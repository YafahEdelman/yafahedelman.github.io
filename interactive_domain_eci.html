<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Interactive Domain-ECI</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>
    <style>
        body { font-family: 'Inter', sans-serif; }
    </style>
</head>
<body class="bg-gray-50 text-gray-900 h-screen flex flex-col">

    <!-- Header -->
    <header class="bg-white border-b border-gray-200 px-6 py-4 flex justify-between items-center shadow-sm z-10">
        <div>
            <h1 class="text-xl font-bold text-gray-800">Domain-ECI Explorer</h1>
            <p class="text-sm text-gray-500">Select benchmarks to estimate capability over time</p>
        </div>
        <div class="flex items-center gap-4">
             <div class="flex items-center space-x-2 bg-gray-100 p-1 rounded-lg">
                <button id="mode-fixed" class="px-3 py-1.5 rounded-md text-sm font-medium transition-all shadow-sm bg-white text-gray-900 border border-gray-200">
                    Fixed Parameters
                </button>
                <button id="mode-refit" class="px-3 py-1.5 rounded-md text-sm font-medium text-gray-500 hover:text-gray-900 transition-all">
                    Refit (JMLE)
                </button>
                 <button id="mode-compare" class="px-3 py-1.5 rounded-md text-sm font-medium text-gray-500 hover:text-gray-900 transition-all">
                    Compare
                </button>
            </div>
            <button id="download-btn" class="bg-blue-600 hover:bg-blue-700 text-white px-4 py-2 rounded-md text-sm font-medium transition-colors">
                Export Data
            </button>
        </div>
    </header>

    <!-- Main Content -->
    <div class="flex flex-1 overflow-hidden">
        
        <!-- Sidebar: Benchmark Selection -->
        <aside class="w-80 bg-white border-r border-gray-200 flex flex-col z-0">
            <div class="p-4 border-b border-gray-100 flex justify-between items-center bg-gray-50">
                <h2 class="font-semibold text-gray-700">Benchmarks</h2>
                <span id="benchmark-count" class="bg-blue-100 text-blue-800 text-xs font-semibold px-2.5 py-0.5 rounded-full">0/0</span>
            </div>
            <div class="p-2 border-b border-gray-100">
                 <input type="text" id="search-benchmarks" placeholder="Filter benchmarks..." class="w-full px-3 py-2 border border-gray-300 rounded-md text-sm focus:outline-none focus:ring-2 focus:ring-blue-500">
            </div>
            <div class="flex-1 overflow-y-auto p-2 space-y-1" id="benchmark-list">
                <!-- Benchmarks injected here -->
            </div>
             <div class="p-4 border-t border-gray-200 bg-gray-50">
                <button id="select-all" class="text-blue-600 text-sm hover:underline mr-4">Select All</button>
                <button id="deselect-all" class="text-gray-500 text-sm hover:underline">Clear</button>
            </div>
        </aside>

        <!-- Visualization Area -->
        <main class="flex-1 flex flex-col bg-white overflow-hidden relative">
             <div id="loading-overlay" class="absolute inset-0 bg-white/80 z-50 flex items-center justify-center hidden backdrop-blur-sm">
                <div class="flex flex-col items-center">
                    <div class="animate-spin rounded-full h-10 w-10 border-b-2 border-blue-600 mb-3"></div>
                    <span class="text-gray-600 font-medium">Refitting Model...</span>
                </div>
            </div>

            <div class="flex-1 p-6" id="chart-container">
                <!-- Plotly Chart -->
            </div>
            
            <div class="px-6 py-4 border-t border-gray-100 bg-gray-50 text-xs text-gray-500 flex justify-between">
                <span id="status-text">Ready.</span>
                <span>Select benchmarks to update the graph.</span>
            </div>
        </main>
    </div>

    <!-- Data Injection -->
    <script>
        const RAW_DATA = {"models": {"Baichuan 2-7B": {"name": "Baichuan 2-7B", "date": "2023-09-20"}, "Baichuan1-7B": {"name": "Baichuan1-7B", "date": "2023-06-01"}, "Baichuan2-13B": {"name": "Baichuan2-13B", "date": "2023-09-06"}, "Cerebras-GPT-13B": {"name": "Cerebras-GPT-13B", "date": "2023-03-20"}, "Claude 2": {"name": "Claude 2", "date": "2023-07-11"}, "Claude 2.1": {"name": "Claude 2.1", "date": "2023-11-21"}, "Claude 3 Haiku": {"name": "Claude 3 Haiku", "date": "2024-03-07"}, "Claude 3 Opus": {"name": "Claude 3 Opus", "date": "2024-02-29"}, "Claude 3 Sonnet": {"name": "Claude 3 Sonnet", "date": "2024-02-29"}, "Claude 3.5 Haiku": {"name": "Claude 3.5 Haiku", "date": "2024-10-22"}, "Claude 3.5 Sonnet": {"name": "Claude 3.5 Sonnet", "date": "2024-06-20"}, "Claude 3.5 Sonnet (October 2024)": {"name": "Claude 3.5 Sonnet (October 2024)", "date": "2024-10-22"}, "Claude 3.7 Sonnet": {"name": "Claude 3.7 Sonnet", "date": "2025-02-24"}, "Claude Haiku 4.5": {"name": "Claude Haiku 4.5", "date": "2025-10-15"}, "Claude Instant": {"name": "Claude Instant", "date": "2023-08-09"}, "Claude Opus 4": {"name": "Claude Opus 4", "date": "2025-05-22"}, "Claude Opus 4.1": {"name": "Claude Opus 4.1", "date": "2025-08-05"}, "Claude Opus 4.5": {"name": "Claude Opus 4.5", "date": "2025-11-24"}, "Claude Sonnet 4": {"name": "Claude Sonnet 4", "date": "2025-05-22"}, "Claude Sonnet 4.5": {"name": "Claude Sonnet 4.5", "date": "2025-09-29"}, "DeepSeek Coder 1.3B": {"name": "DeepSeek Coder 1.3B", "date": "2023-11-02"}, "DeepSeek Coder 33B": {"name": "DeepSeek Coder 33B", "date": "2023-11-02"}, "DeepSeek Coder 6.7B": {"name": "DeepSeek Coder 6.7B", "date": "2023-11-02"}, "DeepSeek-R1": {"name": "DeepSeek-R1", "date": "2025-01-20"}, "DeepSeek-R1 (May 2025)": {"name": "DeepSeek-R1 (May 2025)", "date": "2025-05-28"}, "DeepSeek-V2 (MoE-236B)": {"name": "DeepSeek-V2 (MoE-236B)", "date": "2024-05-07"}, "DeepSeek-V3": {"name": "DeepSeek-V3", "date": "2024-12-26"}, "DeepSeek-V3.1": {"name": "DeepSeek-V3.1", "date": "2025-08-21"}, "DeepSeek-V3.2": {"name": "DeepSeek-V3.2", "date": "2025-12-01"}, "DeepSeek-V3.2-Exp": {"name": "DeepSeek-V3.2-Exp", "date": "2025-09-29"}, "Dolly 2.0-12b": {"name": "Dolly 2.0-12b", "date": "2023-04-11"}, "Falcon 2 11B": {"name": "Falcon 2 11B", "date": "2024-05-09"}, "Falcon-180B": {"name": "Falcon-180B", "date": "2023-09-06"}, "Falcon-40B": {"name": "Falcon-40B", "date": "2023-03-15"}, "Falcon-7B": {"name": "Falcon-7B", "date": "2023-04-24"}, "GPT-3.5 Turbo": {"name": "GPT-3.5 Turbo", "date": "2023-11-06"}, "GPT-4 (Jun 2023)": {"name": "GPT-4 (Jun 2023)", "date": "2023-06-13"}, "GPT-4 (Mar 2023)": {"name": "GPT-4 (Mar 2023)", "date": "2023-03-14"}, "GPT-4 Turbo (Apr 2024)": {"name": "GPT-4 Turbo (Apr 2024)", "date": "2024-04-09"}, "GPT-4.1": {"name": "GPT-4.1", "date": "2025-04-14"}, "GPT-4.1 mini": {"name": "GPT-4.1 mini", "date": "2025-04-14"}, "GPT-4.1 nano": {"name": "GPT-4.1 nano", "date": "2025-04-14"}, "GPT-4.5": {"name": "GPT-4.5", "date": "2025-02-27"}, "GPT-4o (Aug 2024)": {"name": "GPT-4o (Aug 2024)", "date": "2024-08-06"}, "GPT-4o (May 2024)": {"name": "GPT-4o (May 2024)", "date": "2024-05-13"}, "GPT-4o (Nov 2024)": {"name": "GPT-4o (Nov 2024)", "date": "2024-11-20"}, "GPT-4o mini": {"name": "GPT-4o mini", "date": "2024-07-18"}, "GPT-5": {"name": "GPT-5", "date": "2025-08-07"}, "GPT-5 Pro": {"name": "GPT-5 Pro", "date": "2025-10-07"}, "GPT-5 mini": {"name": "GPT-5 mini", "date": "2025-08-07"}, "GPT-5 nano": {"name": "GPT-5 nano", "date": "2025-08-07"}, "GPT-5.1": {"name": "GPT-5.1", "date": "2025-11-13"}, "GPT-5.2": {"name": "GPT-5.2", "date": "2025-12-11"}, "Gemini 1.0 Pro": {"name": "Gemini 1.0 Pro", "date": "2024-02-15"}, "Gemini 1.5 Flash (May 2024)": {"name": "Gemini 1.5 Flash (May 2024)", "date": "2024-05-23"}, "Gemini 1.5 Flash (Sep 2024)": {"name": "Gemini 1.5 Flash (Sep 2024)", "date": "2024-09-24"}, "Gemini 1.5 Pro": {"name": "Gemini 1.5 Pro", "date": "2024-09-24"}, "Gemini 2.0 Flash": {"name": "Gemini 2.0 Flash", "date": "2025-02-05"}, "Gemini 2.0 Flash Thinking (Jan 2025)": {"name": "Gemini 2.0 Flash Thinking (Jan 2025)", "date": "2025-01-21"}, "Gemini 2.0 Pro": {"name": "Gemini 2.0 Pro", "date": "2025-02-05"}, "Gemini 2.5 Flash (Apr 2025)": {"name": "Gemini 2.5 Flash (Apr 2025)", "date": "2025-04-17"}, "Gemini 2.5 Flash (Jun 2025)": {"name": "Gemini 2.5 Flash (Jun 2025)", "date": "2025-06-17"}, "Gemini 2.5 Flash (May 2025)": {"name": "Gemini 2.5 Flash (May 2025)", "date": "2025-05-20"}, "Gemini 2.5 Flash (Sep 2025)": {"name": "Gemini 2.5 Flash (Sep 2025)", "date": "2025-09-25"}, "Gemini 2.5 Pro (Jun 2025)": {"name": "Gemini 2.5 Pro (Jun 2025)", "date": "2025-06-17"}, "Gemini 2.5 Pro (Mar 2025)": {"name": "Gemini 2.5 Pro (Mar 2025)", "date": "2025-03-25"}, "Gemini 2.5 Pro (May 2025)": {"name": "Gemini 2.5 Pro (May 2025)", "date": "2025-05-06"}, "Gemini 3 Flash": {"name": "Gemini 3 Flash", "date": "2025-12-17"}, "Gemini 3 Pro": {"name": "Gemini 3 Pro", "date": "2025-11-18"}, "Gemma 2 27B": {"name": "Gemma 2 27B", "date": "2024-06-24"}, "Gemma 2 9B": {"name": "Gemma 2 9B", "date": "2024-06-24"}, "Gemma 2B": {"name": "Gemma 2B", "date": "2024-02-21"}, "Gemma 3 27B": {"name": "Gemma 3 27B", "date": "2025-03-12"}, "Gemma 7B": {"name": "Gemma 7B", "date": "2024-02-21"}, "Grok 3": {"name": "Grok 3", "date": "2025-04-09"}, "Grok 4": {"name": "Grok 4", "date": "2025-07-09"}, "Grok-2": {"name": "Grok-2", "date": "2024-12-12"}, "Grok-3 mini": {"name": "Grok-3 mini", "date": "2025-04-09"}, "INTELLECT-1": {"name": "INTELLECT-1", "date": "2024-11-29"}, "Kimi K2": {"name": "Kimi K2", "date": "2025-07-12"}, "Kimi K2 Thinking": {"name": "Kimi K2 Thinking", "date": "2025-11-06"}, "LLaMA-13B": {"name": "LLaMA-13B", "date": "2023-02-24"}, "LLaMA-33B": {"name": "LLaMA-33B", "date": "2023-02-24"}, "LLaMA-65B": {"name": "LLaMA-65B", "date": "2023-02-24"}, "LLaMA-7B": {"name": "LLaMA-7B", "date": "2023-02-24"}, "Llama 2-13B": {"name": "Llama 2-13B", "date": "2023-07-18"}, "Llama 2-34B": {"name": "Llama 2-34B", "date": "2023-07-18"}, "Llama 2-70B": {"name": "Llama 2-70B", "date": "2023-07-18"}, "Llama 2-7B": {"name": "Llama 2-7B", "date": "2023-07-18"}, "Llama 3-70B": {"name": "Llama 3-70B", "date": "2024-04-18"}, "Llama 3-8B": {"name": "Llama 3-8B", "date": "2024-04-18"}, "Llama 3.1-405B": {"name": "Llama 3.1-405B", "date": "2024-07-23"}, "Llama 3.1-70B": {"name": "Llama 3.1-70B", "date": "2024-07-23"}, "Llama 3.1-8B": {"name": "Llama 3.1-8B", "date": "2024-07-23"}, "Llama 3.2 90B": {"name": "Llama 3.2 90B", "date": "2024-09-24"}, "Llama 3.3 70B": {"name": "Llama 3.3 70B", "date": "2024-12-06"}, "Llama 4 Maverick": {"name": "Llama 4 Maverick", "date": "2025-04-05"}, "Llama 4 Scout": {"name": "Llama 4 Scout", "date": "2025-04-05"}, "MPT-30B": {"name": "MPT-30B", "date": "2023-06-22"}, "MPT-7B": {"name": "MPT-7B", "date": "2023-05-05"}, "Mistral 7B": {"name": "Mistral 7B", "date": "2023-09-27"}, "Mistral Large": {"name": "Mistral Large", "date": "2024-02-26"}, "Mistral Large 2": {"name": "Mistral Large 2", "date": "2024-11-18"}, "Mistral Medium 3": {"name": "Mistral Medium 3", "date": "2025-05-07"}, "Mistral NeMo": {"name": "Mistral NeMo", "date": "2024-07-18"}, "Mixtral 8x22B": {"name": "Mixtral 8x22B", "date": "2024-04-17"}, "Mixtral 8x7B": {"name": "Mixtral 8x7B", "date": "2023-12-11"}, "Nemotron-4 15B": {"name": "Nemotron-4 15B", "date": "2024-02-26"}, "Phi-1.5": {"name": "Phi-1.5", "date": "2023-09-11"}, "Phi-2": {"name": "Phi-2", "date": "2023-12-12"}, "Phi-4": {"name": "Phi-4", "date": "2024-12-12"}, "Qwen Plus": {"name": "Qwen Plus", "date": "2025-01-25"}, "Qwen-14B": {"name": "Qwen-14B", "date": "2023-09-28"}, "Qwen-7B": {"name": "Qwen-7B", "date": "2023-09-28"}, "Qwen2-72B": {"name": "Qwen2-72B", "date": "2024-06-07"}, "Qwen2.5-72B": {"name": "Qwen2.5-72B", "date": "2024-09-19"}, "Qwen2.5-Coder (1.5B)": {"name": "Qwen2.5-Coder (1.5B)", "date": "2024-09-18"}, "Qwen2.5-Coder (32B)": {"name": "Qwen2.5-Coder (32B)", "date": "2024-09-18"}, "Qwen2.5-Coder (7B)": {"name": "Qwen2.5-Coder (7B)", "date": "2024-09-18"}, "Qwen2.5-Max": {"name": "Qwen2.5-Max", "date": "2025-01-25"}, "Qwen3-235B-A22B": {"name": "Qwen3-235B-A22B", "date": "2025-04-29"}, "Qwen3-235B-A22B-Thinking (Jul 2025)": {"name": "Qwen3-235B-A22B-Thinking (Jul 2025)", "date": "2025-07-25"}, "Qwen3-Coder-480B-A35B": {"name": "Qwen3-Coder-480B-A35B", "date": "2025-07-31"}, "Qwen3-Max": {"name": "Qwen3-Max", "date": "2025-09-24"}, "Stable Beluga 2": {"name": "Stable Beluga 2", "date": "2023-07-20"}, "StarCoder 2 15B": {"name": "StarCoder 2 15B", "date": "2024-02-20"}, "StarCoder 2 3B": {"name": "StarCoder 2 3B", "date": "2024-02-22"}, "StarCoder 2 7B": {"name": "StarCoder 2 7B", "date": "2024-02-20"}, "XGen-7B": {"name": "XGen-7B", "date": "2023-06-27"}, "Yi 6B": {"name": "Yi 6B", "date": "2023-11-02"}, "Yi-34B": {"name": "Yi-34B", "date": "2023-11-22"}, "gpt-oss-120b": {"name": "gpt-oss-120b", "date": "2025-08-05"}, "o1": {"name": "o1", "date": "2024-12-17"}, "o1-mini": {"name": "o1-mini", "date": "2024-09-12"}, "o1-preview": {"name": "o1-preview", "date": "2024-09-12"}, "o3": {"name": "o3", "date": "2025-04-16"}, "o3-mini": {"name": "o3-mini", "date": "2025-01-31"}, "o3-pro": {"name": "o3-pro", "date": "2025-06-10"}, "o4-mini": {"name": "o4-mini", "date": "2025-04-16"}, "phi-3-medium 14B": {"name": "phi-3-medium 14B", "date": "2024-04-23"}, "phi-3-mini 3.8B": {"name": "phi-3-mini 3.8B", "date": "2024-04-23"}, "phi-3-small 7.4B": {"name": "phi-3-small 7.4B", "date": "2024-04-23"}}, "benchmarks": {"ARC AI2": {"name": "ARC AI2", "diff": 105.1930405856702, "disc": 0.0944288087686699}, "BBH": {"name": "BBH", "diff": 113.60350501413456, "disc": 0.084821943823402}, "GSM8K": {"name": "GSM8K", "diff": 105.65084306346868, "disc": 0.0963752526209175}, "HellaSwag": {"name": "HellaSwag", "diff": 76.54614285395945, "disc": 0.0301332309687049}, "LAMBADA": {"name": "LAMBADA", "diff": 32.37583948836671, "disc": 0.0152206717542228}, "MMLU": {"name": "MMLU", "diff": 109.84539034547636, "disc": 0.0666196953012477}, "GPQA diamond": {"name": "GPQA diamond", "diff": 135.905859745637, "disc": 0.1205381131999341}, "MATH level 5": {"name": "MATH level 5", "diff": 128.11393930948424, "disc": 0.1849098233215427}, "OTIS Mock AIME 2024-2025": {"name": "OTIS Mock AIME 2024-2025", "diff": 138.04409483210003, "disc": 0.2353216021056998}, "WeirdML": {"name": "WeirdML", "diff": 145.0710546736157, "disc": 0.0814742311916497}, "Winogrande": {"name": "Winogrande", "diff": 109.73284588313348, "disc": 0.0452915372619513}, "TriviaQA": {"name": "TriviaQA", "diff": 50.24834447104708, "disc": 0.0181355933800949}, "Cybench": {"name": "Cybench", "diff": 147.41076875706423, "disc": 0.1474436270890705}, "SimpleBench": {"name": "SimpleBench", "diff": 149.2864892654243, "disc": 0.1201644182250022}, "The Agent Company": {"name": "The Agent Company", "diff": 146.93112836234894, "disc": 0.14284206272087}, "PIQA": {"name": "PIQA", "diff": 75.44154840421315, "disc": 0.016737696321792}, "OpenBookQA": {"name": "OpenBookQA", "diff": 105.06526736914988, "disc": 0.0516643993159377}, "Balrog": {"name": "Balrog", "diff": 158.7415159126036, "disc": 0.0408850679569183}, "GeoBench": {"name": "GeoBench", "diff": 118.04089607750711, "disc": 0.0320612927702825}, "Fiction.LiveBench": {"name": "Fiction.LiveBench", "diff": 135.46100907423966, "disc": 0.1237881484208301}, "FrontierMath-2025-02-28-Private": {"name": "FrontierMath-2025-02-28-Private", "diff": 155.73473920633722, "disc": 0.1660351142989665}, "Aider polyglot": {"name": "Aider polyglot", "diff": 138.80808212823638, "disc": 0.1766038501623188}, "Lech Mazur Writing": {"name": "Lech Mazur Writing", "diff": 108.8215952802692, "disc": 0.0408249757909401}, "ARC-AGI": {"name": "ARC-AGI", "diff": 146.10321514972603, "disc": 0.2159491664898188}, "SWE-Bench Verified (Bash Only)": {"name": "SWE-Bench Verified (Bash Only)", "diff": 142.8387294620133, "disc": 0.1325078119866445}, "CadEval": {"name": "CadEval", "diff": 139.94976565935565, "disc": 0.1012534267007525}, "SimpleQA Verified": {"name": "SimpleQA Verified", "diff": 149.508369395908, "disc": 0.1451753901944211}, "ANLI": {"name": "ANLI", "diff": 129.76641601907025, "disc": 0.0647584914337646}, "FrontierMath-Tier-4-2025-07-01-Private": {"name": "FrontierMath-Tier-4-2025-07-01-Private", "diff": 161.93389444503427, "disc": 0.1854878328641463}, "VideoMME": {"name": "VideoMME", "diff": 108.55584885904462, "disc": 0.0180996852544703}, "VPCT": {"name": "VPCT", "diff": 145.08306464982996, "disc": 0.0717011218908533}, "OSWorld": {"name": "OSWorld", "diff": 146.8083216994265, "disc": 0.1256180284929259}, "GSO-Bench": {"name": "GSO-Bench", "diff": 164.8551803608777, "disc": 0.1249943043964883}, "Chess Puzzles": {"name": "Chess Puzzles", "diff": 157.73009457333544, "disc": 0.1166623682165005}, "Terminal Bench": {"name": "Terminal Bench", "diff": 150.56323908697456, "disc": 0.1248082673348482}, "DeepResearch Bench": {"name": "DeepResearch Bench", "diff": 148.7751386129299, "disc": 0.0355571039883531}, "ScienceQA": {"name": "ScienceQA", "diff": 110.19904769844744, "disc": 0.0854500706191642}}, "performances": {"Baichuan 2-7B": {"ARC AI2": 0.1, "BBH": 0.2213333333333333, "GSM8K": 0.246, "HellaSwag": 0.5733333333333334, "LAMBADA": 0.733, "MMLU": 0.3888}, "Claude 3 Sonnet": {"GPQA diamond": 0.2079124579124579, "MATH level 5": 0.1817409365558912, "OTIS Mock AIME 2024-2025": 0.024024024024024, "WeirdML": 0.1016, "MMLU": 0.6786666666666666, "Winogrande": 0.502}, "Llama 3.1-405B": {"ARC AI2": 0.9373333333333332, "TriviaQA": 0.827, "GPQA diamond": 0.3455387205387206, "MATH level 5": 0.4977341389728096, "OTIS Mock AIME 2024-2025": 0.0963185407629851, "WeirdML": 0.2138, "Cybench": 0.075, "SimpleBench": 0.076, "BBH": 0.7719999999999999, "The Agent Company": 0.074, "HellaSwag": 0.856, "MMLU": 0.7933333333333333, "PIQA": 0.718, "OpenBookQA": 0.3226666666666666, "Winogrande": 0.784}, "Llama 3.1-70B": {"GPQA diamond": 0.2558922558922558, "MATH level 5": 0.366786253776435, "OTIS Mock AIME 2024-2025": 0.0351462573684795, "WeirdML": 0.0897, "Balrog": 0.279, "The Agent Company": 0.069, "MMLU": 0.7346666666666667}, "Llama 3.1-8B": {"GPQA diamond": 0.0126262626262625, "MATH level 5": 0.2287575528700906, "OTIS Mock AIME 2024-2025": 0.024024024024024, "WeirdML": 0.0173, "Balrog": 0.151, "GSM8K": 0.824, "MMLU": 0.4146666666666667, "PIQA": 0.6240000000000001}, "Llama 3.2 90B": {"GPQA diamond": 0.2138047138047137, "MATH level 5": 0.3943542296072507, "OTIS Mock AIME 2024-2025": 0.0254143031920808, "Balrog": 0.273, "GeoBench": 0.52, "MMLU": 0.7373333333333334}, "Llama 3.3 70B": {"GPQA diamond": 0.2992424242424242, "MATH level 5": 0.4159743202416918, "OTIS Mock AIME 2024-2025": 0.0504393282171059, "WeirdML": 0.1444, "SimpleBench": 0.0388, "Balrog": 0.23, "Fiction.LiveBench": 0.333, "MMLU": 0.8173333333333334}, "Llama 4 Maverick": {"GPQA diamond": 0.5597643097643098, "MATH level 5": 0.7301737160120846, "OTIS Mock AIME 2024-2025": 0.2047603158714269, "WeirdML": 0.2447, "SimpleBench": 0.1324, "FrontierMath-2025-02-28-Private": 0.0068965517241379, "Aider polyglot": 0.156, "GeoBench": 0.52, "Lech Mazur Writing": 0.637, "ARC-AGI": 0.044, "Fiction.LiveBench": 0.462, "SWE-Bench Verified (Bash Only)": 0.2104}, "Llama 4 Scout": {"GPQA diamond": 0.3577441077441077, "MATH level 5": 0.6227341389728097, "OTIS Mock AIME 2024-2025": 0.0768546324101878, "FrontierMath-2025-02-28-Private": 0.0, "ARC-AGI": 0.005, "Fiction.LiveBench": 0.36, "SWE-Bench Verified (Bash Only)": 0.0906}, "MPT-30B": {"ARC AI2": 0.3413333333333333, "TriviaQA": 0.736, "BBH": 0.1733333333333333, "GSM8K": 0.344, "MMLU": 0.3053333333333333, "PIQA": 0.6379999999999999, "OpenBookQA": 0.36, "Winogrande": 0.4199999999999999}, "MPT-7B": {"ARC AI2": 0.2346666666666666, "TriviaQA": 0.616, "BBH": 0.1413333333333333, "GSM8K": 0.091, "HellaSwag": 0.6853333333333333, "LAMBADA": 0.7, "MMLU": 0.0773333333333333, "PIQA": 0.6120000000000001, "OpenBookQA": 0.352, "Winogrande": 0.3720000000000001}, "Claude 3.5 Haiku": {"GPQA diamond": 0.1750841750841751, "MATH level 5": 0.4635574018126888, "OTIS Mock AIME 2024-2025": 0.0420976532087642, "WeirdML": 0.3073, "CadEval": 0.32, "FrontierMath-2025-02-28-Private": 0.0034482758620689, "SimpleQA Verified": 0.067, "Aider polyglot": 0.28, "Balrog": 0.193, "GeoBench": 0.34, "Lech Mazur Writing": 0.735, "MMLU": 0.6573333333333333}, "Mistral 7B": {"ARC AI2": 0.7146666666666667, "TriviaQA": 0.752, "MATH level 5": 0.0368202416918429, "BBH": 0.4146666666666667, "GSM8K": 0.544, "ANLI": 0.2064999999999999, "HellaSwag": 0.7466666666666667, "MMLU": 0.5, "PIQA": 0.6599999999999999, "OpenBookQA": 0.7306666666666667, "Winogrande": 0.506}, "Mistral Large": {"GPQA diamond": 0.1835016835016834, "MATH level 5": 0.2446185800604229, "OTIS Mock AIME 2024-2025": 0.0184629073517961, "MMLU": 0.584}, "Mistral Large 2": {"GPQA diamond": 0.351010101010101, "MATH level 5": 0.5028323262839879, "OTIS Mock AIME 2024-2025": 0.0838060282504726, "SimpleBench": 0.07, "FrontierMath-2025-02-28-Private": 0.0034482758620689, "Lech Mazur Writing": 0.6900000000000001, "MMLU": 0.7333333333333334}, "Mistral Medium 3": {"GPQA diamond": 0.4604377104377105, "MATH level 5": 0.8162764350453172, "OTIS Mock AIME 2024-2025": 0.3215437659882104, "FrontierMath-2025-02-28-Private": 0.0034602076124567}, "Mistral NeMo": {"GPQA diamond": 0.0652356902356902, "MATH level 5": 0.1082892749244713, "Balrog": 0.176, "GSM8K": 0.842, "PIQA": 0.6699999999999999}, "Mixtral 8x22B": {"GPQA diamond": 0.1207912457912457, "MATH level 5": 0.2424471299093655, "WeirdML": 0.0317, "Cybench": 0.075, "MMLU": 0.7040000000000001}, "Mixtral 8x7B": {"ARC AI2": 0.8306666666666667, "TriviaQA": 0.822, "GPQA diamond": 0.0744949494949494, "MATH level 5": 0.0995090634441087, "GSM8K": 0.744, "ANLI": 0.328, "HellaSwag": 0.8226666666666667, "MMLU": 0.608, "PIQA": 0.6719999999999999, "OpenBookQA": 0.8106666666666666, "Winogrande": 0.544}, "Nemotron-4 15B": {"ARC AI2": 0.4066666666666667, "BBH": 0.4493333333333333, "GSM8K": 0.46, "HellaSwag": 0.7653333333333333, "MMLU": 0.4493333333333333, "PIQA": 0.6479999999999999, "Winogrande": 0.56}, "Phi-1.5": {"ARC AI2": 0.2586666666666666, "HellaSwag": 0.3013333333333333, "MMLU": 0.168, "OpenBookQA": 0.1626666666666666, "Winogrande": 0.4679999999999999}, "Claude 3.5 Sonnet": {"GPQA diamond": 0.3872053872053873, "MATH level 5": 0.5168051359516617, "OTIS Mock AIME 2024-2025": 0.0643421198976753, "WeirdML": 0.3097, "Cybench": 0.175, "SimpleBench": 0.13, "FrontierMath-2025-02-28-Private": 0.0103448275862068, "FrontierMath-Tier-4-2025-07-01-Private": 0.0, "VideoMME": 0.4666666666666666, "VPCT": 0.33, "MMLU": 0.82}, "Phi-2": {"ARC AI2": 0.6786666666666666, "TriviaQA": 0.452, "BBH": 0.4586666666666666, "ANLI": 0.1374999999999999, "HellaSwag": 0.3813333333333333, "MMLU": 0.4453333333333333, "OpenBookQA": 0.648, "Winogrande": 0.094}, "Phi-4": {"GPQA diamond": 0.4141414141414141, "MATH level 5": 0.6493580060422961, "OTIS Mock AIME 2024-2025": 0.1366366366366366, "Balrog": 0.116, "Lech Mazur Writing": 0.626, "MMLU": 0.7973333333333333}, "Qwen Plus": {"GPQA diamond": 0.3080808080808081, "MATH level 5": 0.6527567975830816, "OTIS Mock AIME 2024-2025": 0.1769547325102879, "FrontierMath-2025-02-28-Private": 0.0172413793103448}, "Qwen-14B": {"ARC AI2": 0.7919999999999999, "BBH": 0.4000000000000001, "GSM8K": 0.613, "LAMBADA": 0.711, "MMLU": 0.5506666666666667, "PIQA": 0.5980000000000001}, "Qwen-7B": {"ARC AI2": 0.6706666666666666, "BBH": 0.2666666666666666, "GSM8K": 0.517, "LAMBADA": 0.679, "MMLU": 0.2666666666666666, "PIQA": 0.558}, "Qwen2-72B": {"GPQA diamond": 0.2104377104377104, "MATH level 5": 0.3906722054380664, "The Agent Company": 0.011, "MMLU": 0.7653333333333333}, "Qwen2.5-72B": {"ARC AI2": 0.9266666666666666, "TriviaQA": 0.719, "GPQA diamond": 0.3219696969696969, "MATH level 5": 0.631703172205438, "OTIS Mock AIME 2024-2025": 0.0796351907463018, "BBH": 0.7306666666666667, "Balrog": 0.162, "GeoBench": 0.62, "VideoMME": 0.6466666666666666, "The Agent Company": 0.057, "OSWorld": 0.05, "HellaSwag": 0.7973333333333333, "MMLU": 0.8039999999999999, "PIQA": 0.6519999999999999, "Winogrande": 0.6459999999999999}, "Qwen2.5-Coder (1.5B)": {"ARC AI2": 0.2693333333333333, "GSM8K": 0.658, "HellaSwag": 0.4906666666666666, "MMLU": 0.3813333333333333, "Winogrande": 0.2139999999999999}, "Qwen2.5-Coder (32B)": {"ARC AI2": 0.6066666666666666, "Aider polyglot": 0.1639999999999999, "GSM8K": 0.93, "SWE-Bench Verified (Bash Only)": 0.09, "HellaSwag": 0.7733333333333333, "MMLU": 0.7213333333333334, "Winogrande": 0.6160000000000001}, "Qwen2.5-Coder (7B)": {"ARC AI2": 0.4786666666666666, "GSM8K": 0.867, "HellaSwag": 0.6906666666666667, "MMLU": 0.5733333333333334, "Winogrande": 0.4579999999999999}, "Claude 3.5 Sonnet (October 2024)": {"GPQA diamond": 0.404040404040404, "MATH level 5": 0.5694864048338368, "OTIS Mock AIME 2024-2025": 0.0838060282504726, "WeirdML": 0.3997, "CadEval": 0.48, "SimpleBench": 0.2967999999999999, "FrontierMath-2025-02-28-Private": 0.0206896551724137, "Aider polyglot": 0.516, "Balrog": 0.326, "GeoBench": 0.62, "Lech Mazur Writing": 0.8029999999999999, "FrontierMath-Tier-4-2025-07-01-Private": 0.0, "VPCT": 0.33, "GSO-Bench": 0.046, "The Agent Company": 0.24, "MMLU": 0.8306666666666667}, "Qwen2.5-Max": {"GPQA diamond": 0.4149831649831649, "MATH level 5": 0.6718277945619335, "OTIS Mock AIME 2024-2025": 0.1602713824936047, "FrontierMath-2025-02-28-Private": 0.0103448275862068, "Aider polyglot": 0.218, "Lech Mazur Writing": 0.7290000000000001, "Fiction.LiveBench": 0.667}, "Qwen3-235B-A22B": {"GPQA diamond": 0.6094276094276094, "MATH level 5": 0.6885699899295065, "SimpleBench": 0.172, "Aider polyglot": 0.596, "Lech Mazur Writing": 0.8300000000000001, "Fiction.LiveBench": 0.677}, "Qwen3-235B-A22B-Thinking (Jul 2025)": {"GPQA diamond": 0.734006734006734, "OTIS Mock AIME 2024-2025": 0.8665331998665332, "WeirdML": 0.4104, "FrontierMath-2025-02-28-Private": 0.0848056537102473, "SimpleQA Verified": 0.501002004008016, "Lech Mazur Writing": 0.8494999999999999, "FrontierMath-Tier-4-2025-07-01-Private": 0.0, "Fiction.LiveBench": 0.75, "Chess Puzzles": 0.1224489795918367}, "Qwen3-Coder-480B-A35B": {"WeirdML": 0.4117, "GSO-Bench": 0.049, "SWE-Bench Verified (Bash Only)": 0.554, "Terminal Bench": 0.254}, "Qwen3-Max": {"GPQA diamond": 0.6346801346801346, "MATH level 5": 0.9712990936555892, "OTIS Mock AIME 2024-2025": 0.7330663997330663, "SimpleQA Verified": 0.6746746746746747, "Lech Mazur Writing": 0.8711000000000001, "Fiction.LiveBench": 0.667, "Chess Puzzles": 0.04}, "Stable Beluga 2": {"ARC AI2": 0.8146666666666667, "BBH": 0.5906666666666666, "GSM8K": 0.696, "HellaSwag": 0.7879999999999999, "LAMBADA": 0.713, "MMLU": 0.5813333333333334, "PIQA": 0.6659999999999999}, "StarCoder 2 15B": {"ARC AI2": 0.296, "GSM8K": 0.577, "MMLU": 0.5213333333333333, "Winogrande": 0.286}, "StarCoder 2 3B": {"ARC AI2": 0.1226666666666667, "GSM8K": 0.216, "MMLU": 0.1546666666666666, "Winogrande": 0.1419999999999999}, "StarCoder 2 7B": {"ARC AI2": 0.1826666666666666, "GSM8K": 0.327, "MMLU": 0.184, "Winogrande": 0.1419999999999999}, "XGen-7B": {"ARC AI2": 0.2159999999999999, "HellaSwag": 0.656, "MMLU": 0.1506666666666666, "PIQA": 0.51, "OpenBookQA": 0.2026666666666667, "Winogrande": 0.298}, "Claude 3.7 Sonnet": {"GPQA diamond": 0.7297979797979798, "MATH level 5": 0.9116314199395772, "OTIS Mock AIME 2024-2025": 0.5773551329106884, "CadEval": 0.54, "Cybench": 0.2, "SimpleBench": 0.3568, "FrontierMath-2025-02-28-Private": 0.0413793103448275, "Aider polyglot": 0.649, "GeoBench": 0.68, "Lech Mazur Writing": 0.8109999999999999, "VPCT": 0.39, "GSO-Bench": 0.038, "The Agent Company": 0.309, "ARC-AGI": 0.286, "DeepResearch Bench": 0.436, "Fiction.LiveBench": 0.833, "OSWorld": 0.358, "SWE-Bench Verified (Bash Only)": 0.528}, "Yi 6B": {"ARC AI2": 0.3373333333333333, "BBH": 0.296, "GSM8K": 0.449, "HellaSwag": 0.6586666666666666, "MMLU": 0.52, "Winogrande": 0.4259999999999999}, "Yi-34B": {"MATH level 5": 0.0514539274924471, "BBH": 0.6226666666666666, "GSM8K": 0.76, "MMLU": 0.684}, "gpt-oss-120b": {"GPQA diamond": 0.6767676767676768, "OTIS Mock AIME 2024-2025": 0.8887776665554443, "WeirdML": 0.4817, "SimpleBench": 0.0652, "SimpleQA Verified": 0.139, "Aider polyglot": 0.418, "Lech Mazur Writing": 0.7726000000000001, "Fiction.LiveBench": 0.444, "SWE-Bench Verified (Bash Only)": 0.26, "Terminal Bench": 0.187, "Chess Puzzles": 0.2}, "o1": {"GPQA diamond": 0.6902356902356902, "MATH level 5": 0.947129909365559, "OTIS Mock AIME 2024-2025": 0.7330663997330663, "WeirdML": 0.4382, "CadEval": 0.56, "SimpleBench": 0.2812, "FrontierMath-2025-02-28-Private": 0.093103448275862, "Aider polyglot": 0.617, "GeoBench": 0.8, "Lech Mazur Writing": 0.702, "VPCT": 0.37, "ARC-AGI": 0.307, "Fiction.LiveBench": 0.833}, "o1-mini": {"GPQA diamond": 0.4983164983164982, "MATH level 5": 0.8918051359516617, "OTIS Mock AIME 2024-2025": 0.4689133578022466, "WeirdML": 0.3632, "Cybench": 0.1, "SimpleBench": 0.0172, "FrontierMath-2025-02-28-Private": 0.0172413793103448, "Lech Mazur Writing": 0.649, "ARC-AGI": 0.14}, "o1-preview": {"GPQA diamond": 0.3375420875420875, "MATH level 5": 0.8164652567975831, "OTIS Mock AIME 2024-2025": 0.3104215326437549, "WeirdML": 0.4756, "Cybench": 0.1, "SimpleBench": 0.3003999999999999, "ARC-AGI": 0.18}, "o3": {"GPQA diamond": 0.7575757575757577, "MATH level 5": 0.9777190332326284, "OTIS Mock AIME 2024-2025": 0.8387276165053943, "WeirdML": 0.5242, "CadEval": 0.74, "SimpleBench": 0.4372, "FrontierMath-2025-02-28-Private": 0.1868512110726643, "SimpleQA Verified": 0.53, "Aider polyglot": 0.813, "GeoBench": 0.74, "Lech Mazur Writing": 0.8390000000000001, "FrontierMath-Tier-4-2025-07-01-Private": 0.0208333333333333, "VPCT": 0.52, "GSO-Bench": 0.088, "ARC-AGI": 0.608, "DeepResearch Bench": 0.466, "Fiction.LiveBench": 0.889, "OSWorld": 0.23, "SWE-Bench Verified (Bash Only)": 0.584}, "o3-mini": {"GPQA diamond": 0.6936026936026937, "MATH level 5": 0.9648791540785498, "OTIS Mock AIME 2024-2025": 0.769213658102547, "WeirdML": 0.437, "CadEval": 0.54, "Cybench": 0.225, "SimpleBench": 0.0736, "FrontierMath-2025-02-28-Private": 0.1241379310344827, "Aider polyglot": 0.604, "Lech Mazur Writing": 0.617, "FrontierMath-Tier-4-2025-07-01-Private": 0.0416666666666666, "GSO-Bench": 0.013, "ARC-AGI": 0.345, "Fiction.LiveBench": 0.5, "Chess Puzzles": 0.17}, "o3-pro": {"WeirdML": 0.5821, "Aider polyglot": 0.8490000000000001, "Lech Mazur Writing": 0.8628, "ARC-AGI": 0.593, "Fiction.LiveBench": 0.972}, "o4-mini": {"GPQA diamond": 0.728114478114478, "MATH level 5": 0.978285498489426, "OTIS Mock AIME 2024-2025": 0.8164831498164832, "WeirdML": 0.5256, "CadEval": 0.62, "SimpleBench": 0.2644, "FrontierMath-2025-02-28-Private": 0.2482758620689655, "SimpleQA Verified": 0.239, "Aider polyglot": 0.72, "GeoBench": 0.64, "Lech Mazur Writing": 0.75, "FrontierMath-Tier-4-2025-07-01-Private": 0.0625, "VPCT": 0.575, "GSO-Bench": 0.036, "ARC-AGI": 0.587, "Fiction.LiveBench": 0.778, "SWE-Bench Verified (Bash Only)": 0.45, "Chess Puzzles": 0.26}, "Claude Haiku 4.5": {"GPQA diamond": 0.6161616161616162, "MATH level 5": 0.9636087945413192, "OTIS Mock AIME 2024-2025": 0.666332999666333, "WeirdML": 0.454, "FrontierMath-2025-02-28-Private": 0.0590277777777777, "SimpleQA Verified": 0.059, "FrontierMath-Tier-4-2025-07-01-Private": 0.0208333333333333, "Terminal Bench": 0.298}, "phi-3-medium 14B": {"ARC AI2": 0.888, "TriviaQA": 0.739, "GPQA diamond": 0.0345117845117845, "MATH level 5": 0.1756042296072507, "BBH": 0.7519999999999999, "ANLI": 0.3370000000000001, "HellaSwag": 0.7653333333333333, "MMLU": 0.7066666666666667, "OpenBookQA": 0.832, "Winogrande": 0.6299999999999999}, "phi-3-mini 3.8B": {"ARC AI2": 0.7986666666666666, "TriviaQA": 0.64, "BBH": 0.6226666666666666, "ANLI": 0.292, "HellaSwag": 0.6893333333333334, "MMLU": 0.584, "OpenBookQA": 0.84, "Winogrande": 0.4159999999999999}, "phi-3-small 7.4B": {"ARC AI2": 0.876, "TriviaQA": 0.581, "BBH": 0.7213333333333334, "ANLI": 0.3714999999999999, "HellaSwag": 0.6933333333333334, "MMLU": 0.676, "OpenBookQA": 0.84, "Winogrande": 0.6299999999999999}, "Claude Instant": {"ARC AI2": 0.8173333333333334, "TriviaQA": 0.789, "GSM8K": 0.867, "MMLU": 0.6453333333333333}, "Claude Opus 4": {"GPQA diamond": 0.6835016835016835, "MATH level 5": 0.850453172205438, "OTIS Mock AIME 2024-2025": 0.6440885329774219, "WeirdML": 0.434, "Cybench": 0.38, "SimpleBench": 0.5055999999999999, "FrontierMath-2025-02-28-Private": 0.0448275862068965, "Aider polyglot": 0.72, "GeoBench": 0.49, "FrontierMath-Tier-4-2025-07-01-Private": 0.0416666666666666, "VPCT": 0.38, "GSO-Bench": 0.069, "ARC-AGI": 0.357, "DeepResearch Bench": 0.49, "Fiction.LiveBench": 0.611, "SWE-Bench Verified (Bash Only)": 0.676}, "Claude Opus 4.1": {"GPQA diamond": 0.6969696969696969, "OTIS Mock AIME 2024-2025": 0.6885774663552441, "WeirdML": 0.4276, "Cybench": 0.38, "SimpleBench": 0.52, "FrontierMath-2025-02-28-Private": 0.0724137931034482, "SimpleQA Verified": 0.348, "Lech Mazur Writing": 0.8538000000000001, "FrontierMath-Tier-4-2025-07-01-Private": 0.0416666666666666, "VPCT": 0.35, "DeepResearch Bench": 0.497, "Terminal Bench": 0.38}, "Claude Opus 4.5": {"GPQA diamond": 0.813973063973064, "OTIS Mock AIME 2024-2025": 0.8609720831943055, "WeirdML": 0.637, "FrontierMath-2025-02-28-Private": 0.2068965517241379, "SimpleQA Verified": 0.418, "GeoBench": 0.75, "FrontierMath-Tier-4-2025-07-01-Private": 0.0416666666666666, "VPCT": 0.4, "GSO-Bench": 0.265, "ARC-AGI": 0.8, "OSWorld": 0.663, "SWE-Bench Verified (Bash Only)": 0.744, "Terminal Bench": 0.631, "Chess Puzzles": 0.12}, "Baichuan1-7B": {"BBH": 0.0997333333333333, "GSM8K": 0.092, "MMLU": 0.2306666666666666, "PIQA": 0.524}, "Claude Sonnet 4": {"GPQA diamond": 0.7225042301184433, "MATH level 5": 0.8436555891238671, "OTIS Mock AIME 2024-2025": 0.7108219330441553, "WeirdML": 0.4611, "Cybench": 0.35, "SimpleBench": 0.346, "FrontierMath-2025-02-28-Private": 0.0413793103448275, "Aider polyglot": 0.613, "GeoBench": 0.37, "FrontierMath-Tier-4-2025-07-01-Private": 0.0, "VPCT": 0.34, "GSO-Bench": 0.049, "The Agent Company": 0.331, "ARC-AGI": 0.4, "DeepResearch Bench": 0.478, "Fiction.LiveBench": 0.469, "OSWorld": 0.439, "SWE-Bench Verified (Bash Only)": 0.6493}, "Claude Sonnet 4.5": {"GPQA diamond": 0.7643097643097643, "MATH level 5": 0.9773413897280968, "OTIS Mock AIME 2024-2025": 0.7775553331108886, "WeirdML": 0.4771, "Cybench": 0.55, "SimpleBench": 0.4516, "FrontierMath-2025-02-28-Private": 0.1522491349480969, "SimpleQA Verified": 0.236, "FrontierMath-Tier-4-2025-07-01-Private": 0.0416666666666666, "VPCT": 0.398, "GSO-Bench": 0.147, "DeepResearch Bench": 0.526, "OSWorld": 0.629, "SWE-Bench Verified (Bash Only)": 0.706, "Terminal Bench": 0.428, "Chess Puzzles": 0.12}, "DeepSeek Coder 1.3B": {"ARC AI2": 0.0053333333333333, "GSM8K": 0.044, "MMLU": 0.0106666666666666, "Winogrande": 0.066}, "DeepSeek Coder 33B": {"ARC AI2": 0.2293333333333333, "GSM8K": 0.354, "MMLU": 0.192, "Winogrande": 0.24}, "DeepSeek Coder 6.7B": {"ARC AI2": 0.152, "GSM8K": 0.213, "MMLU": 0.152, "Winogrande": 0.1519999999999999}, "DeepSeek-R1": {"GPQA diamond": 0.6228956228956228, "MATH level 5": 0.9305135951661632, "OTIS Mock AIME 2024-2025": 0.5328661995328662, "WeirdML": 0.3649, "SimpleBench": 0.1708, "Aider polyglot": 0.569, "Balrog": 0.349, "Lech Mazur Writing": 0.8300000000000001, "ARC-AGI": 0.158, "Fiction.LiveBench": 0.694}, "DeepSeek-R1 (May 2025)": {"GPQA diamond": 0.6843434343434344, "MATH level 5": 0.9663897280966768, "OTIS Mock AIME 2024-2025": 0.6635524413302191, "WeirdML": 0.4163, "SimpleBench": 0.2895999999999999, "SimpleQA Verified": 0.274, "Aider polyglot": 0.7140000000000001, "ARC-AGI": 0.212, "DeepResearch Bench": 0.351, "Fiction.LiveBench": 0.75}, "DeepSeek-V2 (MoE-236B)": {"ARC AI2": 0.896, "TriviaQA": 0.8, "BBH": 0.7173333333333334, "HellaSwag": 0.828, "MMLU": 0.7120000000000001, "PIQA": 0.6779999999999999, "Winogrande": 0.726}, "DeepSeek-V3": {"ARC AI2": 0.9373333333333332, "TriviaQA": 0.829, "GPQA diamond": 0.5681818181818182, "MATH level 5": 0.75547583081571, "OTIS Mock AIME 2024-2025": 0.3771549327104882, "WeirdML": 0.3608, "SimpleBench": 0.1264, "FrontierMath-2025-02-28-Private": 0.0172413793103448, "BBH": 0.8333333333333334, "Aider polyglot": 0.551, "Lech Mazur Writing": 0.77, "Fiction.LiveBench": 0.5, "HellaSwag": 0.852, "MMLU": 0.8293333333333334, "PIQA": 0.694, "Winogrande": 0.704}, "DeepSeek-V3.1": {"WeirdML": 0.3836, "SimpleBench": 0.28, "Lech Mazur Writing": 0.8517, "Fiction.LiveBench": 0.528}, "Baichuan2-13B": {"ARC AI2": 0.1733333333333333, "BBH": 0.32, "GSM8K": 0.528, "HellaSwag": 0.6106666666666666, "LAMBADA": 0.74, "MMLU": 0.4559999999999999, "PIQA": 0.562}, "DeepSeek-V3.2": {"GPQA diamond": 0.7789802789802791, "OTIS Mock AIME 2024-2025": 0.8780526558304335, "FrontierMath-2025-02-28-Private": 0.221, "SimpleQA Verified": 0.275, "FrontierMath-Tier-4-2025-07-01-Private": 0.021, "Chess Puzzles": 0.14}, "DeepSeek-V3.2-Exp": {"WeirdML": 0.3948, "Aider polyglot": 0.742, "The Agent Company": 0.429, "Fiction.LiveBench": 0.833}, "Dolly 2.0-12b": {"ARC AI2": 0.1946666666666666, "HellaSwag": 0.6106666666666666, "MMLU": 0.016, "PIQA": 0.508, "OpenBookQA": 0.1893333333333333, "Winogrande": 0.236}, "Falcon 2 11B": {"GSM8K": 0.5383, "HellaSwag": 0.7721333333333332, "MMLU": 0.4453333333333333, "Winogrande": 0.5660000000000001}, "Falcon-180B": {"ARC AI2": 0.5706666666666668, "GSM8K": 0.544, "HellaSwag": 0.8533333333333334, "LAMBADA": 0.798, "MMLU": 0.608, "PIQA": 0.698, "OpenBookQA": 0.5226666666666667, "Winogrande": 0.742}, "Falcon-40B": {"ARC AI2": 0.4914666666666667, "TriviaQA": 0.799, "BBH": 0.1613333333333333, "GSM8K": 0.338, "HellaSwag": 0.8037333333333333, "LAMBADA": 0.773, "MMLU": 0.4253333333333333, "PIQA": 0.6599999999999999, "OpenBookQA": 0.4213333333333333, "Winogrande": 0.538}, "Falcon-7B": {"ARC AI2": 0.3048, "TriviaQA": 0.646, "BBH": 0.0502666666666666, "GSM8K": 0.068, "HellaSwag": 0.7080000000000001, "LAMBADA": 0.749, "MMLU": 0.1333333333333333, "PIQA": 0.6060000000000001, "OpenBookQA": 0.3546666666666667, "Winogrande": 0.3440000000000001}, "Cerebras-GPT-13B": {"ARC AI2": 0.0986666666666666, "HellaSwag": 0.4586666666666666, "MMLU": 0.016, "PIQA": 0.47, "OpenBookQA": 0.144, "Winogrande": 0.2159999999999999}, "GPT-3.5 Turbo": {"ARC AI2": 0.832, "TriviaQA": 0.858, "GPQA diamond": 0.0404040404040403, "MATH level 5": 0.158893504531722, "WeirdML": 0.0348, "BBH": 0.4878666666666666, "GSM8K": 0.5777, "ANLI": 0.3714999999999999, "MMLU": 0.6186666666666666, "OpenBookQA": 0.8133333333333334, "Winogrande": 0.6319999999999999}, "GPT-4 (Jun 2023)": {"TriviaQA": 0.848, "GPQA diamond": 0.0753367003367002, "MATH level 5": 0.2297016616314199, "OTIS Mock AIME 2024-2025": 0.0101212323434545, "WeirdML": 0.1244, "BBH": 0.6682666666666667, "GSM8K": 0.8999, "MMLU": 0.7653333333333333}, "GPT-4 (Mar 2023)": {"GPQA diamond": 0.143097643097643, "OTIS Mock AIME 2024-2025": 0.0045601156712267, "GSM8K": 0.92, "HellaSwag": 0.9373333333333332, "MMLU": 0.8186666666666667, "Winogrande": 0.75}, "GPT-4 Turbo (Apr 2024)": {"GPQA diamond": 0.2878787878787878, "MATH level 5": 0.467333836858006, "OTIS Mock AIME 2024-2025": 0.0657323990657323, "WeirdML": 0.1801, "SimpleBench": 0.1012, "MMLU": 0.7506666666666666}, "GPT-4.1": {"GPQA diamond": 0.5589225589225589, "MATH level 5": 0.8300604229607251, "OTIS Mock AIME 2024-2025": 0.382716049382716, "WeirdML": 0.3904, "CadEval": 0.42, "SimpleBench": 0.124, "FrontierMath-2025-02-28-Private": 0.0551724137931034, "Aider polyglot": 0.524, "GeoBench": 0.72, "FrontierMath-Tier-4-2025-07-01-Private": 0.0, "ARC-AGI": 0.055, "Fiction.LiveBench": 0.639, "SWE-Bench Verified (Bash Only)": 0.3958}, "GPT-4.1 mini": {"GPQA diamond": 0.5446127946127945, "MATH level 5": 0.8729229607250756, "OTIS Mock AIME 2024-2025": 0.4466688911133355, "WeirdML": 0.3761, "CadEval": 0.16, "FrontierMath-2025-02-28-Private": 0.0448275862068965, "Aider polyglot": 0.324, "ARC-AGI": 0.035, "Fiction.LiveBench": 0.444, "SWE-Bench Verified (Bash Only)": 0.2394}, "GPT-4.1 nano": {"GPQA diamond": 0.3190235690235689, "MATH level 5": 0.6999622356495468, "OTIS Mock AIME 2024-2025": 0.2881770659548436, "WeirdML": 0.1898, "FrontierMath-2025-02-28-Private": 0.0103448275862068, "Aider polyglot": 0.089, "ARC-AGI": 0.0, "Fiction.LiveBench": 0.25}, "GPT-4.5": {"GPQA diamond": 0.5824915824915825, "MATH level 5": 0.7862537764350453, "OTIS Mock AIME 2024-2025": 0.3771549327104882, "WeirdML": 0.3937, "Cybench": 0.175, "SimpleBench": 0.2139999999999999, "Aider polyglot": 0.449, "Lech Mazur Writing": 0.756, "VPCT": 0.45, "ARC-AGI": 0.103, "Fiction.LiveBench": 0.639}, "GPT-4o (Aug 2024)": {"GPQA diamond": 0.3228114478114477, "MATH level 5": 0.5327605740181269, "OTIS Mock AIME 2024-2025": 0.0629518407296184, "CadEval": 0.26, "SimpleBench": 0.0136, "FrontierMath-2025-02-28-Private": 0.0034482758620689, "Aider polyglot": 0.231, "VideoMME": 0.6253333333333333, "MMLU": 0.7906666666666666}, "GPT-4o (May 2024)": {"GPQA diamond": 0.3186026936026936, "MATH level 5": 0.5104796072507553, "OTIS Mock AIME 2024-2025": 0.0615615615615615, "ScienceQA": 0.8466666666666667, "Balrog": 0.323, "MMLU": 0.7893333333333333}, "GPT-4o (Nov 2024)": {"GPQA diamond": 0.3051346801346801, "MATH level 5": 0.4977341389728096, "OTIS Mock AIME 2024-2025": 0.0615615615615615, "WeirdML": 0.2512, "Cybench": 0.125, "FrontierMath-2025-02-28-Private": 0.0034482758620689, "Aider polyglot": 0.182, "GeoBench": 0.71, "Lech Mazur Writing": 0.8180000000000001, "VPCT": 0.4, "GSO-Bench": 0.0, "The Agent Company": 0.086, "ARC-AGI": 0.045, "SWE-Bench Verified (Bash Only)": 0.2162, "MMLU": 0.8413333333333334}, "GPT-4o mini": {"GPQA diamond": 0.1696127946127945, "MATH level 5": 0.5263406344410876, "OTIS Mock AIME 2024-2025": 0.0685129574018462, "WeirdML": 0.1176, "Aider polyglot": 0.036, "Balrog": 0.174, "GeoBench": 0.64, "Lech Mazur Writing": 0.672, "VideoMME": 0.5306666666666667, "VPCT": 0.34, "GSM8K": 0.913, "MMLU": 0.7573333333333333, "PIQA": 0.774}, "GPT-5": {"GPQA diamond": 0.8156565656565656, "MATH level 5": 0.9813066465256798, "OTIS Mock AIME 2024-2025": 0.9138026915804692, "WeirdML": 0.607, "SimpleBench": 0.4803999999999999, "FrontierMath-2025-02-28-Private": 0.3241379310344827, "SimpleQA Verified": 0.506, "Aider polyglot": 0.88, "Balrog": 0.328, "GeoBench": 0.81, "Lech Mazur Writing": 0.8723000000000001, "FrontierMath-Tier-4-2025-07-01-Private": 0.125, "VPCT": 0.66, "GSO-Bench": 0.069, "ARC-AGI": 0.657, "DeepResearch Bench": 0.51, "Fiction.LiveBench": 0.972, "SWE-Bench Verified (Bash Only)": 0.65, "Terminal Bench": 0.496, "Chess Puzzles": 0.37}, "GPT-5 Pro": {"WeirdML": 0.6039, "SimpleBench": 0.5392, "FrontierMath-Tier-4-2025-07-01-Private": 0.125, "ARC-AGI": 0.702}, "GPT-5 mini": {"GPQA diamond": 0.6666666666666666, "MATH level 5": 0.978474320241692, "OTIS Mock AIME 2024-2025": 0.8665331998665332, "WeirdML": 0.5267, "FrontierMath-2025-02-28-Private": 0.2724137931034482, "SimpleQA Verified": 0.21, "FrontierMath-Tier-4-2025-07-01-Private": 0.0625, "VPCT": 0.402, "Fiction.LiveBench": 0.694, "SWE-Bench Verified (Bash Only)": 0.598, "Terminal Bench": 0.319}, "GPT-5 nano": {"GPQA diamond": 0.5925925925925926, "MATH level 5": 0.952416918429003, "OTIS Mock AIME 2024-2025": 0.8109220331442554, "WeirdML": 0.3806, "FrontierMath-2025-02-28-Private": 0.0827586206896551, "SimpleQA Verified": 0.122, "FrontierMath-Tier-4-2025-07-01-Private": 0.0208333333333333, "VPCT": 0.372, "Fiction.LiveBench": 0.444, "SWE-Bench Verified (Bash Only)": 0.348, "Terminal Bench": 0.115}, "GPT-5.1": {"GPQA diamond": 0.8350168350168351, "OTIS Mock AIME 2024-2025": 0.8859971082193304, "WeirdML": 0.6077, "FrontierMath-2025-02-28-Private": 0.3103448275862069, "SimpleQA Verified": 0.489, "FrontierMath-Tier-4-2025-07-01-Private": 0.125, "VPCT": 0.587, "ARC-AGI": 0.728, "Terminal Bench": 0.476, "Chess Puzzles": 0.32}, "GPT-5.2": {"GPQA diamond": 0.8853333333333334, "OTIS Mock AIME 2024-2025": 0.9610721832944056, "WeirdML": 0.722, "SimpleBench": 0.3495999999999999, "FrontierMath-2025-02-28-Private": 0.407, "SimpleQA Verified": 0.389, "FrontierMath-Tier-4-2025-07-01-Private": 0.292, "VPCT": 0.84, "ARC-AGI": 0.862, "SWE-Bench Verified (Bash Only)": 0.718, "Chess Puzzles": 0.49}, "Claude 2": {"TriviaQA": 0.875, "GPQA diamond": 0.1287878787878788, "MATH level 5": 0.1172583081570997, "OTIS Mock AIME 2024-2025": 0.024024024024024, "MMLU": 0.7133333333333334}, "Gemini 1.0 Pro": {"GPQA diamond": 0.1195286195286194, "MATH level 5": 0.1124433534743202, "OTIS Mock AIME 2024-2025": 0.0101212323434545, "MMLU": 0.6}, "Gemini 1.5 Flash (May 2024)": {"GPQA diamond": 0.2049663299663298, "MATH level 5": 0.2512273413897281, "OTIS Mock AIME 2024-2025": 0.0379268157045933, "VideoMME": 0.604, "GSM8K": 0.824, "MMLU": 0.7053333333333334}, "Gemini 1.5 Flash (Sep 2024)": {"GPQA diamond": 0.2975589225589225, "MATH level 5": 0.6186744712990937, "OTIS Mock AIME 2024-2025": 0.1616616616616616, "WeirdML": 0.2487, "FrontierMath-2025-02-28-Private": 0.0, "Balrog": 0.146, "GeoBench": 0.76, "MMLU": 0.652, "PIQA": 0.75}, "Gemini 1.5 Pro": {"GPQA diamond": 0.4297138047138046, "MATH level 5": 0.7039274924471299, "OTIS Mock AIME 2024-2025": 0.2297853408964519, "WeirdML": 0.222, "CadEval": 0.34, "Cybench": 0.075, "SimpleBench": 0.1252, "Balrog": 0.21, "VideoMME": 0.6666666666666666, "The Agent Company": 0.034, "MMLU": 0.8253333333333334}, "Gemini 2.0 Flash": {"GPQA diamond": 0.521885521885522, "MATH level 5": 0.8216578549848943, "OTIS Mock AIME 2024-2025": 0.3104215326437549, "WeirdML": 0.2577, "CadEval": 0.3, "SimpleBench": 0.1732, "FrontierMath-2025-02-28-Private": 0.0172413793103448, "Aider polyglot": 0.382, "GeoBench": 0.77, "Lech Mazur Writing": 0.7150000000000001, "The Agent Company": 0.114, "Fiction.LiveBench": 0.611, "SWE-Bench Verified (Bash Only)": 0.1352, "MMLU": 0.7293333333333334}, "Gemini 2.0 Flash Thinking (Jan 2025)": {"GPQA diamond": 0.4276094276094276, "OTIS Mock AIME 2024-2025": 0.5773551329106884, "SimpleBench": 0.1684, "Aider polyglot": 0.182, "Lech Mazur Writing": 0.738, "Fiction.LiveBench": 0.528}, "Gemini 2.0 Pro": {"GPQA diamond": 0.5420875420875421, "MATH level 5": 0.8345921450151057, "Aider polyglot": 0.356, "Fiction.LiveBench": 0.417}, "Gemini 2.5 Flash (Apr 2025)": {"OTIS Mock AIME 2024-2025": 0.7302858413969525, "Aider polyglot": 0.471, "GeoBench": 0.73, "Lech Mazur Writing": 0.7650000000000001, "VPCT": 0.38, "ARC-AGI": 0.323, "Fiction.LiveBench": 0.472, "SWE-Bench Verified (Bash Only)": 0.2873}, "Gemini 2.5 Flash (Jun 2025)": {"SimpleBench": 0.2944, "FrontierMath-2025-02-28-Private": 0.0484429065743944, "Balrog": 0.335, "FrontierMath-Tier-4-2025-07-01-Private": 0.0416666666666666}, "Gemini 2.5 Flash (May 2025)": {"OTIS Mock AIME 2024-2025": 0.7080413747080414, "WeirdML": 0.4095, "Aider polyglot": 0.551, "GeoBench": 0.76, "ARC-AGI": 0.333, "Fiction.LiveBench": 0.778}, "Claude 2.1": {"GPQA diamond": 0.1060606060606059, "OTIS Mock AIME 2024-2025": 0.0184629073517961, "WeirdML": 0.0706, "MMLU": 0.6466666666666666}, "Gemini 2.5 Flash (Sep 2025)": {"WeirdML": 0.4191, "VPCT": 0.462, "The Agent Company": 0.411, "Terminal Bench": 0.171}, "Gemini 2.5 Pro (Jun 2025)": {"GPQA diamond": 0.803872053872054, "OTIS Mock AIME 2024-2025": 0.847069291513736, "WeirdML": 0.5403, "SimpleBench": 0.5488000000000001, "FrontierMath-2025-02-28-Private": 0.1413793103448276, "SimpleQA Verified": 0.56, "Aider polyglot": 0.831, "Lech Mazur Writing": 0.8602000000000001, "FrontierMath-Tier-4-2025-07-01-Private": 0.0416666666666666, "VPCT": 0.464, "GSO-Bench": 0.039, "ARC-AGI": 0.313, "DeepResearch Bench": 0.428, "Fiction.LiveBench": 0.917, "Terminal Bench": 0.326, "Chess Puzzles": 0.2}, "Gemini 2.5 Pro (Mar 2025)": {"GPQA diamond": 0.7845117845117845, "MATH level 5": 0.9556268882175226, "CadEval": 0.64, "SimpleBench": 0.4192, "Aider polyglot": 0.7290000000000001, "Balrog": 0.433, "GeoBench": 0.81, "Lech Mazur Writing": 0.8050000000000002, "VPCT": 0.48, "ARC-AGI": 0.33, "Fiction.LiveBench": 0.667}, "Gemini 2.5 Pro (May 2025)": {"GPQA diamond": 0.5555555555555555, "MATH level 5": 0.959025679758308, "Aider polyglot": 0.7690000000000001, "GeoBench": 0.86, "VPCT": 0.405, "The Agent Company": 0.303, "Fiction.LiveBench": 0.667, "SWE-Bench Verified (Bash Only)": 0.536}, "Gemini 3 Flash": {"GPQA diamond": 0.7760942760942761, "OTIS Mock AIME 2024-2025": 0.9277054832610389, "FrontierMath-2025-02-28-Private": 0.356401384083045, "SimpleQA Verified": 0.674, "FrontierMath-Tier-4-2025-07-01-Private": 0.0416666666666666, "VPCT": 0.726, "Chess Puzzles": 0.38}, "Gemini 3 Pro": {"GPQA diamond": 0.9015151515151516, "OTIS Mock AIME 2024-2025": 0.9138026915804692, "WeirdML": 0.6993, "SimpleBench": 0.7168, "FrontierMath-2025-02-28-Private": 0.376, "SimpleQA Verified": 0.729, "GeoBench": 0.84, "FrontierMath-Tier-4-2025-07-01-Private": 0.1875, "VPCT": 0.91, "GSO-Bench": 0.186, "ARC-AGI": 0.75, "SWE-Bench Verified (Bash Only)": 0.742, "Terminal Bench": 0.542, "Chess Puzzles": 0.31}, "Gemma 2 27B": {"GPQA diamond": 0.1531986531986532, "MATH level 5": 0.2788897280966767, "OTIS Mock AIME 2024-2025": 0.0129017906795683, "MMLU": 0.676}, "Gemma 2 9B": {"GPQA diamond": 0.0328282828282828, "MATH level 5": 0.2100641993957704, "OTIS Mock AIME 2024-2025": 0.0045601156712267, "GSM8K": 0.849, "MMLU": 0.628, "PIQA": 0.6739999999999999}, "Gemma 2B": {"ARC AI2": 0.2279999999999999, "TriviaQA": 0.532, "BBH": 0.1359999999999999, "GSM8K": 0.177, "HellaSwag": 0.6186666666666666, "MMLU": 0.2306666666666666, "PIQA": 0.546, "Winogrande": 0.308}, "Gemma 3 27B": {"GPQA diamond": 0.3181818181818181, "MATH level 5": 0.740370090634441, "OTIS Mock AIME 2024-2025": 0.1964186408630852, "Aider polyglot": 0.049, "GeoBench": 0.52, "Lech Mazur Writing": 0.799, "Fiction.LiveBench": 0.333}, "Claude 3 Haiku": {"GPQA diamond": 0.1506734006734006, "MATH level 5": 0.1487915407854985, "OTIS Mock AIME 2024-2025": 0.0170726281837392, "WeirdML": 0.0984, "CadEval": 0.12, "ScienceQA": 0.6266666666666666, "MMLU": 0.6506666666666666, "Winogrande": 0.484}, "Gemma 7B": {"ARC AI2": 0.7106666666666667, "TriviaQA": 0.723, "BBH": 0.4013333333333334, "GSM8K": 0.464, "ANLI": 0.2304999999999999, "HellaSwag": 0.7626666666666666, "MMLU": 0.548, "PIQA": 0.6240000000000001, "OpenBookQA": 0.7146666666666667, "Winogrande": 0.5800000000000001}, "Grok 3": {"GPQA diamond": 0.6767676767676768, "MATH level 5": 0.8874622356495468, "OTIS Mock AIME 2024-2025": 0.5551106662217774, "WeirdML": 0.3724, "SimpleBench": 0.2332, "FrontierMath-2025-02-28-Private": 0.0379310344827586, "Aider polyglot": 0.533, "Balrog": 0.295, "Lech Mazur Writing": 0.764, "FrontierMath-Tier-4-2025-07-01-Private": 0.0, "ARC-AGI": 0.055, "Fiction.LiveBench": 0.583}, "Grok 4": {"GPQA diamond": 0.8266666666666667, "OTIS Mock AIME 2024-2025": 0.8398398398398398, "WeirdML": 0.4573, "SimpleBench": 0.526, "FrontierMath-2025-02-28-Private": 0.196551724137931, "SimpleQA Verified": 0.479, "Aider polyglot": 0.7959999999999999, "Balrog": 0.436, "GeoBench": 0.45, "Lech Mazur Writing": 0.8068, "FrontierMath-Tier-4-2025-07-01-Private": 0.0208333333333333, "DeepResearch Bench": 0.479, "Fiction.LiveBench": 0.944, "Terminal Bench": 0.272, "Chess Puzzles": 0.3506493506493506}, "Grok-2": {"GPQA diamond": 0.3838383838383838, "MATH level 5": 0.6351963746223565, "OTIS Mock AIME 2024-2025": 0.1143921699477254, "WeirdML": 0.2224, "SimpleBench": 0.0724, "FrontierMath-2025-02-28-Private": 0.0068965517241379, "Lech Mazur Writing": 0.6360000000000001}, "Grok-3 mini": {"GPQA diamond": 0.6835016835016835, "MATH level 5": 0.9093655589123868, "OTIS Mock AIME 2024-2025": 0.7775553331108886, "WeirdML": 0.4258, "FrontierMath-2025-02-28-Private": 0.0586206896551724, "SimpleQA Verified": 0.211, "Aider polyglot": 0.493, "Lech Mazur Writing": 0.735, "ARC-AGI": 0.165, "Fiction.LiveBench": 0.667}, "INTELLECT-1": {"ARC AI2": 0.3936, "BBH": 0.1313333333333333, "GSM8K": 0.3858, "HellaSwag": 0.6189333333333332, "MMLU": 0.3318666666666667, "Winogrande": 0.3164}, "Kimi K2": {"WeirdML": 0.3936, "SimpleBench": 0.1156, "Aider polyglot": 0.5910000000000001, "Lech Mazur Writing": 0.8729, "GSO-Bench": 0.049, "Fiction.LiveBench": 0.667, "SWE-Bench Verified (Bash Only)": 0.438, "Terminal Bench": 0.278}, "Kimi K2 Thinking": {"GPQA diamond": 0.7895622895622895, "OTIS Mock AIME 2024-2025": 0.8303859414970527, "WeirdML": 0.4279, "FrontierMath-2025-02-28-Private": 0.2140350877192982, "SimpleQA Verified": 0.316, "FrontierMath-Tier-4-2025-07-01-Private": 0.0, "Terminal Bench": 0.357, "Chess Puzzles": 0.2}, "Claude 3 Opus": {"GPQA diamond": 0.2954545454545454, "MATH level 5": 0.3748111782477341, "OTIS Mock AIME 2024-2025": 0.0462684907129351, "WeirdML": 0.2318, "Cybench": 0.1, "SimpleBench": 0.0819999999999999, "MMLU": 0.7946666666666666, "Winogrande": 0.77}, "LLaMA-13B": {"ARC AI2": 0.3693333333333333, "TriviaQA": 0.779, "ScienceQA": 0.2444, "BBH": 0.172, "GSM8K": 0.2055, "HellaSwag": 0.7226666666666667, "LAMBADA": 0.752, "MMLU": 0.3026666666666666, "PIQA": 0.6020000000000001, "OpenBookQA": 0.4186666666666666, "Winogrande": 0.4599999999999999}, "LLaMA-33B": {"ARC AI2": 0.5666666666666668, "TriviaQA": 0.838, "BBH": 0.3333333333333333, "GSM8K": 0.441, "HellaSwag": 0.7706666666666666, "LAMBADA": 0.772, "MMLU": 0.4493333333333333, "PIQA": 0.6459999999999999, "OpenBookQA": 0.4479999999999999, "Winogrande": 0.52}, "LLaMA-65B": {"ARC AI2": 0.5933333333333333, "TriviaQA": 0.86, "BBH": 0.4453333333333333, "GSM8K": 0.544, "HellaSwag": 0.7893333333333333, "LAMBADA": 0.777, "MMLU": 0.512, "PIQA": 0.6559999999999999, "OpenBookQA": 0.4693333333333333, "Winogrande": 0.54}, "LLaMA-7B": {"ARC AI2": 0.3013333333333333, "TriviaQA": 0.71, "ScienceQA": 0.1492, "BBH": 0.1133333333333333, "GSM8K": 0.11, "HellaSwag": 0.6826666666666666, "LAMBADA": 0.733, "MMLU": 0.1413333333333333, "PIQA": 0.5960000000000001, "OpenBookQA": 0.4293333333333333, "Winogrande": 0.4019999999999999}, "Llama 2-13B": {"ARC AI2": 0.4706666666666666, "TriviaQA": 0.796, "ScienceQA": 0.4103999999999999, "BBH": 0.4426666666666666, "GSM8K": 0.369, "HellaSwag": 0.7426666666666667, "LAMBADA": 0.765, "MMLU": 0.4080000000000001, "PIQA": 0.6160000000000001, "OpenBookQA": 0.4266666666666666, "Winogrande": 0.4559999999999999}, "Llama 2-34B": {"ARC AI2": 0.3933333333333333, "TriviaQA": 0.846, "BBH": 0.2546666666666666, "GSM8K": 0.422, "MMLU": 0.5013333333333333, "PIQA": 0.6379999999999999, "OpenBookQA": 0.4426666666666666, "Winogrande": 0.534}, "Llama 2-70B": {"ARC AI2": 0.7106666666666667, "TriviaQA": 0.876, "GPQA diamond": 0.0176767676767675, "MATH level 5": 0.0328549848942598, "BBH": 0.532, "GSM8K": 0.696, "HellaSwag": 0.8039999999999999, "LAMBADA": 0.789, "MMLU": 0.5986666666666666, "PIQA": 0.6559999999999999, "OpenBookQA": 0.4693333333333333, "Winogrande": 0.6040000000000001}, "Llama 2-7B": {"ARC AI2": 0.2786666666666667, "TriviaQA": 0.737, "ScienceQA": 0.2410666666666666, "BBH": 0.1888, "GSM8K": 0.167, "HellaSwag": 0.6960000000000001, "LAMBADA": 0.733, "MMLU": 0.2773333333333334, "PIQA": 0.5760000000000001, "OpenBookQA": 0.4479999999999999, "Winogrande": 0.3839999999999999}, "Llama 3-70B": {"GPQA diamond": 0.2074915824915824, "MATH level 5": 0.225547583081571, "OTIS Mock AIME 2024-2025": 0.0420976532087642, "Cybench": 0.05, "MMLU": 0.7240000000000001, "OpenBookQA": 0.3013333333333333, "Winogrande": 0.6699999999999999}, "Llama 3-8B": {"ARC AI2": 0.7706666666666666, "TriviaQA": 0.677, "GPQA diamond": 0.0143097643097642, "MATH level 5": 0.0612726586102719, "OTIS Mock AIME 2024-2025": 0.0073406740073406, "ANLI": 0.3594999999999999, "MMLU": 0.584, "OpenBookQA": 0.7679999999999999, "Winogrande": 0.514}}};
    </script>

    <!-- App Logic -->
    <script>
        // State
        const state = {
            selectedBenchmarks: new Set(),
            mode: 'fixed', // 'fixed', 'refit', 'compare'
            models: [], // Computed capabilities
            cachedRefit: null
        };

        // --- Core Math: Sigmoid ---
        function sigmoid(x) {
            // Clip to avoid overflow/underflow
            const xc = Math.max(-100, Math.min(100, x));
            return 1.0 / (1.0 + Math.exp(-xc));
        }

        // --- IRT: Fixed Parameters Solver ---
        function solveFixed(performances, diffs, discs) {
            // performances: array of {model, score} (but here we process one model at a time)
            // Actually, we pass in arrays of values for a single model
            // p: observed performance (0-1), d: difficulty, a: discriminability
            
            // Objective: Minimize sum((pred - obs)^2)
            // Using simple Gradient Descent for scalar optimization
            
            let theta = 100; // Start at mean
            const learningRate = 0.5;
            const iterations = 50; 
            
            for(let i=0; i<iterations; i++) {
                let grad = 0;
                let loss = 0;
                
                for(let j=0; j<performances.length; j++) {
                    const p_obs = performances[j];
                    const d = diffs[j];
                    const a = discs[j];
                    
                    const logit = a * (theta - d);
                    const p_pred = sigmoid(logit);
                    
                    // Derivative of MSE w.r.t theta: 2 * (pred - obs) * pred * (1-pred) * a
                    grad += 2 * (p_pred - p_obs) * p_pred * (1.0 - p_pred) * a;
                }
                
                // Update
                theta = theta - learningRate * grad;
                
                // Simple bounds
                theta = Math.max(50, Math.min(200, theta));
            }
            return theta;
        }

        // --- IRT: Refit Solver (JMLE) ---
        async function solveJMLE(activeBenchmarkIds) {
             // Prepare data structure: sparse matrix logic
             // We need: for each model, list of (benchmark_idx, score)
             //          for each benchmark, list of (model_idx, score)
             
             const models = Object.keys(RAW_DATA.models);
             const benchmarks = Array.from(activeBenchmarkIds);
             
             if(benchmarks.length === 0) return {};
             
             // Init parameters
             // ability: map model_id -> float (init 100)
             // difficulty: map bench_id -> float (init 100)
             // disc: map bench_id -> float (init 1)
             
             let ability = {};
             models.forEach(m => ability[m] = 100.0);
             
             let difficulty = {};
             let discriminability = {};
             benchmarks.forEach(b => {
                 difficulty[b] = 100.0;
                 discriminability[b] = 1.0; // Fixed disc for stability if too sparse, or learn it
             });

             // Pre-process links for fast iteration
             const modelData = {}; // model -> [{b_id, p}]
             models.forEach(m => modelData[m] = []);
             
             const benchData = {}; // bench -> [{m_id, p}]
             benchmarks.forEach(b => benchData[b] = []);

             let totalObs = 0;
             models.forEach(m => {
                 if(RAW_DATA.performances[m]) {
                     benchmarks.forEach(b => {
                         if(RAW_DATA.performances[m][b] !== undefined) {
                             const p = RAW_DATA.performances[m][b];
                             modelData[m].push({b, p});
                             benchData[b].push({m, p});
                             totalObs++;
                         }
                     });
                 }
             });
             
             console.log(`Fitting JMLE on ${totalObs} observations...`);

             // Optimization Loop
             const iterations = 15;
             const lr = 0.5;

             for(let iter=0; iter<iterations; iter++) {
                 
                 // 1. Optimize Abilities (Theta)
                 models.forEach(m => {
                     const obs = modelData[m];
                     if(obs.length === 0) return;
                     
                     let theta = ability[m];
                     // specialized GD for user
                     for(let k=0; k<5; k++) {
                         let grad = 0;
                         obs.forEach(o => {
                             const d = difficulty[o.b];
                             const a = discriminability[o.b];
                             const pred = sigmoid(a * (theta - d));
                             grad += (pred - o.p) * pred * (1-pred) * a; // MSE grad
                             // grad += (pred - o.p) * a; // Log-likelihood grad (if minimizing NLL)
                         });
                         // Using MSE for consistency with "Fixed" logic described
                         // MSE Grad: 2 * error * sigmoid_prime * a
                         // Actually let's just stick to the simple gradient update 
                         theta = theta - 1.0 * grad; // Aggressive LR
                     }
                     ability[m] = theta;
                 });

                 // 2. Optimization Item Params (Diff/Disc)
                 // Stabilize: Center abilities mean=100
                 let meanTheta = 0; 
                 let count = 0;
                 models.forEach(m => { if(modelData[m].length>0) { meanTheta+=ability[m]; count++; }});
                 if(count>0) meanTheta /= count;
                 models.forEach(m => ability[m] += (100 - meanTheta)); // Re-center

                 benchmarks.forEach(b => {
                     const obs = benchData[b];
                     if(obs.length === 0) return;
                     
                     let d = difficulty[b];
                     let a = discriminability[b]; // Often simplified to 1 in simple Rasch, but we can try 2PL

                     for(let k=0; k<5; k++) {
                         let d_grad = 0;
                         // let a_grad = 0; 
                         
                         obs.forEach(o => {
                             const theta = ability[o.m];
                             const pred = sigmoid(a * (theta - d));
                             const err = (pred - o.p);
                             const w = pred * (1-pred);
                             
                             // MSE Gradients
                             // d_pred/d_d = w * (-a)
                             d_grad += 2 * err * w * (-a);
                             
                             // d_pred/d_a = w * (theta - d)
                             // a_grad += 2 * err * w * (theta - d);
                         });
                         
                         d = d - 0.5 * d_grad;
                         // a = a - 0.05 * a_grad; 
                         // a = Math.max(0.1, Math.min(3.0, a)); // Clamp disc
                     }
                     difficulty[b] = d;
                     // discriminability[b] = a;
                 });
                 
                 // Give UI a chance to breathe (optional for async)
                 if(iter % 5 === 0) await new Promise(r => setTimeout(r, 0));
             }
             
             // Final re-center/scale (Scale Identification)
             // We can force std dev to be something like 15 (like IQ/standardized)
             // But for now just centering on 100 is enough for relative comparison.
             
             return ability;
        }

        // --- Processing & Rendering ---
        
        async function updateChart() {
            const list = document.getElementById('benchmark-list');
            const status = document.getElementById('status-text');
            const loader = document.getElementById('loading-overlay');
            
            // Collect Active Data
            const activeBenchmarks = Array.from(state.selectedBenchmarks);
            document.getElementById('benchmark-count').innerText = `${activeBenchmarks.length}/${Object.keys(RAW_DATA.benchmarks).length}`;

            if(activeBenchmarks.length === 0) {
                Plotly.purge('chart-container');
                status.innerText = "Select at least one benchmark.";
                return;
            }

            // Calculation
            const plotData = [];
            
            // 1. FIXED Mode Calculation
            if(state.mode === 'fixed' || state.mode === 'compare') {
                const x = [];
                const y = [];
                const text = [];
                
                Object.keys(RAW_DATA.models).forEach(modelId => {
                     const m = RAW_DATA.models[modelId];
                     const perfs = [];
                     const diffs = [];
                     const discs = [];
                     
                     activeBenchmarks.forEach(bId => {
                         if(RAW_DATA.performances[modelId] && RAW_DATA.performances[modelId][bId] !== undefined) {
                             perfs.push(RAW_DATA.performances[modelId][bId]);
                             diffs.push(RAW_DATA.benchmarks[bId].diff); // Global fixed params
                             discs.push(RAW_DATA.benchmarks[bId].disc);
                         }
                     });
                     
                     if(perfs.length > 0) {
                         const theta = solveFixed(perfs, diffs, discs);
                         x.push(m.date);
                         y.push(theta);
                         text.push(m.name);
                     }
                });
                
                plotData.push({
                    x: x, y: y, text: text,
                    mode: 'markers',
                    type: 'scatter',
                    name: 'Fixed Params (Global)',
                    marker: { color: '#2563EB', size: 10, opacity: 0.7 }
                });
            }

            // 2. REFIT Mode Calculation
            if(state.mode === 'refit' || state.mode === 'compare') {
                loader.classList.remove('hidden');
                status.innerText = "Refitting model...";
                
                // Allow UI render
                await new Promise(r => setTimeout(r, 50));
                
                const refitScores = await solveJMLE(state.selectedBenchmarks);
                
                const x = [];
                const y = [];
                const text = [];
                
                Object.keys(refitScores).forEach(modelId => {
                    const m = RAW_DATA.models[modelId];
                    if(m && m.date) {
                        x.push(m.date);
                        y.push(refitScores[modelId]);
                        text.push(m.name);
                    }
                });
                
                plotData.push({
                    x: x, y: y, text: text,
                    mode: 'markers',
                    type: 'scatter',
                    name: 'Refit Params (Local)',
                    marker: { color: '#DC2626', size: 10, opacity: 0.7, symbol: state.mode==='compare' ? 'triangle-up' : 'circle' }
                });
                
                loader.classList.add('hidden');
            }

            status.innerText = `Displaying ${plotData[0].x.length} models over ${activeBenchmarks.length} benchmarks.`;

            // Config
            const layout = {
                title: 'Model Capability over Time',
                xaxis: { title: 'Release Date' },
                yaxis: { title: 'Estimated ECI' },
                hovermode: 'closest',
                template: 'plotly_white',
                margin: { t: 40, r: 20, l: 60, b: 40 }
            };

            Plotly.newPlot('chart-container', plotData, layout, {responsive: true});
        }

        // --- UI Setup ---
        function init() {
            // Render Benchmark List
            const list = document.getElementById('benchmark-list');
            const allBenches = Object.entries(RAW_DATA.benchmarks).sort((a,b) => a[1].name.localeCompare(b[1].name));
            
            allBenches.forEach(([id, meta]) => {
                const div = document.createElement('div');
                div.className = "flex items-center space-x-2 hover:bg-gray-50 p-1 rounded cursor-pointer";
                div.innerHTML = `
                    <input type="checkbox" id="chk-${id}" value="${id}" class="rounded text-blue-600 focus:ring-blue-500 border-gray-300">
                    <label for="chk-${id}" class="text-sm text-gray-700 flex-1 truncate select-none cursor-pointer" title="${meta.name}">
                        ${meta.name}
                    </label>
                `;
                // Add default to selected if it's "Knowledge" basket usually
                // For now, select all by default? Or first 5?
                // Select "Knowledge" typicals: MMLU, GPQA
                if(['MMLU', 'GPQA diamond', 'Othello'].some(k => meta.name.includes(k))) {
                    div.querySelector('input').checked = true;
                    state.selectedBenchmarks.add(id);
                }

                div.addEventListener('change', (e) => {
                    const chk = e.target.closest('input');
                    if(chk) {
                        if(chk.checked) state.selectedBenchmarks.add(chk.value);
                        else state.selectedBenchmarks.delete(chk.value);
                        updateChart();
                    }
                });
                list.appendChild(div);
            });
            
            // Search filter
            document.getElementById('search-benchmarks').addEventListener('input', (e) => {
                const term = e.target.value.toLowerCase();
                const items = list.children;
                for(let item of items) {
                    const txt = item.innerText.toLowerCase();
                    item.style.display = txt.includes(term) ? 'flex' : 'none';
                }
            });

            // Mode switching
            const btns = ['mode-fixed', 'mode-refit', 'mode-compare'];
            btns.forEach(bid => {
                document.getElementById(bid).addEventListener('click', (e) => {
                    // Reset styles
                    btns.forEach(b => {
                        const el = document.getElementById(b);
                        el.className = "px-3 py-1.5 rounded-md text-sm font-medium text-gray-500 hover:text-gray-900 transition-all";
                    });
                    // Set active
                    e.target.className = "px-3 py-1.5 rounded-md text-sm font-medium transition-all shadow-sm bg-white text-gray-900 border border-gray-200";
                    
                    state.mode = bid.replace('mode-', '');
                    updateChart();
                });
            });
            
            // Bulk actions
            document.getElementById('select-all').addEventListener('click', () => {
                 document.querySelectorAll('#benchmark-list input').forEach(c => {
                     c.checked = true;
                     state.selectedBenchmarks.add(c.value);
                 });
                 updateChart();
            });
            
             document.getElementById('deselect-all').addEventListener('click', () => {
                 document.querySelectorAll('#benchmark-list input').forEach(c => {
                     c.checked = false;
                 });
                 state.selectedBenchmarks.clear();
                 updateChart();
            });

            // Initial Draw
            updateChart();
        }

        init();

    </script>
</body>
</html>
