import csv
import json
import os
import io
import requests
import zipfile

DATASETS_DIR = "datasets_raw"
OUTPUT_FILE = "datasets.js"

DATASET_URLS = {
    "AI Models": "https://epoch.ai/data/ai_models.zip",
    "Benchmarks": "https://epoch.ai/data/benchmark_data.zip",
    "Data Centers": "https://epoch.ai/data/data_centers/data_centers.zip",
    "ML Hardware": "https://epoch.ai/data/ml_hardware.zip",
    "AI Companies": "https://epoch.ai/data/ai_companies.zip",
    "GPU Clusters": "https://epoch.ai/data/gpu_clusters.zip",
    "AI Chip Sales": "https://epoch.ai/data/ai_chip_sales.zip",
    "Public Opinion": "https://epoch.ai/data/polling_on_ai_usage_dec_2025.csv"
}

# The files unzip into the root of DATASETS_DIR without the parent folder in some cases,
# or we need to be flexible.
# Based on `ls -R datasets_raw` output:
# ai_models.zip -> all_ai_models.csv (in root)
# benchmark_data.zip -> epoch_capabilities_index.csv (in root)
# etc.
# So I should update paths to point to root if they are there.

DATASET_FILES = {
    "AI Models": "all_ai_models.csv",
    "Benchmarks": "epoch_capabilities_index.csv",
    "Data Centers": "data_centers.csv",
    "ML Hardware": "ml_hardware.csv",
    "AI Companies": "ai_companies.csv",
    "GPU Clusters": "gpu_clusters.csv",
    "AI Chip Sales": "timelines_by_chip.csv",
    "Public Opinion": "polling_on_ai_usage.csv"
}

def download_and_extract(name, url):
    print(f"Downloading {name} from {url}...")
    try:
        response = requests.get(url)
        response.raise_for_status()

        if url.endswith(".zip"):
            with zipfile.ZipFile(io.BytesIO(response.content)) as z:
                z.extractall(DATASETS_DIR)
        else:
            # It's a file (CSV)
            filename = os.path.basename(url)
            # rename if needed to match DATASET_FILES key
            if name == "Public Opinion":
                filename = "polling_on_ai_usage.csv"

            with open(os.path.join(DATASETS_DIR, filename), 'wb') as f:
                f.write(response.content)

    except Exception as e:
        print(f"Failed to download {name}: {e}")

def ensure_datasets():
    if not os.path.exists(DATASETS_DIR):
        os.makedirs(DATASETS_DIR)

    for name, relative_path in DATASET_FILES.items():
        filepath = os.path.join(DATASETS_DIR, relative_path)
        # Check if file exists. If not, maybe we need to download.
        # But wait, download checks URL.
        # We need to map Name -> URL.

        if not os.path.exists(filepath):
            print(f"File {filepath} missing. Attempting to download...")
            url = DATASET_URLS.get(name)
            if url:
                download_and_extract(name, url)
            else:
                print(f"No URL found for {name}.")

def parse_float(value):
    if not value:
        return None
    try:
        return float(value)
    except ValueError:
        return value

def process_csv(filepath):
    data = []
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                # Attempt to convert numeric values
                new_row = {}
                for k, v in row.items():
                    if k is None: continue # Skip None keys
                    k = k.strip()
                    if not k: continue # Skip empty keys

                    if "date" in k.lower():
                        new_row[k] = v
                    else:
                        num = parse_float(v)
                        new_row[k] = num if num is not None else v

                data.append(new_row)
    except Exception as e:
        print(f"Error processing {filepath}: {e}")
        return []
    return data

def main():
    ensure_datasets()

    datasets = {}
    schemas = {}

    for name, relative_path in DATASET_FILES.items():
        filepath = os.path.join(DATASETS_DIR, relative_path)
        print(f"Processing {name} from {filepath}...")

        if not os.path.exists(filepath):
            print(f"Warning: File {filepath} not found. Skipping.")
            continue

        data = process_csv(filepath)
        datasets[name] = data

        if data:
            schemas[name] = list(data[0].keys())
        else:
            schemas[name] = []

        print(f"  Loaded {len(data)} records.")

    # Generate JavaScript file
    js_content = f"""// Generated by process_datasets.py
// Do not edit manually.

window.DATASET_SCHEMAS = {json.dumps(schemas, indent=2)};

window.EPOCH_DATASETS = {json.dumps(datasets, indent=2)};

// Default to "AI Models" if available, otherwise first one
window.RAW_DATA = window.EPOCH_DATASETS["AI Models"] || Object.values(window.EPOCH_DATASETS)[0] || [];
"""

    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write(js_content)

    print(f"Successfully generated {OUTPUT_FILE}")

if __name__ == "__main__":
    main()
