window.EPOCH_DATASETS = {
  "AI Models": [
    {
      "Model": "Solar Open 100B\n",
      "Organization": "Upstage",
      "Publication date": "2025-12-31",
      "Domain": "Language",
      "Task": "Language modeling/generation,Chat",
      "Parameters": "102000000000.0",
      "Parameters notes": "102B, of which 12B active\n",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "19700000000000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://huggingface.co/upstage/Solar-Open-100B",
      "Reference": "Upstage's flagship 102B-parameter large language model",
      "Citations": "",
      "Authors": "Upstage AI\n",
      "Abstract": "Solar Open is Upstage's flagship 102B-parameter large language model, trained entirely from scratch and released under the Solar-Apache License 2.0 (see LICENSE). As a Mixture-of-Experts (MoE) architecture, it delivers enterprise-grade performance in reasoning, instruction-following, and agentic capabilities\u2014all while prioritizing transparency and customization for the open-source community.",
      "Organization categorization": "Industry",
      "Country (of organization)": "Korea (Republic of)",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "1572.0",
      "Training time notes": "\"Approx. 65.5 days (Net training time excluding downtime)\" per email\n",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "K-EXAONE",
      "Organization": "LG AI Research",
      "Publication date": "2025-12-31",
      "Domain": "Language",
      "Task": "",
      "Parameters": "236000000000.0",
      "Parameters notes": "236B",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "10T tokens",
      "Confidence": "Confident",
      "Link": "https://huggingface.co/LGAI-EXAONE",
      "Reference": "K-EXAONE Technical Report",
      "Citations": "",
      "Authors": "Eunbi Choi, Kibong Choi, Seokhee Hong, Junwon Hwang, Hyojin Jeon, Hyunjik Jo, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Yongil Kim, Haeju Lee, Jinsik Lee, Kyungmin Lee, Sangha Park, Heuiyeen Yeen",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "Korea (Republic of)",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "",
      "Epochs": "1.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA B200 GPUs",
      "Hardware quantity": "512.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "VAETKI\n",
      "Organization": "NC AI",
      "Publication date": "2025-12-30",
      "Domain": "Language",
      "Task": "Language modeling/generation",
      "Parameters": "100000000000.0",
      "Parameters notes": "Total parameters: 100B (Sparse), Active parameters: 10B per token. It uses a Mixture-of-Experts architecture with 1 shared expert and 128 total experts(top-8 routing)",
      "Training compute (FLOP)": "",
      "Training compute notes": "A combination of open-source datasets (primarily Nemotron-Pre-Training-Datasets) and custom web-crawled data pre-processed from various public sources.",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://huggingface.co/NC-AI-consortium-VAETKI/VAETKI",
      "Reference": "",
      "Citations": "",
      "Authors": "",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "Korea (Republic of)",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "",
      "Epochs": "1.0",
      "Training time (hours)": "3238.0",
      "Training time notes": "Trained on 1,016 H100 GPUs for 4.5 months",
      "Training hardware": "Nvidia H100 80G",
      "Hardware quantity": "1016.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "Estimated value of ~$9M - $10M USD (Calculated based on 3.24 million H100 GPU-hours",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "8000.0",
      "Batch size notes": "Multi-stage training strategy:\n    Pre-training Stage 1 (Main): 8000\n    Pre-training Stages 2-3 / Post Training (Annealing/Refinement): 2000",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "A.X K1",
      "Organization": "SK Telecom",
      "Publication date": "2025-12-30",
      "Domain": "Language",
      "Task": "Code generation,Language modeling/generation,Translation,Instruction interpretation,Mathematical reasoning,Chat,Language modeling,Language generation,Text autocompletion",
      "Parameters": "519000000000.0",
      "Parameters notes": "MoE Architecture, Total parameters 519B, Active parameters 33B",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://huggingface.co/skt",
      "Reference": "A.X K1",
      "Citations": "",
      "Authors": "",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "Korea (Republic of)",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "1536.0",
      "Training time notes": "GPU hours (1,536 GPUs)",
      "Training hardware": "NVIDIA H200",
      "Hardware quantity": "1536.0",
      "Hardware utilization (MFU)": "0.283",
      "Training compute cost (2023 USD)": "9642947.0",
      "Compute cost notes": "All cloud compute\n",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "18432.0",
      "Batch size notes": "ramp up from 2,048, step size 1,024",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "0.927",
      "Training compute cost (cloud)": "9642947.0",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "HyperCLOVA X SEED 32B Think",
      "Organization": "NAVER",
      "Publication date": "2025-12-29",
      "Domain": "Multimodal,Language,Vision",
      "Task": "Language modeling/generation,(Visual) Question answering,Image Understanding",
      "Parameters": "32000000000.0",
      "Parameters notes": "32B",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://huggingface.co/naver-hyperclovax/HyperCLOVAX-SEED-Think-32B",
      "Reference": "",
      "Citations": "",
      "Authors": "",
      "Abstract": "Developed by Naver, South Korea\u2019s leading AI research lab, this cutting-edge language model supports multimodal inputs and advanced reasoning.",
      "Organization categorization": "Industry",
      "Country (of organization)": "Korea (Republic of)",
      "Notability criteria": "Significant use",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GLM-4.7",
      "Organization": "Z.ai (Zhipu AI)",
      "Publication date": "2025-12-22",
      "Domain": "Language",
      "Task": "Language modeling/generation,Code generation",
      "Parameters": "358000000000.0",
      "Parameters notes": "mixture of experts (MoE) with 358B total, 32B active",
      "Training compute (FLOP)": "4.42e+24",
      "Training compute notes": "Training tokens not disclosed.\n\nPresuming this is a post-training update from GLM-4.5 (23T tokens) with minor additional compute, training compute is at least:\n\n6 FLOP/parameter/token * 32B active parameters * 23T tokens = 4.42e24 FLOP",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://z.ai/blog/glm-4.7",
      "Reference": "",
      "Citations": "",
      "Authors": "",
      "Abstract": "GLM-4.7, your new coding partner, is coming with the following features:\n\nCore Coding: GLM-4.7 brings clear gains, compared to its predecessor GLM-4.6, in multilingual agentic coding and terminal-based tasks, including (73.8%, +5.8%) on SWE-bench, (66.7%, +12.9%) on SWE-bench Multilingual, and (41%, +16.5%) on Terminal Bench 2.0. GLM-4.7 also supports thinking before acting, with significant improvements on complex tasks in mainstream agent frameworks such as Claude Code, Kilo Code, Cline, and Roo Code.\nVibe Coding: GLM-4.7 takes a major step forward in UI quality. It produces cleaner, more modern webpages and generates better-looking slides with more accurate layout and sizing.\nTool Using: GLM-4.7 achieves significantly improvements in Tool using. Significant better performances can be seen on benchmarks such as \u03c4^2-Bench and on web browsing via BrowseComp.\nComplex Reasoning: GLM-4.7 delivers a substantial boost in mathematical and reasoning capabilities, achieving (42.8%, +12.4%) on the HLE (Humanity\u2019s Last Exam) benchmark compared to GLM-4.6.\nYou can also see significant improvements in many other scenarios such as chat, creative writing, and role-play scenario.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "Discretionary",
      "Notability criteria notes": "near-frontier for open models and models from Chinese developers",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "GLM-4.5",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "MIT license",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Claude Opus 4.5",
      "Organization": "Anthropic",
      "Publication date": "2025-11-24",
      "Domain": "Language,Multimodal,Vision",
      "Task": "Code generation,Language modeling/generation,Quantitative reasoning,Search,Visual question answering,Translation,Image captioning,Instruction interpretation,Mathematical reasoning,Visual puzzles,Code autocompletion,Chat,Character recognition (OCR),Language modeling,Language generation,Text autocompletion,Retrieval-augmented generation,System control",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://www.anthropic.com/news/claude-opus-4-5",
      "Reference": "Introducing Claude Opus 4.5",
      "Citations": "",
      "Authors": "",
      "Abstract": "Our newest model, Claude Opus 4.5, is available today. It\u2019s intelligent, efficient, and the best model in the world for coding, agents, and computer use. It\u2019s also meaningfully better at everyday tasks like deep research and working with slides and spreadsheets. Opus 4.5 is a step forward in what AI systems can do, and a preview of larger changes to how work gets done.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "SOTA on SWE-bench",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Gemini 3 Pro",
      "Organization": "Google DeepMind",
      "Publication date": "2025-11-18",
      "Domain": "Multimodal,Language,Vision",
      "Task": "Language modeling/generation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "Almost certainly >1e25",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://blog.google/products/gemini/gemini-3/",
      "Reference": "A new era of intelligence with Gemini 3",
      "Citations": "",
      "Authors": "",
      "Abstract": "Gemini 3 Pro is the next generation in the Gemini series of models, a suite of\nhighly-capable, natively multimodal, reasoning models. Gemini 3 Pro is now Google\u2019s most advanced\nmodel for complex tasks, and can comprehend vast datasets, challenging problems from different\ninformation sources, including text, audio, images, video, and entire code repositories.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Significant use",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "Google TPU v7 Ironwood",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GPT-5.1",
      "Organization": "OpenAI",
      "Publication date": "2025-11-13",
      "Domain": "Multimodal,Language,Vision",
      "Task": "Language modeling/generation,Question answering",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "Almost certainly > 1e25\n",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://openai.com/index/gpt-5-1/",
      "Reference": "GPT-5.1: A smarter, more conversational ChatGPT",
      "Citations": "",
      "Authors": "",
      "Abstract": "\"Today we\u2019re upgrading the GPT\u20115 series with the release of:\n\nGPT\u20115.1 Instant: our most-used model, now warmer, more intelligent, and better at following your instructions.\nGPT\u20115.1 Thinking: our advanced reasoning model, now easier to understand and faster on simple tasks, more persistent on complex ones.\nWe heard clearly from users that great AI should not only be smart, but also enjoyable to talk to. GPT\u20115.1 improves meaningfully on both intelligence and communication style.\n\nWe\u2019re also making it easier for you to shape ChatGPT\u2019s tone. Preferences on chat style vary\u2014from person to person and even from conversation to conversation\u2014so we\u2019re introducing more intuitive and effective controls so ChatGPT can better match the tone you want in responses.\"",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Significant use",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "\n",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Kimi K2 Thinking",
      "Organization": "Moonshot",
      "Publication date": "2025-11-06",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering,Search,Quantitative reasoning,Code generation",
      "Parameters": "1000000000000.0",
      "Parameters notes": "Total Parameters\t1T\nActivated Parameters\t32B",
      "Training compute (FLOP)": "4.2e+24",
      "Training compute notes": "Assuming the additional post-training contributed between 1% and 100% of Kimi K2's training compute (which we confidently estimate at 2.976e+24), we get a range of 3.0e24 to 6.0e24, and a geometric mean of 4.2e24.",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "\"Kimi K2 was pre-trained on 15.5T tokens\" \nTraining dataset size for Kimi K2 Thinking is not reported",
      "Confidence": "Likely",
      "Link": "https://moonshotai.github.io/Kimi-K2/thinking",
      "Reference": "Introducing Kimi K2 Thinking",
      "Citations": "",
      "Authors": "",
      "Abstract": "Today, we are introducing Kimi K2 Thinking, our best open-source thinking model.\nBuilt as a thinking agent, it reasons step by step while using tools, achieving state-of-the-art performance on Humanity's Last Exam (HLE), BrowseComp, and other benchmarks, with major gains in reasoning, agentic search, coding, writing, and general capabilities.\nKimi K2 Thinking can execute up to 200 \u2013 300 sequential tool calls without human interference, reasoning coherently across hundreds of steps to solve complex problems.\nIt marks our latest efforts in test-time scaling, by scaling both thinking tokens and tool calling steps.\nK2 Thinking is now live on kimi.com under the chat mode [1], with its full agentic mode available soon. It is also accessible through the Kimi K2 Thinking API.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Kimi K2 Thinking sets new records across benchmarks that assess reasoning, coding, and agent capabilities. K2 Thinking achieves 44.9% on HLE with tools, 60.2% on BrowseComp, and 71.3% on SWE-Bench Verified, demonstrating strong generalization as a state-of-the-art thinking agent model.",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "Kimi K2",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "modified MIT license (capped by 100M MAU or $20M monthly revenue)\nhttps://huggingface.co/moonshotai/Kimi-K2-Thinking",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Tongyi DeepResearch",
      "Organization": "Alibaba",
      "Publication date": "2025-10-28",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering,Quantitative reasoning,Search,System control",
      "Parameters": "30500000000.0",
      "Parameters notes": "30.5 billion total parameters,\nwith only 3.3 billion activated per token",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2510.24701",
      "Reference": "Tongyi DeepResearch Technical Report",
      "Citations": "",
      "Authors": "Baixuan Li, Bo Zhang, Dingchu Zhang, Fei Huang, Guangyu Li, Guoxin Chen, Huifeng Yin, Jialong Wu, Jingren Zhou, Kuan Li, Liangcai Su, Litu Ou, Liwen Zhang, Pengjun Xie, Rui Ye, Wenbiao Yin, Xinmiao Yu, Xinyu Wang, Xixi Wu, Xuanzhong Chen, Yida Zhao, Zhen Zhang, Zhengwei Tao, Zhongwang Zhang, Zile Qiao, Chenxi Wang, Donglei Yu, Gang Fu, Haiyang Shen, Jiayin Yang, Jun Lin, Junkai Zhang, Kui Zeng, Li Yang, Hailong Yin, Maojia Song, Ming Yan, Peng Xia, Qian Xiao, Rui Min, Ruixue Ding, Runnan Fang, Shaowei Chen, Shen Huang, Shihang Wang, Shihao Cai, Weizhou Shen, Xiaobin Wang, Xin Guan, Xinyu Geng, Yingcheng Shi, Yuning Wu, Zhuo Chen, Zijian Li, Yong Jiang",
      "Abstract": "We present Tongyi DeepResearch, an agentic large language model, which is specifically designed for long-horizon, deep information-seeking research tasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is developed through an end-to-end training framework that combines agentic mid-training and agentic post-training, enabling scalable reasoning and information seeking across complex tasks. We design a highly scalable data synthesis pipeline that is fully automatic, without relying on costly human annotation, and empowers all training stages. By constructing customized environments for each stage, our system enables stable and consistent interactions throughout. Tongyi DeepResearch, featuring 30.5 billion total parameters, with only 3.3 billion activated per token, achieves state-of-the-art performance across a range of agentic deep research benchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH, WebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We open-source the model, framework, and complete solutions to empower the community.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\" achieves state-of-the-art performance across a range of agentic deep research benchmarks, including Humanity\u2019s Last\nExam, BrowseComp, BrowseComp-ZH, WebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "Qwen3-30B-A3B",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache 2.0\nhttps://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B\nhttps://github.com/Alibaba-NLP/DeepResearch",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "MiniMax-M2",
      "Organization": "MiniMax",
      "Publication date": "2025-10-27",
      "Domain": "Language",
      "Task": "Code generation,System control,Search,Language modeling/generation,Question answering",
      "Parameters": "229000000000.0",
      "Parameters notes": "\"maintaining activations around 10B\"\nsafetensors: 229B params",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://www.minimax.io/news/minimax-m2",
      "Reference": "MiniMax M2 & Agent: Ingenious in Simplicity",
      "Citations": "",
      "Authors": "",
      "Abstract": "Today, we are officially open-sourcing and launching MiniMax M2, a model born for Agents and code. At only 8% of the price of Claude Sonnet and twice the speed, it's available for free for a limited time!\nTop-tier Coding Capabilities: Built for end-to-end development workflows, it excels in various applications such as Claude Code, Cursor, Cline, Kilo Code, and Droid.\nPowerful Agentic Performance: It demonstrates outstanding planning and stable execution of complex, long-chain tool-calling tasks, coordinating calls to the Shell, Browser, Python code interpreter, and various MCP tools.\nUltimate Cost-Effectiveness & Speed: Through efficient design of activated parameters, we have achieved the optimal balance of intelligence, speed, and cost.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "Discretionary",
      "Notability criteria notes": " high performing open source model",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "MIT license\nhttps://huggingface.co/MiniMaxAI/MiniMax-M2",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Veo 3.1",
      "Organization": "Google DeepMind",
      "Publication date": "2025-10-15",
      "Domain": "Video,Vision",
      "Task": "Image-to-video,Video generation,Text-to-video,Audio generation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://blog.google/technology/ai/veo-updates-flow/",
      "Reference": "Introducing Veo 3.1 and advanced capabilities in Flow",
      "Citations": "",
      "Authors": "",
      "Abstract": "We\u2019re also introducing Veo 3.1, which brings richer audio, more narrative control, and enhanced realism that captures true-to-life textures. Veo 3.1 is state-of-the-art and builds on Veo 3, with stronger prompt adherence and improved audiovisual quality when turning images into videos.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Veo 3.1 has achieved state of the art results in head-to-head comparisons of outputs by human raters over top video generation models.\"\nhttps://deepmind.google/models/veo/evals/",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "we're opening up new possibilities for richer, more powerful video storytelling right inside Flow.\n\nThe Veo 3.1 model is also available via the Gemini API for developers, Vertex AI for enterprise customers, and the Gemini app. New capabilities are available in both Gemini API 2 and Vertex AI 3\n\n",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Claude Haiku 4.5",
      "Organization": "Anthropic",
      "Publication date": "2025-10-15",
      "Domain": "Language",
      "Task": "Chat,Code generation,Language modeling/generation,Question answering",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://www.anthropic.com/news/claude-haiku-4-5",
      "Reference": "Introducing Claude Haiku 4.5",
      "Citations": "",
      "Authors": "",
      "Abstract": "Claude Haiku 4.5, our latest small model, is available today to all users.\n\nWhat was recently at the frontier is now cheaper and faster. Five months ago, Claude Sonnet 4 was a state-of-the-art model. Today, Claude Haiku 4.5 gives you similar levels of coding performance but at one-third the cost and more than twice the speed.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Significant use",
      "Notability criteria notes": "Likely significant use as one of Anthropic's flagship models",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Ling-1T",
      "Organization": "Ant Group",
      "Publication date": "2025-10-10",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering,Quantitative reasoning,Code generation,Mathematical reasoning",
      "Parameters": "1000000000000.0",
      "Parameters notes": "1 trillion total parameters with 50 billion activated parameters",
      "Training compute (FLOP)": "6.000001e+24",
      "Training compute notes": "6 FLOP/parameter/token * 50000000000 active parameters * 20000000000000 tokens = 6e+24 FLOP",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "20000000000000",
      "Dataset size notes": "\"Pre-trained on 20 trillion+ high-quality, reasoning-dense tokens\"",
      "Confidence": "Confident",
      "Link": "https://huggingface.co/inclusionAI/Ling-1T\nhttps://arxiv.org/pdf/2510.22115",
      "Reference": "Ling-1T",
      "Citations": "",
      "Authors": "",
      "Abstract": "Ling-1T is the first flagship non-thinking model in the Ling 2.0 series, featuring 1 trillion total parameters with \u2248 50 billion active parameters per token. Built on the Ling 2.0 architecture, Ling-1T is designed to push the limits of efficient reasoning and scalable cognition.\n\nPre-trained on 20 trillion+ high-quality, reasoning-dense tokens, Ling-1T-base supports up to 128K context length and adopts an evolutionary chain-of-thought (Evo-CoT) process across mid-training and post-training. This curriculum greatly enhances the model\u2019s efficiency and reasoning depth, allowing Ling-1T to achieve state-of-the-art performance on multiple complex reasoning benchmarks\u2014balancing accuracy and efficiency.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We comprehensively evaluated Ling-1T against leading flagship models, including both open-source giants (e.g., DeepSeek-V3.1-Terminus, Kimi-K2-Instruct-0905) and closed-source APIs (GPT-5-main, Gemini-2.5-Pro). Across code generation, software development, competition-level mathematics, professional math, and logical reasoning, Ling-1T consistently demonstrates superior complex reasoning ability and overall advantage.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "74317824.0",
      "Batch size notes": "18144 * 4096",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "MIT license\nhttps://huggingface.co/inclusionAI/Ling-1T",
      "Numerical format": "FP8",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GPT-5 Pro",
      "Organization": "OpenAI",
      "Publication date": "2025-10-07",
      "Domain": "Multimodal,Language,Vision",
      "Task": "",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://cdn.openai.com/gpt-5-system-card.pdf",
      "Reference": "GPT-5 System Card",
      "Citations": "",
      "Authors": "",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Significant use,Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Sora 2.0",
      "Organization": "OpenAI",
      "Publication date": "2025-09-30",
      "Domain": "Video",
      "Task": "Video generation,Text-to-video",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://openai.com/index/sora-2/",
      "Reference": "Sora 2 is here",
      "Citations": "",
      "Authors": "",
      "Abstract": "Our latest video generation model is more physically accurate, realistic, and more controllable than prior systems. It also features synchronized dialogue and sound effects. Create with it in the new Sora app.\n\nToday we\u2019re releasing Sora 2, our flagship video and audio generation model.\n\nThe original Sora model\u2060 from February 2024 was in many ways the GPT\u20111 moment for video\u2014the first time video generation started to seem like it was working, and simple behaviors like object permanence emerged from scaling up pre-training compute. Since then, the Sora team has been focused on training models with more advanced world simulation capabilities. We believe such systems will be critical for training AI models that deeply understand the physical world. A major milestone for this is mastering pre-training and post-training on large-scale video data, which are in their infancy compared to language.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Significant use",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Hosted access (no API)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GLM-4.6",
      "Organization": "Z.ai (Zhipu AI),Tsinghua University",
      "Publication date": "2025-09-30",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering,Code generation,Quantitative reasoning,System control",
      "Parameters": "357000000000.0",
      "Parameters notes": "Similarly to GLM 4.5: 355 billion total parameters (reported) with 32 billion active parameters (assumed)",
      "Training compute (FLOP)": "4.42e+24",
      "Training compute notes": "6 FLOP/parameter/token * 32000000000 active parameters [very likely assumption - everything else is reported to be same as at GLM 4.5] * 23000000000000 tokens = 4.42e24 FLOP",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "23000000000000",
      "Dataset size notes": "23T tokens (from Jaime's correspondence with the GLM team)",
      "Confidence": "Likely",
      "Link": "https://z.ai/blog/glm-4.6",
      "Reference": "GLM-4.6: Advanced Agentic, Reasoning and Coding Capabilities",
      "Citations": "",
      "Authors": "",
      "Abstract": "Today, we are releasing the latest version of our flagship model: GLM-4.6. Compared with GLM-4.5, this generation brings several key improvements:\n\nLonger context window: The context window has been expanded from 128K to 200K tokens, enabling the model to handle more complex agentic tasks.\nSuperior coding performance: The model achieves higher scores on code benchmarks and demonstrates better real-world performance in applications such as Claude Code\u3001Cline\u3001Roo Code and Kilo Code, including improvements in generating visually polished front-end pages.\nAdvanced reasoning: GLM-4.6 shows a clear improvement in reasoning performance and supports tool use during inference, leading to stronger overall capability.\nMore capable agents: GLM-4.6 exhibits stronger performance in tool using and search-based agents, and integrates more effectively within agent frameworks.\nRefined writing: Better aligns with human preferences in style and readability, and performs more naturally in role-playing scenarios.\nWe evaluated GLM-4.6 across eight public benchmarks covering agents, reasoning, and coding. Results show clear gains over GLM-4.5, with GLM-4.6 also holding competitive advantages over leading domestic and international models such as DeepSeek-V3.2-Exp and Claude Sonnet 4, but still lags behind Claude Sonnet 4.5 in coding ability.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "China,China",
      "Notability criteria": "Discretionary",
      "Notability criteria notes": "\"We evaluated GLM-4.6 across eight public benchmarks covering agents, reasoning, and coding. Results show clear gains over GLM-4.5, with GLM-4.6 also holding competitive advantages over leading domestic and international models such as DeepSeek-V3.1-Terminus and Claude Sonnet 4.\"",
      "Epochs": "",
      "Training time (hours)": "2880.0",
      "Training time notes": "4 months (from Jaime's correspondence with the GLM team)\n\n4 months * 30 days * 24 hours = 2880 hours",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "Not listed",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "MIT license\nhttps://huggingface.co/zai-org/GLM-4.6",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Claude Sonnet 4.5",
      "Organization": "Anthropic",
      "Publication date": "2025-09-29",
      "Domain": "Language,Vision,Multimodal",
      "Task": "Language modeling/generation,Code generation,System control,Question answering,Quantitative reasoning,Mathematical reasoning,Visual question answering",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://www.anthropic.com/news/claude-sonnet-4-5",
      "Reference": "Introducing Claude Sonnet 4.5",
      "Citations": "",
      "Authors": "",
      "Abstract": "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It\u2019s the best model at using computers. And it shows substantial gains in reasoning and math.\nCode is everywhere. It runs every application, spreadsheet, and software tool you use. Being able to use those tools and reason through hard problems is how modern work gets done.\nClaude Sonnet 4.5 makes this possible. We're releasing it along with a set of major upgrades to our products. In Claude Code, we've added checkpoints\u2014one of our most requested features\u2014that save your progress and allow you to roll back instantly to a previous state. We've refreshed the terminal interface and shipped a native VS Code extension. We've added a new context editing feature and memory tool to the Claude API that lets agents run even longer and handle even greater complexity. In the Claude apps, we've brought code execution and file creation (spreadsheets, slides, and documents) directly into the conversation. And we've made the Claude for Chrome extension available to Max users who joined the waitlist last month.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Claude Sonnet 4.5 is the best coding model in the world\"\n\n\"Claude Sonnet 4.5 is state-of-the-art on the SWE-bench Verified evaluation\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Gemini Robotics-ER 1.5",
      "Organization": "Google DeepMind",
      "Publication date": "2025-09-25",
      "Domain": "Vision,Language,Speech",
      "Task": "Instruction interpretation,Robotic manipulation,Image captioning,Object detection,Search,Language modeling/generation,Question answering,Speech recognition (ASR)",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://deepmind.google/discover/blog/gemini-robotics-15-brings-ai-agents-into-the-physical-world/\n\nhttps://storage.googleapis.com/deepmind-media/gemini-robotics/Gemini-Robotics-1-5-Tech-Report.pdf",
      "Reference": "Gemini Robotics 1.5 brings AI agents into the physical world",
      "Citations": "",
      "Authors": "Abbas Abdolmaleki, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Ashwin Balakrishna, Nathan Batchelor, Alex Bewley, Jeff Bingham, Michael Bloesch, Konstantinos Bousmalis, Philemon Brakel, Anthony Brohan, Thomas Buschmann, Arunkumar Byravan, Serkan Cabi, Ken Caluwaerts, Federico Casarini, Christine Chan, Oscar Chang, London Chappellet-Volpini, Jose Enrique Chen, Xi Chen, Hao-Tien Lewis Chiang, Krzysztof Choromanski, Adrian Collister, David B. D'Ambrosio, Sudeep Dasari, Todor Davchev, Meet Kirankumar Dave, Coline Devin, Norman Di Palo, Tianli Ding, Carl Doersch, Adil Dostmohamed, Yilun Du, Debidatta Dwibedi, Sathish Thoppay Egambaram, Michael Elabd, Tom Erez, Xiaolin Fang, Claudio Fantacci, Cody Fong, Erik Frey, Chuyuan Fu, Ruiqi Gao, Marissa Giustina, Keerthana Gopalakrishnan, Laura Graesser, Oliver Groth, Agrim Gupta, Roland Hafner, Steven Hansen, Leonard Hasenclever, Sam Haves, Nicolas Heess, Brandon Hernaez, Alex Hofer, Jasmine Hsu, Lu Huang, Sandy H. Huang, Atil Iscen, Mithun George Jacob, Deepali Jain, Sally Jesmonth, Abhishek Jindal, Ryan Julian, Dmitry Kalashnikov, Stefani Karp, Matija Kecman, J. Chase Kew, Donnie Kim, Frank Kim, Junkyung Kim, Thomas Kipf, Sean Kirmani, Ksenia Konyushkova, Yuheng Kuang, Thomas Lampe, Antoine Laurens, Tuan Anh Le, Isabel Leal, Alex X. Lee, Tsang-Wei Edward Lee, Guy Lever, Jacky Liang, Li-Heng Lin, Fangchen Liu, Shangbang Long, Caden Lu, Sharath Maddineni, Anirudha Majumdar, Kevis-Kokitsi Maninis, Andrew Marmon, Sergio Martinez, Assaf Hurwitz Michaely, Niko Milonopoulos, Joss Moore, Robert Moreno, Michael Neunert, Francesco Nori, Joy Ortiz, Kenneth Oslund, Carolina Parada, Emilio Parisotto, Peter Pastor Sampedro, Acorn Pooley, Thomas Power, Alessio Quaglino, Haroon Qureshi, Rajkumar Vasudeva Raju, Helen Ran, Dushyant Rao, Kanishka Rao, Isaac Reid, David Rendleman, Krista Reymann, Miguel Rivas, Francesco Romano, Yulia Rubanova, Pannag R Sanketi, Dhruv Shah, Mohit Sharma, Kathryn Shea, Mohit Shridhar, Charles Shu, Vikas Sindhwani, Sumeet Singh, Radu Soricut, Rachel Sterneck, Ian Storz, Razvan Surdulescu, Jie Tan, Jonathan Tompson, Saran Tunyasuvunakool, Jake Varley, Grace Vesom, Giulia Vezzani, Maria Bauza Villalonga, Oriol Vinyals, Ren\u00e9 Wagner, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Chengda Wu, Markus Wulfmeier, Fei Xia, Ted Xiao, Annie Xie, Jinyu Xie, Peng Xu, Sichun Xu, Ying Xu, Zhuo Xu, Jimmy Yan, Sherry Yang, Skye Yang, Yuxiang Yang, Hiu Hong Yu, Wenhao Yu, Li Yang Ku, Wentao Yuan, Yuan Yuan, Jingwei Zhang, Tingnan Zhang, Zhiyuan Zhang, Allan Zhou, Guangyao Zhou and Yuxiang Zhou",
      "Abstract": "Our most capable vision-language model (VLM) reasons about the physical world, natively calls digital tools and creates detailed, multi-step plans to complete a mission. This model now achieves state-of-the-art performance across spatial understanding benchmarks.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Table 19\n\"Our model achieves the highest aggregated performance on 15 academic embodied reasoning benchmarks, including Point-Bench, RefSpatial, RoboSpatial-Pointing, Where2Place, BLINK, CV-Bench, ERQA, EmbSpatial, MindCube, RoboSpatial-VQA, SAT, Cosmos-Reason1, Min Video Pairs, OpenEQA and VSI-Bench.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "Google TPU v4,Google TPU v5e,Google TPU v6e Trillium",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Starting today, we\u2019re making Gemini Robotics-ER 1.5 available to developers via the Gemini API in Google AI Studio.",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Qwen3-Omni-30B-A3B",
      "Organization": "Alibaba",
      "Publication date": "2025-09-22",
      "Domain": "Multimodal,Language,Vision,Speech,Video",
      "Task": "Language modeling/generation,Question answering,Visual question answering,Image captioning,Video description,Speech recognition (ASR),Speech synthesis,Speech-to-text,Text-to-speech (TTS)",
      "Parameters": "35300000000.0",
      "Parameters notes": "Audio Encoder AuT 650M \nVision Encoder SigLIP2-So400M 540M \nThinker MoE Transformer 30B-A3B\nTalker MoE Transformer 3B-A0.3B\nMTP Dense Transformer 80M\nCode2wav ConvNet 200M",
      "Training compute (FLOP)": "3.6e+22",
      "Training compute notes": "6 FLOP/parameter/token * 3000000000 active parameters * 2000000000000 tokens = 3.6e+22 FLOP",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "2000000000000",
      "Dataset size notes": "\"our AuT (Audio Transformer) encoder, trained from scratch on 20 million hours of supervised audio\"\n\n\"The second phase of pretraining utilizes a large-scale dataset containing\napproximately 2 trillion tokens, with the following distribution across modalities: text (0.57 trillion), audio (0.77 trillion), image (0.82 trillion), video (0.05 trillion), and video-audio (0.05 trillion)\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2509.17765",
      "Reference": "Qwen3-Omni Technical Report",
      "Citations": "",
      "Authors": "Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, Yuanjun Lv, Yongqi Wang, Dake Guo, He Wang, Linhan Ma, Pei Zhang, Xinyu Zhang, Hongkun Hao, Zishan Guo, Baosong Yang, Bin Zhang, Ziyang Ma, Xipin Wei, Shuai Bai, Keqin Chen, Xuejing Liu, Peng Wang, Mingkun Yang, Dayiheng Liu, Xingzhang Ren, Bo Zheng, Rui Men, Fan Zhou, Bowen Yu, Jianxin Yang, Le Yu, Jingren Zhou, Junyang Lin",
      "Abstract": "We present Qwen3-Omni, a single multimodal model that, for the first time, maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages, and speech generation in 10 languages. To reduce first-packet latency in streaming synthesis, Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms. To further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Qwen3-Omni achieves SOTA on 32 benchmarks and overall SOTA on 22 across 36 audio and audio-visual benchmarks, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "not listed",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Apache 2.0\nhttps://huggingface.co/Qwen/Qwen3-Omni-30B-A3B-Instruct",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "AgentFounder-30B",
      "Organization": "Alibaba",
      "Publication date": "2025-09-16",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering,System control,Quantitative reasoning,Mathematical reasoning,Code generation,Search",
      "Parameters": "30000000000.0",
      "Parameters notes": "30B-A3B",
      "Training compute (FLOP)": "6.5367e+23",
      "Training compute notes": "6.48e+23 FLOP [base model compute] + 5.67e+21 FLOP = 6.5367e+23 FLOP",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "315000000000",
      "Dataset size notes": "We train AgentFounder models with data volumes ranging from 0B to 315B tokens\n\nAgentic CPT Stage 1: We process approximately 200B tokens\nAgentic CPT Stage 2: We further refine these capabilities using 100B tokens",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2509.13310",
      "Reference": "Scaling Agents via Continual Pre-training",
      "Citations": "",
      "Authors": "Liangcai Su, Zhen Zhang, Guangyu Li, Zhuo Chen, Chenxi Wang, Maojia Song, Xinyu Wang, Kuan Li, Jialong Wu, Xuanzhong Chen, Zile Qiao, Zhongwang Zhang, Huifeng Yin, Shihao Cai, Runnan Fang, Zhengwei Tao, Wenbiao Yin, Chenxiong Qian, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou",
      "Abstract": "Large language models (LLMs) have evolved into agentic systems capable of autonomous tool use and multi-step reasoning for complex problem-solving. However, post-training approaches building upon general-purpose foundation models consistently underperform in agentic tasks, particularly in open-source implementations. We identify the root cause: the absence of robust agentic foundation models forces models during post-training to simultaneously learn diverse agentic behaviors while aligning them to expert demonstrations, thereby creating fundamental optimization tensions. To this end, we are the first to propose incorporating Agentic Continual Pre-training (Agentic CPT) into the deep research agents training pipeline to build powerful agentic foundational models. Based on this approach, we develop a deep research agent model named AgentFounder. We evaluate our AgentFounder-30B on 10 benchmarks and achieve state-of-the-art performance while retains strong tool-use ability, notably 39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We evaluate our AgentFounder-30B on 10 benchmarks and achieve state-of-the-art performance while retains strong tool-use ability, notably 39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "Qwen3-30B-A3B",
      "Finetune compute (FLOP)": "5.67e+21",
      "Finetune compute notes": "6 FLOP/parameter/token * 3000000000 active parameters * 315000000000 tokens = 5.67e+21 FLOP",
      "Batch size": "",
      "Batch size notes": "Batch size not stated",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache 2.0\nhttps://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B\n\nhttps://github.com/Alibaba-NLP/DeepResearch",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Qwen3-Max",
      "Organization": "Alibaba",
      "Publication date": "2025-09-05",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering,Mathematical reasoning,Code generation,Quantitative reasoning,Retrieval-augmented generation,Translation",
      "Parameters": "1000000000000.0",
      "Parameters notes": "MoE architecture",
      "Training compute (FLOP)": "1.512e+25",
      "Training compute notes": "6ND with:\n36T tokens is taken from the qwen3 technical report\n70B active params is based on it having >1T params, and the architectures of Qwen3-235B-A22B and Qwen3-Coder-480B-A35B",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "36000000000000",
      "Dataset size notes": "\"was pretrained on 36 trillion tokens\"",
      "Confidence": "Speculative",
      "Link": "https://modelstudio.console.alibabacloud.com/?tab=doc#/doc/?type=model&url=2840914_2&modelId=qwen3-max-preview\n\nhttps://qwen.ai/blog?id=87dc93fc8a590dc718c77e1f6e84c07b474f6c5a&from=home.latest-research-list",
      "Reference": "Introducing Qwen3-Max-Preview (Instruct) \u2014 our biggest model yet, with over 1 trillion parameters! ",
      "Citations": "",
      "Authors": "",
      "Abstract": "Following the release of the Qwen3-2507 series, we are thrilled to introduce Qwen3-Max \u2014 our largest and most capable model to date. The preview version of Qwen3-Max-Instruct currently ranks third on the Text Arena leaderboard, surpassing GPT-5-Chat. The official release further enhances performance in coding and agent capabilities, achieving state-of-the-art results across a comprehensive suite of benchmarks \u2014 including knowledge, reasoning, coding, instruction following, human preference alignment, agent tasks, and multilingual understanding. We invite you to try Qwen3-Max-Instruct via its API on Alibaba Cloud or explore it directly on Qwen Chat. Meanwhile, Qwen3-Max-Thinking \u2014 still under active training \u2014 is already demonstrating remarkable potential. When augmented with tool usage and scaled test-time compute, the Thinking variant has achieved 100% on challenging reasoning benchmarks such as AIME 25 and HMMT. We look forward to releasing it publicly in the near future.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "Discretionary",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "not listed",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "LongCat-Flash",
      "Organization": "Meituan Inc",
      "Publication date": "2025-09-01",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering,Chat,Code generation,Quantitative reasoning,Instruction interpretation",
      "Parameters": "560000000000.0",
      "Parameters notes": "\"560 billion total parameters, featuring an innovative Mixture-of-Experts (MoE) architecture. The model incorporates a dynamic computation mechanism that activates 18.6B\u223c31.3B parameters (averaging\u223c27B)\"",
      "Training compute (FLOP)": "3.726e+24",
      "Training compute notes": "6 FLOP/parameter/token * 27000000000 active parameters * 23000000000000 tokens [\"Likely\" confidence, see dataset size notes] = 3.726e+24 FLOP",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "23000000000000",
      "Dataset size notes": "\"(1) We train the model on approximately 20 trillion tokens with 8192 sequence length to establish a robust base model. \n(2) Reasoning and coding capabilities are further enhanced using trillions of data. \n(3) The context length is extended to 128k through training on long context corpora.\"\n\nWith \"Likely\" confidence we may assume theytrained on total of 23T tokens",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2509.01322",
      "Reference": "LongCat-Flash Technical Report",
      "Citations": "",
      "Authors": "Meituan LongCat Team, Bayan, Bei Li, Bingye Lei, Bo Wang, Bolin Rong, Chao Wang, Chao Zhang, Chen Gao, Chen Zhang, Cheng Sun, Chengcheng Han, Chenguang Xi, Chi Zhang, Chong Peng, Chuan Qin, Chuyu Zhang, Cong Chen, Congkui Wang, Dan Ma, Daoru Pan, Defei Bu, Dengchang Zhao, Deyang Kong, Dishan Liu, Feiye Huo, Fengcun Li, Fubao Zhang, Gan Dong, Gang Liu, Gang Xu, Ge Li, Guoqiang Tan, Guoyuan Lin, Haihang Jing, Haomin Fu, Haonan Yan, Haoxing Wen, Haozhe Zhao, Hong Liu, Hongmei Shi, Hongyan Hao, Hongyin Tang, Huantian Lv, Hui Su, Jiacheng Li, Jiahao Liu, Jiahuan Li, Jiajun Yang, Jiaming Wang, Jian Yang, Jianchao Tan, Jiaqi Sun, Jiaqi Zhang, Jiawei Fu, Jiawei Yang, Jiaxi Hu, Jiayu Qin, Jingang Wang, Jiyuan He, Jun Kuang, Junhui Mei, Kai Liang, Ke He, Kefeng Zhang, Keheng Wang, Keqing He, Liang Gao, Liang Shi, Lianhui Ma, Lin Qiu, Lingbin Kong, Lingtong Si, Linkun Lyu, Linsen Guo, Liqi Yang, Lizhi Yan, Mai Xia, Man Gao, Manyuan Zhang, Meng Zhou, Mengxia Shen, Mingxiang Tuo, Mingyang Zhu, Peiguang Li, Peng Pei, Peng Zhao, Pengcheng Jia, Pingwei Sun, Qi Gu, Qianyun Li, Qingyuan Li, Qiong Huang, Qiyuan Duan, Ran Meng, Rongxiang Weng, Ruichen Shao, Rumei Li, Shizhe Wu, Shuai Liang et al. (82 additional authors not shown)",
      "Abstract": "We introduce LongCat-Flash, a 560-billion-parameter Mixture-of-Experts (MoE) language model designed for both computational efficiency and advanced agentic capabilities. Stemming from the need for scalable efficiency, LongCat-Flash adopts two novel designs: (a) Zero-computation Experts, which enables dynamic computational budget allocation and activates 18.6B-31.3B (27B on average) per token depending on contextual demands, optimizing resource usage. (b) Shortcut-connected MoE, which enlarges the computation-communication overlap window, demonstrating notable gains in inference efficiency and throughput compared to models of a comparable scale. We develop a comprehensive scaling framework for large models that combines hyperparameter transfer, model-growth initialization, a multi-pronged stability suite, and deterministic computation to achieve stable and reproducible training. Notably, leveraging the synergy among scalable architectural design and infrastructure efforts, we complete model training on more than 20 trillion tokens within 30 days, while achieving over 100 tokens per second (TPS) for inference at a cost of $0.70 per million output tokens. To cultivate LongCat-Flash towards agentic intelligence, we conduct a large-scale pre-training on optimized mixtures, followed by targeted mid- and post-training on reasoning, code, and instructions, with further augmentation from synthetic data and tool use tasks. Comprehensive evaluations demonstrate that, as a non-thinking foundation model, LongCat-Flash delivers highly competitive performance among other leading models, with exceptional strengths in agentic tasks. The model checkpoint of LongCat-Flash is open-sourced to foster community research.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Table 3\n\"LongCat-Flash Base model achieves performance on par with state-of-the-art base models despite its compact active/total parameter size. Although Llama-4-Maverick has fewer activated and total parameters, LongCat-Flash Base surpasses both on nearly all benchmarks.\"\n\n\"It achieves the\nhighest score of 89.65 on IFEval, outperforming all other models and demonstrating superior reliability in adhering to\ncomplex and nuanced directives. Furthermore, it secures the best score on COLLIE (57.10) and Meeseeks-zh (43.03),\"\n\n\"LongCat-Flash demonstrates a clear advantage in using agentic tool use domain, notably outperforming other models on \u03c4\n2\n-Bench even when compared to models with more parameters.\"",
      "Epochs": "",
      "Training time (hours)": "720.0",
      "Training time notes": "\"completing training within 30 days\" (=720 hours)",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "MIT license\nhttps://huggingface.co/meituan-longcat/LongCat-Flash-Chat",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Gemini 2.5 Flash Image (Nano Banana)",
      "Organization": "Google",
      "Publication date": "2025-08-26",
      "Domain": "Image generation",
      "Task": "Image generation,Text-to-image,Image-to-image",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "Likely very widely used, qualitatively SOTA in image generation and editing as of late 2025",
      "Confidence": "Unknown",
      "Link": "https://gemini.google/overview/image-generation/\n\nhttps://aistudio.google.com/models/gemini-2-5-flash-image",
      "Reference": "Nano Banana\nImage editing in Gemini just got a major upgrade",
      "Citations": "",
      "Authors": "",
      "Abstract": "Text-to-Image: Generate high-quality images from simple or complex text descriptions.\nImage + Text-to-Image (Editing): Provide an image and use text prompts to add, remove, or modify elements, change the style, or adjust the color grading.\nMulti-Image to Image (Composition & Style Transfer): Use multiple input images to compose a new scene or transfer the style from one image to another.\nIterative Refinement: Engage in a conversation to progressively refine your image over multiple turns, making small adjustments until it's perfect.\nHigh-Fidelity Text Rendering: Accurately generate images that contain legible and well-placed text, ideal for logos, diagrams, and posters.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Discretionary",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "Gemini 2.5 Flash (Apr 2025)",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GPT-5 nano",
      "Organization": "OpenAI",
      "Publication date": "2025-08-07",
      "Domain": "Multimodal,Language,Vision",
      "Task": "",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://cdn.openai.com/gpt-5-system-card.pdf",
      "Reference": "GPT-5 System Card",
      "Citations": "",
      "Authors": "",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Significant use",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GPT-5 mini",
      "Organization": "OpenAI",
      "Publication date": "2025-08-07",
      "Domain": "Multimodal,Language,Vision",
      "Task": "",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://cdn.openai.com/gpt-5-system-card.pdf",
      "Reference": "GPT-5 System Card",
      "Citations": "",
      "Authors": "",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Significant use",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GPT-5",
      "Organization": "OpenAI",
      "Publication date": "2025-08-07",
      "Domain": "Multimodal,Language,Vision",
      "Task": "",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "6.6e+25",
      "Training compute notes": "Likely around 6e25 [CI: 2e25 to 2e26] FLOP. See document below for details\n\nhttps://docs.google.com/document/d/1V2jIk365LnhH4WDoCw5dYJjZr1Htw8IHaK1noMf5Y48/edit?tab=t.z871imftkus",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Speculative",
      "Link": "https://cdn.openai.com/gpt-5-system-card.pdf\nhttps://openai.com/index/introducing-gpt-5/ ",
      "Reference": "GPT-5 System Card",
      "Citations": "",
      "Authors": "",
      "Abstract": "\"We are introducing GPT\u20115, our best AI system yet. GPT\u20115 is a significant leap in intelligence over all our previous models, featuring state-of-the-art performance across coding, math, writing, health, visual perception, and more. It is a unified system that knows when to respond quickly and when to think longer to provide expert-level responses. GPT\u20115 is available to all users, with Plus subscribers getting more usage, and Pro subscribers getting access to GPT\u20115 pro, a version with extended reasoning for even more comprehensive and accurate answers.\"",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Significant use,Historical significance",
      "Notability criteria notes": "Flagship OpenAI model starting August 2025, with presumably hundreds of millions of active users",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "gpt-oss-120b",
      "Organization": "OpenAI",
      "Publication date": "2025-08-05",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering",
      "Parameters": "116830000000.0",
      "Parameters notes": "Total parameters: 116.83B",
      "Training compute (FLOP)": "4.94e+24",
      "Training compute notes": "\"The training run for gpt-oss-120b required 2.1 million H100-hours to complete\"\n(2.1e6 hours)*(1,979 H100 FLOP/s)*(30% utilization)*(60*60) = 4.49e24\nThey also do post training similar to o3, which we assume adds at least 10% as much compute, so we multiply this estimate by 1.1 to get 4.94e24\n\n",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "(pretraining FLOPs)/(6*5.1B active parameters)",
      "Confidence": "Confident",
      "Link": "https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf",
      "Reference": "gpt-oss-120b & gpt-oss-20b Model Card",
      "Citations": "",
      "Authors": "",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Discretionary",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA H100 SXM5 80GB",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "gpt-oss-20b",
      "Organization": "OpenAI",
      "Publication date": "2025-08-05",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering",
      "Parameters": "20910000000.0",
      "Parameters notes": "Total parameters: 20.91B",
      "Training compute (FLOP)": "5.49e+23",
      "Training compute notes": "\"The training run for gpt-oss-120b required 2.1\nmillion H100-hours to complete, with gpt-oss-20b needing almost 10x fewer\"\nassuming \"almost 10x fewer\" means ~9x fewer: 4.94e24/9 = 5.49e23",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "(pretraining FLOPs)/(6*3.6B active parameters)",
      "Confidence": "Confident",
      "Link": "https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf",
      "Reference": "gpt-oss-120b & gpt-oss-20b Model Card",
      "Citations": "",
      "Authors": "",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Discretionary",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA H100 SXM5 80GB",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GLM-4.5",
      "Organization": "Z.ai (Zhipu AI),Tsinghua University",
      "Publication date": "2025-08-05",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering,Code generation,Quantitative reasoning",
      "Parameters": "355000000000.0",
      "Parameters notes": "355 billion total parameters with 32 billion active parameters",
      "Training compute (FLOP)": "4.42e+24",
      "Training compute notes": "(6 FLOP/token/parameter) * (23T tokens) * (32B active parameters) = 4.42e24 FLOP",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "23100000000000",
      "Dataset size notes": "23T tokens",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2508.06471",
      "Reference": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models",
      "Citations": "",
      "Authors": "Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, Kedong Wang, Lucen\nZhong, Mingdao Liu, Rui Lu, Shulin Cao, Xiaohan Zhang, Xuancheng Huang, Yao Wei, Yean Cheng,\nYifan An, Yilin Niu, Yuanhao Wen, Yushi Bai, Zhengxiao Du, Zihan Wang (\u6c6a\u5b50\u6db5), Zilin Zhu",
      "Abstract": "We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes.\nThrough multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on\nTAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer parameters than several competitors, GLM-4.5 ranks 3rd overall among all evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance research in reasoning and agentic AI systems. Code, models, and more information are available at https://github.com/zai-org/GLM-4.5.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "China,China",
      "Notability criteria": "Discretionary",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "64000000.0",
      "Batch size notes": "\"We used a batch size warmup strategy, where the batch size was gradually increased from 16M tokens to 64M tokens in the training of the first 500B token\"",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT license\nhttps://huggingface.co/zai-org/GLM-4.5\n\nApache 2.0 (Inference code)\nhttps://github.com/zai-org/GLM-4.5?tab=readme-ov-file",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Claude Opus 4.1",
      "Organization": "Anthropic",
      "Publication date": "2025-08-05",
      "Domain": "Language,Multimodal,Vision",
      "Task": "Language modeling/generation,Question answering,System control,Code generation,Search,Quantitative reasoning,Mathematical reasoning,Visual question answering",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://www.anthropic.com/news/claude-opus-4-1",
      "Reference": "Claude Opus 4.1",
      "Citations": "",
      "Authors": "",
      "Abstract": "Today we're releasing Claude Opus 4.1, an upgrade to Claude Opus 4 on agentic tasks, real-world coding, and reasoning. We plan to release substantially larger improvements to our models in the coming weeks.\n\nOpus 4.1 is now available to paid Claude users and in Claude Code. It's also on our API, Amazon Bedrock, and Google Cloud's Vertex AI. Pricing is the same as Opus 4.\n\nOpus 4.1 advances our state-of-the-art coding performance to 74.5% on SWE-bench Verified. It also improves Claude\u2019s in-depth research and data analysis skills, especially around detail tracking and agentic search.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Unreleased",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Qwen Image",
      "Organization": "Alibaba",
      "Publication date": "2025-08-04",
      "Domain": "Image generation,Vision",
      "Task": "Image generation,Text-to-image,Image-to-image",
      "Parameters": "27000000000.0",
      "Parameters notes": "Table 1",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/Qwen_Image.pdf",
      "Reference": "Qwen-Image Technical Report",
      "Citations": "",
      "Authors": "Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Shengming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang",
      "Abstract": "We present Qwen-Image, an image generation foundation model in the Qwen series that achieves significant advances in complex text rendering and precise image editing. To address the challenges of complex text rendering, we design a comprehensive data pipeline that includes large-scale data collection, filtering, annotation, synthesis, and balancing. Moreover, we adopt a progressive training strategy that starts with nontext-to-text rendering, evolves from simple to complex textual inputs, and gradually scales up to paragraph-level descriptions. This curriculum learning approach substantially enhances the model\u2019s native text rendering capabilities. As a result, Qwen-Image not only performs exceptionally well in alphabetic languages such as English, but also achieves remarkable progress on more challenging logographic languages like Chinese. To enhance image editing consistency, we introduce an improved multi-task training paradigm that incorporates not only traditional text-to-image (T2I) and text-image-toimage (TI2I) tasks but also image-to-image (I2I) reconstruction, effectively aligning the latent representations between Qwen2.5-VL and MMDiT. Furthermore, we separately feed the original image into Qwen2.5-VL and the VAE encoder to obtain semantic and reconstructive representations, respectively. This dual-encoding mechanism enables the editing module to strike a balance between  preserving semantic consistency and maintaining visual fidelity. We present a comprehensive evaluation of Qwen-Image across multiple public benchmarks, including GenEval, DPG, and OneIG-Bench for general image generation, as well as GEdit, ImgEdit, and GSO for image editing. QwenImage achieves state-of-the-art performance, demonstrating its strong capabilities in both image generation and editing. Furthermore, results on LongText-Bench, ChineseWord, and CVTG-2K show that it excels in text rendering\u2014particularly in Chinese text generation\u2014outperforming existing state-of-the-art models by a significant margin. This highlights Qwen-Image\u2019s unique position as a leading image generation model that combines broad general capability with exceptional text rendering precision.\n",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Tables 3-12",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "Qwen2.5-VL-7B ",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache 2.0\nhttps://huggingface.co/Qwen/Qwen-Image\nhttps://github.com/QwenLM/Qwen-Image",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Hierarchical Reasoning Model (HPM)",
      "Organization": "Sapient Intelligence",
      "Publication date": "2025-08-04",
      "Domain": "Language,Vision,Multimodal",
      "Task": "Visual puzzles",
      "Parameters": "27000000.0",
      "Parameters notes": "27M",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "ARC-AGI",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "1000 training samples",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2506.21734",
      "Reference": "Hierarchical Reasoning Model",
      "Citations": "",
      "Authors": "Guan Wang, Jin Li, Yuhao Sun, Xing Chen, Changling Liu, Yue Wu, Meng Lu, Sen Song, Yasin Abbasi Yadkori",
      "Abstract": "Reasoning, the process of devising and executing complex goal-oriented action sequences, remains a critical challenge in AI. Current large language models (LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from brittle task decomposition, extensive data requirements, and high latency. Inspired by the hierarchical and multi-timescale processing in the human brain, we propose the Hierarchical Reasoning Model (HRM), a novel recurrent architecture that attains significant computational depth while maintaining both training stability and efficiency. HRM executes sequential reasoning tasks in a single forward pass without explicit supervision of the intermediate process, through two interdependent recurrent modules: a high-level module responsible for slow, abstract planning, and a low-level module handling rapid, detailed computations. With only 27 million parameters, HRM achieves exceptional performance on complex reasoning tasks using only 1000 training samples. The model operates without pre-training or CoT data, yet achieves nearly perfect performance on challenging tasks including complex Sudoku puzzles and optimal path finding in large mazes. Furthermore, HRM outperforms much larger models with significantly longer context windows on the Abstraction and Reasoning Corpus (ARC), a key benchmark for measuring artificial general intelligence capabilities. These results underscore HRM's potential as a transformative advancement toward universal computation and general-purpose reasoning systems.",
      "Organization categorization": "",
      "Country (of organization)": "Singapore",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"With only about 1000 training examples, the HRM (~27M parameters) surpasses state-of-the-art\nCoT models on inductive benchmarks (ARC-AGI) and challenging symbolic tree-search puzzles (Sudoku-Extreme, Maze-Hard) where CoT models failed completely.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "",
      "Accessibility notes": "Apache 2.0\nhttps://github.com/sapientinc/HRM",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "MindLink-72B",
      "Organization": "Kunlun Inc.",
      "Publication date": "2025-08-01",
      "Domain": "Language",
      "Task": "Code generation,Mathematical reasoning",
      "Parameters": "72000000000.0",
      "Parameters notes": "72B",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://github.com/SkyworkAI/MindLink/blob/main/mindlink.pdf",
      "Reference": "MindLink: Adaptive Multi-step Planning for Large Language Models",
      "Citations": "",
      "Authors": "",
      "Abstract": "We introduce MindLink, a new family of large language models developed by Kunlun Inc. Built on Qwen, these models incorporate our latest advances in post-training techniques. MindLink demonstrates strong performance across various common benchmarks and is widely applicable in diverse AI scenarios. We welcome feedback to help us continuously optimize and improve our models.\n\n",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Table 4\nMindLink achieves superior performance overs SOTA methods in terms of MMLU-Pro and various mathematics and reasoning benchmarks - better performance than DeepSeek R1",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA A800 PCIe 80 GB",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "Qwen2.5-72B",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Gemini 2.5 Deep Think",
      "Organization": "Google,Google DeepMind",
      "Publication date": "2025-08-01",
      "Domain": "Language,Multimodal,Vision,Video,Audio,Mathematics",
      "Task": "Language modeling/generation,Mathematical reasoning,Code generation,Visual question answering,Question answering,Visual puzzles,Video description,Speech recognition (ASR),Speech-to-text",
      "Parameters": "",
      "Parameters notes": "The Gemini 2.5 models are sparse mixture-of-experts (MoE) transformers with native multimodal support for text, vision, and audio inputs.",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-2-5-Deep-Think-Model-Card.pdf\n\nhttps://storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_report.pdf",
      "Reference": "Gemini 2.5 Deep Think - Model Card",
      "Citations": "",
      "Authors": "",
      "Abstract": "To advance Gemini\u2019s capabilities towards solving hard reasoning problems, we developed a novel reasoning approach, called Deep Think, that naturally blends in parallel thinking techniques during response generation. Deep Think enables Gemini to creatively produce multiple hypotheses and carefully critique them before arriving at the final answer, achieving state-of-the-art performances in challenging benchmarks such as Olympiad math (USAMO 2025), competitive coding (LiveCodeBench), and multimodality (MMMU), see more details at (Doshi, 2025b). We announced Gemini 2.5 Deep Think at Google I/O and launched an experimental version to trusted testers and advanced users in June 2025.\n",
      "Organization categorization": "Industry,Industry",
      "Country (of organization)": "United States of America,United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "see p4 of the model card https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-2-5-Deep-Think-Model-Card.pdf",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Hosted access (no API)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Qwen3-235B-A22B-Thinking (Jul 2025)",
      "Organization": "Alibaba",
      "Publication date": "2025-07-25",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation,Translation",
      "Parameters": "235000000000.0",
      "Parameters notes": "235 billion total parameters and 22 billion activated parameters\n\nNumber of Layers: 94\nNumber of Attention Heads (GQA): 64 for Q and 4 for KV\nNumber of Experts: 128\nNumber of Activated Experts: 8\nContext Length: 32,768 natively and 131,072 tokens with YaRN.",
      "Training compute (FLOP)": "4.752e+24",
      "Training compute notes": "6 FLOP / parameter / token * 22*10^9 active parameters * 36000000000000 tokens = 4.752e+24 FLOP",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "36000000000000",
      "Dataset size notes": "36T",
      "Confidence": "Likely",
      "Link": "https://qwenlm.github.io/blog/qwen3/",
      "Reference": "Qwen3: Think Deeper, Act Faster",
      "Citations": "",
      "Authors": "",
      "Abstract": "Today, we are excited to announce the release of Qwen3, the latest addition to the Qwen family of large language models. Our flagship model, Qwen3-235B-A22B, achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro. Additionally, the small MoE model, Qwen3-30B-A3B, outcompetes QwQ-32B with 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of Qwen2.5-72B-Instruct.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "Discretionary",
      "Notability criteria notes": "Major Alibaba release",
      "Epochs": "1.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "not listed",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Apache 2.0\n\nhttps://huggingface.co/Qwen/Qwen3-235B-A22B",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Qwen3-235B-A22B-Instruct (Jul 2025)",
      "Organization": "Alibaba",
      "Publication date": "2025-07-25",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation,Translation",
      "Parameters": "235000000000.0",
      "Parameters notes": "235 billion total parameters and 22 billion activated parameters\n\nNumber of Layers: 94\nNumber of Attention Heads (GQA): 64 for Q and 4 for KV\nNumber of Experts: 128\nNumber of Activated Experts: 8\nContext Length: 32,768 natively and 131,072 tokens with YaRN.",
      "Training compute (FLOP)": "4.752e+24",
      "Training compute notes": "6 FLOP / parameter / token * 22*10^9 active parameters * 36000000000000 tokens = 4.752e+24 FLOP",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "36000000000000",
      "Dataset size notes": "36T",
      "Confidence": "Likely",
      "Link": "https://qwenlm.github.io/blog/qwen3/",
      "Reference": "Qwen3: Think Deeper, Act Faster",
      "Citations": "",
      "Authors": "",
      "Abstract": "Today, we are excited to announce the release of Qwen3, the latest addition to the Qwen family of large language models. Our flagship model, Qwen3-235B-A22B, achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro. Additionally, the small MoE model, Qwen3-30B-A3B, outcompetes QwQ-32B with 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of Qwen2.5-72B-Instruct.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "Discretionary",
      "Notability criteria notes": "Major Alibaba release",
      "Epochs": "1.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "not listed",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Apache 2.0\n\nhttps://huggingface.co/Qwen/Qwen3-235B-A22B",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Qwen3-Coder-480B-A35B",
      "Organization": "Alibaba",
      "Publication date": "2025-07-22",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering,Code generation,System control",
      "Parameters": "480000000000.0",
      "Parameters notes": "\"a 480B-parameter Mixture-of-Experts model with 35B active parameters which supports the context length of 256K tokens natively and 1M tokens with extrapolation methods\"",
      "Training compute (FLOP)": "1.575e+24",
      "Training compute notes": "6 FLOP / parameter / token * 35 * 10^9 active parameters * 7.5 * 10^12 tokens = 1.575e+24 FLOP`",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "7500000000000",
      "Dataset size notes": "\"Scaling Tokens: 7.5T tokens \"",
      "Confidence": "Confident",
      "Link": "https://qwenlm.github.io/blog/qwen3-coder/",
      "Reference": "Qwen3-Coder: Agentic Coding in the World",
      "Citations": "",
      "Authors": "",
      "Abstract": "Today, we're announcing Qwen3-Coder, our most agentic code model to date. Qwen3-Coder is available in multiple sizes, but we're excited to introduce its most powerful variant first: Qwen3-Coder-480B-A35B-Instruct. featuring the following key enhancements:\n\nSignificant Performance among open models on Agentic Coding, Agentic Browser-Use, and other foundational coding tasks, achieving results comparable to Claude Sonnet.\nLong-context Capabilities with native support for 256K tokens, extendable up to 1M tokens using Yarn, optimized for repository-scale understanding.\nAgentic Coding supporting for most platform such as Qwen Code, CLINE, featuring a specially designed function call format.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "Discretionary",
      "Notability criteria notes": "Major Alibaba release",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "not listed",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Apache 2.0\nhttps://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "EXAONE 4.0 (32B)",
      "Organization": "LG AI Research",
      "Publication date": "2025-07-15",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering,Code generation,Quantitative reasoning,Translation",
      "Parameters": "32000000000.0",
      "Parameters notes": "32B",
      "Training compute (FLOP)": "2.69000000000001e+24",
      "Training compute notes": "Reported in Table 2.\n\nfrom communication with the authors:\n\"EXAONE 4.0 32B: NVIDIA H200 GPUs x 512 EA for 19 weeks (FP8 mode training)\"",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "14000000000000",
      "Dataset size notes": "max sequence length 131,072 (Table 1)\n\nsize of pretraining data: 14T (Table 2)",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2507.11407",
      "Reference": "EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes",
      "Citations": "",
      "Authors": "LG AI Research: Kyunghoon Bae, Eunbi Choi, Kibong Choi, Stanley Jungkyu Choi, Yemuk Choi, Kyubeen Han, Seokhee Hong, Junwon Hwang, Taewan Hwang, Joonwon Jang, Hyojin Jeon, Kijeong Jeon, Gerrard Jeongwon Jo, Hyunjik Jo, Jiyeon Jung, Euisoon Kim, Hyosang Kim, Jihoon Kim, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Yongil Kim, Youchul Kim, Edward Hwayoung Lee, Gwangho Lee, Haeju Lee, Honglak Lee, Jinsik Lee, Kyungmin Lee, Sangha Park, Young Min Paik, Yongmin Park, Youngyong Park, Sanghyun Seo, Sihoon Yang, Heuiyeen Yeen, Sihyuk Yi, Hyeongu Yun",
      "Abstract": "This technical report introduces EXAONE 4.0, which integrates a Non-reasoning mode and a Reasoning mode to achieve both the excellent usability of EXAONE 3.5 and the advanced reasoning abilities of EXAONE Deep. To pave the way for the agentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool use, and its multilingual capabilities are extended to support Spanish in addition to English and Korean. The EXAONE 4.0 model series consists of two sizes: a mid-size 32B model optimized for high performance, and a small-size 1.2B model designed for on-device applications. The EXAONE 4.0 demonstrates superior performance compared to open-weight models in its class and remains competitive even against frontier-class models. The models are publicly available for research purposes and can be easily downloaded via this https URL.",
      "Organization categorization": "Industry",
      "Country (of organization)": "Korea (Republic of)",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "2e24 FLOPs of training compute",
      "Epochs": "",
      "Training time (hours)": "3192.0",
      "Training time notes": "from communication with the authors:\n\"EXAONE 4.0 32B: NVIDIA H200 GPUs x 512 EA for 19 weeks (FP8 mode training)\"\n\n19 weeks = 3192 hours",
      "Training hardware": "NVIDIA H200 SXM",
      "Hardware quantity": "512.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "701356.0100875632",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Exaone license (permits only academic, research, or educational usage) \nhttps://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B",
      "Numerical format": "FP8",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Gemini Embedding",
      "Organization": "Google DeepMind",
      "Publication date": "2025-07-14",
      "Domain": "Language",
      "Task": "Semantic embedding",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/2503.07891",
      "Reference": "Gemini Embedding: Generalizable Embeddings from Gemini",
      "Citations": "",
      "Authors": "Jinhyuk Lee, Feiyang Chen, Sahil Dua, Daniel Cer, Madhuri Shanbhogue, Iftekhar Naim, Gustavo Hern\u00e1ndez \u00c1brego, Zhe Li, Kaifeng Chen, Henrique Schechter Vera, Xiaoqi Ren, Shanfeng Zhang, Daniel Salz, Michael Boratko, Jay Han, Blair Chen, Shuo Huang, Vikram Rao, Paul Suganthan, Feng Han, Andreas Doumanoglou, Nithi Gupta, Fedor Moiseev, Cathy Yip, Aashi Jain, Simon Baumgartner, Shahrokh Shahi, Frank Palma Gomez, Sandeep Mariserla, Min Choi, Parashar Shah, Sonam Goenka, Ke Chen, Ye Xia, Koert Chen, Sai Meher Karthik Duddu, Yichang Chen, Trevor Walker, Wenlei Zhou, Rakesh Ghiya, Zach Gleicher, Karan Gill, Zhe Dong, Mojtaba Seyedhosseini, Yunhsuan Sung, Raphael Hoffmann, Tom Duerig",
      "Abstract": "In this report, we introduce Gemini Embedding, a state-of-the-art embedding model leveraging the power of Gemini, Google's most capable large language model. Capitalizing on Gemini's inherent multilingual and code understanding capabilities, Gemini Embedding produces highly generalizable embeddings for text spanning numerous languages and textual modalities. The representations generated by Gemini Embedding can be precomputed and applied to a variety of downstream tasks including classification, similarity, clustering, ranking, and retrieval. Evaluated on the Massive Multilingual Text Embedding Benchmark (MMTEB), which includes over one hundred tasks across 250+ languages, Gemini Embedding substantially outperforms prior state-of-the-art models, demonstrating considerable improvements in embedding quality. Achieving state-of-the-art performance across MMTEB's multilingual, English, and code benchmarks, our unified model demonstrates strong capabilities across a broad selection of tasks and surpasses specialized domain-specific models.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Table 1\n\"This embedding model has consistently held a top spot on the Massive Text Embedding Benchmark (MTEB) Multilingual leaderboard since the experimental launch in March.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "https://developers.googleblog.com/en/gemini-embedding-available-gemini-api/",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Kimi K2",
      "Organization": "Moonshot",
      "Publication date": "2025-07-11",
      "Domain": "Language",
      "Task": "Language modeling/generation,Code generation,Question answering,Quantitative reasoning,Search,Table tasks",
      "Parameters": "1000000000000.0",
      "Parameters notes": "MoE with 1T total parameters and 32B parameters active per forward pass",
      "Training compute (FLOP)": "2.976e+24",
      "Training compute notes": "6 FLOP / parameter / token * 32 * 10^9 activated parameters * 15.5 * 10^12 tokens = 2.976e+24 FLOP",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "15500000000000",
      "Dataset size notes": "\"Kimi K2 was pre-trained on 15.5T tokens\"",
      "Confidence": "Confident",
      "Link": "https://moonshotai.github.io/Kimi-K2/",
      "Reference": "Kimi K2: Open Agentic Intelligence",
      "Citations": "",
      "Authors": "",
      "Abstract": "Kimi K2 is our latest Mixture-of-Experts model with 32 billion activated parameters and 1 trillion total parameters. It achieves state-of-the-art performance in frontier knowledge, math, and coding among non-thinking models. But it goes further \u2014 meticulously optimized for agentic tasks, Kimi K2 does not just answer; it acts.\nAnd now, it is within your reach. Today, we are open-sourcing:\nKimi-K2-Base: The foundation model, a strong start for researchers and builders who want full control for fine-tuning and custom solutions.\nKimi-K2-Instruct: The post-trained model best for drop-in, general-purpose chat and agentic experiences. It is a reflex-grade model without long thinking.\nWith Kimi K2, advanced agentic intelligence is more open and accessible than ever. We can't wait to see what you build.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA H800 SXM5",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "67000000.0",
      "Batch size notes": "\"global batch size was held at 67M token\"",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open access (restricted use)",
      "Accessibility notes": "Modified MIT license (separare license is required for entities with 100M+ MAU or $20M+ in annual revenue) for weights and inference code:\nhttps://huggingface.co/moonshotai/Kimi-K2-Instruct\nhttps://github.com/MoonshotAI/Kimi-K2?tab=readme-ov-file#4-deployment\n\nAPI:\nhttps://platform.moonshot.cn/docs/pricing/chat#%E8%AE%A1%E8%B4%B9%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Grok 4 Heavy",
      "Organization": "xAI",
      "Publication date": "2025-07-10",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://x.ai/news/grok-4",
      "Reference": "Grok 4 Heavy - the most powerful version of Grok 4",
      "Citations": "",
      "Authors": "",
      "Abstract": "We have made further progress on parallel test-time compute, which allows Grok to consider multiple hypotheses at once. We call this model Grok 4 Heavy, and it sets a new standard for performance and reliability. Grok 4 Heavy saturates most academic benchmarks and is the first model to score 50% on Humanity's Last Exam, a benchmark \"designed to be the final closed-ended academic benchmark of its kind.\"",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Grok 4 Heavy leads USAMO'25 with 61.9%, and is the first to score 50.7% on Humanity's Last Exam (text-only subset), demonstrating unparalleled capabilities in complex reasoning through scaled reinforcement learning and native tool use.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Grok 4",
      "Organization": "xAI",
      "Publication date": "2025-07-09",
      "Domain": "Language,Multimodal,Vision",
      "Task": "Language modeling/generation,Question answering,Search,Visual question answering,Character recognition (OCR),Image captioning,Quantitative reasoning",
      "Parameters": "3000000000000.0",
      "Parameters notes": "Rumored to be 2.4T params (https://x.com/kalomaze/status/1942996555088134592)",
      "Training compute (FLOP)": "5.0000000000001e+26",
      "Training compute notes": "We think that RL relative to pre-compute is between our estimate for o3 (10% of pre-training) and the 100% implied by this slide in the launch ( https://archive.is/f0vJU ). Assuming the same pre-training as Grok 3 (also implied by that slide, and much more consistent) and that Grok 3 used a tenth as much RL, we get:\n\n2 * (grok3/1.1) in the high case (rl is 10% of grok 3, so grok3/1.1 is grok3 precompute, and in this case twice that is grok 4)\n1.1 * (grok3/1.01) in the low case\nThe geometric mean is (rounded to one sig fig): 5e26\n",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Speculative",
      "Link": "https://x.ai/news/grok-4",
      "Reference": "Grok 4",
      "Citations": "",
      "Authors": "",
      "Abstract": "Grok 4 is the most intelligent model in the world. It includes native tool use and real-time search integration, and is available now to SuperGrok and Premium+ subscribers, as well as through the xAI API. We are also introducing a new SuperGrok Heavy tier with access to Grok 4 Heavy - the most powerful version of Grok 4.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "200000.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "387842678.08636147",
      "Compute cost notes": "See https://colab.research.google.com/drive/1nAl9CJi6VFLYZszzVEOLIlkxx1NMc_Lv?usp=sharing\n\nand \nhttps://docs.google.com/document/d/1gF9VLRaQx__TN2pdgs-P4K_UEW5PhMQ2tGwJPwPcV6A/edit?tab=t.0",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": "10717356897.243427"
    },
    {
      "Model": "EXAONE Path 2.0",
      "Organization": "LG AI Research",
      "Publication date": "2025-07-09",
      "Domain": "Vision,Medicine",
      "Task": "Cancer diagnosis,Image classification,Medical diagnosis",
      "Parameters": "175000000.0",
      "Parameters notes": "~175M parameters based on Figure 1",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "144450",
      "Dataset size notes": "\"EXAONE Path 2.0 is trained on 37,195 Formalin-Fixed, Paraffin-Embedded (FFPE) Hematoxylin and Eosin (H&E) stained WSIs. These WSIs generate 144,450 image-label pairs across 16 training tasks, with each WSI contributing multiple labels corresponding to different prediction objectives including cancer subtyping, tissue classification, and biomarker prediction.\"\n\n\"gigapixel scale\"\n\n\"In the first curriculum stage, we apply 256\u00d7256 DINO loss to the first-stage ViT and 1024\u00d71024 DINO loss to the second-stage ViT\"\n\nunknown number of epochs or training steps",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2507.06639",
      "Reference": "EXAONE Path 2.0: Pathology Foundation Model with End-to-End Supervision",
      "Citations": "",
      "Authors": "Myungjang Pyeon, Janghyeon Lee, Minsoo Lee, Juseung Yun, Hwanil Choi, Jonghyun Kim, Jiwon Kim, Yi Hu, Jongseong Jang, Soonyoung Lee",
      "Abstract": "In digital pathology, whole-slide images (WSIs) are often difficult to handle due to their gigapixel scale, so most approaches train patch encoders via self-supervised learning (SSL) and then aggregate the patch-level embeddings via multiple instance learning (MIL) or slide encoders for downstream tasks. However, patch-level SSL may overlook complex domain-specific features that are essential for biomarker prediction, such as mutation status and molecular characteristics, as SSL methods rely only on basic augmentations selected for natural image domains on small patch-level area. Moreover, SSL methods remain less data efficient than fully supervised approaches, requiring extensive computational resources and datasets to achieve competitive performance. To address these limitations, we present EXAONE Path 2.0, a pathology foundation model that learns patch-level representations under direct slide-level supervision. Using only 37k WSIs for training, EXAONE Path 2.0 achieves state-of-the-art average performance across 10 biomarker prediction tasks, demonstrating remarkable data efficiency.",
      "Organization categorization": "Industry",
      "Country (of organization)": "Korea (Republic of)",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Author correspondence:\n\"Our technical report for the EXAONE Path 2.0 model presents results across ten benchmark datasets.\nAmong them, the benchmarks ending in ***-USA1 and ***-KOR are private datasets collected from hospitals,\nwhile those ending in ***-CPTAC are based on publicly available data from the following paper:\n\nLi, Yize, Lazar, Alexander J., et al., \"Proteogenomic data and resources for pan-cancer analysis\", Cancer Cell, Volume 41, Issue 8, 1397\u20131406\nhttps://www.cell.com/cancer-cell/fulltext/S1535-6108(23)00219-2\n\nAccording to Google Scholar, this paper has been cited 142 times. (link)\nThe benchmark table below, which includes only data derived from this publication, shows the average performance comparison across competing models.\nAs you can see, EXAONE Path 2.0 achieves the highest performance in this evaluation.",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "EXAONEPath AI Model License Agreement 1.0 - NC\nhttps://huggingface.co/LGAI-EXAONE/EXAONE-Path-2.0",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "FGN",
      "Organization": "Google DeepMind",
      "Publication date": "2025-06-12",
      "Domain": "Earth science",
      "Task": "Weather forecasting",
      "Parameters": "720000000.0",
      "Parameters notes": "\"In this work we train \ud835\udc3d = 4 models. To ensemble their predictions, we generate an equal number of ensemble member trajectories from each model\"\n\"FGN is a larger model, with \u223c180m parameters per model seed\"\n\nThey train a 4 model ensemble with 180M parameters per model. Total parameters: 4*180=720M",
      "Training compute (FLOP)": "9.618950880000001e+21",
      "Training compute notes": "TPU v5 FLOPs: 459000000000000\nTPU v6 FLOPs: 918000000000000\n\nThey train for 490 TPU days, with an unspecified mix of v5 and v6. Assuming same proportion of both TPU and a utilization of 0.33. \n\nMean TPU FLOPs: 688500000000000\n\nCompute: 490*24*60*60*688500000000000*0.33=9618950880000001000000",
      "Training dataset": "ERA5",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2506.10772v1",
      "Reference": "Skillful joint probabilistic weather forecasting from marginals",
      "Citations": "",
      "Authors": "Ferran Alet, Ilan Price, Andrew El-Kadi, Dominic Masters, Stratis Markou, Tom R. Andersson, Jacklynn Stott, Remi Lam, Matthew Willson, Alvaro Sanchez-Gonzalez, Peter Battaglia",
      "Abstract": "Machine learning (ML)-based weather models have rapidly risen to prominence due to their greater accuracy and speed than traditional forecasts based on numerical weather prediction (NWP), recently outperforming traditional ensembles in global probabilistic weather forecasting. This paper presents FGN, a simple, scalable and flexible modeling approach which significantly outperforms the current state-of-the-art models. FGN generates ensembles via learned model-perturbations with an ensemble of appropriately constrained models. It is trained directly to minimize the continuous rank probability score (CRPS) of per-location forecasts. It produces state-of-the-art ensemble forecasts as measured by a range of deterministic and probabilistic metrics, makes skillful ensemble tropical cyclone track predictions, and captures joint spatial structure despite being trained only on marginals.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"This paper presents FGN, a simple, scalable and flexible modeling approach which significantly outperforms the current state-of-the-art models\"\n\n\"FGN achieves state-of-the-art cyclone track prediction\"\n\n\"Our results show FGN offers substantial improvements over previous ML-based probabilistic weather models, and sets a new state-of-the-art in ensemble forecasting. It outperforms GenCast with both the skill of its marginal forecasts, including on extreme weather, as well as its skill in forecasting quantities dependent on the dependency structure of the joint forecast distribution, notably including tropical cyclone tracks.\"",
      "Epochs": "",
      "Training time (hours)": "72.0",
      "Training time notes": "\"Training FGN takes approximately 3 wall clock days, using a combined total of 490 TPUv5p and TPUv6e days of compute. \"\n\nIt's unclear if training a single model or the whole ensemble takes 3 days.",
      "Training hardware": "Google TPU v5p,Google TPU v6e Trillium",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Seed-1.6-Thinking",
      "Organization": "ByteDance",
      "Publication date": "2025-06-11",
      "Domain": "Multimodal,Language,Vision",
      "Task": "Language modeling/generation,Vision-language generation",
      "Parameters": "230000000000.0",
      "Parameters notes": "Model has 230B parameters total, with 23B active parameters (sparse MoE setting).",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://seed.bytedance.com/en/blog/introduction-to-techniques-used-in-seed1-6",
      "Reference": "Introduction to Techniques Used in Seed1.6\n",
      "Citations": "",
      "Authors": "",
      "Abstract": "Seed1.6 is the latest general-purpose model series unveiled by the ByteDance Seed team. It incorporates multimodal capabilities, supporting adaptive deep thinking, multimodal understanding, GUI-based interaction, and deep reasoning with a 256K context window. Seed1.6 is now available through the open API of Volcano Engine. You can try it out via the links provided at the end of this article.\n\n\nIn Seed1.6, we introduced Adaptive Chain-of-Thought (AdaCoT), a novel technique for initiating thinking processes adaptively based on question difficulty. This adaptive approach enables a better trade-off between model effectiveness and reasoning performance.\n\n\nSeed1.6 models demonstrated competitive performance across diverse benchmarks. They matched or even outperformed Seed1.5-VL in multiple visual tasks, while also achieving high scores in generalization tests such as college entrance exams in China and abroad.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Best performing model on CompassBench LLM Leaderboard (OpenCompass) in terms of Language https://rank.opencompass.org.cn/leaderboard-llm/?m=25-07",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Qwen3 Embedding",
      "Organization": "Alibaba",
      "Publication date": "2025-06-05",
      "Domain": "Language",
      "Task": "Semantic embedding",
      "Parameters": "8000000000.0",
      "Parameters notes": "8B",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "\"Finally, we create a total of approximately 150 million pairs of multi-task weak supervision training data. <..> Ultimately, approximately 12 million high-quality supervised training data pairs are selected for further training\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2506.05176v1",
      "Reference": "Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models",
      "Citations": "",
      "Authors": "Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, Fei Huang, Jingren Zhou",
      "Abstract": "In this work, we introduce the Qwen3 Embedding series, a significant advancement over its predecessor, the GTE-Qwen series, in text embedding and reranking capabilities, built upon the Qwen3 foundation models. Leveraging the Qwen3 LLMs' robust capabilities in multilingual text understanding and generation, our innovative multi-stage training pipeline combines large-scale unsupervised pre-training with supervised fine-tuning on high-quality datasets. Effective model merging strategies further ensure the robustness and adaptability of the Qwen3 Embedding series. During the training process, the Qwen3 LLMs serve not only as backbone models but also play a crucial role in synthesizing high-quality, rich, and diverse training data across multiple domains and languages, thus enhancing the training pipeline. The Qwen3 Embedding series offers a spectrum of model sizes (0.6B, 4B, 8B) for both embedding and reranking tasks, addressing diverse deployment scenarios where users can optimize for either efficiency or effectiveness. Empirical evaluations demonstrate that the Qwen3 Embedding series achieves state-of-the-art results across diverse benchmarks. Notably, it excels on the multilingual evaluation benchmark MTEB for text embedding, as well as in various retrieval tasks, including code retrieval, cross-lingual retrieval and multilingual retrieval. To facilitate reproducibility and promote community-driven research and development, the Qwen3 Embedding models are publicly available under the Apache 2.0 license.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"For example, the flagship model Qwen3-8B-Embedding attains a score of 70.58 on the MTEB Multilingual benchmark (Enevoldsen et al., 2025) and 80.68 on the MTEB Code benchmark (Enevoldsen et al., 2025), surpassing the previous state-of-the-art proprietary\nembedding model, Gemini-Embedding (Lee et al., 2025b).\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "Qwen3-8B",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache 2.0\nhttps://huggingface.co/Qwen/Qwen3-Embedding-8B\n\nhttps://github.com/QwenLM/Qwen3-Embedding",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Gemini 2.5 Pro (Jun 2025)",
      "Organization": "Google DeepMind",
      "Publication date": "2025-06-05",
      "Domain": "Language,Vision,Video,Multimodal,Speech",
      "Task": "Language modeling/generation,Question answering,Code generation,Quantitative reasoning,Visual question answering,Translation,Image captioning,Video description,Speech recognition (ASR)",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/#gemini-2-5-thinking",
      "Reference": "Gemini 2.5: Our most intelligent AI model",
      "Citations": "",
      "Authors": "",
      "Abstract": "Gemini 2.5 Pro Experimental is our most advanced model for complex tasks. It tops the LMArena leaderboard \u2014 which measures human preferences \u2014 by a significant margin, indicating a highly capable model equipped with high-quality style. 2.5 Pro also shows strong reasoning and code capabilities, leading on common coding, math and science benchmarks.\n\nGemini 2.5 Pro is available now in Google AI Studio and in the Gemini app for Gemini Advanced users, and will be coming to Vertex AI soon. We\u2019ll also introduce pricing in the coming weeks, enabling people to use 2.5 Pro with higher rate limits for scaled production use.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Gemini also continues to stand out for achieving the SoTA score on both the LOFT and MRCR long-context tasks at 128k context, and is the only one, amongst the models examined in the above table, to support context lengths of 1M+ tokens.\"\n\nTable 4",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Availability\t\nGoogle AI Studio\nGemini API\nGemini App",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DeepSeek-R1 (May 2025)",
      "Organization": "DeepSeek",
      "Publication date": "2025-05-28",
      "Domain": "Language",
      "Task": "Language modeling/generation,Code generation,Quantitative reasoning,Question answering",
      "Parameters": "671000000000.0",
      "Parameters notes": "671B total\n37B activated\nhttps://github.com/deepseek-ai/DeepSeek-R1/tree/main",
      "Training compute (FLOP)": "4.020010000000001e+24",
      "Training compute notes": "Estimates by Ege Erdil in Gradient Updates:\nhttps://epoch.ai/gradient-updates/what-went-into-training-deepseek-r1\n\"A dataset size of 14.8 trillion tokens is reasonable and in line with other models of this scale. Assuming that\u2019s valid, the pretraining of this model would have required 6 * (37 billion) * (14.8 trillion) = 3e24 FLOP. If we assume DeepSeek\u2019s training cluster consists of H800s with the PCIe form factor, then each should be capable of 1.5e15 FP8 per second, and the implied model FLOP utilization (MFU) of DeepSeek v3\u2019s 55 day training run ends up being around 23%.\"\n\n6 FLOP/token/param * 14.8T tokens * 37B active params = 3.29e24 FLOP (pretraining)\n1.2e23 FLOP (post-training)\n6.1e23 FLOP (fine-tuning)\n\nTotal compute: 3.29e24 + 1.2e23 + 6.1e23 = 4.02e24",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://api-docs.deepseek.com/news/news250120",
      "Reference": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
      "Citations": "",
      "Authors": "",
      "Abstract": "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.\nThrough RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "Training cost,SOTA improvement",
      "Notability criteria notes": "Best score on SuperCLUE Math6o in Jan 2025\nhttps://www.superclueai.com/",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "6770000.0",
      "Compute cost notes": "$5.576M 2024 USD (estimated training cost of Deepseek v3) + $1M (estimated training cost of RL) = $6.576M 2024 USD = $6.77M 2023 USD\n\nhttps://www.usinflationcalculator.com/\n\nhttps://epoch.ai/gradient-updates/what-went-into-training-deepseek-r1",
      "Training power draw (W)": "",
      "Base model": "DeepSeek-V3",
      "Finetune compute (FLOP)": "6.1e+23",
      "Finetune compute notes": "6.1e23 FLOP from these estimations: https://epoch.ai/gradient-updates/what-went-into-training-deepseek-r1",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "MIT licensed\nhttps://huggingface.co/deepseek-ai/DeepSeek-R1",
      "Numerical format": "FP8",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Claude Sonnet 4",
      "Organization": "Anthropic",
      "Publication date": "2025-05-22",
      "Domain": "Language,Multimodal,Vision",
      "Task": "Code generation,Language modeling/generation,Quantitative reasoning,Search,Visual question answering,Translation,Image captioning,Instruction interpretation,Mathematical reasoning,Visual puzzles,Code autocompletion,Chat,Character recognition (OCR),Language modeling,Language generation,Text autocompletion,Retrieval-augmented generation,System control",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://www.anthropic.com/claude/sonnet\n\nhttps://www.anthropic.com/news/claude-4",
      "Reference": "Hybrid reasoning model with superior intelligence for high-volume use cases, and 200K context window",
      "Citations": "",
      "Authors": "",
      "Abstract": "Claude Sonnet 4 can understand nuanced instructions and context, recognize and correct its own mistakes, and create sophisticated analysis and insights from complex data. Combined with superior coding, vision, and writing skills, you can use Claude Sonnet 4 for a variety of use cases.\n\nClaude Sonnet 4 can produce near-instant responses or extended, step-by-step thinking that is made visible to the user. API users also have fine-grained control over how long the model thinks for.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Claude Opus 4",
      "Organization": "Anthropic",
      "Publication date": "2025-05-22",
      "Domain": "Language,Multimodal,Vision",
      "Task": "Code generation,Language modeling/generation,Quantitative reasoning,Search,Visual question answering,Translation,Image captioning,Instruction interpretation,Mathematical reasoning,Visual puzzles,Code autocompletion,Chat,Character recognition (OCR),Language modeling,Language generation,Text autocompletion,Retrieval-augmented generation,System control",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://www.anthropic.com/claude/opus\n\nhttps://www.anthropic.com/news/claude-4",
      "Reference": "Hybrid reasoning model that pushes the frontier for coding and AI agents, featuring a 200K context window",
      "Citations": "",
      "Authors": "",
      "Abstract": "Claude Opus 4 is our most powerful model yet and the best coding model in the world, leading on SWE-bench (72.5%) and Terminal-bench (43.2%). It delivers sustained performance on long-running tasks that require focused effort and thousands of steps, with the ability to work continuously for several hours\u2014dramatically outperforming all Sonnet models and significantly expanding what AI agents can accomplish.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Veo 3",
      "Organization": "Google DeepMind",
      "Publication date": "2025-05-21",
      "Domain": "Video,Vision",
      "Task": "Video generation,Image-to-video,Text-to-video",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://deepmind.google/models/veo/",
      "Reference": "Our state-of-the-art video generation model",
      "Citations": "",
      "Authors": "Abhishek Sharma, Alina Kuznetsova, Ali Razavi, Aleksander Holynski, Alina Kuznetsova, Ankush Gupta, Austin Waters, Ben Poole, Daniel Tanis, Derek Gasaway, Dumitru Erhan, Enric Corona, Frank Belletti, Gabe Barth-Maron, Hakan Erdogan, Henna Nandwani, Hernan Moraldo, Ilya Figotin, Igor Saprykin, Jason Baldridge, Jeff Donahue, Jimmy Shi, Kurtis David, Mai Gimenez, Medhini Narasimhan, Miaosen Wang, Mingda Zhang, Mohammad Babaeizadeh, Mukul Bhutani, Nikhil Khadke, Nilpa Jha, Pieter-Jan Kindermans, Poorva Rane, Rachel Hornung, Ricky Wong, Ruben Villegas, Ruiqi Gao, Ryan Poplin, Salah Zaiem, Sayna Ebrahimi, Scott Wisdom, Shlomi Fruchter, Sophia Sanchez, Vikas Verma, Viral Carpenter, Xinchen Yan, Xinyu Wang, Yiwen Luo, Zhichao Yin, Zu Kim",
      "Abstract": "Re-designed for greater realism\nGreater realism and fidelity, including 4k output and Veo 3\u2019s real world physics and audio\n\nFollows prompts like never before\nImproved prompt adherence, meaning more accurate responses to your instructions.\n\nImproved creative control\nNew capabilities to achieve new levels of control, consistency, and creativity.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "https://deepmind.google/models/veo/evals/",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "https://cloud.google.com/vertex-ai/generative-ai/docs/models/veo/3-0-generate-preview",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Imagen 4",
      "Organization": "Google",
      "Publication date": "2025-05-20",
      "Domain": "Image generation",
      "Task": "Image generation,Text-to-image",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://storage.googleapis.com/deepmind-media/Model-Cards/Imagen-4-Model-Card.pdf",
      "Reference": "Imagen 4 Model Card",
      "Citations": "",
      "Authors": "Gabriel Barcik, Jakob Bauer, Dana Berman, Nicole Brichtova, Lluis Castrejon, Matan Cohen, Sander Dieleman, Yuqing Du, Praneet Dutta, Jess Gallegos, Yilin Gao, Evgeny Gladchenko, Susan Hao, Ruba Haroun, Ed Hirst, Tobenna Peter Igwe, Xuhui Jia, Siavash Khodadadeh, Pavel Khrushkov, Karol Langner, Rory Lawton, Yinxiao Li, Yandong Li, Shixin Luo, Michael Mathieu, So\u0148a Mokr\u00e1, A\u00e4ron van den Oord, Lily Pagan, Zarana Parekh, Noam Petrank, Jordi Pont-Tuset, Hang Qi, Deepak Ramachandran, Poorva Rane, Ali Razavi, Robert Riachi, Dirk Robinson, James Thornton, Felix Riedel, Evgeny Sluzhaev, Hansa Srinivasan, Srivatsan Srinivasan, Benigno Uria, Cristina Vasconcelos, Oliver Wang, Simon Wang, Austin Waters, Daniel Winter, Chris Wolff, Xin Yuan, Zhisheng Xiao, Keyang Xu, Andrew Xue, Katie Zhang, Yang Zhao",
      "Abstract": "Description: Imagen 4 is a latent diffusion model that generates high quality images from text prompts. Imagen 4 performs well in photorealistic composition seings and has improved spelling and typography, instruction following and richer colors, textures and details compared to previous Imagen models.\nInputs: The inputs consist of natural-language text strings (e.g. instructions for creating a synthetic image using a visual description) or image files.\nOutputs: Outputs are generated high quality images in response to text and image inputs. \nArchitecture: Imagen 4 utilises latent diffusion, which is the de facto standard approach for modern image and video models, achieving high quality performance in generative media applications.\n",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Significant use",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Seed1.5-VL",
      "Organization": "ByteDance",
      "Publication date": "2025-05-11",
      "Domain": "Vision,Language,Multimodal,Video",
      "Task": "Visual question answering,Video description,Language modeling/generation,Question answering,Character recognition (OCR)",
      "Parameters": "",
      "Parameters notes": "\"Seed1.5-VL is composed with a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B active parameters.\"",
      "Training compute (FLOP)": "1.388556e+24",
      "Training compute notes": "989000000000000 FLOP / GPU / sec [H800, bf16 assumed] * 1300000 GPU-hours [see training time notes] * 3600 sec / hour * 0.3 [assumed utilization] = 1.388556e+24 FLOP",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "3000000000000",
      "Dataset size notes": "\"more than 3T multimodal data tokens\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2505.07062",
      "Reference": "Seed1.5-VL Technical Report",
      "Citations": "",
      "Authors": "Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, Jingji Chen, Jingjia Huang, Kang Lei, Liping Yuan, Lishu Luo, Pengfei Liu, Qinghao Ye, Rui Qian, Shen Yan, Shixiong Zhao, Shuai Peng, Shuangye Li, Sihang Yuan, Sijin Wu, Tianheng Cheng, Weiwei Liu, Wenqian Wang, Xianhan Zeng, Xiao Liu, Xiaobo Qin, Xiaohan Ding, Xiaojun Xiao, Xiaoying Zhang, Xuanwei Zhang, Xuehan Xiong, Yanghua Peng, Yangrui Chen, Yanwei Li, Yanxu Hu, Yi Lin, Yiyuan Hu, Yiyuan Zhang, Youbin Wu, Yu Li, Yudong Liu, Yue Ling, Yujia Qin, Zanbo Wang, Zhiwu He, Aoxue Zhang, Bairen Yi, Bencheng Liao, Can Huang, Can Zhang, Chaorui Deng, Chaoyi Deng, Cheng Lin, Cheng Yuan, Chenggang Li, Chenhui Gou, Chenwei Lou, Chengzhi Wei, Chundian Liu, Chunyuan Li, Deyao Zhu, Donghong Zhong, Feng Li, Feng Zhang, Gang Wu, Guodong Li, Guohong Xiao, Haibin Lin, Haihua Yang, Haoming Wang, Heng Ji, Hongxiang Hao, Hui Shen, Huixia Li, Jiahao Li, Jialong Wu, Jianhua Zhu, Jianpeng Jiao, Jiashi Feng, Jiaze Chen, Jianhui Duan, Jihao Liu, Jin Zeng, Jingqun Tang, Jingyu Sun, Joya Chen, Jun Long, Junda Feng, Junfeng Zhan, Junjie Fang, Junting Lu, Kai Hua, Kai Liu, Kai Shen, Kaiyuan Zhang, Ke Shen et al. (97 additional authors not shown)",
      "Abstract": "We present Seed1.5-VL, a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning. Seed1.5-VL is composed with a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B active parameters. Despite its relatively compact architecture, it delivers strong performance across a wide spectrum of public VLM benchmarks and internal evaluation suites, achieving the state-of-the-art performance on 38 out of 60 public benchmarks. Moreover, in agent-centric tasks such as GUI control and gameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAI CUA and Claude 3.7. Beyond visual and video understanding, it also demonstrates strong reasoning abilities, making it particularly effective for multimodal reasoning challenges such as visual puzzles. We believe these capabilities will empower broader applications across diverse tasks. In this report, we mainly provide a comprehensive review of our experiences in building Seed1.5-VL across model design, data construction, and training at various stages, hoping that this report can inspire further research. Seed1.5-VL is now accessible at this https URL (Volcano Engine Model ID: doubao-1-5-thinking-vision-pro-250428)",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Table 6",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "\" The pretraining phase consumes 1.3\nmillion GPU hours in total\"\n\n\" all computational costs mentioned in this report are normalized to GPU hours based on the H800\"",
      "Training hardware": "NVIDIA H800 SXM5",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "API (log in and select Doubao-1.5-thinking-vision-pro to experience): https://www.volcengine.com/experience/ark?model=doubao-1-5-thinking-vision-pro-250428\n\n\nGitHub sample code (Apache 2.0): https://github.com/ByteDance-Seed/Seed1.5-VL",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Gemini 2.5 Pro (May 2025)",
      "Organization": "Google DeepMind",
      "Publication date": "2025-05-06",
      "Domain": "Language,Vision,Video,Multimodal,Speech",
      "Task": "Language modeling/generation,Question answering,Code generation,Quantitative reasoning,Visual question answering,Translation,Image captioning,Video description,Speech recognition (ASR)",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/#gemini-2-5-thinking",
      "Reference": "Gemini 2.5: Our most intelligent AI model",
      "Citations": "",
      "Authors": "",
      "Abstract": "Gemini 2.5 Pro Experimental is our most advanced model for complex tasks. It tops the LMArena leaderboard \u2014 which measures human preferences \u2014 by a significant margin, indicating a highly capable model equipped with high-quality style. 2.5 Pro also shows strong reasoning and code capabilities, leading on common coding, math and science benchmarks.\n\nGemini 2.5 Pro is available now in Google AI Studio and in the Gemini app for Gemini Advanced users, and will be coming to Vertex AI soon. We\u2019ll also introduce pricing in the coming weeks, enabling people to use 2.5 Pro with higher rate limits for scaled production use.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Gemini also continues to stand out for achieving the SoTA score on both the LOFT and MRCR long-context tasks at 128k context, and is the only one, amongst the models examined in the above table, to support context lengths of 1M+ tokens.\"\n\nTable 4",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Availability\t\nGoogle AI Studio\nGemini API\nGemini App",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Qwen3-235B-A22B",
      "Organization": "Alibaba",
      "Publication date": "2025-04-29",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation,Translation",
      "Parameters": "235000000000.0",
      "Parameters notes": "235 billion total parameters and 22 billion activated parameters\n\nNumber of Layers: 94\nNumber of Attention Heads (GQA): 64 for Q and 4 for KV\nNumber of Experts: 128\nNumber of Activated Experts: 8\nContext Length: 32,768 natively and 131,072 tokens with YaRN.",
      "Training compute (FLOP)": "4.752e+24",
      "Training compute notes": "6 FLOP / parameter / token * 22*10^9 active parameters * 36000000000000 tokens = 4.752e+24 FLOP",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "36000000000000",
      "Dataset size notes": "36T",
      "Confidence": "Likely",
      "Link": "https://qwenlm.github.io/blog/qwen3/",
      "Reference": "Qwen3: Think Deeper, Act Faster",
      "Citations": "",
      "Authors": "",
      "Abstract": "Today, we are excited to announce the release of Qwen3, the latest addition to the Qwen family of large language models. Our flagship model, Qwen3-235B-A22B, achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro. Additionally, the small MoE model, Qwen3-30B-A3B, outcompetes QwQ-32B with 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of Qwen2.5-72B-Instruct.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "Discretionary",
      "Notability criteria notes": "Major Alibaba release",
      "Epochs": "1.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "not listed",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Apache 2.0\n\nhttps://huggingface.co/Qwen/Qwen3-235B-A22B",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "gpt-image-1",
      "Organization": "OpenAI",
      "Publication date": "2025-04-23",
      "Domain": "Image generation,Vision",
      "Task": "Image generation,Text-to-image",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://openai.com/index/image-generation-api/",
      "Reference": "Introducing our latest image generation model in the API",
      "Citations": "",
      "Authors": "",
      "Abstract": "Today, we\u2019re bringing the natively multimodal model that powers this experience in ChatGPT to the API via gpt-image-1, enabling developers and businesses to easily integrate high-quality, professional-grade image generation directly into their own tools and platforms. The model\u2019s versatility allows it to create images across diverse styles, faithfully follow custom guidelines, leverage world knowledge, and accurately render text\u2014unlocking countless practical applications across multiple domains.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Significant use",
      "Notability criteria notes": "\"When we introduced image generation in ChatGPT last month, it quickly became one of our most popular features: over 130 million users around the world created more than 700 million images in just the first week.\"\n\nThey also describe the model as SOTA:\nhttps://platform.openai.com/docs/models/gpt-image-1",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Pangu Ultra",
      "Organization": "Huawei",
      "Publication date": "2025-04-10",
      "Domain": "Language",
      "Task": "Code generation,Language modeling/generation",
      "Parameters": "135000000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "1.0692e+25",
      "Training compute notes": "When compared to Llama 3.1 405B, Pangu Ultra achieves better scores on most of the challenging benchmarks, while utilizing only about 29% of the training FLOPs required by Llama 405B.\n\nCompute = 6 FLOP/token/param *  135e9 params *13.2e12 tokens = 1.069200e+25 FLOP\nThis is consistent with 29% of Llama 405B's compute: 3.8e25*0.29=1.1e25.",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "13200000000000",
      "Dataset size notes": " 13.2 trillion tokens",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2504.07866",
      "Reference": "Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs",
      "Citations": "12.0",
      "Authors": "Yichun Yin, Wenyong Huang, Kaikai Song, Yehui Tang, Xueyu Wu, Wei Guo, Peng Guo, Yaoyuan Wang, Xiaojun Meng, Yasheng Wang, Dong Li, Can Chen, Dandan Tu, Yin Li, Fisher Yu, Ruiming Tang, Yunhe Wang, Baojun Wang, Bin Wang, Bo Wang, Boxiao Liu, Changzheng Zhang, Duyu Tang, Fei Mi, Hui Jin, Jiansheng Wei, Jiarui Qin, Jinpeng Li, Jun Zhao, Liqun Deng, Lin Li, Minghui Xu, Naifu Zhang, Nianzu Zheng, Qiang Li, Rongju Ruan, Shengjun Cheng, Tianyu Guo, Wei He, Wei Li, Weiwen Liu, Wulong Liu, Xinyi Dai, Yonghan Dong, Yu Pan, Yue Li, Yufei Wang, Yujun Li, Yunsheng Ni, Zhe Liu, Zhenhe Zhang, Zhicheng Liu",
      "Abstract": "We present Pangu Ultra, a Large Language Model (LLM) with 135 billion parameters and dense Transformer modules trained on Ascend Neural Processing Units (NPUs). Although the field of LLM has been witnessing unprecedented advances in pushing the scale and capability of LLM in recent years, training such a large-scale model still involves significant optimization and system challenges. To stabilize the training process, we propose depth-scaled sandwich normalization, which effectively eliminates loss spikes during the training process of deep models. We pre-train our model on 13.2 trillion diverse and high-quality tokens and further enhance its reasoning capabilities during post-training. To perform such large-scale training efficiently, we utilize 8,192 Ascend NPUs with a series of system optimizations. Evaluations on multiple diverse benchmarks indicate that Pangu Ultra significantly advances the state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral Large 2, and even achieves competitive results with DeepSeek-R1, whose sparse model structure contains much more parameters. Our exploration demonstrates that Ascend NPUs are capable of efficiently and effectively training dense models with more than 100 billion parameters. Our model and system will be available for our commercial customers.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "A closer examination reveals that Pangu Ultra excels on Chinese benchmarks, surpassing both Qwen 2.5 72B and DeepSeek V3, the current best-performing Chinese model. In addition, when compared to Llama 3.1 405B, Pangu Ultra achieves better scores on most of the challenging benchmarks, while utilizing only about 29% of the training FLOPs required by Llama 405B. These results suggest the effectiveness of our model architecture and the high quality of our training data.\n\nTable 3, Tabke 4: seems that they are comparing just to 'representative' models, not SOTA",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "Huawei Ascend 910B",
      "Hardware quantity": "8192.0",
      "Hardware utilization (MFU)": "0.52",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "6426121.277196856",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "8388608.0",
      "Batch size notes": "2048 * 4096",
      "Model accessibility": "Hosted access (no API)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "\"Our model and system will be available for our commercial customers\"",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "105834295.27828689",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Llama 4 Scout",
      "Organization": "Meta AI",
      "Publication date": "2025-04-05",
      "Domain": "Multimodal,Language,Vision",
      "Task": "Chat,Code generation,Visual question answering,Language modeling/generation,Question answering",
      "Parameters": "109000000000.0",
      "Parameters notes": "\"Our smaller model, Llama 4 Scout, is a general purpose model with 17 billion active parameters, 16 experts, and 109 billion total parameters that delivers state-of-the-art performance for its class.\"",
      "Training compute (FLOP)": "4.08e+24",
      "Training compute notes": "40T training tokens per model card:\n\nhttps://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md  \n\nEstimating training compute from parameters and tokens:\n6 FLOP per token per parameter * 17B active parameters * 40T tokens = 4.08e24 FLOP\n(Implying mean throughput was 227 TFLOPS/GPU, or 11.5% MFU in FP8)\n\n\nThe model card also states that Llama 4 Scout used 5.0M H100-hours.\nThe blog post gives a figure of 390 TFLOPS/GPU, but this may have been the utilization rate for Behemoth, or all of the models together. Using this utilization, we have:\nCompute = 390 TFLOP/s * 5 million hours = 7.02e24 FLOP\n(This value is higher than the compute implied by parameters and tokens, and suggests utilization may have been lower for Scout than for Behemoth.)",
      "Training dataset": "",
      "Training dataset size (gradients)": "30000000000000",
      "Dataset size notes": "\"The overall data mixture for training consisted of more than 30 trillion tokens, which is more than double the Llama 3 pre-training mixture and includes diverse text, image, and video datasets.\"",
      "Confidence": "Likely",
      "Link": "https://ai.meta.com/blog/llama-4-multimodal-intelligence/",
      "Reference": "The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation",
      "Citations": "",
      "Authors": "",
      "Abstract": "We\u2019re sharing the first models in the Llama 4 herd, which will enable people to build more personalized multimodal experiences.\nLlama 4 Scout, a 17 billion active parameter model with 16 experts, is the best multimodal model in the world in its class and is more powerful than all previous generation Llama models, while fitting in a single NVIDIA H100 GPU. Additionally, Llama 4 Scout offers an industry-leading context window of 10M and delivers better results than Gemma 3, Gemini 2.0 Flash-Lite, and Mistral 3.1 across a broad range of widely reported benchmarks.\nLlama 4 Maverick, a 17 billion active parameter model with 128 experts, is the best multimodal model in its class, beating GPT-4o and Gemini 2.0 Flash across a broad range of widely reported benchmarks, while achieving comparable results to the new DeepSeek v3 on reasoning and coding\u2014at less than half the active parameters. Llama 4 Maverick offers a best-in-class performance to cost ratio with an experimental chat version scoring ELO of 1417 on LMArena.\nThese models are our best yet thanks to distillation from Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the world\u2019s smartest LLMs. Llama 4 Behemoth outperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on several STEM benchmarks. Llama 4 Behemoth is still training, and we\u2019re excited to share more details about it even while it\u2019s still in flight.\nDownload the Llama 4 Scout and Llama 4 Maverick models today on llama.com and Hugging Face. Try Meta AI built with Llama 4 in WhatsApp, Messenger, Instagram Direct, and on the web.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Discretionary",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Llama 4 license (branding requirements, size cap 700M MAU)\nhttps://huggingface.co/meta-llama/Llama-4-Scout-17B-16E\n\nno training code here \nhttps://github.com/meta-llama/llama-models/tree/main/models/llama4",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Llama 4 Maverick",
      "Organization": "Meta AI",
      "Publication date": "2025-04-05",
      "Domain": "Multimodal,Language,Vision",
      "Task": "Chat,Code generation,Visual question answering,Language modeling/generation,Question answering",
      "Parameters": "400000000000.0",
      "Parameters notes": "\"Llama 4 Maverick models have 17B active parameters and 400B total parameters.\"\n\nhttps://ai.meta.com/blog/llama-4-multimodal-intelligence/",
      "Training compute (FLOP)": "2.244000000001e+24",
      "Training compute notes": "22T training tokens per model card:\n\nhttps://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md   \n\nMaverick was trained using co-distillation from Llama 4 Behemoth. It isn't 100% clear that all 22T tokens used distillation, but we assume this for the time being.\n\nEstimating training compute from parameters and tokens:\nCompute = 6 FLOP per token per parameter * 17B active parameters * 22T tokens = 2.244e24 FLOP\n(Implying mean throughput was 262 TFLOPS/GPU, or 13.2% MFU in FP8)\n\n\nThe model card also states that Llama 4 Maverick used 2.38M H100-hours.\nThe blog post gives a figure of 390 TFLOPS/GPU, but this may have been the utilization rate for Behemoth, or all of the models together. Using this utilization, we have:\nCompute = 390 TFLOP/s * 2.38 million hours = 3.342e24 FLOP\n(This value is higher than the compute implied by parameters and tokens, and suggests utilization may have been lower for Maverick than for Behemoth.)",
      "Training dataset": "",
      "Training dataset size (gradients)": "30000000000000",
      "Dataset size notes": "\"The overall data mixture for training consisted of more than 30 trillion tokens, which is more than double the Llama 3 pre-training mixture and includes diverse text, image, and video datasets.\"",
      "Confidence": "Likely",
      "Link": "https://ai.meta.com/blog/llama-4-multimodal-intelligence/",
      "Reference": "The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation",
      "Citations": "",
      "Authors": "",
      "Abstract": "We\u2019re sharing the first models in the Llama 4 herd, which will enable people to build more personalized multimodal experiences.\nLlama 4 Scout, a 17 billion active parameter model with 16 experts, is the best multimodal model in the world in its class and is more powerful than all previous generation Llama models, while fitting in a single NVIDIA H100 GPU. Additionally, Llama 4 Scout offers an industry-leading context window of 10M and delivers better results than Gemma 3, Gemini 2.0 Flash-Lite, and Mistral 3.1 across a broad range of widely reported benchmarks.\nLlama 4 Maverick, a 17 billion active parameter model with 128 experts, is the best multimodal model in its class, beating GPT-4o and Gemini 2.0 Flash across a broad range of widely reported benchmarks, while achieving comparable results to the new DeepSeek v3 on reasoning and coding\u2014at less than half the active parameters. Llama 4 Maverick offers a best-in-class performance to cost ratio with an experimental chat version scoring ELO of 1417 on LMArena.\nThese models are our best yet thanks to distillation from Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the world\u2019s smartest LLMs. Llama 4 Behemoth outperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on several STEM benchmarks. Llama 4 Behemoth is still training, and we\u2019re excited to share more details about it even while it\u2019s still in flight.\nDownload the Llama 4 Scout and Llama 4 Maverick models today on llama.com and Hugging Face. Try Meta AI built with Llama 4 in WhatsApp, Messenger, Instagram Direct, and on the web.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Discretionary",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Llama 4 license (branding requirements, size cap 700M MAU)\nhttps://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Original\n\nno training code here \nhttps://github.com/meta-llama/llama-models/tree/main/models/llama4",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Llama 4 Behemoth (preview)",
      "Organization": "Meta AI",
      "Publication date": "2025-04-05",
      "Domain": "Multimodal,Language,Vision",
      "Task": "Chat,Code generation,Visual question answering,Translation,Language modeling/generation,Quantitative reasoning,Question answering",
      "Parameters": "2000000000000.0",
      "Parameters notes": "\"Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the world\u2019s smartest LLMs.\"",
      "Training compute (FLOP)": "5.18400000000001e+25",
      "Training compute notes": "Behemoth's training dataset is at least 30T tokens:\nhttps://ai.meta.com/blog/llama-4-multimodal-intelligence/ \n\n6 FLOP / parameter / token * 288 * 10^9 activated parameters * 30 * 10^12 tokens = 5.184e+25 FLOP",
      "Training dataset": "",
      "Training dataset size (gradients)": "30000000000000",
      "Dataset size notes": "\"The overall data mixture for training consisted of more than 30 trillion tokens, which is more than double the Llama 3 pre-training mixture and includes diverse text, image, and video datasets.\"",
      "Confidence": "Likely",
      "Link": "https://ai.meta.com/blog/llama-4-multimodal-intelligence/",
      "Reference": "The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation",
      "Citations": "",
      "Authors": "",
      "Abstract": "We\u2019re sharing the first models in the Llama 4 herd, which will enable people to build more personalized multimodal experiences.\n...\nLlama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the world\u2019s smartest LLMs. Llama 4 Behemoth outperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on several STEM benchmarks. Llama 4 Behemoth is still training, and we\u2019re excited to share more details about it even while it\u2019s still in flight.\nDownload the Llama 4 Scout and Llama 4 Maverick models today on llama.com and Hugging Face. Try Meta AI built with Llama 4 in WhatsApp, Messenger, Instagram Direct, and on the web.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "Based on the model cards for Llama 4 Scout and Maverick, they seem to be using H100-80GB GPUs, despite the article saying that 390 TFLOPS/GPU was a high MFU (it is high throughput, but <20% MFU in FP8).",
      "Training hardware": "NVIDIA H100 SXM5 80GB",
      "Hardware quantity": "32000.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "44588963.624007165",
      "Compute cost notes": "",
      "Training power draw (W)": "43933454.99810563",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "\"While we\u2019re not yet releasing Llama 4 Behemoth as it is still training\"",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "946150165.8699855",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": "1780094856.8197687"
    },
    {
      "Model": "GPT-4o (Mar 2025)",
      "Organization": "OpenAI",
      "Publication date": "2025-03-27",
      "Domain": "Multimodal,Language,Audio,Speech,Vision",
      "Task": "Chat,Image generation,Audio generation,Vision-language generation,Table tasks,Language modeling/generation,Question answering,Speech recognition (ASR),Speech-to-text",
      "Parameters": "",
      "Parameters notes": "Not known.\n\nInference costs in the API are 2x cheaper than GPT-4 Turbo",
      "Training compute (FLOP)": "",
      "Training compute notes": "Training compute estimated to be 3.8e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Speculative",
      "Link": "https://help.openai.com/en/articles/6825453-chatgpt-release-notes",
      "Reference": "Improvements to GPT-4o",
      "Citations": "",
      "Authors": "Aidan Clark, Alex Paino, Jacob Menick, Liam Fedus, Luke Metz, Clemens Winter, Lia Guy, Sam Schoenholz, Daniel Levy, Nitish Keskar, Alex Carney, Alex Paino, Ian Sohl, Qiming Yuan, Reimar Leike, Arka Dhar, Brydon Eastman, Mia Glaese, Ben Sokolowsky, Andrew Kondrich, Felipe Petroski Such, Henrique Ponde de Oliveira Pinto, Jiayi Weng, Randall Lin, Youlong Cheng, Nick Ryder, Lauren Itow, Barret Zoph, John Schulman, Mianna Chen, Adam Lerer, Adam P. Goucher, Adam Perelman, Akila Welihinda, Alec Radford, Alex Borzunov, Alex Carney, Alex Chow, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexi Christakis, Ali Kamali, Allison Moyer, Allison Tam, Amin Tootoonchian, Ananya Kumar, Andrej Karpathy, Andrey Mishchenko, Andrew Cann, Andrew Kondrich, Andrew Tulloch, Angela Jiang, Antoine Pelisse, Anuj Gosalia, Avi Nayak, Avital Oliver, Behrooz Ghorbani, Ben Leimberger, Ben Wang, Blake Samic, Brian Guarraci, Brydon Eastman, Camillo Lugaresi, Chak Li, Charlotte Barette, Chelsea Voss, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christopher Hesse, Colin Wei, Daniel Kappler, Daniel Levin, Daniel Levy, David Farhi, David Mely, David Sasaki, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Peterson, Eric Sigler, Eugene Brevdo, Farzad Khorasani, Francis Zhang, Gene Oden, Geoff Salmon, Hadi Salman, Haiming Bao, Heather Schmidt, Hongyu Ren, Hyung Won Chung, Ian Kivlichan, Ian O'Connell, Ian Osband, Ilya Kostrikov, Ingmar Kanitscheider, Jacob Coxon, James Crooks, James Lennon, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jiayi Weng, Jie Tang, Joanne Jang, Jonathan Ward, Jonathan McKay, Jong Wook Kim, Josh Gross, Josh Kaplan, Joy Jiao, Joyce Lee, Juntang Zhuang, Kai Fricke, Kavin Karthik, Kenny Hsu, Kiel Howe, Kyle Luther, Larry Kai, Lauren Itow, Leo Chen, Lia Guy, Lien Mamitsuka, Lilian Weng, Long Ouyang, Louis Feuvrier, Lukas Kondraciuk, Lukasz Kaiser, Lyric Doshi, Mada Aflak, Maddie Simens, Madeleine Thompson, Marat Dukhan, Marvin Zhang, Mateusz Litwin, Max Johnson, Mayank Gupta, Mia Glaese, Michael Janner, Michael Petrov, Michael Wu, Michelle Fradin, Michelle Pokrass, Miguel Oom Temudo de Castro, Mikhail Pavlov, Minal Khan, Mo Bavarian, Natalia Gimelshein, Natalie Staudacher, Nick Stathas, Nik Tezak, Nithanth Kudige, Noel Bundick, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivier Godement, Owen Campbell-Moore, Philip Pronin, Philippe Tillet, Rachel Lim, Rajan Troll, Randall Lin, Rapha gontijo lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Rob Honsby, Rohit Ramchandani, Rory Carmichael, Ruslan Nigmatullin, Ryan Cheu, Scott Gray, Sean Grove, Sean Metzger, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shuaiqi (Tony) Xia, Sonia Phene, Spencer Papay, Steve Coffey, Steve Lee, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tarun Gogineni, Ted Sanders, Thomas Cunninghman, Thomas Dimson, Thomas Raoux, Tianhao Zheng, Tina Kim, Todd Underwood, Tristan Heywood, Valerie Qi, Vinnie Monaco, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wojciech Zaremba, Yash Patil, Yilei, Qian, Yongjik Kim, Youlong Cheng, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, Yury Malkov",
      "Abstract": "We\u2019re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.\n\nGPT-4o (\u201co\u201d for \u201comni\u201d) is a step towards much more natural human-computer interaction\u2014it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement,Significant use",
      "Notability criteria notes": "Outperforms GPT-4 Turbo and other models on text and especially on multimodal benchmarks, such as MMLU, GPQA, HumanEval, MMMU, etc See Model Evaluations: https://openai.com/index/hello-gpt-4o/ \n\nGPT-4o is now the default model in ChatGPT, so it's one of the most widely used models.",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "GPT-4o (Jan 2025)",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "Definitely a new model, not a GPT-4 finetune",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Gemini 2.5 Pro (Mar 2025)",
      "Organization": "Google DeepMind",
      "Publication date": "2025-03-25",
      "Domain": "Language,Vision,Video,Multimodal,Speech",
      "Task": "Language modeling/generation,Question answering,Code generation,Quantitative reasoning,Visual question answering,Translation,Image captioning,Video description,Speech recognition (ASR)",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/#gemini-2-5-thinking",
      "Reference": "Gemini 2.5: Our most intelligent AI model",
      "Citations": "",
      "Authors": "",
      "Abstract": "Gemini 2.5 Pro Experimental is our most advanced model for complex tasks. It tops the LMArena leaderboard \u2014 which measures human preferences \u2014 by a significant margin, indicating a highly capable model equipped with high-quality style. 2.5 Pro also shows strong reasoning and code capabilities, leading on common coding, math and science benchmarks.\n\nGemini 2.5 Pro is available now in Google AI Studio and in the Gemini app for Gemini Advanced users, and will be coming to Vertex AI soon. We\u2019ll also introduce pricing in the coming weeks, enabling people to use 2.5 Pro with higher rate limits for scaled production use.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Gemini also continues to stand out for achieving the SoTA score on both the LOFT and MRCR long-context tasks at 128k context, and is the only one, amongst the models examined in the above table, to support context lengths of 1M+ tokens.\"\n\nTable 4",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Availability\t\nGoogle AI Studio\nGemini API\nGemini App",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Diffusion Renderer",
      "Organization": "NVIDIA,University of Toronto,Vector Institute,University of Illinois Urbana-Champaign (UIUC)",
      "Publication date": "2025-03-22",
      "Domain": "Video",
      "Task": "Video editing,Video-to-video,3D segmentation",
      "Parameters": "1100000000.0",
      "Parameters notes": "\"This model has 1.1B model parameters.\"",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased,Objaverse",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "\"In total, we generate 150,000 videos with paired ground-truth G-buffers and environment maps, at 24 frames\nper video in 512x512 resolution\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2501.18590",
      "Reference": "DiffusionRenderer: Neural Inverse and Forward Rendering with Video Diffusion Models",
      "Citations": "",
      "Authors": "Ruofan Liang, Zan Gojcic, Huan Ling, Jacob Munkberg, Jon Hasselgren, Zhi-Hao Lin, Jun Gao, Alexander Keller, Nandita Vijaykumar, Sanja Fidler, Zian Wang",
      "Abstract": "Understanding and modeling lighting effects are fundamental tasks in computer vision and graphics. Classic physically-based rendering (PBR) accurately simulates the light transport, but relies on precise scene representations--explicit 3D geometry, high-quality material properties, and lighting conditions--that are often impractical to obtain in real-world scenarios. Therefore, we introduce DiffusionRenderer, a neural approach that addresses the dual problem of inverse and forward rendering within a holistic framework. Leveraging powerful video diffusion model priors, the inverse rendering model accurately estimates G-buffers from real-world videos, providing an interface for image editing tasks, and training data for the rendering model. Conversely, our rendering model generates photorealistic images from G-buffers without explicit light transport simulation. Experiments demonstrate that DiffusionRenderer effectively approximates inverse and forwards rendering, consistently outperforming the state-of-the-art. Our model enables practical applications from a single video input--including relighting, material editing, and realistic object insertion.",
      "Organization categorization": "Industry,Academia,Academia,Academia",
      "Country (of organization)": "United States of America,Canada,Canada,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Table 2\n\n\"In Table 2 and Fig. 6, we compare with recent state-of-the-art relighting methods DiLightNet [82] and Neural Gaffer [30].\nOur method outperforms these baselines, particularly in scenes with complex shadows and inter-reflections.\"",
      "Epochs": "",
      "Training time (hours)": "48.0",
      "Training time notes": "\"The training takes around 2 days on 32 A100 GPUs.\"",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "32.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "25112.659611092717",
      "Base model": "Stable Video Diffusion",
      "Finetune compute (FLOP)": "1.2934287e+20",
      "Finetune compute notes": "77970000000000 FLOP / GPU / sec [A100, fp16 reported] * 32 GPUs * 48 hours * 3600 sec / hour * 0.3 [assumed utilization] = 1.2934287e+20 FLOP",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open access (non-commercial)",
      "Accessibility notes": "NVIDIA license (non commercial)\nhttps://huggingface.co/nexuslrf/diffusion_renderer-forward-svd\nhttps://github.com/nv-tlabs/diffusion-renderer/tree/main",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "EXAONE Deep 32B",
      "Organization": "LG AI Research",
      "Publication date": "2025-03-16",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering,Quantitative reasoning,Code generation",
      "Parameters": "32000000000.0",
      "Parameters notes": "32B",
      "Training compute (FLOP)": "1.26e+24",
      "Training compute notes": "1.25 \u00d7 10^24 (base model reported training compute) + 7.04 \u00d7 10^21 (finetune compute) = 1.26 \u00d7 10^24 FLOP\n\nTable 1",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "12000000000",
      "Dataset size notes": "\"To enhance the reasoning capabilities of language models, we have utilized 1.6M instances for SFT and 20K instances of preference data for DPO. The SFT dataset contains approximately 12B tokens\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2503.12524",
      "Reference": "EXAONE Deep: LLMs with Enhanced Reasoning Performance",
      "Citations": "",
      "Authors": "LG AI Research, Kyunghoon Bae, Eunbi Choi, Kibong Choi, Stanley Jungkyu Choi, Yemuk Choi, Seokhee Hong, Junwon Hwang, Hyojin Jeon, Kijeong Jeon, Gerrard Jeongwon Jo, Hyunjik Jo, Jiyeon Jung, Hyosang Kim, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Yongil Kim, Youchul Kim, Edward Hwayoung Lee, Haeju Lee, Honglak Lee, Jinsik Lee, Kyungmin Lee, Sangha Park, Yongmin Park, Sihoon Yang, Heuiyeen Yeen, Sihyuk Yi, Hyeongu Yun",
      "Abstract": "We present EXAONE Deep series, which exhibits superior capabilities in various reasoning tasks, including math and coding benchmarks. We train our models mainly on the reasoning-specialized dataset that incorporates long streams of thought processes. Evaluation results show that our smaller models, EXAONE Deep 2.4B and 7.8B, outperform other models of comparable size, while the largest model, EXAONE Deep 32B, demonstrates competitive performance against leading open-weight models. All EXAONE Deep models are openly available for research purposes and can be downloaded from this https URL",
      "Organization categorization": "Industry",
      "Country (of organization)": "Korea (Republic of)",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "512 H100 for 3 months\n\nMath \u2013 EXAONE Deep 32B Outperforms Competitors in High-Difficulty Math Benchmarks Even at Just 5% of Their Size\n\nMMLU \u2013 EXAONE Deep 32B Achieves 83.0 score, Proving the Best Performance Among Domestic Models",
      "Epochs": "",
      "Training time (hours)": "2160.0",
      "Training time notes": "512 H100 GPUs were used for three months",
      "Training hardware": "NVIDIA H100 SXM5 80GB",
      "Hardware quantity": "512.0",
      "Hardware utilization (MFU)": "0.3156",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "703248.4282353076",
      "Base model": "EXAONE 3.5 32B",
      "Finetune compute (FLOP)": "7.04e+21",
      "Finetune compute notes": "Table 1 (reported): 7.04 \u00d7 10^21 FLOP\n\n6ND = 6*32B parameters * 12B tokens = 2.304e+21 FLOP",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "https://huggingface.co/LGAI-EXAONE/EXAONE-Deep-32B\n\nExaone License",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "15357606.081416206",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ERNIE-4.5-VL-424B-A47B (\u6587\u5fc3\u5927\u6a21\u578b4.5)",
      "Organization": "Baidu",
      "Publication date": "2025-03-16",
      "Domain": "Multimodal,Language,Vision,Video",
      "Task": "Language modeling/generation,Visual question answering,Video description,Speech recognition (ASR),Quantitative reasoning,Code generation,Translation,Question answering,Character recognition (OCR)",
      "Parameters": "424000000000.0",
      "Parameters notes": "MoE:\ntotal parameters - 424B\nactive parameters - 47B\n\n\"ViT encoder comprising 630 million parameters\"",
      "Training compute (FLOP)": "",
      "Training compute notes": "Unlikely over 1e25 FLOP, as the ERNIE-4.5 LLM pretraining used 3e24 FLOP.",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://www.datacamp.com/blog/ernie-4-5-x1\n\nhttps://yiyan.baidu.com/blog/publication/ERNIE_Technical_Report.pdf",
      "Reference": "Baidu's ERNIE 4.5 & X1: Features, Access, DeepSeek Comparison\n",
      "Citations": "",
      "Authors": "",
      "Abstract": "In this report, we introduce ERNIE 4.5, a new family of large-scale multimodal models comprising 10 distinct variants. The model family consist of Mixture-of-Experts (MoE) models with 47B and 3B active parameters, with the largest model having 424B total parameters, as well as a 0.3B dense model. For the MoE architecture, we propose a novel heterogeneous modality structure, which supports parameter sharing across modalities while also allowing dedicated parameters for each individual modality.\nThis MoE architecture has the advantage to enhance multimodal understanding without compromising, and even improving, performance on text-related tasks. All of our models are trained with optimal efficiency using the PaddlePaddle deep learning framework, which also enables high-performance inference and streamlined deployment for them. We achieve 47% Model FLOPs Utilization (MFU) in our largest ERNIE 4.5 language model pre-training. Experimental results show that our models achieve state-of-the-art performance across multiple text and multimodal benchmarks, especially in instruction following, world knowledge memorization, visual understanding and multimodal reasoning. All models are publicly accessible under Apache 2.0 to support future research and development in the field. Additionally, we open source the development toolkits for ERNIE 4.5, featuring industrial-grade capabilities, resourceefficient training and inference workflows, and multi-hardware compatibility.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "ERNIE 4.5 surpassed GPT-4o in six out of seven evaluated benchmarks:\n[Table 7, Table 8]\nCCBench: Evaluates common-sense reasoning across text and images. ERNIE 4.5 scored approximately 81, slightly outperforming GPT-4o\u2019s ~79.\nOCRBench: Assesses optical character recognition capabilities, focusing on text extraction from images. ERNIE 4.5 achieved around 88, surpassing GPT-4o\u2019s ~81.\nChartQA: Tests understanding of data presented in charts. ERNIE 4.5 scored ~82, marginally ahead of GPT-4o\u2019s ~81.\nMMMU: Measures multimodal reasoning across various topics. Here, GPT-4o led with ~70, while ERNIE 4.5 scored ~64, indicating an area for improvement.\nMathVista: Evaluates mathematical reasoning in visual contexts. ERNIE 4.5 scored ~69, outperforming GPT-4o\u2019s ~61.\nDocVQA: Assesses the ability to answer questions based on document visuals. ERNIE 4.5 excelled with a score of ~91, compared to GPT-4o\u2019s ~85.\nMVBench: Focuses on temporal understanding in dynamic video tasks, requiring reasoning over sequences of frames. ERNIE 4.5 scored ~72, significantly outperforming GPT-4o\u2019s ~63.\nText-only benchmarks\nOn text-only tasks, ERNIE 4.5 achieved an average score of 79.6, slightly ahead of GPT-4.5\u2019s average of 79.14, and also surpassing DeepSeek-V3 (~77).",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "ERNIE-4.5-300B-A47B",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache 2.0:\nhttps://huggingface.co/baidu/ERNIE-4.5-VL-424B-A47B-Base-PT\n\nApache 2.0 for inference code\nhttps://github.com/PaddlePaddle/ERNIE",
      "Numerical format": "FP8",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Hunyuan-TurboS",
      "Organization": "Tencent",
      "Publication date": "2025-03-11",
      "Domain": "Language",
      "Task": "Language modeling/generation,Quantitative reasoning,Question answering,Code generation,Text summarization",
      "Parameters": "560000000000.0",
      "Parameters notes": " the model scales to 56B activated\nparameters and 560B total parameters",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "16000000000000",
      "Dataset size notes": "pre-trained on 16T high-quality tokens",
      "Confidence": "Confident",
      "Link": "https://web.archive.org/web/20250408105622/https://www.dapingtime.com/article/2171.html\n\nhttps://medium.com/data-science-in-your-pocket/tencent-hunyuan-turbo-s-the-fastest-reasoning-llm-d64a02bed5c8\n\nhttps://arxiv.org/abs/2505.15431",
      "Reference": "Tencent HunYuan Turbo S: The fastest reasoning LLM\nAt par with DeepSeek, Claude 3.5 and GPT-4o",
      "Citations": "",
      "Authors": "Tencent Hunyuan Team: Ao Liu, Botong Zhou, Can Xu, Chayse Zhou, ChenChen Zhang, Chengcheng Xu, Chenhao Wang, Decheng Wu, Dengpeng Wu, Dian Jiao, Dong Du, Dong Wang, Feng Zhang, Fengzong Lian, Guanghui Xu, Guanwei Zhang, Hai Wang, Haipeng Luo, Han Hu, Huilin Xu, Jiajia Wu, Jianchen Zhu, Jianfeng Yan, Jiaqi Zhu, Jihong Zhang, Jinbao Xue, Jun Xia, Junqiang Zheng, Kai Liu, Kai Zhang, Kai Zheng, Kejiao Li, Keyao Wang, Lan Jiang, Lixin Liu, Lulu Wu, Mengyuan Huang, Peijie Yu, Peiqi Wang, Qian Wang, Qianbiao Xiang, Qibin Liu, Qingfeng Sun, Richard Guo, Ruobing Xie, Saiyong Yang, Shaohua Chen, Shihui Hu, Shuai Li, Shuaipeng Li, Shuang Chen, Suncong Zheng, Tao Yang, Tian Zhang, Tinghao Yu, Weidong Han, Weijie Liu, Weijin Zhou, Weikang Wang, Wesleye Chen, Xiao Feng, Xiaoqin Ren, Xingwu Sun, Xiong Kuang, Xuemeng Huang, Xun Cao, Yanfeng Chen, Yang Du, Zhen Yang, Yangyu Tao, Yaping Deng, Yi Shen, Yigeng Hong, Yiqi Chen, Yiqing Huang, Yuchi Deng, Yue Mao, Yulong Wang, Yuyuan Zeng, Zenan Xu, Zhanhui Kang, Zhe Zhao, ZhenXiang Yan, Zheng Fang, Zhichao Hu, Zhongzhi Chen, Zhuoyu Li, Zongwei Li, Alex Yan, Ande Liang, Baitong Liu, Beiping Pan, Bin Xing, Binghong Wu, Bingxin Qu, Bolin Ni, Boyu Wu, Chen Li, Cheng Jiang , Cheng Zhang, Chengjun Liu, Chengxu Yang, Chengzhong Xu, Chiyu Wang, Chong Zha, Daisy Yi, Di Wang, Fanyang Lu, Fei Chen, Feifei Liu, Feng Zheng, Guanghua Yu, Guiyang Li, Guohua Wang, Haisheng Lin, Han Liu, Han Wang, Hao Fei, Hao Lu, Haoqing Jiang, Haoran Sun, Haotian Zhu, Huangjin Dai, Huankui Chen, Huawen Feng, Huihui Cai, Huxin Peng, Jackson Lv, Jiacheng Shi, Jiahao Bu, Jianbo Li, Jianglu Hu, Jiangtao Guan, Jianing Xu, Jianwei Cai, Jiarong Zhang, Jiawei Song, Jie Jiang, Jie Liu, Jieneng Yang, Jihong Zhang, Jin lv, Jing Zhao, Jinjian Li, Jinxing Liu, Jun Zhao, Juntao Guo, Kai Wang, Kan Wu, Lei Fu, Lei He, Lei Wang, Li Liu, Liang Dong, Liya Zhan, Long Cheng, Long Xu, Mao Zheng, Meng Liu, Mengkang Hu, Nanli Chen, Peirui Chen, Peng He, Pengju Pan, Pengzhi Wei, Qi Yang, Qi Yi, Roberts Wang, Rongpeng Chen, Rui Sun, Rui Yang, Ruibin Chen, Ruixu Zhou, Shaofeng Zhang, Sheng Zhang, Shihao Xu, Shuaishuai Chang, Shulin Liu, SiQi Wang, Songjia Feng, Songling Yuan, Tao Zhang, Tianjiao Lang, Tongkai Li, Wei Deng, Wei Li, Weichao Wang, Weigang Zhang, Weixuan Sun, Wen Ouyang, Wenxiang Jiao, Wenzhi Sun, Wenzhuo Jia, Xiang Zhang, Xiangyu He, Xianshun Ren, XiaoYing Zhu, Xiaolong Guo, Xiaoxue Li, Xiaoyu Ma, Xican Lu, Xinhua Feng, Xinting Huang, Xinyu Guan, Xirui Li, Xu Zhang, Xudong Gao, Xun Luo, Xuxiang Qi, Yangkun Chen, Yangyu Tao, Yanling Xiao, Yantao Mai, Yanze Chen, Yao Ding, Yeting Yang, YiFan Song, Yifan Yang, Yijiao Zhu, Yinhe Wu, Yixian Liu, Yong Yang, Yuanjun Cai, Yuanlin Tu, Yue Zhang, Yufei Huang, Yuhang Zhou, Yuhao Jiang, Yuhong Liu, Yuhui Hu, Yujin Lin, Yun Yang, Yunhao Wang, Yusong Zhang, Zekun Wu, Zelong Zhang, Zhan Yu, Zhaoliang Yang, Zhe Zhao, Zheng Li, Zhenyu Huang, Zhiguang Liu, Zhijiang Xu, Zhiqing Kui, Zhiyin Zeng, Zhiyuan Xiong, Zhuo Han, Zifan Wu, Zigang Geng, Zilong Zhao, Ziyan Tang, Ziyuan Zhu, Zonglei Zhu, Zhijiang Xu",
      "Abstract": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS, a novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It synergistically combines Mamba's long-sequence processing efficiency with Transformer's superior contextual understanding. Hunyuan-TurboS features an adaptive long-short chain-of-thought (CoT) mechanism, dynamically switching between rapid responses for simple queries and deep \"thinking\" modes for complex problems, optimizing computational resources. Architecturally, this 56B activated (560B total) parameter model employs 128 layers (Mamba2, Attention, FFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear complexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE structure. Pre-trained on 16T high-quality tokens, it supports a 256K context length and is the first industry-deployed large-scale Mamba model. Our comprehensive post-training strategy enhances capabilities via Supervised Fine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method, Multi-round Deliberation Learning for iterative improvement, and a two-stage Large-scale Reinforcement Learning process targeting STEM and general instruction-following. Evaluations show strong performance: overall top 7 rank on LMSYS Chatbot Arena with a score of 1356, outperforming leading models like Gemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves an average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances high performance and efficiency, offering substantial capabilities at lower inference costs than many reasoning models, establishing a new paradigm for efficient large-scale pre-trained models.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Based on the photos published by Tencent on X, it achieved top performance on several benchmarks - MMLU, C-Eval, MATH. (https://x.com/TencentHunyuan/status/1899105803073958010/photo/1)",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "found nowhere",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "API via Tencent Cloud",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "QwQ-32B",
      "Organization": "Alibaba",
      "Publication date": "2025-03-06",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering,Quantitative reasoning,Code generation",
      "Parameters": "32500000000.0",
      "Parameters notes": "Architecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\nNumber of Parameters: 32.5B\nNumber of Paramaters (Non-Embedding): 31.0B\nNumber of Layers: 64\nNumber of Attention Heads (GQA): 40 for Q and 8 for KV",
      "Training compute (FLOP)": "3.51e+24",
      "Training compute notes": "Assuming the same dataset size as for Qwen2.5 training (18T tokens):\n\n6ND = 6 * 32500000000 parameters * 18 * 10^12 tokens =  3.51 \u00d7 10^24\n\n'Speculative' confidence",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "Speculatively: might be similar to Qwen2.5 models (18T tokens)",
      "Confidence": "Speculative",
      "Link": "https://qwenlm.github.io/blog/qwq-32b/",
      "Reference": "QwQ-32B: Embracing the Power of Reinforcement Learning",
      "Citations": "",
      "Authors": "Qwen Team",
      "Abstract": "QwQ is the reasoning model of the Qwen series. Compared with conventional instruction-tuned models, QwQ, which is capable of thinking and reasoning, can achieve significantly enhanced performance in downstream tasks, especially hard problems. QwQ-32B is the medium-sized reasoning model, which is capable of achieving competitive performance against state-of-the-art reasoning models, e.g., DeepSeek-R1, o1-mini.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Blog (https://qwenlm.github.io/blog/qwq-32b-preview/) lists AIME and MATH-500 scores superior to o1-preview",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "Qwen2.5-Coder (32B)",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "https://huggingface.co/Qwen/QwQ-32B\nApache 2",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Mistral OCR",
      "Organization": "Mistral AI",
      "Publication date": "2025-03-06",
      "Domain": "Multimodal,Vision,Language",
      "Task": "Character recognition (OCR),Chat,Language generation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://mistral.ai/news/mistral-ocr",
      "Reference": "Mistral OCR\n",
      "Citations": "",
      "Authors": "",
      "Abstract": "Mistral OCR is an Optical Character Recognition API that sets a new standard in document understanding. Unlike other models, Mistral OCR comprehends each element of documents\u2014media, text, tables, equations\u2014with unprecedented accuracy and cognition. It takes images and PDFs as input and extracts content in an ordered interleaved text and images.",
      "Organization categorization": "Industry",
      "Country (of organization)": "France",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Mistral OCR has consistently outperformed other leading OCR models in rigorous benchmark tests.\n\nNo evaluations on standard benchmarks are mentioned",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "available on la Plateforme",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Mercury",
      "Organization": "Inception Labs",
      "Publication date": "2025-02-27",
      "Domain": "Language",
      "Task": "Code generation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://machine-learning-made-simple.medium.com/is-the-mercury-llm-the-first-of-a-new-generation-of-llms-b64de1d36029\n\nhttps://www.inceptionlabs.ai/news\n\nhttps://www.inceptionlabs.ai/introducing-mercury-our-general-chat-model\n",
      "Reference": "Is the Mercury LLM the first of a new Generation of LLMs?",
      "Citations": "",
      "Authors": "",
      "Abstract": "Today, we\u2019re excited to announce that Mercury, our first general chat model, is available to support a wider range of text generation applications.\n\nWhen benchmarked by Artificial Analysis, a leading third-party model evaluator, Mercury matches the performance of speed-optimized frontier models like GPT-4.1 Nano and Claude 3.5 Haiku while running over 7x faster. ",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\u201cWhen evaluated on standard coding benchmarks, Mercury Coder achieves excellent quality across numerous benchmarks, often surpassing the performance of speed-optimized autoregressive models like GPT-4o Mini and Claude 3.5 Haiku while being up to 10x faster.\u201d\n\n\"When benchmarked on Copilot Arena, Mercury Coder Mini is tied for second place, surpassing the performance of speed-optimized models like GPT-4o Mini and Gemini-1.5-Flash and even of larger models like GPT-4o. At the same time, it is the fastest model, about 4 times faster than GPT-4o Mini.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GPT-4.5",
      "Organization": "OpenAI",
      "Publication date": "2025-02-27",
      "Domain": "Language,Vision,Multimodal",
      "Task": "Language modeling/generation,Question answering,Quantitative reasoning,Translation,Visual question answering,Code generation,Instruction interpretation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "3.8e+26",
      "Training compute notes": "Analysis of GPT-4.5's training cluster yields a median estimate of 187M H100-hours of training. The utilization assumptions we used for the Grok 3 estimate (probably worth revisiting) were 20 to 40% under the H100 FP8 spec of 2000 teraflop/s. This leads to an estimate of 2.7e26 to 5.4e26 FLOP, or a geomean of 3.8e26\n\nAlternatively, using a plausible range of 20 to 50% utilization, given the possibility of FP8 training, yields a median estimate of ~2e25 FLOP. See notebook below for details.\n\nhttps://colab.research.google.com/drive/1QBmVPm64Ti0xucN0EsZTgSz_I7Mj9hAZ#scrollTo=NYH1ABJuLJlw \n\nThis is consistent with OpenAI's statement that GPT-4.5 was a \u201cnew order of magnitude in compute\u201d compared to previous models (e.g. GPT-4, which was ~2e25), suggesting around 2e26 FLOP. But they could have meant this somewhat loosely. \n\nIn the \"Pretraining GPT-4.5\" interview, they state they used multi-cluster training: https://youtu.be/6nJZopACRuQ?si=FFJC-gEmGPZjvoPM&t=617 ",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://openai.com/index/introducing-gpt-4-5/",
      "Reference": "Introducing GPT-4.5",
      "Citations": "",
      "Authors": "Foundational contributors\nAlex Paino, Ali Kamali, Amin Tootoonchian, Andrew Tulloch, Ben Sokolowsky, Clemens Winter, Colin Wei, Daniel Kappler, Daniel Levy, Felipe Petroski Such, Geoff Salmon, Ian O\u2019Connell, Jason Teplitz, Kai Chen, Nik Tezak, Prafulla Dhariwal, Rapha Gontijo Lopes, Sam Schoenholz, Youlong Cheng, Yujia Jin, Yunxing Dai\n\nResearch\nCore contributors\n\nAiden Low, Alec Radford, Alex Carney, Alex Nichol, Alexis Conneau, Ananya Kumar, Ben Wang, Charlotte Cole , Elizabeth Yang, Gabriel Goh, Hadi Salman, Haitang Hu, Heewoo Jun, Ian Sohl, Ishaan Gulrajani, Jacob Coxon, James Betker, Jamie Kiros, Jessica Landon, Kyle Luther, Lia Guy, Lukas Kondraciuk, Lyric Doshi, Mikhail Pavlov, Qiming Yuan, Reimar Leike, Rowan Zellers, Sean Metzger, Shengjia Zhao, Spencer Papay, Tao Wang\n\nContributors\n\nAdam Lerer, Aidan McLaughlin, Alexander Prokofiev, Alexandra Barr, Allan Jabri, Ananya Kumar, Andrew Gibiansky, Andrew Schmidt, Casey Chu, Chak Li, Chelsea Voss, Chris Hallacy, Chris Koch, Christine McLeavey, David Mely, Dimitris Tsipras, Eric Sigler, Erin Kavanaugh, Farzad Khorasani, Huiwen Chang, Ilya Kostrikov, Ishaan Singal, Ji Lin, Jiahui Yu, Jing Yu Zhang, John Rizzo, Jong Wook Kim, Joyce Lee, Juntang Zhuang, Leo Liu, Li Jing, Long Ouyang, Louis Feuvrier, Mo Bavarian, Nick Stathas, Nitish Keskar, Oleg Murk, Preston Bowman, Scottie Yan, SQ Mah, Tao Xu, Taylor Gordon, Valerie Qi, Wenda Zhou, Yu Zhang\n\nScaling\nCore contributors\n\nAdam Goucher, Alex Chow, Alex Renzin, Aleksandra Spyra, Avi Nayak, Ben Leimberger, Christopher Hesse, Duc Phong Nguyen, Dinghua Li, Eric Peterson, Francis Zhang, Gene Oden, Kai Fricke, Kai Hayashi, Larry Lv, Leqi Zou, Lin Yang, Madeleine Thompson, Michael Petrov, Miguel Castro, Natalia Gimelshein, Phil Tillet, Reza Zamani, Ryan Cheu Stanley Hsieh, Steve Lee, Stewart Hall, Thomas Raoux, Tianhao Zheng, Vishal Kuo, Yongjik Kim, Yuchen Zhang, Zhuoran Liu\n\nContributors\n\nAlvin Wan, Andrew Cann, Antoine Pelisse, Anuj Kalia, Aaron Hurst, Avital Oliver, Brad Barnes, Brian Hsu, Chen Ding, Chen Shen, Cheng Chang, Christian Gibson, Duncan Findlay, Fan Wang, Fangyuan Li, Gianluca Borello, Heather Schmidt, Henrique Ponde de Oliveira Pinto, Ikai Lan, Jiayi Weng, James Crooks, Jos Kraaijeveld, Junru Shao, Kenny Hsu, Kenny Nguyen, Kevin King, Leah Burkhardt, Leo Chen, Linden Li, Lu Zhang, Mahmoud Eariby, Marat Dukhan, Mateusz Litwin, Miki Habryn, Natan LaFontaine, Pavel Belov, Peng Su, Prasad Chakka, Rachel Lim, Rajkumar Samuel, Renaud Gaubert, Rory Carmichael, Sarah Dong, Shantanu Jain, Stephen Logsdon, Todd Underwood, Weixing Zhang, Will Sheu, Weiyi Zheng, Yinghai Lu, Yunqiao Zhang\n\nSafety Systems\nAndrea Vallone, Andy Applebaum, Cameron Raymond, Chong Zhang, Dan Mossing, Elizabeth Proehl, Eric Wallace, Evan Mays, Grace Zhao, Ian Kivlichan, Irina Kofman, Joel Parish, Kevin Liu, Keren Gu-Lemberg, Kristen Ying, Lama Ahmad, Lilian Weng , Leon Maksin, Leyton Ho, Meghan Shah, Michael Lampe, Michele Wang, Miles Wang, Olivia Watkins, Phillip Guo, Samuel Miserendino, Sam Toizer, Sandhini Agarwal, Tejal Patwardhan, Tom Dupr\u00e9 la Tour, Tong Mu, Tyna Eloundou, Yunyun Wang\n\nDeployment\nAdam Brandon, Adam Perelman, Adele Li, Akshay Nathan, Alan Hayes, Alfred Xue, Alison Ben, Alec Gorge, Alex Guziel, Alex Iftimie, Ally Bennett, Andrew Chen, Andy Wang, Andy Wood, Angad Singh, Anoop Kotha, Antonia Woodford, Anuj Saharan, Ashley Tyra, Atty Eleti, Ben Schneider, Bessie Ji, Beth Hoover, Bill Chen, Blake Samic, Britney Smith, Brian Yu, Caleb Wang, Cary Bassin, Cary Hudson, Charlie Jatt, Chengdu Huang, Chris Beaumont, Christina Huang, Cristina Scheau, Dana Palmie, Daniel Levine, Daryl Neubieser, Dave Cummings, David Sasaki, Dibya Bhattacharjee, Dylan Hunn, Edwin Arbus, Elaine Ya Le, Enis Sert, Eric Kramer, Fred von Lohmann, Gaby Janatpour, Garrett McGrath, Garrett Ollinger, Gary Yang, Hao Sheng, Harold Hotelling, Janardhanan Vembunarayanan, Jeff Harris, Jeffrey Sabin Matsumoto, Jennifer Robinson, Jessica Liang, Jessica Shieh, Jiacheng Yang, Joel Morris, Joseph Florencio, Josh Kaplan, Kan Wu, Karan Sharma, Karen Li, Katie Pypes, Kendal Simon, Kendra Rimbach, Kevin Park, Kevin Rao, Laurance Fauconnet, Lauren Workman, Leher Pathak, Liang Wu, Liang Xiong, Lien Mamitsuka, Lindsay McCallum, Lukas Gross, Manoli Liodakis, Matt Nichols, Michelle Fradin, Minal Khan, Mingxuan Wang, Nacho Soto, Natalie Staudacher, Nikunj Handa, Niko Felix, Ning Liu, Olivier Godement, Oona Gleeson, Philip Pronin, Raymond Li, Reah Miyara, Rohan Nuttall, R.J. Marsan, Sara Culver, Scott Ethersmith, Sean Fitzgerald, Shamez Hemani, Sherwin Wu, Shiao Lee, Shuyang Cheng, Siyuan Fu, Spug Golden, Steve Coffey, Steven Heidel, Sundeep Tirumalareddy, Tabarak Khan, Thomas Degry, Thomas Dimson, Tom Stasi, Tomo Hiratsuka, Trevor Creech, Uzair Navid Iftikhar, Victoria Chernova, Victoria Spiegel, Wanning Jiang, Wenlei Xie, Yaming Lin, Yara Khakbaz, Yilei Qian, Yilong Qin, Yo Shavit, Zhi Bie\n\nExecutive Leadership\nBob McGrew, Greg Brockman, Hannah Wong, Jakub Pachocki, Johannes Heidecke, Joanne Jang, Kate Rouch, Kevin Weil, Lauren Itow, Liam Fedus, Mark Chen, Mia Glaese, Mira Murati, Nick Ryder, Sam Altman, Srinivas Narayanan, Tal Broda",
      "Abstract": "We advance AI capabilities by scaling two complementary paradigms: unsupervised learning and reasoning. These represent two axes of intelligence.\n\nUnsupervised learning increases world model accuracy and intuition. Models like GPT\u20113.5, GPT\u20114, and GPT\u20114.5 advance this paradigm.\nScaling reasoning\u2060, on the other hand, teaches models to think and produce a chain of thought before they respond, allowing them to tackle complex STEM or logic problems. Models like OpenAI o1 and OpenAI o3\u2011mini advance this paradigm.\nGPT\u20114.5 is an example of scaling unsupervised learning by scaling up compute and data, along with architecture and optimization innovations. GPT\u20114.5 was trained on Microsoft Azure AI supercomputers. The result is a model that has broader knowledge and a deeper understanding of the world, leading to reduced hallucinations and more reliability across a wide range of topics.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "Described by OpenAI as a \"new order of magnitude of compute\"\n\nhttps://openai.com/index/introducing-gpt-4-5/",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "339957479.93550694",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Claude 3.7 Sonnet",
      "Organization": "Anthropic",
      "Publication date": "2025-02-24",
      "Domain": "Language,Vision,Multimodal",
      "Task": "Language modeling/generation,Question answering,Code generation,Quantitative reasoning,Translation,Instruction interpretation,Visual question answering",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "3.35e+25",
      "Training compute notes": "https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://www.anthropic.com/news/claude-3-7-sonnet",
      "Reference": "Claude 3.7 Sonnet",
      "Citations": "",
      "Authors": "",
      "Abstract": "Today, we\u2019re announcing Claude 3.7 Sonnet1, our most intelligent model to date and the first hybrid reasoning model on the market. Claude 3.7 Sonnet can produce near-instant responses or extended, step-by-step thinking that is made visible to the user. API users also have fine-grained control over how long the model can think for.\n\nClaude 3.7 Sonnet shows particularly strong improvements in coding and front-end web development. ",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Grok 3",
      "Organization": "xAI",
      "Publication date": "2025-02-17",
      "Domain": "Language,Vision,Multimodal",
      "Task": "Chat,Language modeling/generation,Question answering,Code generation,Visual question answering",
      "Parameters": "3000000000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "3.5e+26",
      "Training compute notes": "Estimate based on a cluster of 80,000 H100s per the xai website and an estimated training time of approximately three months.\n\nFull estimate here: https://docs.google.com/document/d/1MIUFviULJ3YI_XjyzL8cwG0cBRANKNxVEB4DrUcFiNs/edit?usp=sharing",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://x.ai/blog/grok-3",
      "Reference": "Grok 3 Beta \u2014 The Age of Reasoning Agents",
      "Citations": "",
      "Authors": "",
      "Abstract": "We are pleased to introduce Grok 3, our most advanced model yet: blending strong reasoning with extensive pretraining knowledge. Trained on our Colossus supercluster with 10x the compute of previous state-of-the-art models, Grok 3 displays significant improvements in reasoning, mathematics, coding, world knowledge, and instruction-following tasks. Grok 3's reasoning capabilities, refined through large scale reinforcement learning, allow it to think for seconds to minutes, correcting errors, exploring alternatives, and delivering accurate answers. Grok 3 has leading performance across both academic benchmarks and real-world user preferences, achieving an Elo score of 1402 in the Chatbot Arena. Alongside it, we\u2019re unveiling Grok 3 mini, which represents a new frontier in cost-efficient reasoning. Both models are still in training and will evolve rapidly with your feedback. We are rolling out Grok 3 to users in the coming days, along with an early preview of its reasoning capabilities.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "2160.0",
      "Training time notes": "Estimated to be approximately 3 months. See compute estimate notes for more details.\n",
      "Training hardware": "NVIDIA H100 SXM5 80GB",
      "Hardware quantity": "80000.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "217835545.5267339",
      "Compute cost notes": "",
      "Training power draw (W)": "109948656.20196915",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "2446652193.900951",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "849737163.9529365",
      "Training compute cost (upfront)": "4446013316.520631"
    },
    {
      "Model": "Eurus-2-7B-PRIME",
      "Organization": "Tsinghua University,University of Illinois Urbana-Champaign (UIUC),Shanghai AI Lab,Peking University,Shanghai Jiao Tong University,CUHK Shenzhen Research Institute",
      "Publication date": "2025-02-03",
      "Domain": "Mathematics",
      "Task": "Mathematical reasoning,Code generation",
      "Parameters": "7000000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Eurus-2-RL-Data",
      "Training dataset size (gradients)": "830000",
      "Dataset size notes": "\"We finally obtain 230K SFT data and the average response length is 1390 tokens.\"\nDoes not include the RL finetuning data",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/2502.01456",
      "Reference": "Process Reinforcement through Implicit Rewards",
      "Citations": "221.0",
      "Authors": "Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, Jiarui Yuan, Huayu Chen, Kaiyan Zhang, Xingtai Lv, Shuo Wang, Yuan Yao, Xu Han, Hao Peng, Yu Cheng, Zhiyuan Liu, Maosong Sun, Bowen Zhou, Ning Ding",
      "Abstract": "Dense process rewards have proven a more effective alternative to the sparse outcome-level rewards in the inference-time scaling of large language models (LLMs), particularly in tasks requiring complex multi-step reasoning. While dense rewards also offer an appealing choice for the reinforcement learning (RL) of LLMs since their fine-grained rewards have the potential to address some inherent issues of outcome rewards, such as training efficiency and credit assignment, this potential remains largely unrealized. This can be primarily attributed to the challenges of training process reward models (PRMs) online, where collecting high-quality process labels is prohibitively expensive, making them particularly vulnerable to reward hacking. To address these challenges, we propose PRIME (Process Reinforcement through IMplicit rEwards), which enables online PRM updates using only policy rollouts and outcome labels through implict process rewards. PRIME combines well with various advantage functions and forgoes the dedicated reward model training phase that existing approaches require, substantially reducing the development overhead. We demonstrate PRIME\u2019s effectiveness on competitional math and coding. Starting from Qwen2.5-Math-7B-Base, PRIME achieves a 15.1% average improvement across several key reasoning benchmarks over the SFT model. Notably, our resulting model, Eurus-2-7B-PRIME, surpasses Qwen2.5-Math-7B-Instruct on seven reasoning benchmarks with 10% of its training data.1",
      "Organization categorization": "Academia,Academia,Academia,Academia,Academia",
      "Country (of organization)": "China,United States of America,China,China,China,China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Table 2\n\n\"Eurus-2-7B-PRIME excels at competition-level mathematics benchmarks, outperforming advanced math models and larger models.\"\n\n\"Eurus-2-7B-PRIME achieves 26.7%\npass@1 on AIME 2024, surpassing GPT-4o, Llama-3.1-70B-Instruct, and Qwen2.5-Math-7B-Instruct, demonstrating its excellent reasoning ability.\"",
      "Epochs": "3.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "Qwen2.5-Math-7B-Base",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache 2.0\nhttps://huggingface.co/PRIME-RL/Eurus-2-7B-PRIME\n\nApache 2.0\nhttps://github.com/PRIME-RL/PRIME",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "o3-mini",
      "Organization": "OpenAI",
      "Publication date": "2025-01-31",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering,Quantitative reasoning,Code generation",
      "Parameters": "",
      "Parameters notes": "Can't get an exact estimate, but we suspect total parameter count around 60B-120B, active parameters around 10B-30B. \n\nGiven these models are served at 150-200 tok/s, at $4.40/Mtok output, inference economics (https://epoch.ai/blog/inference-economics-of-language-models) suggests total parameter count around 60-120B parameters, with mixture-of-experts active parameters around 10-30B. MoEs make a given model roughly comparable to a ~50% smaller dense model (https://epoch.ai/gradient-updates/moe-vs-dense-models-inference), which lines up decently with Magistral Small pricing (24B dense, served at a similar speed for the cheaper $1.50/Mtok). ",
      "Training compute (FLOP)": "",
      "Training compute notes": "We can\u2019t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://openai.com/index/openai-o3-mini/",
      "Reference": "Pushing the frontier of cost-effective reasoning.",
      "Citations": "",
      "Authors": "Training\nBrian Zhang, Eric Mitchell, Hongyu Ren, Kevin Lu, Max Schwarzer, Michelle Pokrass, Shengjia Zhao, Ted Sanders\n\nEval\nAdam Kalai, Alex Tachard Passos, Ben Sokolowsky, Elaine Ya Le, Erik Ritter, Hao Sheng, Hanson Wang, Ilya Kostrikov, James Lee, Johannes Ferstad, Michael Lampe, Prashanth Radhakrishnan, Sean Fitzgerald, Sebastien Bubeck, Yann Dubois, Yu Bai\n\nFrontier Evals & Preparedness\nAndy Applebaum, Elizabeth Proehl, Evan Mays, Joel Parish, Kevin Liu, Leon Maksin, Leyton Ho, Miles Wang, Michele Wang, Olivia Watkins, Patrick Chao, Samuel Miserendino, Tejal Patwardhan\n\nEngineering\nAdam Walker, Akshay Nathan, Alyssa Huang, Andy Wang, Ankit Gohel, Ben Eggers, Brian Yu, Bryan Ashley, Chengdu Huang, Christian Hoareau, Davin Bogan, Emily Sokolova, Eric Horacek, Eric Jiang, Felipe Petroski Such, Jonah Cohen, Josh Gross, Justin Becker, Kan Wu, Kevin Whinnery, Larry Lv, Lee Byron, Manoli Liodakis, Max Johnson, Mike Trpcic, Murat Yesildal, Rasmus Rygaard, RJ Marsan, Rohit Ramchandani, Rohan Kshirsagar, Roman Huet, Sara Conlon, Shuaiqi (Tony) Xia, Siyuan Fu, Srinivas Narayanan, Sulman Choudhry, Tomer Kaftan, Trevor Creech\n\nSearch\nAdam Fry, Adam Perelman, Brandon Wang, Cristina Scheau, Philip Pronin, Sundeep Tirumalareddy, Will Ellsworth, Zewei Chu\n\nProduct\nAntonia Woodford, Beth Hoover, Jake Brill, Kelly Stirman, Minnia Feng, Neel Ajjarapu, Nick Turley, Nikunj Handa, Olivier Godement\n\nSafety\nAndrea Vallone, Andrew Duberstein, Enis Sert, Eric Wallace, Grace Zhao, Irina Kofman, Jieqi Yu, Joaquin Quinonero Candela, Madelaine Boyd, Mehmet Yatbaz, Mike McClay, Mingxuan Wang, Saachi Jain, Sandhini Agarwal, Sam Toizer, Santiago Hern\u00e1ndez, Steve Mostovoy, Young Cha, Tao Li, Yunyun Wang\n\nExternal Redteaming\nLama Ahmad, Troy Peterson\n\n\nResearch Program Managers\nCarpus Chang, Kristen Ying\n\nLeadership\nAidan Clark, Dane Stuckey, Jerry Tworek, Jakub Pachocki, Johannes Heidecke, Kevin Weil, Liam Fedus, Mark Chen, Sam Altman, Wojciech Zaremba",
      "Abstract": "We\u2019re releasing OpenAI o3-mini, the newest, most cost-efficient model in our reasoning series, available in both ChatGPT and the API today. Previewed in December 2024\u2060, this powerful and fast model advances the boundaries of what small models can achieve, delivering exceptional STEM capabilities\u2014with particular strength in science, math, and coding\u2014all while maintaining the low cost and reduced latency of OpenAI o1-mini.\n\nOpenAI o3-mini is our first small reasoning model that supports highly requested developer features including function calling\u2060(opens in a new window), Structured Outputs\u2060(opens in a new window), and developer messages\u2060(opens in a new window), making it production-ready out of the gate. Like OpenAI o1-mini and OpenAI o1-preview, o3-mini will support streaming\u2060(opens in a new window). Also, developers can choose between three reasoning effort\u2060(opens in a new window) options\u2014low, medium, and high\u2014to optimize for their specific use cases. This flexibility allows o3-mini to \u201cthink harder\u201d when tackling complex challenges or prioritize speed when latency is a concern. o3-mini does not support vision capabilities, so developers should continue using OpenAI o1 for visual reasoning tasks. o3-mini is rolling out in the Chat Completions API, Assistants API, and Batch API starting today to select developers in API usage tiers 3-5\u2060",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Significant use",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GPT-4o (Jan 2025)",
      "Organization": "OpenAI",
      "Publication date": "2025-01-29",
      "Domain": "Multimodal,Language,Audio,Speech,Vision",
      "Task": "Chat,Image generation,Audio generation,Vision-language generation,Table tasks,Language modeling/generation,Question answering,Speech recognition (ASR),Speech-to-text",
      "Parameters": "",
      "Parameters notes": "Not known.\n\nInference costs in the API are 2x cheaper than GPT-4 Turbo",
      "Training compute (FLOP)": "",
      "Training compute notes": "Training compute estimated to be 3.8e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Speculative",
      "Link": "https://help.openai.com/en/articles/9624314-model-release-notes",
      "Reference": "Updates to GPT-4o in ChatGPT (January 29, 2025)",
      "Citations": "",
      "Authors": "Aidan Clark, Alex Paino, Jacob Menick, Liam Fedus, Luke Metz, Clemens Winter, Lia Guy, Sam Schoenholz, Daniel Levy, Nitish Keskar, Alex Carney, Alex Paino, Ian Sohl, Qiming Yuan, Reimar Leike, Arka Dhar, Brydon Eastman, Mia Glaese, Ben Sokolowsky, Andrew Kondrich, Felipe Petroski Such, Henrique Ponde de Oliveira Pinto, Jiayi Weng, Randall Lin, Youlong Cheng, Nick Ryder, Lauren Itow, Barret Zoph, John Schulman, Mianna Chen, Adam Lerer, Adam P. Goucher, Adam Perelman, Akila Welihinda, Alec Radford, Alex Borzunov, Alex Carney, Alex Chow, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexi Christakis, Ali Kamali, Allison Moyer, Allison Tam, Amin Tootoonchian, Ananya Kumar, Andrej Karpathy, Andrey Mishchenko, Andrew Cann, Andrew Kondrich, Andrew Tulloch, Angela Jiang, Antoine Pelisse, Anuj Gosalia, Avi Nayak, Avital Oliver, Behrooz Ghorbani, Ben Leimberger, Ben Wang, Blake Samic, Brian Guarraci, Brydon Eastman, Camillo Lugaresi, Chak Li, Charlotte Barette, Chelsea Voss, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christopher Hesse, Colin Wei, Daniel Kappler, Daniel Levin, Daniel Levy, David Farhi, David Mely, David Sasaki, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Peterson, Eric Sigler, Eugene Brevdo, Farzad Khorasani, Francis Zhang, Gene Oden, Geoff Salmon, Hadi Salman, Haiming Bao, Heather Schmidt, Hongyu Ren, Hyung Won Chung, Ian Kivlichan, Ian O'Connell, Ian Osband, Ilya Kostrikov, Ingmar Kanitscheider, Jacob Coxon, James Crooks, James Lennon, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jiayi Weng, Jie Tang, Joanne Jang, Jonathan Ward, Jonathan McKay, Jong Wook Kim, Josh Gross, Josh Kaplan, Joy Jiao, Joyce Lee, Juntang Zhuang, Kai Fricke, Kavin Karthik, Kenny Hsu, Kiel Howe, Kyle Luther, Larry Kai, Lauren Itow, Leo Chen, Lia Guy, Lien Mamitsuka, Lilian Weng, Long Ouyang, Louis Feuvrier, Lukas Kondraciuk, Lukasz Kaiser, Lyric Doshi, Mada Aflak, Maddie Simens, Madeleine Thompson, Marat Dukhan, Marvin Zhang, Mateusz Litwin, Max Johnson, Mayank Gupta, Mia Glaese, Michael Janner, Michael Petrov, Michael Wu, Michelle Fradin, Michelle Pokrass, Miguel Oom Temudo de Castro, Mikhail Pavlov, Minal Khan, Mo Bavarian, Natalia Gimelshein, Natalie Staudacher, Nick Stathas, Nik Tezak, Nithanth Kudige, Noel Bundick, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivier Godement, Owen Campbell-Moore, Philip Pronin, Philippe Tillet, Rachel Lim, Rajan Troll, Randall Lin, Rapha gontijo lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Rob Honsby, Rohit Ramchandani, Rory Carmichael, Ruslan Nigmatullin, Ryan Cheu, Scott Gray, Sean Grove, Sean Metzger, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shuaiqi (Tony) Xia, Sonia Phene, Spencer Papay, Steve Coffey, Steve Lee, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tarun Gogineni, Ted Sanders, Thomas Cunninghman, Thomas Dimson, Thomas Raoux, Tianhao Zheng, Tina Kim, Todd Underwood, Tristan Heywood, Valerie Qi, Vinnie Monaco, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wojciech Zaremba, Yash Patil, Yilei, Qian, Yongjik Kim, Youlong Cheng, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, Yury Malkov",
      "Abstract": "We\u2019re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.\n\nGPT-4o (\u201co\u201d for \u201comni\u201d) is a step towards much more natural human-computer interaction\u2014it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement,Significant use",
      "Notability criteria notes": "Outperforms GPT-4 Turbo and other models on text and especially on multimodal benchmarks, such as MMLU, GPQA, HumanEval, MMMU, etc See Model Evaluations: https://openai.com/index/hello-gpt-4o/ \n\nGPT-4o is now the default model in ChatGPT, so it's one of the most widely used models.",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "GPT-4o (Nov 2024)",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "Definitely a new model, not a GPT-4 finetune",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Computer-Using Agent (CUA)",
      "Organization": "OpenAI",
      "Publication date": "2025-01-23",
      "Domain": "Vision,Language,Multimodal",
      "Task": "Instruction interpretation,System control",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://openai.com/index/computer-using-agent/",
      "Reference": "Powering Operator with Computer-Using Agent, a universal interface for AI to interact with the digital world.",
      "Citations": "",
      "Authors": "",
      "Abstract": "Today we introduced a research preview of Operator\u2060(opens in a new window), an agent that can go to the web to perform tasks for you. Powering Operator is Computer-Using Agent (CUA), a model that combines GPT\u20114o's vision capabilities with advanced reasoning through reinforcement learning. CUA is trained to interact with graphical user interfaces (GUIs)\u2014the buttons, menus, and text fields people see on a screen\u2014just as humans do. This gives it the flexibility to perform digital tasks without using OS-or web-specific APIs. \n\nCUA builds off of years of foundational research at the intersection of multimodal understanding and reasoning. By combining advanced GUI perception with structured problem-solving, it can break tasks into multi-step plans and adaptively self-correct when challenges arise. This capability marks the next step in AI development, allowing models to use the same tools humans rely on daily and opening the door to a vast range of new applications.\n\nWhile CUA is still early and has limitations, it sets new state-of-the-art benchmark results, achieving a 38.1% success rate on OSWorld for full computer use tasks, and 58.1% on WebArena and 87% on WebVoyager for web-based tasks. These results highlight CUA\u2019s ability to navigate and operate across diverse environments using a single general action space. ",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "SOTA at OSWorld",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Hosted access (no API)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Kimi k1.5",
      "Organization": "Moonshot",
      "Publication date": "2025-01-22",
      "Domain": "Multimodal,Language,Vision",
      "Task": "Language modeling/generation,Code generation,Quantitative reasoning,Question answering,Visual question answering,Translation,Image captioning,Visual puzzles",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/2501.12599v1",
      "Reference": "Kimi k1.5: Scaling Reinforcement Learning with LLMs",
      "Citations": "671.0",
      "Authors": "Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, Zonghan Yang",
      "Abstract": "Language model pretraining with next token prediction has proved effective for scaling compute but is limited to the amount of available training data. Scaling reinforcement learning (RL) unlocks a new axis for the continued improvement of artificial intelligence, with the promise that large language models (LLMs) can scale their training data by learning to explore with rewards. However, prior published work has not produced competitive results. In light of this, we report on the training practice of Kimi k1.5, our latest multi-modal LLM trained with RL, including its RL training techniques, multi-modal data recipes, and infrastructure optimization. Long context scaling and improved policy optimization methods are key ingredients of our approach, which establishes a simplistic, effective RL framework without relying on more complex techniques such as Monte Carlo tree search, value functions, and process reward models. Notably, our system achieves state-of-the-art reasoning performance across multiple benchmarks and modalities\u2014e.g., 77.5 on AIME, 96.2 on MATH 500, 94-th percentile on Codeforces, 74.9 on MathVista\u2014matching OpenAI\u2019s o1. Moreover, we present effective long2short methods that use long-CoT techniques to improve short-CoT models, yielding state-of-the-art short-CoT reasoning results\u2014e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 on LiveCodeBench\u2014outperforming existing short-CoT models such as GPT-4o and Claude Sonnet 3.5 by a large margin (up to +550%).",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Sota short-CoT performance, outperforming GPT-4o and Claude Sonnet 3.5 on AIME,  MATH-500, LiveCodeBench by a large margin (up to +550%)\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "https://github.com/MoonshotAI/Kimi-k1.5",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Doubao-1.5-pro",
      "Organization": "ByteDance",
      "Publication date": "2025-01-22",
      "Domain": "Language",
      "Task": "Language generation",
      "Parameters": "",
      "Parameters notes": "Not directly reported. We are told it is a MoE model, and that it matches the performance of a dense model trained on the same data, while using 1/7th of the activated parameters. Additionally they say \"The number of parameters of the Doubao dense model is also much smaller than that of Llama3.1-405B\", which suggests that the number of activated parameters on the forward pass is \"much less\" than 405B/7 = 58B parameters.",
      "Training compute (FLOP)": "",
      "Training compute notes": "The model appears to have been trained on 9T tokens; since we believe the MoE model uses \"much less\" than 58B parameters (see parameter notes), training compute is likely to be less than 6 * 9T * 58B = 3.132e24\n\nIt is possible the 9T token training run was for comparison sake against the dense model (they label it as \"doubao-MoE\", not doubao-1.5-pro), and that they continued training beyond this. They would need to train for at least 29T tokens to ",
      "Training dataset": "",
      "Training dataset size (gradients)": "9000000000000",
      "Dataset size notes": "9T tokens",
      "Confidence": "Confident",
      "Link": "https://team.doubao.com/zh/special/doubao_1_5_pro",
      "Reference": "Doubao-1.5-pro",
      "Citations": "",
      "Authors": "",
      "Abstract": "The model uses the MoE architecture and explores the ultimate balance between model performance and reasoning performance through integrated training-thinking design. Doubao-1.5-pro can use only a smaller activation parameter to exceed the performance of a first-class super-large pre-training model and achieve excellent results on multiple evaluation benchmarks.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "",
      "Epochs": "1.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Hosted access (no API)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Eagle 2",
      "Organization": "NVIDIA,Nanjing University,Tsinghua University,Hong Kong Polytechnic University,Johns Hopkins University,New York University (NYU)",
      "Publication date": "2025-01-20",
      "Domain": "Vision,Robotics,Language",
      "Task": "",
      "Parameters": "8930000000.0",
      "Parameters notes": "Table 4\n\nhttps://huggingface.co/nvidia/Eagle2-9B\n\"8.93B params\"\n",
      "Training compute (FLOP)": "4.7156e+22",
      "Training compute notes": "Appendix A. Compute: \"We show our training resource for Eagle2-9B in Tab. A. In actual development, we rarely iterate the Stage-1 model. Usually, we iterate Stage-1.5 once after iterating Stage-2 >10 times.\"\nAssume \">10 times\" -> 12\nAssume \"H100\" -> NVIDIA H100 SXM5 80GB\nAssume bf16\nAssume 0.4 utilization (NVIDIA in-house)\nH100 SXM5 performance = 989400000000000 FLOP/s = 9.894e14 FLOP/s\n\nStage 1:     (H100 * 128) * (2.5 hr * 1)\nStage 1.5:  (H100 * 256) * (28 hr * 2)\nStage 2:     (H100 * 256) * (6 hr * 12)\n\nStage 1:      0.4 * (9.894e14 FLOP/s * 128) * (3600 s / 1 hr) * (2.5 hr *1) ~= 4.56e20 FLOP\nStage 1.5:   0.4 * (9.894e14 FLOP/s * 256) * (3600 s / 1 hr) * (28 hr * 2) ~= 2.04e22 FLOP\nStage 2:      0.4 * (9.894e14 FLOP/s * 256) * (3600 s / 1 hr) * (6 hr * 12) ~= 2.63e22 FLOP\n\n(4.56e20 + 2.04e22 + 2.63e22) FLOP ~= 4.72e22 FLOP\n",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "Table 4: ",
      "Confidence": "Confident",
      "Link": "arxiv.org/abs/2501.14818",
      "Reference": "Eagle 2: Building Post-Training Data Strategies from Scratch for Frontier Vision-Language Models",
      "Citations": "35.0",
      "Authors": "Zhiqi Li, Guo Chen, Shilong Liu, Shihao Wang, Vibashan VS, Yishen Ji, Shiyi Lan, Hao Zhang, Yilin Zhao, Subhashree Radhakrishnan, Nadine Chang, Karan Sapra, Amala Sanjay Deshmukh, Tuomas Rintamaki, Matthieu Le, Ilia Karmanov, Lukas Voegtle, Philipp Fischer, De-An Huang, Timo Roman, Tong Lu, Jose M. Alvarez, Bryan Catanzaro, Jan Kautz, Andrew Tao, Guilin Liu, Zhiding Yu",
      "Abstract": "Recently, promising progress has been made by open-source vision-language models (VLMs) in bringing their capabilities closer to those of proprietary frontier models. However, most open-source models only publish their final model weights, leaving the critical details of data strategies and implementation largely opaque. In this work, we address VLM post-training from a data-centric perspective, showing the key role of data strategy in developing frontier VLMs. By studying and building our post-training data strategy from scratch, we share detailed insights into the development processes, aiming to benefit the development of competitive models for the open-source community. Our introduced data strategy, together with training recipes and model design, leads to a family of performant VLMs named Eagle2. Specifically, Eagle2-9B achieves state-of-the-art results across various multimodal benchmarks, matching certain competitive models with up to 70B parameters.",
      "Organization categorization": "Industry,Academia,Academia,Academia,Academia,Academia",
      "Country (of organization)": "United States of America,China,China,Hong Kong,United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Figure 1: \"Overview of Eagle2-9B\u2019s result across different multimodal benchmarks, in comparison to state-of-the-art open-source and commercial frontier models.\"\nClaim SOTA on OCRBench, InfoVQA, ChartQA (Test), MathVista, AI2D (Test), MMStar (relative to the selected open-source + commercial frontier models)\n\nTodo: check Table 7 to confirm they didn't just omit larger models from Figure 1\n\nVLM backbone of GR00T N1",
      "Epochs": "",
      "Training time (hours)": "130.5",
      "Training time notes": "Appendix A. Compute: \"We show our training resource for Eagle2-9B in Tab. A. In actual development, we rarely iterate the Stage-1 model. Usually, we iterate Stage-1.5 once after iterating Stage-2 >10 times.\nAssume \">10 times\" -> 12\nStage 1.0:  2.5 hr * 1\nStage 1.5   28 hr * 2\nStage 2.0:  6 hr * 12\n(2.5 hr * 1) + (28 hr * 2) + (6 hr * 12) = 130.5 hr\n ",
      "Training hardware": "NVIDIA H100 SXM5 80GB",
      "Hardware quantity": "256.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "352055.152733459",
      "Base model": "Qwen2.5-7B",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "https://huggingface.co/nvidia/Eagle2-9B\n\"Creative Commons Attribution Non Commercial 4.0\"\n\nApache 2.0 for code\nhttps://github.com/NVlabs/EAGLE",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "7988460.174824487",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DeepSeek-R1",
      "Organization": "DeepSeek",
      "Publication date": "2025-01-20",
      "Domain": "Language",
      "Task": "Language modeling/generation,Code generation,Quantitative reasoning,Question answering",
      "Parameters": "671000000000.0",
      "Parameters notes": "671B total\n37B activated\nhttps://github.com/deepseek-ai/DeepSeek-R1/tree/main",
      "Training compute (FLOP)": "4.020010000000001e+24",
      "Training compute notes": "Estimates by Ege Erdil in Gradient Updates:\nhttps://epoch.ai/gradient-updates/what-went-into-training-deepseek-r1\n\"A dataset size of 14.8 trillion tokens is reasonable and in line with other models of this scale. Assuming that\u2019s valid, the pretraining of this model would have required 6 * (37 billion) * (14.8 trillion) = 3e24 FLOP. If we assume DeepSeek\u2019s training cluster consists of H800s with the PCIe form factor, then each should be capable of 1.5e15 FP8 per second, and the implied model FLOP utilization (MFU) of DeepSeek v3\u2019s 55 day training run ends up being around 23%.\"\n\n6 FLOP/token/param * 14.8T tokens * 37B active params = 3.29e24 FLOP (pretraining)\n1.2e23 FLOP (post-training)\n6.1e23 FLOP (fine-tuning)\n\nTotal compute: 3.29e24 + 1.2e23 + 6.1e23 = 4.02e24",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://api-docs.deepseek.com/news/news250120",
      "Reference": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
      "Citations": "",
      "Authors": "",
      "Abstract": "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.\nThrough RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "Training cost,SOTA improvement",
      "Notability criteria notes": "Best score on SuperCLUE Math6o in Jan 2025\nhttps://www.superclueai.com/",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "6770000.0",
      "Compute cost notes": "$5.576M 2024 USD (estimated training cost of Deepseek v3) + $1M (estimated training cost of RL) = $6.576M 2024 USD = $6.77M 2023 USD\n\nhttps://www.usinflationcalculator.com/\n\nhttps://epoch.ai/gradient-updates/what-went-into-training-deepseek-r1",
      "Training power draw (W)": "",
      "Base model": "DeepSeek-V3",
      "Finetune compute (FLOP)": "6.1e+23",
      "Finetune compute notes": "6.1e23 FLOP from these estimations: https://epoch.ai/gradient-updates/what-went-into-training-deepseek-r1",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "MIT licensed\nhttps://huggingface.co/deepseek-ai/DeepSeek-R1",
      "Numerical format": "FP8",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "INTELLECT-MATH",
      "Organization": "Prime Intellect",
      "Publication date": "2025-01-17",
      "Domain": "Mathematics",
      "Task": "Mathematical reasoning",
      "Parameters": "7000000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "\"The resulting SFT dataset consisted of 733k questions with corresponding responses.\"",
      "Confidence": "Confident",
      "Link": "https://huggingface.co/PrimeIntellect/INTELLECT-MATH\nhttps://www.primeintellect.ai/blog/intellect-math",
      "Reference": "INTELLECT-MATH: Frontier Mathematical Reasoning through Better Initializations for Reinforcement Learning\n",
      "Citations": "",
      "Authors": "",
      "Abstract": "INTELLECT-MATH is a 7B parameter model optimized for mathematical reasoning. It was trained in two stages, an SFT stage, in which the model was fine-tuned on verified QwQ outputs, and an RL stage, in which the model was trained using the PRIME-RL recipe.\n\nWe demonstrate that the quality of our SFT data can impact the performance and training speed of the RL stage: Due to its better synthetic SFT dataset that encourages the model to imitate the reasoning behavior of a strong teacher model, INTELLECT-MATH outperforms Eurus-2-PRIME, the previous state-of-the-art trained with PRIME-RL, and matches its performance with 10x faster training.\n\n",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Outperforms other models on several benchmarks - Math-500, OlympiadBench, AMC, MINERVA MATH, AVG.\n\nSOTA among same size models:\n\"a model that outperforms Eurus-2-7B-PRIME on several mathematical reasoning benchmarks and is the best 7B parameter model for mathematical reasoning (prior to the release of Deepseek-R1).\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "Qwen2.5-Math-7B-Base",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "",
      "Accessibility notes": "MIT license\nhttps://huggingface.co/PrimeIntellect/INTELLECT-MATH\n\nhttps://github.com/PrimeIntellect-ai/INTELLECT-MATH",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Stable Point Aware 3D (SPAR3D)",
      "Organization": "Stability AI,University of Illinois Urbana-Champaign (UIUC)",
      "Publication date": "2025-01-08",
      "Domain": "3D modeling",
      "Task": "3D reconstruction",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/2501.04689",
      "Reference": "SPAR3D: Stable Point-Aware Reconstruction of 3D Objects from Single Images",
      "Citations": "",
      "Authors": "Zixuan Huang, Mark Boss, Aaryaman Vasishta, James M. Rehg, Varun Jampani",
      "Abstract": "We study the problem of single-image 3D object reconstruction. Recent works have diverged into two directions: regression-based modeling and generative modeling. Regression methods efficiently infer visible surfaces, but struggle with occluded regions. Generative methods handle uncertain regions better by modeling distributions, but are computationally expensive and the generation is often misaligned with visible surfaces. In this paper, we present SPAR3D, a novel two-stage approach aiming to take the best of both directions. The first stage of SPAR3D generates sparse 3D point clouds using a lightweight point diffusion model, which has a fast sampling speed. The second stage uses both the sampled point cloud and the input image to create highly detailed meshes. Our two-stage design enables probabilistic modeling of the ill-posed single-image 3D task while maintaining high computational efficiency and great output fidelity. Using point clouds as an intermediate representation further allows for interactive user edits. Evaluated on diverse datasets, SPAR3D demonstrates superior performance over previous state-of-the-art methods, at an inference speed of 0.7 seconds. Project page with code and model: this https URL",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Table 1. Quantitative Comparisons on GSO [15]. SPAR3D performs favorably to other state-of-the-art methods\n\nTable 2. Quantitative Comparisons on OmniObject3D [63]. SPAR3D performs favorably to other state-of-the-art methods.",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open access (restricted use)",
      "Accessibility notes": "This model is free for both commercial and non-commercial use under the permissive Stability AI Community License. \n\nhttps://huggingface.co/stabilityai/stable-point-aware-3d\n\nhttps://github.com/Stability-AI/stable-point-aware-3d",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "STORM-B/8",
      "Organization": "University of Southern California,Georgia Institute of Technology,Stanford University,NVIDIA",
      "Publication date": "2024-12-31",
      "Domain": "3D modeling",
      "Task": "3D reconstruction,3D segmentation",
      "Parameters": "100598707.0",
      "Parameters notes": "A.3 Baseline Implementations: \"The number of trainable parameters of these models are controlled to be similar, i.e., GS-LRM has 86.68M parameters, LGM has 103.29M parameters, while our default STORM has 100.60M parameters.\n\n3.3 Implementation, Model architecture: \"By default, we use a 12-layer Vision Transformer (ViT-B) (Dosovitskiy, 2020) with full attention and a patch size of 8, along with M = 16 motion tokens.\"\n\nhttps://github.com/NVlabs/GaussianSTORM?tab=readme-ov-file#training\nTraining: \"Multi-GPU example that reproduces the paper's STORM-B/8 model:\n\nhttps://github.com/NVlabs/GaussianSTORM/blob/main/storm/models/storm.py\ndef STORM_B_8(**kwargs):\n    return STORM(patch_size=8, embed_dim=768, depth=12, num_heads=12, **kwargs)\ndef STORM_L_8(**kwargs):\n    return STORM(patch_size=8, embed_dim=1024, depth=24, num_heads=16, **kwargs)\ndef STORM_B_16(**kwargs):\n    return STORM(patch_size=16, embed_dim=768, depth=12, num_heads=12, **kwargs)\ndef STORM_L_16(**kwargs):\n    return STORM(patch_size=16, embed_dim=1024, depth=24, num_heads=16, **kwargs)\ndef STORM_XL_8(**kwargs):\n    return STORM(patch_size=8, embed_dim=1152, depth=28, num_heads=16, **kwargs)\ndef STORM_H_8(**kwargs):\n    return STORM(patch_size=8, embed_dim=1280, depth=32, num_heads=16, **kwargs)\ndef STORM_H_16(**kwargs):\n    return STORM(patch_size=16, embed_dim=1280, depth=32, num_heads=16, **kwargs)\n\nPyTorch model.parameters(): \n\nwith \"dummy decoder\":\nSTORM-B/8:  100.60M (100,598,707)\nSTORM-L/8:  327.08M (327,081,139)\nSTORM-XL/8: 476.55M (476,547,379)\nSTORM-H/8:  665.91M (665,905,587)\nSTORM-B/16: 103.42M (103,417,907)\nSTORM-L/16: 330.82M (330,817,331)\nSTORM-H/16: 670.56M (670,558,771)\n\nwith \"conv decoder\", i.e. \"Latent-STORM\" (adds ~30M):\nSTORM-B/8:  133.23M (133,230,019)\nSTORM-L/8:  359.13M (359,125,699)\nSTORM-XL/8: 508.30M (508,298,563)\nSTORM-H/8:  697.36M (697,363,395)\nSTORM-B/16: 134.21M (134,211,523)\nSTORM-L/16: 360.43M (360,434,371)\nSTORM-H/16: 699.00M (698,999,235)",
      "Training compute (FLOP)": "",
      "Training compute notes": "A.1 STORM implementation details, Training: \"We train our model for 100,000 iterations with a global batch size of 64 on NVIDIA A100 GPUs... Gradient checkpointing is enabled by default to reduce memory usage. Behind the scene, we observe that STORM benefits from longer training durations and larger model sizes. We maintain the default setup to ensure alignment with our baseline in this work. However, an attractive direction for future work is to explore the scaling laws of STORM.\"\n\nassume 100,000 iterations -> 100,000 (global) batch\n64 examples/batch * 100e3 batch = 6.4e6 examples",
      "Training dataset": "Waymo Open Dataset",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "4 Experiments, Datasets: \"We primarily conduct experiments on the Waymo Open Dataset (Sun et al., 2020), which contains 1,000 sequences of driving logs: 798 sequences for training and 202 for validation. Each sequence consists of a 20-second video recorded at 10FPS.\"\n\n10 frames/s * 20 s/scene = 200 frames/scene\n200 frames/scene *  798 scene  =  159600 frames (training)\n\nscene range: [0.1,20] s, or [1,200] frames\n\nSampling scheme:\n3.3 Implementation, Supervision and loss functions: \"During training, we randomly select a starting timestep t and sample 4 target timesteps t\u2032 within the range [t, t+2s] for supervision.\"\n4 Experiments, Datasets: \"The input to our model consists of 4 context timesteps, evenly spaced at t+0s, t+0.5s, t+1.0s, and t+1.5s, where t is a randomly chosen starting timestep.\"\n\n-> t may be chosen in the range [0.1,180] s, or [1,180] frame (i.e. 180 possibilities)",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2501.00602\nhttps://github.com/NVlabs/GaussianSTORM\nhttps://jiawei-yang.github.io/STORM",
      "Reference": "STORM: Spatio-Temporal Reconstruction Model for Large-Scale Outdoor Scenes",
      "Citations": "25.0",
      "Authors": "Jiawei Yang, Jiahui Huang, Yuxiao Chen, Yan Wang, Boyi Li, Yurong You, Apoorva Sharma, Maximilian Igl, Peter Karkus, Danfei Xu, Boris Ivanovic, Yue Wang, Marco Pavone",
      "Abstract": "We present STORM, a spatio-temporal reconstruction model designed for reconstructing dynamic outdoor scenes from sparse observations. Existing dynamic reconstruction methods often rely on per-scene optimization, dense observations across space and time, and strong motion supervision, resulting in lengthy optimization times, limited generalization to novel views or scenes, and degenerated quality caused by noisy pseudo-labels for dynamics. To address these challenges, STORM leverages a data-driven Transformer architecture that directly infers dynamic 3D scene representations--parameterized by 3D Gaussians and their velocities--in a single forward pass. Our key design is to aggregate 3D Gaussians from all frames using self-supervised scene flows, transforming them to the target timestep to enable complete (i.e., \"amodal\") reconstructions from arbitrary viewpoints at any moment in time. As an emergent property, STORM automatically captures dynamic instances and generates high-quality masks using only reconstruction losses. Extensive experiments on public datasets show that STORM achieves precise dynamic scene reconstruction, surpassing state-of-the-art per-scene optimization methods (+4.3 to 6.6 PSNR) and existing feed-forward approaches (+2.1 to 4.7 PSNR) in dynamic regions. STORM reconstructs large-scale outdoor scenes in 200ms, supports real-time rendering, and outperforms competitors in scene flow estimation, improving 3D EPE by 0.422m and Acc5 by 28.02%. Beyond reconstruction, we showcase four additional applications of our model, illustrating the potential of self-supervised learning for broader dynamic scene understanding.",
      "Organization categorization": "Academia,Academia,Academia,Industry",
      "Country (of organization)": "United States of America,United States of America,United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Abstract: \"Extensive experiments on public datasets show that STORM achieves precise dynamic scene reconstruction, surpassing state-of-the-art per-scene optimization methods (+4.3 to 6.6 PSNR) and existing feed-forward approaches (+2.1 to 4.7 PSNR) in dynamic regions. STORM reconstructs large-scale outdoor scenes in 200ms, supports real-time rendering, and outperforms competitors in scene flow estimation, improving 3D EPE by 0.422m and Acc5 by 28.02%.\"\n\nSee Tables 1,2,3 - comparisons to EmerNeRF, 3DGS, PVG, DeformableGS, LGM, GS-LRM, NSFP, NSFP++ on Waymo, NuScenes, and Argoverse2 datasets.",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "16.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "12578.999592690692",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "64.0",
      "Batch size notes": "Global batch size is 64; repo suggests per-GPU split of 4",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "TF32",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DeepSeek-V3",
      "Organization": "DeepSeek",
      "Publication date": "2024-12-24",
      "Domain": "Language",
      "Task": "Language modeling/generation,Code generation,Quantitative reasoning,Question answering",
      "Parameters": "671000000000.0",
      "Parameters notes": "Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.",
      "Training compute (FLOP)": "3.4078e+24",
      "Training compute notes": "\"At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.\"\n\n6 * 37B (active params) * 14.8T = 3.2856e24 for pretraining.\n\nWe know they trained in FP8. H800s get 1.513e15 FLOP/s in FP8:\n2.688M * 3600 * 1.513e15 * MFU = 3.2856e24\n\nSuggests a MFU of 0.2244 in pre-training. If we assume MFU was the same in post-training, that adds an additional:\n\n0.1M * 3600 * 1.513e15 * 0.2244 = 1.222e23 FLOP from post-training\n\nTotal: 3.2856e24 + 1.222e23 = 3.4078e24 FLOP",
      "Training dataset": "",
      "Training dataset size (gradients)": "14800000000000",
      "Dataset size notes": "\"We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2412.19437",
      "Reference": "DeepSeek-V3 Technical Report",
      "Citations": "",
      "Authors": "DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J.L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R.J. Chen, R.L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S.S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W.L. Xiao, Wangding Zeng et al. (100 additional authors not shown)",
      "Abstract": "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "training cost was $5.3million USD (Table 1)",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "\"DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training\"",
      "Training hardware": "NVIDIA H800 SXM5",
      "Hardware quantity": "2048.0",
      "Hardware utilization (MFU)": "0.1947",
      "Training compute cost (2023 USD)": "5390000.0",
      "Compute cost notes": "Table 1\n$5.576M 2024 USD = $5.39M 2023 USD (https://www.usinflationcalculator.com/)",
      "Training power draw (W)": "2818135.1812142837",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "62914560.0",
      "Batch size notes": "15360 * 4096",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "MIT and deepseek license\nhttps://github.com/deepseek-ai/DeepSeek-V3?tab=readme-ov-file\n\nI cannot see training code in this repo https://github.com/deepseek-ai/DeepSeek-V3?tab=readme-ov-file",
      "Numerical format": "FP8",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "o3",
      "Organization": "OpenAI",
      "Publication date": "2024-12-20",
      "Domain": "Language,Vision,Multimodal",
      "Task": "Language modeling/generation,Question answering,Quantitative reasoning,Code generation,Visual question answering,Search,Instruction interpretation,Visual puzzles",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://openai.com/index/introducing-o3-and-o4-mini/",
      "Reference": "Our most powerful reasoning model with leading performance on coding, math, science, and vision",
      "Citations": "",
      "Authors": "",
      "Abstract": "Today, we\u2019re releasing OpenAI o3 and o4-mini, the latest in our o-series of models trained to think for longer before responding. These are the smartest models we\u2019ve released to date, representing a step change in ChatGPT's capabilities for everyone from curious users to advanced researchers. For the first time, our reasoning models can agentically use and combine every tool within ChatGPT\u2014this includes searching the web, analyzing uploaded files and other data with Python, reasoning deeply about visual inputs, and even generating images. Critically, these models are trained to reason about when and how to use tools to produce detailed and thoughtful answers in the right output formats, typically in under a minute, to solve more complex problems. This allows them to tackle multi-faceted questions more effectively, a step toward a more agentic ChatGPT that can independently execute tasks on your behalf. The combined power of state-of-the-art reasoning with full tool access translates into significantly stronger performance across academic benchmarks and real-world tasks, setting a new standard in both intelligence and usefulness.\n<..>\nOpenAI o3 is our most powerful reasoning model that pushes the frontier across coding, math, science, visual perception, and more. It sets a new SOTA on benchmarks including Codeforces, SWE-bench (without building a custom model-specific scaffold), and MMMU. It\u2019s ideal for complex queries requiring multi-faceted analysis and whose answers may not be immediately obvious. It performs especially strongly at visual tasks like analyzing images, charts, and graphics. In evaluations by external experts, o3 makes 20 percent fewer major errors than OpenAI o1 on difficult, real-world tasks\u2014especially excelling in areas like programming, business/consulting, and creative ideation. Early testers highlighted its analytical rigor as a thought partner and emphasized its ability to generate and critically evaluate novel hypotheses\u2014particularly within biology, math, and engineering contexts.\n\n________\nmodel was announced 2024/12/20 \nfrom ARS technica: \"On Friday, during Day 12 of its \"12 days of OpenAI,\" OpenAI CEO Sam Altman announced its latest AI \"reasoning\" models, o3 and o3-mini, which build upon the o1 models launched earlier this year. The company is not releasing them yet but will make these models available for public safety testing and research access today.\"\n\nhttps://arstechnica.com/information-technology/2024/12/openai-announces-o3-and-o3-mini-its-next-simulated-reasoning-models/\n\nmodel was released 2025/04/16",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Significant use",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "\"Both o3 and o4-mini are also available to developers today via the Chat Completions API and Responses API (some developers will need to verify their organizations\u2060(opens in a new window) to access these models)\"",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Veo 2",
      "Organization": "Google DeepMind",
      "Publication date": "2024-12-16",
      "Domain": "Video,Vision",
      "Task": "Video generation,Text-to-video,Image-to-video",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://deepmind.google/technologies/veo/veo-2/",
      "Reference": "Our state-of-the-art video generation model",
      "Citations": "",
      "Authors": "Agrim Gupta, Ali Razavi, Ankush Gupta, Dumitru Erhan, Eric Lau, Frank Belletti, Gabe Barth-Maron, Hakan Erdogan, Hakim Sidahmed, Henna Nandwani, Hernan Moraldo, Hyunjik Kim, Jeff Donahue, Jos\u00e9 Lezama, Kory Mathewson, Kurtis David, Marc van Zee, Medhini Narasimhan, Miaosen Wang, Mohammad Babaeizadeh, Nelly Papalampidi, Nick Pezzotti, Nilpa Jha, Parker Barnes, Pieter-Jan Kindermans, Rachel Hornung, Ruben Villegas, Ryan Poplin, Salah Zaiem, Sander Dieleman, Sayna Ebrahimi, Scott Wisdom, Serena Zhang, Shlomi Fruchter, Weizhe Hua, Xinchen Yan, Yuqing Du and Yutian Chen.",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Veo has achieved state of the art results in head-to-head comparisons of outputs by human raters over top video generation models.\n\nParticipants viewed 1003 prompts and respective videos on MovieGenBench, a benchmark dataset released by Meta. Veo 2 performs best on overall preference, and for its capability to follow prompts accurately.\"\n\nSOTA qualification is unclear solely from MovieGenBench, which is subjective and depends on human raters. But Veo 2 seems to be SOTA over Meta Movie Gen, Kling, Minimax, and Sora Turbo.\nUpdated Sora could be better, but was released later this same month.",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "https://cloud.google.com/vertex-ai/generative-ai/docs/models/veo/2-0-generate-001",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Apollo 7B",
      "Organization": "Meta AI,Stanford University",
      "Publication date": "2024-12-13",
      "Domain": "Video,Language,Multimodal",
      "Task": "Video description",
      "Parameters": "7000000000.0",
      "Parameters notes": "\"We employed the Qwen2.5 (Yang et al., 2024) series of Large Language Models (LLMs) at varying scales to serve as the backbone for Apollo. Specifically, we utilized models with 1.5B, 3B, and 7B parameters. Following our analysis in Sec. 4, we used a SigLIP-SO400M (Zhai et al., 2023) encoder combined with an InternVideo2 (Wang et al., 2024d) video encoder\"\n\nthere is a confusion with Qwen2.5 link - it points to Qwen 2 paper instead of Qwen 2.5 release notes",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "1. Alignment: In this phase, we trained on a 198K mixture of 50/50 image and video captions.\n2. Vision Pretraining: We tuned the encoders using a video-only caption dataset of 396K samples.\n3. Supervised Fine-tuning (SFT): We trained on a mixture of text, image, multi-image, and video data, with a total of 3.2 million samples.\n\n3200000 videos *36%*~40 sec*~128 tokens / sec = 5898240000 video tokens during SFT\n\nassuming about the same amount of image and text tokens. total size of the dataset was likely around 12B tokens\n",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2412.10360",
      "Reference": "Apollo: An Exploration of Video Understanding in Large Multimodal Models",
      "Citations": "",
      "Authors": "Orr Zohar, Xiaohan Wang, Yann Dubois, Nikhil Mehta, Tong Xiao, Philippe Hansen-Estruch, Licheng Yu, Xiaofang Wang, Felix Juefei-Xu, Ning Zhang, Serena Yeung-Levy, Xide Xia",
      "Abstract": "Despite the rapid integration of video perception capabilities into Large Multimodal Models (LMMs), the underlying mechanisms driving their video understanding remain poorly understood. Consequently, many design decisions in this domain are made without proper justification or analysis. The high computational cost of training and evaluating such models, coupled with limited open research, hinders the development of video-LMMs. To address this, we present a comprehensive study that helps uncover what effectively drives video understanding in LMMs.\nWe begin by critically examining the primary contributors to the high computational requirements associated with video-LMM research and discover Scaling Consistency, wherein design and training decisions made on smaller models and datasets (up to a critical size) effectively transfer to larger models. Leveraging these insights, we explored many video-specific aspects of video-LMMs, including video sampling, architectures, data composition, training schedules, and more. For example, we demonstrated that fps sampling during training is vastly preferable to uniform frame sampling and which vision encoders are the best for video representation.\nGuided by these findings, we introduce Apollo, a state-of-the-art family of LMMs that achieve superior performance across different model sizes. Our models can perceive hour-long videos efficiently, with Apollo-3B outperforming most existing 7B models with an impressive 55.1 on LongVideoBench. Apollo-7B is state-of-the-art compared to 7B LMMs with a 70.9 on MLVU, and 63.3 on Video-MME.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Apollo-7B is state-of-the-art compared to 7B LMMs with a 70.9 on MLVU, and 63.3 on Video-MME",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "128.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "100672.34303072089",
      "Base model": "Qwen2.5-7B,SigLIP 400M",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Gemini 2.0 Pro",
      "Organization": "Google DeepMind",
      "Publication date": "2024-12-11",
      "Domain": "Language,Multimodal,Vision,Video,Audio",
      "Task": "Code generation,Language modeling/generation,Question answering,Visual question answering,Speech recognition (ASR),Video description",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "Flagship model from a leading developer in early 2025; very likely it used >1e25 FLOP.",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://deepmind.google/technologies/gemini/pro/",
      "Reference": "Our best model yet for coding performance and complex prompts",
      "Citations": "",
      "Authors": "Gemini Team",
      "Abstract": "Today, we\u2019re releasing an experimental version of Gemini 2.0 Pro that responds to that feedback. It has the strongest coding performance and ability to handle complex prompts, with better understanding and reasoning of world knowledge, than any model we\u2019ve released so far. It comes with our largest context window at 2 million tokens, which enables it to comprehensively analyze and understand vast amounts of information, as well as the ability to call tools like Google Search and code execution.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Hosted access (no API)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Sora Turbo",
      "Organization": "OpenAI",
      "Publication date": "2024-12-09",
      "Domain": "Video,Vision",
      "Task": "Video generation,Text-to-video",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://openai.com/index/sora-is-here/",
      "Reference": "Sora is here",
      "Citations": "",
      "Authors": "",
      "Abstract": "Our video generation model is rolling out at sora.com\u2060.\n\nEarlier this year, we introduced Sora\u2060, our model that can create realistic videos from text, and shared our initial research progress\u2060 on world simulation. Sora serves as a foundation for AI that understands and simulates reality\u2014an important step towards developing models that can interact with the physical world.\n\nWe developed a new version of Sora\u2014Sora Turbo\u2014that is significantly faster than the model we previewed in February. We\u2019re releasing it today as a standalone product at Sora.com to ChatGPT Plus and Pro users.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Significant use",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Hosted access (no API)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "EXAONE 3.5 32B",
      "Organization": "LG AI Research",
      "Publication date": "2024-12-09",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering,Translation",
      "Parameters": "32000000000.0",
      "Parameters notes": "32B",
      "Training compute (FLOP)": "1.25e+24",
      "Training compute notes": "1.25 \u00d7 10^24 (Table 2) ",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "6500000000000",
      "Dataset size notes": "6.5T tokens (Table 2)",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2412.04862",
      "Reference": "EXAONE 3.5: Series of Large Language Models for Real-world Use Cases",
      "Citations": "",
      "Authors": "Soyoung An, Kyunghoon Bae, Eunbi Choi, Kibong Choi, Stanley Jungkyu Choi, Seokhee Hong, Junwon Hwang, Hyojin Jeon, Gerrard Jeongwon Jo, Hyunjik Jo, Jiyeon Jung, Yountae Jung, Hyosang Kim, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Yongil Kim, Youchul Kim, Edward Hwayoung Lee, Haeju Lee, Honglak Lee, Jinsik Lee, Kyungmin Lee, Woohyung Lim, Sangha Park, Sooyoun Park, Yongmin Park, Sihoon Yang, Heuiyeen Yeen, Hyeongu Yun",
      "Abstract": "This technical report introduces the EXAONE 3.5 instruction-tuned language models, developed and released by LG AI Research. The EXAONE 3.5 language models are offered in three configurations: 32B, 7.8B, and 2.4B. These models feature several standout capabilities: 1) exceptional instruction following capabilities in real-world scenarios, achieving the highest scores across seven benchmarks, 2) outstanding long-context comprehension, attaining the top performance in four benchmarks, and 3) competitive results compared to state-of-the-art open models of similar sizes across nine general benchmarks. The EXAONE 3.5 language models are open to anyone for research purposes and can be downloaded from this https URL. For commercial use, please reach out to the official contact point of LG AI Research: contact_us@lgresearch.ai.",
      "Organization categorization": "Industry",
      "Country (of organization)": "Korea (Republic of)",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "Not listed",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Exaone license (allows only non-commercial usage)",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Llama 3.3 70B",
      "Organization": "Meta AI",
      "Publication date": "2024-12-06",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering,Translation,Code generation",
      "Parameters": "70000000000.0",
      "Parameters notes": "70B",
      "Training compute (FLOP)": "6.8649768e+24",
      "Training compute notes": "6ND = 6 FLOP / parameter / token * 70*10^9 parameters * 15*10^12 tokens = 6.3e+24 FLOP\n\n7000000 GPU-hours * 3600 sec / hour * 989500000000000 FLOP / second * 0.3 [assumed utilization]= 7.48062e+24 FLOP\n\nsqrt(7.48062e+24*6.3e+24) = 6.8649768e+24",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "15000000000000",
      "Dataset size notes": "\"Overview: Llama 3.3 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples.\n\nData Freshness: The pretraining data has a cutoff of December 2023.\"",
      "Confidence": "Confident",
      "Link": "https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_3/",
      "Reference": "Meta Llama 3.3 multilingual large language model (LLM) ",
      "Citations": "",
      "Authors": "",
      "Abstract": "The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperforms many of the available open source and closed chat models on common industry benchmarks.\n\nModel developer: Meta\n\nModel Architecture: Llama 3.3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "\"Training utilized a cumulative of 39.3M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.\"\n\nLlama 3.3 70B: Training Time (GPU hours): 7M\n",
      "Training hardware": "NVIDIA H100 SXM5 80GB",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "License A custom commercial license, the Llama 3.3 Community License Agreement, is available at: https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/LICENSE\n\n\"Llama 3.3 is intended for commercial and research use in multiple languages.\"",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "o1",
      "Organization": "OpenAI",
      "Publication date": "2024-12-05",
      "Domain": "Language,Mathematics,Multimodal",
      "Task": "Code generation,Language modeling/generation,Quantitative reasoning,Chat,Question answering,Translation,Mathematical reasoning",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://openai.com/index/introducing-chatgpt-pro/",
      "Reference": "Introducing ChatGPT Pro: Broadening usage of frontier AI.",
      "Citations": "",
      "Authors": "",
      "Abstract": "We've developed a new series of AI models designed to spend more time thinking before they respond. They can reason through complex tasks and solve harder problems than previous models in science, coding, and math.\n\nToday, we are releasing the first of this series in ChatGPT and our API. This is a preview and we expect regular updates and improvements. Alongside this release, we\u2019re also including evaluations for the next update, currently in development.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement,Significant use",
      "Notability criteria notes": "SOTA in GPQA among others: https://openai.com/index/learning-to-reason-with-llms/ \n\n\"we recruited experts with PhDs to answer GPQA-diamond questions. We found that o1 surpassed the performance of those human experts, becoming the first model to do so on this benchmark.\"\n\n\"With its vision perception capabilities enabled, o1 scored 78.2% on MMMU, making it the first model to be competitive with human experts.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "NVILA 15B",
      "Organization": "NVIDIA,Massachusetts Institute of Technology (MIT),University of California (UC) Berkeley,University of California San Diego,University of Washington,Tsinghua University",
      "Publication date": "2024-12-05",
      "Domain": "Vision,Language,Multimodal,Video",
      "Task": "Visual question answering,Video description,Language modeling/generation,Question answering,Character recognition (OCR)",
      "Parameters": "15000000000.0",
      "Parameters notes": "15B",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Docmatrix,PDFA,COYO-700M,ShareGPT4V,MMC4 / Multimodal C4",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2412.04468",
      "Reference": "NVILA: Efficient Frontier Visual Language Models",
      "Citations": "",
      "Authors": "Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, Xiuyu Li, Yunhao Fang, Yukang Chen, Cheng-Yu Hsieh, De-An Huang, An-Chieh Cheng, Vishwesh Nath, Jinyi Hu, Sifei Liu, Ranjay Krishna, Daguang Xu, Xiaolong Wang, Pavlo Molchanov, Jan Kautz, Hongxu Yin, Song Han, Yao Lu",
      "Abstract": "Visual language models (VLMs) have made significant advances in accuracy in recent years. However, their efficiency has received much less attention. This paper introduces NVILA, a family of open VLMs designed to optimize both efficiency and accuracy. Building on top of VILA, we improve its model architecture by first scaling up the spatial and temporal resolutions, and then compressing visual tokens. This \"scale-then-compress\" approach enables NVILA to efficiently process high-resolution images and long videos. We also conduct a systematic investigation to enhance the efficiency of NVILA throughout its entire lifecycle, from training and fine-tuning to deployment. NVILA matches or surpasses the accuracy of many leading open and proprietary VLMs across a wide range of image and video benchmarks. At the same time, it reduces training costs by 4.5X, fine-tuning memory usage by 3.4X, pre-filling latency by 1.6-2.2X, and decoding latency by 1.2-2.8X. We will soon make our code and models available to facilitate reproducibility.",
      "Organization categorization": "Industry,Academia,Academia,Academia,Academia,Academia",
      "Country (of organization)": "United States of America,United States of America,United States of America,United States of America,United States of America,China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Table 8\nhttps://github.com/NVLabs/VILA\n\nSOTA on many benchmarks among models of the same size, absolute SOTA on SEED",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "\"We train all models using 128 NVIDIA H100 GPUs with a global batch size of 2048 across all stages.\"",
      "Training hardware": "NVIDIA H100 SXM5 80GB",
      "Hardware quantity": "128.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "176207.9898367568",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "Creative Commons Attribution Non Commercial 4.0\n\nhttps://huggingface.co/Efficient-Large-Model/NVILA-15B",
      "Numerical format": "FP8",
      "Frontier model": "",
      "Hardware acquisition cost": "4128507.357445165",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Infinity",
      "Organization": "ByteDance",
      "Publication date": "2024-12-05",
      "Domain": "Image generation",
      "Task": "Image generation,Text-to-image",
      "Parameters": "2000000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "LAION,COYO-700M",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2412.04431v1",
      "Reference": "Infinity\u221e: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis",
      "Citations": "170.0",
      "Authors": "Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, Xiaobing Liu",
      "Abstract": "We present Infinity, a Bitwise Visual AutoRegressive Modeling capable of generating high-resolution, photorealistic images following language instruction. Infinity redefines visual autoregressive model under a bitwise token prediction framework with an infinite-vocabulary tokenizer & classifier and bitwise self-correction mechanism, remarkably improving the generation capacity and details. By theoretically scaling the tokenizer vocabulary size to infinity and concurrently scaling the transformer size, our method significantly unleashes powerful scaling capabilities compared to vanilla VAR. Infinity sets a new record for autoregressive text-to-image models, outperforming top-tier diffusion models like SD3-Medium and SDXL. Notably, Infinity surpasses SD3-Medium by improving the GenEval benchmark score from 0.62 to 0.73 and the ImageReward benchmark score from 0.87 to 0.96, achieving a win rate of 66%. Without extra optimization, Infinity generates a high-quality 1024x1024 image in 0.8 seconds, making it 2.6x faster than SD3-Medium and establishing it as the fastest text-to-image model. Models and codes will be released to promote further exploration of Infinity for visual generation and unified tokenizer modeling.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Infinity achieved best performance at severrat text-to-image benchmarks, including GenEval and ImageReward. \n\n\"Infinity sets a new record for AutoRegressive models, and also surpasses leading diffusion models including SDXL[43], PixArt-Sigma[12],DALL-E3[7] and Stable-Diffusion 3[21] on several challenging text-to-image benchmarks. Notably, Infinity surpasses SD3 by improving the GenEval benchmark score from 0.62 to 0.73, ImageReward benchmark score from 0.87 to 0.96, HPSv2.1 benchmark score from 30.9 to 32.3, achieving a win rate of 66% for human evaluation and a 2.6\u00d7 reduction in inference latency with the same model size.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "",
      "Accessibility notes": "MIT license\n\nhttps://huggingface.co/FoundationVision/Infinity\nhttps://github.com/FoundationVision/Infinity",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Amazon Nova Pro",
      "Organization": "Amazon",
      "Publication date": "2024-12-03",
      "Domain": "Multimodal,Language,Video,Vision",
      "Task": "Language modeling/generation,Retrieval-augmented generation,Visual question answering,Image captioning,Video description,Character recognition (OCR),Code generation,Translation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "6.000010000000001e+24",
      "Training compute notes": "\"probably just below 1e25 stemming from the Llama 70B serving speed.  If Llama 70B is trained proportionally to 405B, then it's at ~ 6.6e24. Nova Pro is served at 100tk/s, while Llama 70B is served at 70tk/s on average, and 100tk/s by together.ai at FP8. So Nova Pro would be >1e25 if they roughly 2x the amount of training compared to Llama 70B which [seems unlikely]\"",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Speculative",
      "Link": "https://aws.amazon.com/es/blogs/aws/introducing-amazon-nova-frontier-intelligence-and-industry-leading-price-performance/\n\nhttps://assets.amazon.science/96/7d/0d3e59514abf8fdcfafcdc574300/nova-tech-report-20250317-0810.pdf",
      "Reference": "Introducing Amazon Nova foundation models: Frontier intelligence and industry leading price performance",
      "Citations": "",
      "Authors": "",
      "Abstract": " A highly capable multimodal model with the best combination of accuracy, speed, and cost for a wide range of tasks. Amazon Nova Pro is capable of processing up to 300K input tokens and sets new standards in multimodal intelligence and agentic workflows that require calling APIs and tools to complete complex workflows. It achieves state-of-the-art performance on key benchmarks including visual question answering (TextVQA) and video understanding (VATEX). Amazon Nova Pro demonstrates strong capabilities in processing both visual and textual information and excels at analyzing financial documents. With an input context of 300K tokens, it can process code bases with over fifteen thousand lines of code. Amazon Nova Pro also serves as a teacher model to distill custom variants of Amazon Nova Micro and Lite.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"It achieves state-of-the-art performance on key benchmarks including visual question answering (TextVQA) and video understanding (VATEX).\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "Amazon Trainium1,NVIDIA A100,NVIDIA H100 SXM5 80GB",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Fugatto 1",
      "Organization": "NVIDIA",
      "Publication date": "2024-11-25",
      "Domain": "Multimodal,Language,Audio",
      "Task": "Audio generation",
      "Parameters": "2500000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "\"our dataset is comprised of at least 50,000 hours of audio\"",
      "Confidence": "Confident",
      "Link": "https://research.nvidia.com/publication/2024-11_fugatto-1-foundational-generative-audio-transformer-opus-1",
      "Reference": "Fugatto 1 - Foundational Generative Audio Transformer Opus 1\n",
      "Citations": "",
      "Authors": "Rafael Valle, Rohan Badlani, Zhifeng Kong, Sang-gil Lee, Arushi Goel, Sungwon Kim,\nJoao Felipe Santos, Shuqi Dai, Siddharth Gururani, Aya AlJa'fari, Alex Liu, Kevin Shih, Wei Ping, Bryan Catanzaro\n",
      "Abstract": "Fugatto is a versatile audio synthesis and transformation model capable of following free-form text instructions with optional audio inputs. While large language models (LLMs) trained with text on a simple next-token prediction objective can learn to infer instructions directly from the data, models trained solely on audio data lack this capacity. This is because audio data does not inherently contain the instructions that were used to generate it. To overcome this challenge, we introduce a specialized dataset generation approach optimized for producing a wide range of audio generation and transformation tasks, ensuring the data reveals meaningful relationships between audio and language. Another challenge lies in achieving compositional abilities \u2013 such as combining, interpolating between, or negating instructions \u2013 using data alone. To address it, we propose ComposableART, an inference-time technique that extends classifier-free guidance to compositional guidance. It enables the seamless and flexible composition of instructions, leading to highly customizable audio outputs outside the training distribution. Our evaluations across a diverse set of tasks demonstrate that Fugatto performs competitively with specialized models, while ComposableART enhances its sonic palette and control over synthesis. Most notably, we highlight our framework\u2019s ability to synthesize emergent sounds \u2013 sonic phenomena that transcend conventional audio generation \u2013 unlocking new creative possibilities. Demo Website.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We showcase Fugatto\u2019s performance on traditional TTA benchmarks that measure a model\u2019s ability to synthesize general sounds (AudioCAPS) and music (MusicCAPS) that follow instructions provided in text. We use the metrics (FD, FAD, and IS) and data splits (train, test) used in Kong et al. (2024b). Results in Table 3a and Table 3b shows that our model achieves strictly better scores than existing generalist models, while occasionally outperforming expert models\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "32.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "25178.17637397875",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Unreleased",
      "Accessibility notes": "demos only\nhttps://fugatto.github.io/",
      "Numerical format": "Unknown",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GPT-4o (Nov 2024)",
      "Organization": "OpenAI",
      "Publication date": "2024-11-20",
      "Domain": "Multimodal,Language,Audio,Speech,Vision",
      "Task": "Chat,Image generation,Audio generation,Vision-language generation,Table tasks,Language modeling/generation,Question answering,Speech recognition (ASR),Speech-to-text",
      "Parameters": "",
      "Parameters notes": "Not known.\n\nInference costs in the API are 2x cheaper than GPT-4 Turbo",
      "Training compute (FLOP)": "",
      "Training compute notes": "Training compute estimated to be 3.8e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Speculative",
      "Link": "https://help.openai.com/en/articles/9624314-model-release-notes",
      "Reference": "Update to GPT-4o (November 20, 2024)",
      "Citations": "",
      "Authors": "Aidan Clark, Alex Paino, Jacob Menick, Liam Fedus, Luke Metz, Clemens Winter, Lia Guy, Sam Schoenholz, Daniel Levy, Nitish Keskar, Alex Carney, Alex Paino, Ian Sohl, Qiming Yuan, Reimar Leike, Arka Dhar, Brydon Eastman, Mia Glaese, Ben Sokolowsky, Andrew Kondrich, Felipe Petroski Such, Henrique Ponde de Oliveira Pinto, Jiayi Weng, Randall Lin, Youlong Cheng, Nick Ryder, Lauren Itow, Barret Zoph, John Schulman, Mianna Chen, Adam Lerer, Adam P. Goucher, Adam Perelman, Akila Welihinda, Alec Radford, Alex Borzunov, Alex Carney, Alex Chow, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexi Christakis, Ali Kamali, Allison Moyer, Allison Tam, Amin Tootoonchian, Ananya Kumar, Andrej Karpathy, Andrey Mishchenko, Andrew Cann, Andrew Kondrich, Andrew Tulloch, Angela Jiang, Antoine Pelisse, Anuj Gosalia, Avi Nayak, Avital Oliver, Behrooz Ghorbani, Ben Leimberger, Ben Wang, Blake Samic, Brian Guarraci, Brydon Eastman, Camillo Lugaresi, Chak Li, Charlotte Barette, Chelsea Voss, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christopher Hesse, Colin Wei, Daniel Kappler, Daniel Levin, Daniel Levy, David Farhi, David Mely, David Sasaki, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Peterson, Eric Sigler, Eugene Brevdo, Farzad Khorasani, Francis Zhang, Gene Oden, Geoff Salmon, Hadi Salman, Haiming Bao, Heather Schmidt, Hongyu Ren, Hyung Won Chung, Ian Kivlichan, Ian O'Connell, Ian Osband, Ilya Kostrikov, Ingmar Kanitscheider, Jacob Coxon, James Crooks, James Lennon, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jiayi Weng, Jie Tang, Joanne Jang, Jonathan Ward, Jonathan McKay, Jong Wook Kim, Josh Gross, Josh Kaplan, Joy Jiao, Joyce Lee, Juntang Zhuang, Kai Fricke, Kavin Karthik, Kenny Hsu, Kiel Howe, Kyle Luther, Larry Kai, Lauren Itow, Leo Chen, Lia Guy, Lien Mamitsuka, Lilian Weng, Long Ouyang, Louis Feuvrier, Lukas Kondraciuk, Lukasz Kaiser, Lyric Doshi, Mada Aflak, Maddie Simens, Madeleine Thompson, Marat Dukhan, Marvin Zhang, Mateusz Litwin, Max Johnson, Mayank Gupta, Mia Glaese, Michael Janner, Michael Petrov, Michael Wu, Michelle Fradin, Michelle Pokrass, Miguel Oom Temudo de Castro, Mikhail Pavlov, Minal Khan, Mo Bavarian, Natalia Gimelshein, Natalie Staudacher, Nick Stathas, Nik Tezak, Nithanth Kudige, Noel Bundick, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivier Godement, Owen Campbell-Moore, Philip Pronin, Philippe Tillet, Rachel Lim, Rajan Troll, Randall Lin, Rapha gontijo lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Rob Honsby, Rohit Ramchandani, Rory Carmichael, Ruslan Nigmatullin, Ryan Cheu, Scott Gray, Sean Grove, Sean Metzger, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shuaiqi (Tony) Xia, Sonia Phene, Spencer Papay, Steve Coffey, Steve Lee, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tarun Gogineni, Ted Sanders, Thomas Cunninghman, Thomas Dimson, Thomas Raoux, Tianhao Zheng, Tina Kim, Todd Underwood, Tristan Heywood, Valerie Qi, Vinnie Monaco, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wojciech Zaremba, Yash Patil, Yilei, Qian, Yongjik Kim, Youlong Cheng, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, Yury Malkov",
      "Abstract": "We\u2019re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.\n\nGPT-4o (\u201co\u201d for \u201comni\u201d) is a step towards much more natural human-computer interaction\u2014it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement,Significant use",
      "Notability criteria notes": "Outperforms GPT-4 Turbo and other models on text and especially on multimodal benchmarks, such as MMLU, GPQA, HumanEval, MMMU, etc See Model Evaluations: https://openai.com/index/hello-gpt-4o/ \n\nGPT-4o is now the default model in ChatGPT, so it's one of the most widely used models.",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "GPT-4o (Aug 2024)",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "Definitely a new model, not a GPT-4 finetune",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Suno v4",
      "Organization": "Suno",
      "Publication date": "2024-11-19",
      "Domain": "Audio",
      "Task": "Audio generation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://suno.com/blog/v4",
      "Reference": "Introducing v4",
      "Citations": "",
      "Authors": "Team Suno",
      "Abstract": "Today, we\u2019re excited to introduce v4\u2014the next step toward enabling you to make music at the speed of your ideas. When we launched v3 earlier this year, it opened up new possibilities for music creation. Post v3 launch, we\u2019ve refined what worked and added more where it mattered most. The result is v4\u2014a major update that takes music creation to the next level. v4 delivers cleaner audio, sharper lyrics, and more dynamic song structures.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Significant use",
      "Notability criteria notes": "https://www.semrush.com/website/suno.ai/overview/",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Hosted access (no API)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Unreleased",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Pixtral Large",
      "Organization": "Mistral AI",
      "Publication date": "2024-11-18",
      "Domain": "Multimodal,Language,Vision",
      "Task": "Vision-language generation,Visual question answering,Mathematical reasoning,Character recognition (OCR),Language modeling/generation,Question answering",
      "Parameters": "124000000000.0",
      "Parameters notes": "123B multimodal decoder, 1B parameter vision encoder",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://mistral.ai/news/pixtral-large/",
      "Reference": "Pixtral Large",
      "Citations": "",
      "Authors": "Albert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Alok Kothari, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Augustin Garreau, Austin Birky, Bam4d, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Carole Rambaud, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Diogo Costa, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gaspard Blanchet, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Henri Roussez, Hichem Sattouf, Ian Mack, Jean-Malo Delignon, Jessica Chudnovsky, Justus Murke, Kartik Khandelwal, Lawrence Stewart, Louis Martin, Louis Ternon, Lucile Saulnier, L\u00e9lio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Marjorie Janiewicz, Micka\u00ebl Seznec, Nicolas Schuhl, Niklas Muhs, Olivier de Garrigues, Patrick von Platen, Paul Jacob, Pauline Buche, Pavan Kumar Reddy, Perry Savas, Pierre Stock, Romain Sauvestre, Sagar Vaze, Sandeep Subramanian, Saurabh Garg, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibault Schueller, Thibaut Lavril, Thomas Wang, Th\u00e9ophile Gervet, Timoth\u00e9e Lacroix, Valera Nemychnikova, Wendy Shang, William El Sayed, William Marshall",
      "Abstract": "Today we announce Pixtral Large, a 124B open-weights multimodal model built on top of Mistral Large 2. Pixtral Large is the second model in our multimodal family and demonstrates frontier-level image understanding. Particularly, the model is able to understand documents, charts and natural images, while maintaining the leading text-only understanding of Mistral Large 2.",
      "Organization categorization": "Industry",
      "Country (of organization)": "France",
      "Notability criteria": "Significant use,SOTA improvement",
      "Notability criteria notes": "Number of downloads not visible\n\n\"State-of-the-art on MathVista, DocVQA, VQAv2\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "Mistral Large 2",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "mrl license (research only), separate license is needed for commercial usage\n\nhttps://huggingface.co/mistralai/Pixtral-Large-Instruct-2411",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "k0-math",
      "Organization": "Moonshot",
      "Publication date": "2024-11-16",
      "Domain": "Mathematics,Language",
      "Task": "Mathematical reasoning,Quantitative reasoning",
      "Parameters": "",
      "Parameters notes": "This tweet mentions that this LLM has 100B params but I have not found the information anywhere else. https://x.com/kimmonismus/status/1902710969534460109",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://web.archive.org/web/20250124103542/https://www.globaltimes.cn/page/202411/1323248.shtml",
      "Reference": "Chinese AI start-up unveils latest reasoning model comparable to OpenAI o1 series",
      "Citations": "",
      "Authors": "",
      "Abstract": "Artificial general intelligence start-up Kimi, owned by Chinese AI start-up Moonshot AI, on Saturday launched its first reasoning AI model k0-math. The model can be compared to US-based OpenAI's reasoning AI series o1, including the o1-mini and o1-preview, in certain mathematics tests.\n\nIn four mathematical benchmark tests - China's high school entrance examination, college entrance examination, postgraduate entrance examination and math with introductory competition problems - the k0-math initial model outperformed OpenAI's o1-mini and o1-preview models, according to a statement sent from Moonshot AI to the Global Times on Sunday. \n\nIn the two more challenging competition-level math problem sets - OMNI-MATH and AIME tests - the performance of the k0-math initial model reached 90 percent and 83 percent of the highest scores achieved by o1-mini, respectively.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "In four mathematical benchmark tests - China's high school entrance examination, college entrance examination, postgraduate entrance examination and math with introductory competition problems - the k0-math initial model outperformed OpenAI's o1-mini and o1-preview models, according to a statement sent from Moonshot AI to the Global Times on Sunday. \n\nhttps://recodechinaai.substack.com/p/openais-o1-faces-competition-meet",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Gemini-Exp-1114",
      "Organization": "Google DeepMind",
      "Publication date": "2024-11-15",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://www.tomsguide.com/ai/google-gemini/google-drops-new-gemini-model-and-it-goes-straight-to-the-top-of-the-llm-leaderboard",
      "Reference": "Google drops new Gemini model and it goes straight to the top of the LLM leaderboard",
      "Citations": "",
      "Authors": "Gemini Team",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Training cost,Significant use",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Hosted access (no API)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "SeedEdit",
      "Organization": "ByteDance",
      "Publication date": "2024-11-11",
      "Domain": "Vision",
      "Task": "Image generation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://lf3-static.bytednsdoc.com/obj/eden-cn/lapzild-tss/ljhwZthlaukjlkulzlp/SeedEdit.pdf\nhttps://team.doubao.com/en/special/seededit",
      "Reference": "SeedEdit: Align Image Re-Generation to Image Editing",
      "Citations": "",
      "Authors": "Yichun Shi, Peng Wang, Weilin Huang",
      "Abstract": "We introduce SeedEdit, a diffusion model that is able to revise a given image with any text prompts. In our perspective, the key to such a task is to obtain an optimal balance between maintaining the original image, i.e. image reconstruction, and generating a new image, i.e. image re-generation. To this end, we start from a weak generator (text-to-image model) that creates diverse pairs between such two directions and gradually align it into a strong image editor that well balances between the two tasks. SeedEdit can achieve more diverse and stable editing capability over prior image editing methods, enabling sequential revision over images generated by diffusion models. Our website is\nhttps://team.doubao.com/seededit.\n",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Unclear how many users\n\n\"Table 1 shows the quantitative results of the baselines and our method. Overall, our method shows a significantly higher editing score on both benchmarks than open-source baselines.\"\n\nnot absolute SOTA",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Hunyuan-Large",
      "Organization": "Tencent",
      "Publication date": "2024-11-06",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering,Code generation,Translation",
      "Parameters": "389000000000.0",
      "Parameters notes": "\"a total of 389 billion parameters and 52 billion activation parameters\"",
      "Training compute (FLOP)": "3.49237e+24",
      "Training compute notes": "52B activated parameters\n\n6ND = 6*52*10^9*7*10^12 = 2.184 \u00d7 10^24\n\nThey also suggest more precise formula to calculate MoE compute budget:\n\n9.59ND + 2.3 \u00d7 10^8D = 9.59*52*10^9*7*10^12 + 2.3 \u00d7 10^8 \u00d7  7*10^12 = 3.49237\u00d710^24\n\nwhich seems closer to projected compute on Figure 3",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "7000000000000",
      "Dataset size notes": "\"# Trained Tokens 7T\"  Table 1",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2411.02265",
      "Reference": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent",
      "Citations": "",
      "Authors": "Xingwu Sun, Yanfeng Chen, Yiqing Huang, Ruobing Xie, Jiaqi Zhu, Kai Zhang, Shuaipeng Li, Zhen Yang, Jonny Han, Xiaobo Shu, Jiahao Bu, Zhongzhi Chen, Xuemeng Huang, Fengzong Lian,\nSaiyong Yang, Jianfeng Yan, Yuyuan Zeng, Xiaoqin Ren, Chao Yu, Lulu Wu, Yue Mao, Jun Xia, Tao Yang, Suncong Zheng, Kan Wu, Dian Jiao, Jinbao Xue, Xipeng Zhang, Decheng Wu, Kai Liu, Dengpeng Wu, Guanghui Xu, Shaohua Chen, Shuang Chen, Xiao Feng, Yigeng Hong, Junqiang Zheng, Chengcheng Xu, Zongwei Li, Xiong Kuang, Jianglu Hu, Yiqi Chen, Yuchi Deng, Guiyang Li, Ao Liu, Chenchen Zhang, Shihui Hu, Zilong Zhao, Zifan Wu, Yao Ding, Weichao Wang, Han Liu, Roberts Wang, Hao Fei, Peijie Yu, Ze Zhao, Xun Cao, Hai Wang, Fusheng Xiang, Mengyuan Huang, Zhiyuan Xiong, Bin Hu, Xuebin Hou, Lei Jiang, Jianqiang Ma, Jiajia Wu, Yaping Deng, Yi Shen, Qian Wang, Weijie Liu, Jie Liu, Meng Chen, Liang Dong, Weiwen Jia, Hu Chen, Feifei Liu, Rui Yuan, Huilin Xu, Zhenxiang Yan, Tengfei Cao, Zhichao Hu, Xinhua Feng, Dong Du, Tinghao Yu, Yangyu Tao, Feng Zhang, Jianchen Zhu, Chengzhong Xu, Xirui Li, Chong Zha, Wen Ouyang, Yinben Xia, Xiang Li, Zekun He, Rongpeng Chen, Jiawei Song, Ruibin Chen, Fan Jiang, Chongqing Zhao, Bo Wang, Hao Gong, Rong Gan, Winston Hu, Zhanhui Kang, Yong Yang, Yuhong Liu, Di Wang, and Jie Jiang.\n",
      "Abstract": "In this paper, we introduce Hunyuan-Large, which is currently the largest open-source Transformer-based mixture of experts model, with a total of 389 billion parameters and 52 billion activation parameters, capable of handling up to 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior performance across various benchmarks including language understanding and generation, logical reasoning, mathematical problem-solving, coding, long-context, and aggregated tasks, where it outperforms LLama3.1-70B and exhibits comparable performance when compared to the significantly larger LLama3.1-405B model. Key practice of Hunyuan-Large include large-scale synthetic data that is orders larger than in previous literature, a mixed expert routing strategy, a key-value cache compression technique, and an expert-specific learning rate strategy. Additionally, we also investigate the scaling laws and learning rate schedule of mixture of experts models, providing valuable insights and guidances for future model development and optimization. The code and checkpoints of Hunyuan-Large are released to facilitate future innovations and applications.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "not listed",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Open (restricted use)",
      "Inference code accessibility": "",
      "Accessibility notes": "the license doesn't regulate usage in the EU\nalso requires additional licensing in case of massive commercial use",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Doubao-pro",
      "Organization": "ByteDance",
      "Publication date": "2024-10-28",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering,Text summarization,Text classification",
      "Parameters": "500000000000.0",
      "Parameters notes": "[Speculative] Doubao's large language model has scaled up from 35 billion parameters to 800 billion, with 500 billion and 800 billion parameter models currently under training.\nhttps://xueqiu.com/9637001584/309910396?md5__1038=7qmx2DyDuie4cDBqDTQEWqDtMvO4iTphD\n",
      "Training compute (FLOP)": "2.505e+25",
      "Training compute notes": "6ND = 6 * 500*10^9 * 8350*10^9 = 2.505e+25",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "8350000000000",
      "Dataset size notes": "[Speculative] Doubao's pre-training data volume is approximately 500TB, with only about 10% of this data actually used for training. The current version employs a non-Mixture-of-Experts (MoE) architecture. In the future, MoE architecture may be introduced to increase parameter count and performance, while also integrating multimodal data solutions.\n\nSo this model is dense, and the training data is probably all text tokens, not multimodal.\n\n50TB * 167M tokens/GB ~= 8.35 trillion tokens\n",
      "Confidence": "Likely",
      "Link": "https://www.volcengine.com/docs/6360/1264663",
      "Reference": "Doubao General Model Pro (Doubao-pro)",
      "Citations": "",
      "Authors": "",
      "Abstract": "A professional-grade, self-developed LLM supporting up to 128k tokens, enabling fine-tuning across the entire series. ",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "NVLM-X 72B",
      "Organization": "NVIDIA",
      "Publication date": "2024-10-22",
      "Domain": "Vision,Language",
      "Task": "Language modeling/generation,Vision-language generation,Question answering,Code generation,Translation,Quantitative reasoning",
      "Parameters": "72000000000.0",
      "Parameters notes": "72B",
      "Training compute (FLOP)": "3.0398181e+24",
      "Training compute notes": "3.02e24 FLOP (Qwen2-72B compute) + 19818086000000000000000 = 3.0398181e+24",
      "Training dataset": "COCO,Conceptual Captions (CC3M),SBU,VQAv2,VisualGenome,TextVQA,OCR-VQA",
      "Training dataset size (gradients)": "45875200000",
      "Dataset size notes": "Pre-training\nGlobal batch size 2,048\nSequence length in the LLM decoder 512\nDownsampling of visual tokens 1024->256\n# of visual token per tile 256\n# of tiles 1\n# of training steps 20K\n\n2048 * (512 + 256 * 1) * 20000 = 31,457,280,000\n\nSFT:\nGlobal batch size 256\nSequence length in the LLM decoder  1,024\n# of visual token per tile 256\n# of tiles 6+1\n# of training steps 20K\n\n256 * (1,024 + 256*7) * 20000 = 14417920000\n\n31,457,280,000 +14417920000 = 45875200000",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2409.11402",
      "Reference": "NVLM: Open Frontier-Class Multimodal LLMs",
      "Citations": "",
      "Authors": "Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping",
      "Abstract": "We introduce NVLM 1.0, a family of frontier-class multimodal large language models (LLMs) that achieve state-of-the-art results on vision-language tasks, rivaling the leading proprietary models (e.g., GPT-4o) and open-access models (e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved text-only performance over its LLM backbone after multimodal training. In terms of model design, we perform a comprehensive comparison between decoder-only multimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g., Flamingo). Based on the strengths and weaknesses of both approaches, we propose a novel architecture that enhances both training efficiency and multimodal reasoning capabilities. Furthermore, we introduce a 1-D tile-tagging design for tile-based dynamic high-resolution images, which significantly boosts performance on multimodal reasoning and OCR-related tasks. Regarding training data, we meticulously curate and provide detailed information on our multimodal pretraining and supervised fine-tuning datasets. Our findings indicate that dataset quality and task diversity are more important than scale, even during the pretraining phase, across all architectures. Notably, we develop production-grade multimodality for the NVLM-1.0 models, enabling them to excel in vision-language tasks while maintaining and even improving text-only performance compared to their LLM backbones. To achieve this, we craft and integrate a high-quality text-only dataset into multimodal training, alongside a substantial amount of multimodal math and reasoning data, leading to enhanced math and coding capabilities across modalities. To advance research in the field, we release the model weights at this https URL and will open-source the training code for the community soon.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "",
      "Epochs": "1.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA H100 SXM5 80GB",
      "Hardware quantity": "128.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "Model uses ",
      "Training power draw (W)": "176380.73226445544",
      "Base model": "Qwen2-72B,InternViT-6B",
      "Finetune compute (FLOP)": "1.9818086e+22",
      "Finetune compute notes": "6*72B*45875200000 = 1.9818086e+22",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "4261168.421244739",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "NVLM-H 72B",
      "Organization": "NVIDIA",
      "Publication date": "2024-10-22",
      "Domain": "Vision,Language",
      "Task": "Language modeling/generation,Vision-language generation,Question answering,Code generation,Translation,Quantitative reasoning",
      "Parameters": "72000000000.0",
      "Parameters notes": "72B",
      "Training compute (FLOP)": "3.02e+24",
      "Training compute notes": "Additional compute in this paper is negligible relative to the compute used to train the language model backbone (Qwen2-72B at 3.02e24 FLOP)",
      "Training dataset": "COCO,Conceptual Captions (CC3M),SBU,VQAv2,VisualGenome,TextVQA,OCR-VQA",
      "Training dataset size (gradients)": "125829120000",
      "Dataset size notes": "Pre-training:\nGlobal batch size 2,048\nSequence length in the LLM decoder 512\nDownsampling of visual tokens 1024->256\n# of visual token per tile 256\n# of tiles 6+1\n# of training steps 20K\n\n2048 * (512+256*7) * 20000 = 94,371,840,000\n\nSFT:\nGlobal batch size  256\nSequence length in the LLM decoder  1,280\n# of visual token per tile 256\n# of tiles 6+1\n# of training steps 40K\n\n256*(1280+256*7)*40000 = 31,457,280,000\n\n94,371,840,000 + 31,457,280,000 = 125,829,120,000",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2409.11402",
      "Reference": "NVLM: Open Frontier-Class Multimodal LLMs",
      "Citations": "",
      "Authors": "Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping",
      "Abstract": "We introduce NVLM 1.0, a family of frontier-class multimodal large language models (LLMs) that achieve state-of-the-art results on vision-language tasks, rivaling the leading proprietary models (e.g., GPT-4o) and open-access models (e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved text-only performance over its LLM backbone after multimodal training. In terms of model design, we perform a comprehensive comparison between decoder-only multimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g., Flamingo). Based on the strengths and weaknesses of both approaches, we propose a novel architecture that enhances both training efficiency and multimodal reasoning capabilities. Furthermore, we introduce a 1-D tile-tagging design for tile-based dynamic high-resolution images, which significantly boosts performance on multimodal reasoning and OCR-related tasks. Regarding training data, we meticulously curate and provide detailed information on our multimodal pretraining and supervised fine-tuning datasets. Our findings indicate that dataset quality and task diversity are more important than scale, even during the pretraining phase, across all architectures. Notably, we develop production-grade multimodality for the NVLM-1.0 models, enabling them to excel in vision-language tasks while maintaining and even improving text-only performance compared to their LLM backbones. To achieve this, we craft and integrate a high-quality text-only dataset into multimodal training, alongside a substantial amount of multimodal math and reasoning data, leading to enhanced math and coding capabilities across modalities. To advance research in the field, we release the model weights at this https URL and will open-source the training code for the community soon.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "",
      "Epochs": "1.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA H100 SXM5 80GB",
      "Hardware quantity": "128.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "176380.73226445544",
      "Base model": "Qwen2-72B,InternViT-6B",
      "Finetune compute (FLOP)": "5.436e+22",
      "Finetune compute notes": "6ND = 6*125,829,120,000*72000000000.00 = 5.436e22\n",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "4261168.421244739",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "NVLM-D 72B",
      "Organization": "NVIDIA",
      "Publication date": "2024-10-22",
      "Domain": "Vision,Language",
      "Task": "Language modeling/generation,Vision-language generation,Question answering,Code generation,Translation,Quantitative reasoning",
      "Parameters": "72000000000.0",
      "Parameters notes": "72B",
      "Training compute (FLOP)": "3.02e+24",
      "Training compute notes": "Uses Qwen2-72B as a backbone, which trained with 3.02e24 FLOP, as well as InternViT-6B. It's unclear how many FLOP were spent training but probably negligible; e.g. PaLI trained ViT-e with ~4B parameters using 1.07e23 FLOP.\n\nFine-tuning FLOPs:\n57,016,320,000 image/text tokens over all stages\n6 * 72B * 57,016,320,000 = 2.463e22\n",
      "Training dataset": "COCO,Conceptual Captions (CC3M),SBU,VQAv2,VisualGenome,TextVQA,OCR-VQA",
      "Training dataset size (gradients)": "57016320000",
      "Dataset size notes": "Pre-training\nGlobal batch size 2,048\nSequence length in the LLM decoder 512\nDownsampling of visual tokens 1024->256\n# of visual token per tile 256\n# of tiles 1\n# of training steps 20K\n\n2048 * (512 + 256 * 1) * 20000 = 31,457,280,000\n\nSFT:\nGlobal batch size 128\nSequence length in the LLM decoder 3,200\n# of visual token per tile 256\n# of tiles 6+1\n# of training steps 40K\n\n128 * (3200 + 256*7) * 40000 = 25,559,040,000\n\n31,457,280,000 + 25,559,040,000 = 57,016,320,000",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2409.11402",
      "Reference": "NVLM: Open Frontier-Class Multimodal LLMs",
      "Citations": "",
      "Authors": "Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping",
      "Abstract": "We introduce NVLM 1.0, a family of frontier-class multimodal large language models (LLMs) that achieve state-of-the-art results on vision-language tasks, rivaling the leading proprietary models (e.g., GPT-4o) and open-access models (e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved text-only performance over its LLM backbone after multimodal training. In terms of model design, we perform a comprehensive comparison between decoder-only multimodal LLMs (e.g., LLaVA) and cross-attention-based models (e.g., Flamingo). Based on the strengths and weaknesses of both approaches, we propose a novel architecture that enhances both training efficiency and multimodal reasoning capabilities. Furthermore, we introduce a 1-D tile-tagging design for tile-based dynamic high-resolution images, which significantly boosts performance on multimodal reasoning and OCR-related tasks. Regarding training data, we meticulously curate and provide detailed information on our multimodal pretraining and supervised fine-tuning datasets. Our findings indicate that dataset quality and task diversity are more important than scale, even during the pretraining phase, across all architectures. Notably, we develop production-grade multimodality for the NVLM-1.0 models, enabling them to excel in vision-language tasks while maintaining and even improving text-only performance compared to their LLM backbones. To achieve this, we craft and integrate a high-quality text-only dataset into multimodal training, alongside a substantial amount of multimodal math and reasoning data, leading to enhanced math and coding capabilities across modalities. To advance research in the field, we release the model weights at this https URL and will open-source the training code for the community soon.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "SOTA on OCRBench and VQAv2",
      "Epochs": "1.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA H100 SXM5 80GB",
      "Hardware quantity": "128.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "Model uses ",
      "Training power draw (W)": "176380.73226445544",
      "Base model": "Qwen2-72B,InternViT-6B",
      "Finetune compute (FLOP)": "2.463e+22",
      "Finetune compute notes": "Fine-tuning FLOPs:\n57,016,320,000 image/text tokens over all stages\n6 * 72B * 57,016,320,000 = 2.463e22",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "",
      "Accessibility notes": "https://huggingface.co/nvidia/NVLM-D-72B\nCreative Commons Attribution: Non-Commercial 4.0 International\n\nhttps://github.com/NVIDIA/Megatron-LM/tree/NVLM-1.0/examples/multimodal/nvlm\nlicense for code seems to be Apache 2.0",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "4261168.421244739",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Yi-Lightning",
      "Organization": "01.AI",
      "Publication date": "2024-10-18",
      "Domain": "Language",
      "Task": "Language modeling/generation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "1.5e+24",
      "Training compute notes": "The CEO of 01.AI tweeted that Yi-Lightning was trained for 1 month on 2000 H100s: https://x.com/kaifulee/status/1846310645849047524\nAssuming this is accurate:\n(9.9e14 * 2000) FLOP/s * 1 month * 30.5 days/month * 24hr/day * 3600 s/hr * 0.3 utilization assumption = 1.565e24",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://www.lingyiwanwu.com/en https://platform.lingyiwanwu.com/",
      "Reference": "Yi-Lightning",
      "Citations": "",
      "Authors": "",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "On the blind test list LMSYS, Yi-Lightning surpassed GPT-4o-2024-05-13 released by OpenAI and Anthropic, as well as Claude 3.5 Sonnet, ranking sixth in the world and first in China.",
      "Epochs": "",
      "Training time (hours)": "720.0",
      "Training time notes": "https://x.com/kaifulee/status/1846310645849047524\n\"it was trained on 2000 H100s for 1 month\"",
      "Training hardware": "NVIDIA H100 SXM5 80GB",
      "Hardware quantity": "2000.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "2756194.445567179",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "https://platform.lingyiwanwu.com/",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "66772466.69831805",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "CHAI-1",
      "Organization": "Chai discovery",
      "Publication date": "2024-10-15",
      "Domain": "Biology",
      "Task": "Protein folding prediction,Protein-ligand contact prediction",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "7.7605724e+21",
      "Training compute notes": "From paper: 128 A100s for 30 days; assumptions: 30% utilization rate, FP16 precision",
      "Training dataset": "PDB (Protein Data Bank), AlphaFold database (AFDB)",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://www.chaidiscovery.com/blog/introducing-chai-1\nhttps://www.biorxiv.org/content/10.1101/2024.10.10.615955v2",
      "Reference": "Introducing Chai-1: Decoding the molecular interactions of life",
      "Citations": "0.0",
      "Authors": "Jacques Boitreaud, Jack Dent, Matthew McPartlon, Joshua Meier, Vinicius Reis, Alex Rogozhnikov, Kevin Wu",
      "Abstract": "We introduce Chai-1, a multi-modal foundation model for molecular structure prediction that performs at the state-of-the-art across a variety of tasks relevant to drug discovery. Chai-1 can optionally be prompted with experimental restraints (e.g. derived from wet-lab data) which boostsperformance by double-digit percentage points. Chai-1 can also be run in single-sequence mode without MSAs while preserving most of its performance. We release Chai-1 model weights and inference code as a Python package for non-commercial use and via a web interface where it can be used for free including for commercial drug discovery purposes.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Matches or beats AF3 on Ligand PoseBusters",
      "Epochs": "",
      "Training time (hours)": "720.0",
      "Training time notes": "Taken from paper: 128 A100s for 30 days",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "128.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "100804.70264211958",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "128.0",
      "Batch size notes": "Taken from paper",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "",
      "Accessibility notes": "https://github.com/chaidiscovery/chai-lab?tab=License-1-ov-file",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "RDT-1B",
      "Organization": "Tsinghua University",
      "Publication date": "2024-10-10",
      "Domain": "Robotics",
      "Task": "Robotic manipulation",
      "Parameters": "1200000000.0",
      "Parameters notes": "Model Training and Inference: \"We scale the size of RDT up to 1.2B parameters, establishing it as the currently largest diffusion-based robotic foundation model.\"",
      "Training compute (FLOP)": "4.06e+22",
      "Training compute notes": "Model Training and Inference: \"The model is pre-trained on 48 H100 80GB GPUs for a month, giving a total of 1M training iteration steps. It takes three days to fine-tune this model using the same GPUs for 130K steps.\"\nTable 10: \"Mixed Precision, bf16\"\nAssume 48xGPU, \"scheduling reasons\"  -> NVIDIA H100 SXM5 -> 9.894e14 FLOP/s/GPU\nAssume 0.3 utilization\nAssume 1 month + 3 days = 33 days\n0.3 * 48 GPU * (33 days) * 9.894e14 FLOP/s/GPU ~=  4.02e22 FLOP\n\n",
      "Training dataset": "RDT [pre-train]",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "Appendix D: \"Our pre-training dataset collection includes 46 datasets, with a total scale of 1M+ trajectories and 21TB, making it the largest pre-training collection of robotics datasets to date.\"\n\n\n\n",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2410.07864",
      "Reference": "RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation",
      "Citations": "340.0",
      "Authors": "Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan, Huayu Chen, Zhengyi Wang, Ke Xu, Hang Su, Jun Zhu",
      "Abstract": "Bimanual manipulation is essential in robotics, yet developing foundation models is extremely challenging due to the inherent complexity of coordinating two robot arms (leading to multi-modal action distributions) and the scarcity of training data. In this paper, we present the Robotics Diffusion Transformer (RDT), a pioneering diffusion foundation model for bimanual manipulation. RDT builds on diffusion models to effectively represent multi-modality, with innovative designs of a scalable Transformer to deal with the heterogeneity of multi-modal inputs and to capture the nonlinearity and high frequency of robotic data. To address data scarcity, we further introduce a Physically Interpretable Unified Action Space, which can unify the action representations of various robots while preserving the physical meanings of original actions, facilitating learning transferrable physical knowledge. With these designs, we managed to pre-train RDT on the largest collection of multi-robot datasets to date and scaled it up to 1.2B parameters, which is the largest diffusion-based foundation model for robotic manipulation. We finally fine-tuned RDT on a self-created multi-task bimanual dataset with over 6K+ episodes to refine its manipulation capabilities. Experiments on real robots demonstrate that RDT significantly outperforms existing methods. It exhibits zero-shot generalization to unseen objects and scenes, understands and follows language instructions, learns new skills with just 1~5 demonstrations, and effectively handles complex, dexterous tasks. We refer to this https URL for the code and videos.",
      "Organization categorization": "Academia",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Results show that RDT achieves state-of-the-art performance, outperforming baselines by achieving an improvement of 56% in success rates across a wide spectrum of challenging tasks.\"",
      "Epochs": "",
      "Training time (hours)": "720.0",
      "Training time notes": "Model Training and Inference: \"The model is pre-trained on 48 H100 80GB GPUs for a month, giving a total of 1M training iteration steps. It takes three days to fine-tune this model using the same GPUs for 130K steps.\"",
      "Training hardware": "NVIDIA H100 PCIe",
      "Hardware quantity": "48.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "33080.22622858553",
      "Base model": "",
      "Finetune compute (FLOP)": "3.69e+21",
      "Finetune compute notes": "Model Training and Inference: \"The model is pre-trained on 48 H100 80GB GPUs for a month, giving a total of 1M training iteration steps. It takes three days to fine-tune this model using the same GPUs for 130K steps.\"\nTable 10: \"Mixed Precision, bf16\"\nAssume 48xGPU, \"scheduling reasons\"  -> NVIDIA H100 SXM5 -> 9.894e14 FLOP/s/GPU\nAssume 0.3 utilization\n3 days = 72 hr = 259200 s\n0.3 * 48 GPU * 259200 s *  9.894e14 FLOP/s/GPU ~= 3.69e21 FLOP",
      "Batch size": "1536.0",
      "Batch size notes": "Table 10: Batch size, 32\u00d748",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "https://huggingface.co/robotics-diffusion-transformer/rdt-1b\n\"All the code, pre-trained model weights, and data are licensed under the MIT license.\"",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Palmyra X 004",
      "Organization": "Writer",
      "Publication date": "2024-10-09",
      "Domain": "Language",
      "Task": "Language modeling,Language modeling/generation,Question answering,Code generation,Retrieval-augmented generation",
      "Parameters": "150000000000.0",
      "Parameters notes": "Source: https://venturebeat.com/ai/writers-palmyra-x-004-takes-the-lead-in-ai-function-calling-surpassing-tech-giants/",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://writer.com/engineering/actions-with-palmyra-x-004/",
      "Reference": "Introducing actions with Palmyra X 004",
      "Citations": "",
      "Authors": "Sam Julien / Writer",
      "Abstract": "Palmyra X4 boasts state-of-the-art reasoning through novel training techniques. By leveraging synthetic data, we\u2019ve trained our model more efficiently and at a fraction of the cost reported by major AI labs. Palmyra X4\u2019s suite of new features and capabilities include:\n\nTaking action in systems external to the LLM via tool calling\nAutomatic data integration with a built-in RAG tool\nCode generation\nA 128k context window\nStructured output generation for simpler system integration (coming in a few weeks)\nEarly results on tool calling benchmarks put our new frontier model as the leader on the Berkeley Function Calling Leaderboard by a significant margin, besting model providers including OpenAI, Anthropic, Meta, and Google, and is top-ranked on Stanford HELM.\n\nPalmyra X4 is available today on AI Studio and Ask Writer, our prebuilt chat experience. You can also use apps powered by Palmyra X4 in Slack via our new Slack integration.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "SOTA on Berkeley\u2019s Tool Calling Leaderboard",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GR-2",
      "Organization": "ByteDance",
      "Publication date": "2024-10-08",
      "Domain": "Robotics",
      "Task": "Video,Action recognition,Video generation,Instruction interpretation,Robotic manipulation",
      "Parameters": "230000000.0",
      "Parameters notes": "the default GR-2 model contains 230M parameters, of which 95M are trainable",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "This large-scale pre-training, involving 38 million video clips\nand over 50 billion tokens",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2410.06158v1",
      "Reference": "GR-2: A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation",
      "Citations": "162.0",
      "Authors": "Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, Hanbo Zhang, Minzhao Zhu",
      "Abstract": "We present GR-2, a state-of-the-art generalist robot agent for versatile and generalizable robot manipulation. GR-2 is first pre-trained on a vast number of Internet videos to capture the dynamics of the world. This large-scale pre-training, involving 38 million video clips and over 50 billion tokens, equips GR-2 with the ability to generalize across a wide range of robotic tasks and environments during subsequent policy learning. Following this, GR-2 is fine-tuned for both video generation and action prediction using robot trajectories. It exhibits impressive multi-task learning capabilities, achieving an average success rate of 97.7% across more than 100 tasks. Moreover, GR-2 demonstrates exceptional generalization to new, previously unseen scenarios, including novel backgrounds, environments, objects, and tasks. Notably, GR-2 scales effectively with model size, underscoring its potential for continued growth and application. Project page: \\url{this https URL}.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Top perfoming model on CALVIN benchmark. ",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Movie Gen Video",
      "Organization": "Meta AI",
      "Publication date": "2024-10-04",
      "Domain": "Video,Vision",
      "Task": "Video generation,Text-to-video,Image-to-video",
      "Parameters": "30000000000.0",
      "Parameters notes": "30B",
      "Training compute (FLOP)": "1.65e+24",
      "Training compute notes": "Model size = 30B\nBroken down by training stage (table 3):\n256px T2I: samples seen = 1.94E9; sample token length = 256; flops = 6ND = 8.94E22\n256px T2I/V: samples seen = 3.95E8; sample token length = 8192; flops = 6ND = 5.82E23\n768px T2I/V: samples seen = 7.38E7; sample token length = 73,728; flops = 6ND = 9.79E23\nTotal flops = 1.65E24",
      "Training dataset": "",
      "Training dataset size (gradients)": "3400000000",
      "Dataset size notes": "O(1B) images\nO(100M) videos, each with 256 frames ~= 25M images",
      "Confidence": "Confident",
      "Link": "https://ai.meta.com/static-resource/movie-gen-research-paper",
      "Reference": "Movie Gen: A Cast of Media Foundation Models",
      "Citations": "",
      "Authors": "Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen\nShi, Chih-Yao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan\nPang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat\nSingh, Mary Williamson, Matt Le, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit\nGirdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen,\nSean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Tao Xu, Tingbo Hou, Wei-Ning\nHsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval\nKirstain, Zecheng He, Zijian He",
      "Abstract": "We present Movie Gen, a cast of foundation models that generates high-quality, 1080p HD videos with different aspect ratios and synchronized audio. We also show additional capabilities such as precise instruction-based video editing and generation of personalized videos based on a user\u2019s image.\nOur models set a new state-of-the-art on multiple tasks: text-to-video synthesis, video personalization, video editing, video-to-audio generation, and text-to-audio generation. Our largest video generation model is a 30B parameter transformer trained with a maximum context length of 73K video tokens, corresponding to a generated video of 16 seconds at 16 frames-per-second. We show multiple technical innovations and simplifications on the \n architecture, latent spaces, training objectives and recipes, data curation, evaluation protocols, parallelization techniques, and inference optimizations that allow us to reap the benefits of scaling pre-training data, model size, and training compute for training large scale media generation models. We hope this paper helps the research community to accelerate progress and innovation in media generation models.\nAll videos from this paper are available at https://go.fb.me/MovieGenResearchVideos.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "BOTE estimate of cost is ~$3 million",
      "Epochs": "",
      "Training time (hours)": "331.0",
      "Training time notes": "54 hours for 256px T2I\n128 hours for 256px T2I/V\n149 hours for 768px T2I/V",
      "Training hardware": "NVIDIA H100 SXM5 80GB",
      "Hardware quantity": "6144.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "8469669.524206791",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "207199670.99910247",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "PixelDance",
      "Organization": "ByteDance",
      "Publication date": "2024-09-24",
      "Domain": "Video,Vision",
      "Task": "Video generation,Text-to-video,Image-to-video",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "114229250000000",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://pixeldance.io/ (LINK BROKEN)",
      "Reference": "PixelDance AI - Leading AI Video Generation Platform",
      "Citations": "",
      "Authors": "ByteDance",
      "Abstract": "PixelDance V1.4 is a video generation model developed by the ByteDance Research team, using the DiT structure. It supports both text-to-video and image-to-video generation, capable of producing impressive 10-second video clips. The model has excellent semantic understanding, capable of handling complex narratives and subtle emotional expressions. It can perform sequential multi-shot actions, support complex interactions between multiple subjects, and offer a variety of camera effects. Compatible with multiple styles and proportions, it can quickly generate high-quality video clips, empowering film creation, advertising, short videos, live streaming, and e-commerce.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Currently invite only, I would expect a large number of users when released",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Hosted access (no API)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Llama 3.2 11B",
      "Organization": "Meta AI",
      "Publication date": "2024-09-24",
      "Domain": "Multimodal,Vision,Language",
      "Task": "Visual question answering,Image captioning,Object detection",
      "Parameters": "10600000000.0",
      "Parameters notes": "https://huggingface.co/meta-llama/Llama-3.2-11B-Vision",
      "Training compute (FLOP)": "5.79e+23",
      "Training compute notes": "Tensor type is BF16 (https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct).\n\n\u201cTraining utilized a cumulative of 2.02M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency\u2026 Training time: Stage 1 pretraining: 147K H100 hours Stage 2 annealing: 98K H100 hours SFT: 896 H100 hours RLHF: 224 H100 hours.\u201d (https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md#hardware-and-software).\n\nThe Nvidia H100 80GB is the H100 SXM. BFLOAT16 Tensor Core peak FLOPS with sparsity is 1,979 teraFLOPS (https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-datasheet?ncid=no-ncid).\n\nAssuming 33% utilization rate,\nTraining compute\n~= 0.33 * ( 147000 + 98000 + 896 + 224 ) hours * 3600 s / hour * 1979e12 FLOPS / GPU\n~= 5.79e23 FLOPS",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "Since the model can also be used for image captioning, I assume the dataset size is measured in numbers of image-caption pairs (https://docs.google.com/document/d/1XWLyMzcVfDv4eFQX3yPgM8MZ3_Q1phtIFz9GKv4_KaM/edit?tab=t.0#heading=h.ny4fw3njk98p). \"\"Llama 3.2-Vision was pretrained on 6B image and text pairs\" (https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md#training-data).",
      "Confidence": "Confident",
      "Link": "https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/",
      "Reference": "Llama 3.2: Revolutionizing edge AI and vision with open, customizable models",
      "Citations": "",
      "Authors": "Meta AI",
      "Abstract": "Today, we\u2019re releasing Llama 3.2, which includes small and medium-sized vision LLMs (11B and 90B), and lightweight, text-only models (1B and 3B) that fit onto edge and mobile devices, including pre-trained and instruction-tuned versions.\nSupported by a broad ecosystem, the Llama 3.2 11B and 90B vision models are drop-in replacements for their corresponding text model equivalents, while exceeding on image understanding tasks compared to closed models, such as Claude 3 Haiku. Unlike other open multimodal models, both pre-trained and aligned models are available to be fine-tuned for custom applications using torchtune and deployed locally using torchchat. They\u2019re also available to try using our smart assistant, Meta AI.\nWe\u2019re sharing the first official Llama Stack distributions, which will greatly simplify the way developers work with Llama models in different environments, including single-node, on-prem, cloud, and on-device, enabling turnkey deployment of retrieval-augmented generation (RAG) and tooling-enabled applications with integrated safety.\nWe\u2019ve been working closely with partners like AWS, Databricks, Dell Technologies, Fireworks, Infosys, and Together AI to build Llama Stack distributions for their downstream enterprise clients. On-device distribution is via PyTorch ExecuTorch, and single-node distribution is via Ollama.\nWe continue to share our work because we believe openness drives innovation and is good for developers, Meta, and the world. Llama is already leading the way on openness, modifiability, and cost efficiency\u2014enabling more people to have creative, useful, and life-changing breakthroughs using generative AI.\nWe\u2019re making Llama 3.2 models available for download on llama.com and Hugging Face, as well as available for immediate development on our broad ecosystem of partner platforms, including AMD, AWS, Databricks, Dell, Google Cloud, Groq, IBM, Intel, Microsoft Azure, NVIDIA, Oracle Cloud, Snowflake, and more.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Significant use",
      "Notability criteria notes": "1,019,539 downloads on HuggingFace last month at time of writing (https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct).",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "\"Stage 1 pretraining: 147K H100 hours Stage 2 annealing: 98K H100 hours SFT: 896 H100 hours RLHF: 224 H100 hours\"\n\nhttps://huggingface.co/meta-llama/Llama-3.2-11B-Vision",
      "Training hardware": "NVIDIA H100 SXM5 80GB",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "Llama 3.1-8B",
      "Finetune compute (FLOP)": "3.50010000000001e+23",
      "Finetune compute notes": "147000+98000+896+224 GPU-hours => 246120 GPU-hours * 60 * 60 * 989e12 FLOP * 0.4 (utilization) = 3.5e23 FLOP",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "LLAMA 3.2 COMMUNITY LICENSE AGREEMENT\n\nhttps://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Telechat2-115B",
      "Organization": "China Telecom",
      "Publication date": "2024-09-20",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering,Quantitative reasoning,Chat,Text summarization,Code generation",
      "Parameters": "115000000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "6.9e+24",
      "Training compute notes": "6ND: 6 * 115B * 10T = 6.9e24",
      "Training dataset": "",
      "Training dataset size (gradients)": "10000000000000",
      "Dataset size notes": "The open source TeleChat2-115B model is trained using 10 trillion tokens of high-quality Chinese and English corpus",
      "Confidence": "Confident",
      "Link": "https://huggingface.co/Tele-AI/TeleChat2-115B",
      "Reference": "TeleChat Technical Report",
      "Citations": "",
      "Authors": "Zihan Wang and Xinzhang Liu and Shixuan Liu and Yitong Yao and Yuyao Huang and Zhongjiang He and Xuelong Li and Yongxiang Li and Zhonghao Che and Zhaoxi Zhang and Yan Wang and Xin Wang and Luwen Pu and Huihan Xu and Ruiyu Fang and Yu Zhao and Jie Zhang and Xiaomeng Huang and Zhilong Lu and Jiaxin Peng and Wenjun Zheng and Shiquan Wang and Bingkai Yang and Xuewei he and Zhuoru Jiang and Qiyi Xie and Yanhan Zhang and Zhongqiu Li and Lingling Shi and Weiwei Fu and Yin Zhang and Zilu Huang and Sishi Xiong and Yuxiang Zhang and Chao Wang and Shuangyong Song",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "",
      "Epochs": "1.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "6000000.0",
      "Batch size notes": "table 2",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Open (restricted use)",
      "Inference code accessibility": "Open access (restricted use)",
      "Accessibility notes": "Apache 2.0\nhttps://huggingface.co/Tele-AI/TeleChat2-115B\n\n\"TeleChat model supports commercial use if you plan to treat TeleChat The model or its derivatives are used for commercial purposes, you need to contact the mailbox below tele_ai@chinatelecom.cn\"\n\nno clear license but same disclaimer as above\nhttps://github.com/Tele-AI/TeleChat2/\n\nthis is seems to be pre-training code:\nhttps://github.com/Tele-AI/TeleChat2/tree/main/deepspeed",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Qwen2.5-72B",
      "Organization": "Alibaba",
      "Publication date": "2024-09-19",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering,Quantitative reasoning",
      "Parameters": "72700000000.0",
      "Parameters notes": "72.7B",
      "Training compute (FLOP)": "7.8e+24",
      "Training compute notes": "Training dataset size was 18 trillion\n\n6ND = 6 * 72.7 billion parameters * 18 trillion tokens = 7.8e24",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "18000000000000",
      "Dataset size notes": "\"In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens\"",
      "Confidence": "Confident",
      "Link": "https://qwenlm.github.io/blog/qwen2.5/",
      "Reference": "Qwen2.5: A Party of Foundation Models!",
      "Citations": "",
      "Authors": "Qwen Team",
      "Abstract": "In the past three months since Qwen2\u2019s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history! Let\u2019s get the party started!",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "High compute, near 1e25",
      "Epochs": "1.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "not listed",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "license: allows commercial. weights only\nhttps://huggingface.co/Qwen/Qwen2.5-72B/blob/main/LICENSE ",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Qwen2.5 Instruct (72B)",
      "Organization": "Alibaba",
      "Publication date": "2024-09-19",
      "Domain": "Language",
      "Task": "Code generation,Code autocompletion,Quantitative reasoning,Question answering,Language modeling/generation",
      "Parameters": "72700000000.0",
      "Parameters notes": "Number of Parameters: 72.7B\nNumber of Paramaters (Non-Embedding): 70.0B",
      "Training compute (FLOP)": "7.8516e+24",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://qwenlm.github.io/blog/qwen2.5/",
      "Reference": "Qwen2.5: A Party of Foundation Models!",
      "Citations": "",
      "Authors": "Qwen Team",
      "Abstract": "Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\n\nSignificantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\nSignificant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\nLong-context Support up to 128K tokens and can generate up to 8K tokens.\nMultilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "Qwen2.5-72B",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "requires permission to use in applications with 100M+ users\n\nhttps://huggingface.co/Qwen/Qwen2.5-72B-Instruct\n\nseems that there is no pretraining code here\nhttps://github.com/QwenLM/Qwen3",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Oryx 34B",
      "Organization": "Tsinghua University,Tencent,Nanyang Technological University",
      "Publication date": "2024-09-19",
      "Domain": "Multimodal,Vision,3D modeling,Video,Language",
      "Task": "Visual question answering,Video compression,Image captioning,Video description,Language modeling/generation",
      "Parameters": "34000000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Oryx-SFT-Data",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2409.12961v1",
      "Reference": "Oryx MLLM: On-Demand Spatial-Temporal Understanding at Arbitrary Resolution\n",
      "Citations": "131.0",
      "Authors": "Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, Yongming Rao",
      "Abstract": "Visual data comes in various forms, ranging from small icons of just a few pixels to long videos spanning hours. Existing multi-modal LLMs usually standardize these diverse visual inputs to a fixed resolution for visual encoders and yield similar numbers of tokens for LLMs. This approach is non-optimal for multimodal understanding and inefficient for processing inputs with long and short visual contents. To solve the problem, we propose Oryx, a unified multimodal architecture for the spatial-temporal understanding of images, videos, and multi-view 3D scenes. Oryx offers an on-demand solution to seamlessly and efficiently process visual inputs with arbitrary spatial sizes and temporal lengths through two core innovations: 1) a pre-trained OryxViT model that can encode images at any resolution into LLM-friendly visual representations; 2) a dynamic compressor module that supports 1x to 16x compression on visual tokens by request. These design features enable Oryx to accommodate extremely long visual contexts, such as videos, with lower resolution and high compression while maintaining high recognition precision for tasks like document understanding with native resolution and no compression. Beyond the architectural improvements, enhanced data curation and specialized training on long-context retrieval and spatial-aware data help Oryx achieve strong capabilities in image, video, and 3D multimodal understanding simultaneously",
      "Organization categorization": "Academia,Industry,Academia",
      "Country (of organization)": "China,China,Singapore",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Best performance on MLVU benchmark (long-form temporal understanding), MMBench and TextVQA (image understanding).",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA A800 SXM",
      "Hardware quantity": "64.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "50431.54291680728",
      "Base model": "Yi-1.5-34B",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "128.0",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache 2.0\nhttps://huggingface.co/THUdyh/Oryx-1.5-32B\n\nMIT license\nhttps://github.com/Oryx-mllm/Oryx",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Qwen2.5-32B",
      "Organization": "Alibaba",
      "Publication date": "2024-09-17",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering,Quantitative reasoning",
      "Parameters": "32500000000.0",
      "Parameters notes": "32.5B",
      "Training compute (FLOP)": "3.51e+24",
      "Training compute notes": "6 FLOP / parameter / token * 32.5B parameters * 18 trillion tokens = 3.51 \u00d7 10^24 FLOP",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "18000000000000",
      "Dataset size notes": "\"In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens\"",
      "Confidence": "Confident",
      "Link": "https://qwenlm.github.io/blog/qwen2.5/ ",
      "Reference": "Qwen2.5: A Party of Foundation Models!",
      "Citations": "",
      "Authors": "Qwen Team",
      "Abstract": "In the past three months since Qwen2\u2019s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history! Let\u2019s get the party started!\n\nThe Qwen2.5-7B model surpasses its predecessors and counterparts in numerous benchmarks, despite having fewer non-embedding parameters. It demonstrates significant improvements across various tasks, achieving 74.2 on general benchmarks like MMLU, 49.8 on math challenges such as MATH, and 57.9 on coding tasks like HumanEval.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "",
      "Epochs": "1.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "not listed",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Apache 2.0\nhttps://huggingface.co/Qwen/Qwen2.5-32B",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "o1-preview",
      "Organization": "OpenAI",
      "Publication date": "2024-09-12",
      "Domain": "Language,Mathematics,Biology",
      "Task": "Code generation,Language modeling/generation,Quantitative reasoning,Chat,Question answering,Translation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://openai.com/index/introducing-openai-o1-preview/",
      "Reference": "A new series of reasoning models for solving hard problems.",
      "Citations": "",
      "Authors": "",
      "Abstract": "We've developed a new series of AI models designed to spend more time thinking before they respond. They can reason through complex tasks and solve harder problems than previous models in science, coding, and math.\n\nToday, we are releasing the first of this series in ChatGPT and our API. This is a preview and we expect regular updates and improvements. Alongside this release, we\u2019re also including evaluations for the next update, currently in development.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement,Significant use",
      "Notability criteria notes": "SOTA in GPQA among others: https://openai.com/index/learning-to-reason-with-llms/ ",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "o1-mini",
      "Organization": "OpenAI",
      "Publication date": "2024-09-12",
      "Domain": "Language",
      "Task": "Code generation,Language modeling/generation,Quantitative reasoning,Chat,Question answering,Translation",
      "Parameters": "",
      "Parameters notes": "Can't get an exact estimate, but we suspect total parameter count around 60B-120B, active parameters around 10B-30B. \n\nGiven these models are served at 150-200 tok/s, at $4.40/Mtok output, inference economics (https://epoch.ai/blog/inference-economics-of-language-models) suggests total parameter count around 60-120B parameters, with mixture-of-experts active parameters around 10-30B. MoEs make a given model roughly comparable to a ~50% smaller dense model (https://epoch.ai/gradient-updates/moe-vs-dense-models-inference), which lines up decently with Magistral Small pricing (24B dense, served at a similar speed for the cheaper $1.50/Mtok). ",
      "Training compute (FLOP)": "",
      "Training compute notes": "We can\u2019t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://openai.com/index/learning-to-reason-with-llms/",
      "Reference": "Learning to reason with LLMs",
      "Citations": "",
      "Authors": "",
      "Abstract": "We've developed a new series of AI models designed to spend more time thinking before they respond. They can reason through complex tasks and solve harder problems than previous models in science, coding, and math.\n\n...\n\nWe\u2019re also releasing OpenAI o1-mini, a faster, cheaper reasoning model that is particularly effective at coding. As a smaller model, o1-mini is 80% cheaper than o1-preview, making it a powerful, cost-effective model for applications that require reasoning but not broad world knowledge.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Significant use",
      "Notability criteria notes": "Model available in ChatGPT, likely widely used ",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DeepSeek-V2.5",
      "Organization": "DeepSeek",
      "Publication date": "2024-09-06",
      "Domain": "Language",
      "Task": "Language modeling/generation,Chat,Code generation",
      "Parameters": "236000000000.0",
      "Parameters notes": "21B active params, 236B total",
      "Training compute (FLOP)": "1.7892e+24",
      "Training compute notes": "V2.5 is a merge of V2-coder and V2-chat\nV2-coder is trained for 6T additional tokens from an intermediate checkpoint of V2, which had been trained for 4.2T tokens. Total: 10.2T\nV2-chat is fine-tuned from V2, saw 8.2T tokens in pre-training\nUnique steps: 8.2T + 6T = 14.2T\nFLOPs: 6 * 21B * 14.2T = 1.7892e24",
      "Training dataset": "GitHub,Common Crawl",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "The original V2 had a dataset of 8.1T unique tokens, and coder-V2 added an additional 1.391T unique tokens of code and math. But it appears no additional training was done to combine them into this model.",
      "Confidence": "Confident",
      "Link": "https://huggingface.co/deepseek-ai/DeepSeek-V2.5",
      "Reference": "DeepSeek-V2.5",
      "Citations": "",
      "Authors": "DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Hanwei Xu, Hao Yang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J.L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jin Chen, Jingyang Yuan, Junjie Qiu, Junxiao Song, Kai Dong, Kaige Gao, Kang Guan, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qihao Zhu, Qinyu Chen, Qiushi Du, R.J. Chen, R.L. Jin, Ruiqi Ge, Ruizhe Pan, Runxin Xu, Ruyi Chen, S.S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Size Zheng, T. Wang, Tian Pei, Tian Yuan, Tianyu Sun, W.L. Xiao, Wangding Zeng, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wentao Zhang, X.Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "36864000.0",
      "Batch size notes": "Maximum batch size comes from training of V2-coder, which used long context training with 288 batches of 128k tokens = 36,864,000",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Hunyuan Turbo",
      "Organization": "Tencent",
      "Publication date": "2024-09-05",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://zhidx.com/p/442250.html",
      "Reference": "\u817e\u8baf\u7248\u201cGPT-4o\u201d\u6765\u4e86\uff01\u6df7\u5143Turbo\u9996\u53d1\u5e76\u4e0a\u7ebf\uff0c\u6548\u7387\u7ffb\u500d\u4ef7\u683c\u780d\u534a\n",
      "Citations": "",
      "Authors": "",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Best score on SuperCLUE\u603b\u6392\u884c\u699c\uff082024\u5e748\u6708\uff09- SuperCLUE general benchmak from Aug 2024 (https://www.superclueai.com/) in terms of \"science\" and \"liberal arts\" evaluation.\n\nhttps://zhidx.com/p/442250.html",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Available via Tencent Cloud\n\nhttps://www.tencentcloud.com/document/product/647/68338",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Harrison.rad.1",
      "Organization": "Harrison.ai",
      "Publication date": "2024-09-05",
      "Domain": "Vision,Medicine,Language,Multimodal",
      "Task": "Visual question answering,Medical diagnosis",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://harrison.ai/news/harrison-ai-launches-world-leading-ai-model-to-transform-healthcare/",
      "Reference": "Harrison.ai launches world-leading AI model to transform healthcare",
      "Citations": "",
      "Authors": "",
      "Abstract": "Harrison.rad.1 significantly outperforms other major models on radiology tasks\n\nThe new model is being made accessible to select healthcare professionals and regulators to open up the conversation on responsible use of AI in healthcare \n\nSydney, 5th September, 2024 \u2013 Healthcare AI technology company, harrison.ai, today announced the launch of Harrison.rad.1, a radiology-specific vision language model. It represents a major breakthrough in applying AI to tackle the global healthcare challenge. The model is now being made accessible to selected industry partners, healthcare professionals, and regulators around the world to spark collective conversations about the safe, responsible, and ethical use of AI to revolutionise healthcare access and capability, and to improve patient outcomes.",
      "Organization categorization": "Industry",
      "Country (of organization)": "Australia",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "It surpasses other foundational models on the challenging Fellowship of the Royal College of Radiologists (FRCR) 2B Rapids examination \u2013 an exam that only 40-59% of human radiologists manage to pass on their first attempt. When reattempted within a year of passing, radiologists score an average of 50.88 out of 601. Harrison.rad.1 performed on par with accredited and experienced radiologists at 51.4 out of 60, while other competing models such as OpenAI\u2019s GPT-4o, Anthropic\u2019s Claude-3.5-sonnet, Google\u2019s Gemini-1.5 Pro and Microsoft\u2019s LLaVA-Med scored below 30 on average2.\n\nAdditionally, when assessing Harrison.rad.1 using the VQA-Rad benchmark, a dataset of clinically generated visual questions and answers on radiological images, Harrison.rad.1 achieved an impressive 82% accuracy on closed questions, outperforming other leading foundational models\n\n2025: https://www.businesswire.com/news/home/20250818451065/en/Harrison.ais-Foundation-Model-Achieves-Breakthrough-Results-in-Independent-US-Healthcare-AI-Challenge",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "AlphaProteo",
      "Organization": "Google DeepMind",
      "Publication date": "2024-09-05",
      "Domain": "Biology",
      "Task": "Protein generation,Proteins",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "PDB (Protein Data Bank)",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaproteo-generates-novel-proteins-for-biology-and-health-research/AlphaProteo2024.pdf",
      "Reference": "De novo design of high-affinity protein binders with AlphaProteo",
      "Citations": "",
      "Authors": "Vinicius Zambaldi, David La, Alexander E. Chu, Harshnira Patani, Amy E. Danson, Tristan O. C. Kwan, Thomas Frerix, Rosalia G. Schneider, David Saxton, Ashok Thillaisundaram, Zachary Wu, Isabel Moraes, Oskar Lange, Eliseo Papa, Gabriella Stanton, Victor Martin, Sukhdeep Singh, Lai H. Wong, Russ Bates, Simon A. Kohl, Josh Abramson, Andrew W. Senior, Yilmaz Alguel, Mary Y. Wu,\nIrene M. Aspalter, Katie Bentley, David L.V. Bauer, Peter Cherepanov, Demis Hassabis, Pushmeet Kohli, Rob Fergus, Jue Wang",
      "Abstract": "Computational design of protein-binding proteins is a fundamental capability with broad utility in biomedical research and biotechnology. Recent methods have made strides against some target proteins, but on-demand creation of high-affinity binders without multiple rounds of experimental testing remains\nan unsolved challenge. This technical report introduces AlphaProteo, a family of machine learning models for protein design, and details its performance on the de novo binder design problem. With AlphaProteo, we achieve 3- to 300-fold better binding affinities and higher experimental success rates than the best existing methods on seven target proteins. Our results suggest that AlphaProteo can generate binders \"ready-to-use\" for many research applications using only one round of medium-throughput screening\nand no further optimization.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "Economic impacts from development of commercially and socially valuable protein designs and materials",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Hairuo",
      "Organization": "Inspur",
      "Publication date": "2024-08-29",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://encloud.inspur.com/hairuo/cp/index.html\n\nhttps://www.eguizhou.gov.cn/guiyang/2024-08/29/c_1016827.htm",
      "Reference": "",
      "Citations": "",
      "Authors": "",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Highest score at SuperGLUE Benchmark https://super.gluebenchmark.com/leaderboard/",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Hosted access (no API)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GLM-4-Plus",
      "Organization": "Z.ai (Zhipu AI)",
      "Publication date": "2024-08-29",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "Estimated to be 3.6e+25 FLOP using benchmark imputation. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Speculative",
      "Link": "https://bigmodel.cn/dev/howuse/glm-4",
      "Reference": "GLM-4-Plus",
      "Citations": "",
      "Authors": "Zhipu AI",
      "Abstract": "At the KDD International Conference on Data Mining and Knowledge Discovery, the Zhipu GLM team unveiled the new generation of base large model\u2014GLM-4-Plus. As the latest version of Zhipu\u2019s fully self-developed GLM large model, GLM-4-Plus signifies Zhipu AI\u2019s continuous dedication in the field of general artificial intelligence, advancing the independent and autonomous innovation of large model technology.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Jamba 1.5-Large",
      "Organization": "AI21 Labs",
      "Publication date": "2024-08-22",
      "Domain": "Language",
      "Task": "Language modeling/generation,Chat,Translation,Question answering",
      "Parameters": "398000000000.0",
      "Parameters notes": "94B active/398B total",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2408.12570\nhttps://www.ai21.com/blog/announcing-jamba-model-family\nhttps://huggingface.co/ai21labs/AI21-Jamba-1.5-Large",
      "Reference": "Jamba-1.5: Hybrid Transformer-Mamba Models at Scale",
      "Citations": "",
      "Authors": "Barak Lenz, Alan Arazi, Amir Bergman, Avshalom Manevich, Barak Peleg, Ben Aviram, Chen Almagor, Clara Fridman, Dan Padnos, Daniel Gissin, Daniel Jannai, Dor Muhlgay, Dor Zimberg, Edden M Gerber, Elad Dolev, Eran Krakovsky, Erez Safahi, Erez Schwartz, Gal Cohen, Gal Shachaf, Haim Rozenblum, Hofit Bata, Ido Blass, Inbal Magar, Itay Dalmedigos, Jhonathan Osin, Julie Fadlon, Maria Rozman, Matan Danos, Michael Gokhman, Mor Zusman, Naama Gidron, Nir Ratner, Noam Gat, Noam Rozen, Oded Fried, Ohad Leshno, Omer Antverg, Omri Abend, Opher Lieber, Or Dagan, Orit Cohavi, Raz Alon, Ro'i Belson, Roi Cohen, Rom Gilad, Roman Glozman, Shahar Lev, Shaked Meirom, Tal Delbari, Tal Ness, Tomer Asida, Tom Ben Gal, Tom Braude, Uriya Pumerantz, Yehoshua Cohen, Yonatan Belinkov, Yuval Globerson, Yuval Peleg Levy, Yoav Shoham",
      "Abstract": "We present Jamba-1.5, new instruction-tuned large language models based on our Jamba architecture. Jamba is a hybrid Transformer-Mamba mixture of experts architecture, providing high throughput and low memory usage across context lengths, while retaining the same or better quality as Transformer models. We release two model sizes: Jamba-1.5-Large, with 94B active parameters, and Jamba-1.5-Mini, with 12B active parameters. Both models are fine-tuned for a variety of conversational and instruction-following capabilties, and have an effective context length of 256K tokens, the largest amongst open-weight models. To support cost-effective inference, we introduce ExpertsInt8, a novel quantization technique that allows fitting Jamba-1.5-Large on a machine with 8 80GB GPUs when processing 256K-token contexts without loss of quality. When evaluated on a battery of academic and chatbot benchmarks, Jamba-1.5 models achieve excellent results while providing high throughput and outperforming other open-weight models on long-context benchmarks. The model weights for both sizes are publicly available under the Jamba Open Model License and we release ExpertsInt8 as open source.",
      "Organization categorization": "Industry",
      "Country (of organization)": "Israel",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA H100 SXM5 80GB",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Commercial use allowed up to $50M USD annual revenue.",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Grok-2",
      "Organization": "xAI",
      "Publication date": "2024-08-13",
      "Domain": "Language,Vision,Multimodal",
      "Task": "Chat,Language modeling/generation,Question answering,Code generation,Visual question answering",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "2.96e+25",
      "Training compute notes": "Estimate based on xAI statements comparing Grok-2 compute to GPT-4 and Grok-3. Full estimate here: https://docs.google.com/document/d/1C_dABuZrAqYE_ui4_GZ4bRLtq3TBjIGoBSktaPElhEU/edit?usp=sharing",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://x.ai/blog/grok-2",
      "Reference": "Grok-2 Beta Release",
      "Citations": "",
      "Authors": "",
      "Abstract": "Grok-2 is our frontier language model with state-of-the-art reasoning capabilities. This release includes two members of the Grok family: Grok-2 and Grok-2 mini. Both models are now being released to Grok users on the \ud835\udd4f platform.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA H100 SXM5 80GB",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "31602310.530835517",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Table Tennis Agent",
      "Organization": "Google DeepMind",
      "Publication date": "2024-08-07",
      "Domain": "Robotics",
      "Task": "Sports",
      "Parameters": "185000.0",
      "Parameters notes": "17 low level controllers with 10k parameters each: \n\"Each policy is a dilated-gated CNN\n[22] following the architecture in [23] with 10k parameters... The final\nsystem contained 17 LLCs\"\n\nOne high-level controller with 4.5k parameters: \"The style policy architecture, similar to the LLC but with\nonly 4.5k parameters, has a (8, 128) observation space\"\n\nspin classifier that is a 2-layer MLP of hidden sizes (128, 64) and input size 18, which is 10k parameters per o1 and Claude.\n\nSo ~185k parameters total",
      "Training compute (FLOP)": "",
      "Training compute notes": "unclear",
      "Training dataset": "",
      "Training dataset size (gradients)": "2400000000",
      "Dataset size notes": "~18k ball states\n\n\"This iterative cycle of training models in simulation on\nthe latest dataset, evaluating it in the real world, and using\nthe annotated evaluation data to extend the dataset, can be\nrepeated as many times as needed. We completed 7 cycles\nfor rally balls and 2 cycles for serving balls over the course\nof 3 months with over 50 different human opponents, leading\nto a final dataset size of 14.2k initial ball states for rallies\nand 3.4k for serves. A summary of the dataset evolution is\npresented in Table I and Figure 6.\"",
      "Confidence": "Likely",
      "Link": "https://deepmind.google/research/publications/107741/\nhttps://arxiv.org/abs/2408.03906",
      "Reference": "Achieving Human Level Competitive Robot Table Tennis",
      "Citations": "",
      "Authors": "David B. D'Ambrosio, Saminda Abeyruwan, Laura Graesser, Atil Iscen, Heni Ben Amor, Alex Bewley, Barney J. Reed, Krista Reymann, Leila Takayama, Yuval Tassa, Krzysztof Choromanski, Erwin Coumans, Deepali Jain, Navdeep Jaitly, Natasha Jaques, Satoshi Kataoka, Yuheng Kuang, Nevena Lazic, Reza Mahjourian, Sherry Moore, Kenneth Oslund, Anish Shankar, Vikas Sindhwani, Vincent Vanhoucke, Grace Vesom, Peng Xu, and Pannag R. Sanketi",
      "Abstract": "Achieving human-level speed and performance on real world tasks is a north star for the robotics research community. This work takes a step towards that goal and presents the first learned robot agent that reaches amateur human-level performance in competitive table tennis. Table tennis is a physically demanding sport which requires human players to undergo years of training to achieve an advanced level of proficiency. In this paper, we contribute (1) a hierarchical and modular policy architecture consisting of (i) low level controllers with their detailed skill descriptors which model the agent's capabilities and help to bridge the sim-to-real gap and (ii) a high level controller that chooses the low level skills, (2) techniques for enabling zero-shot sim-to-real including an iterative approach to defining the task distribution that is grounded in the real-world and defines an automatic curriculum, and (3) real time adaptation to unseen opponents. Policy performance was assessed through 29 robot vs. human matches of which the robot won 45% (13/29). All humans were unseen players and their skill level varied from beginner to tournament level. Whilst the robot lost all matches vs. the most advanced players it won 100% matches vs. beginners and 55% matches vs. intermediate players, demonstrating solidly amateur human-level performance. Videos of the matches can be viewed at this https URL",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"first learned robot agent that reaches amateur human-level performance in competitive table tennis\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "LLaVA-OV-72B",
      "Organization": "ByteDance,Nanyang Technological University,Chinese University of Hong Kong (CUHK),Hong Kong University of Science and Technology (HKUST)",
      "Publication date": "2024-08-06",
      "Domain": "Multimodal,Vision,Language,Video",
      "Task": "Image captioning,Visual question answering,Video description,Object recognition,Action recognition,Language modeling/generation",
      "Parameters": "72000000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "3.036551985824e+24",
      "Training compute notes": "FineTune: 6*72000000000*38314782000=1.655199e+22\n\nBase model: 3.02e+24\nTotal: 3.036552e+24",
      "Training dataset": "LLaVA-OneVision",
      "Training dataset size (gradients)": "38314782000",
      "Dataset size notes": "Stage 1: 558000*729=406782000\nStage 1.5: 4M*729*5=14580000000\nStage 2-single: 3.2M*729*5=11664000000\nStage 2-one vision: 1.6M*729*10=11664000000\nTotal: 38314782000",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2408.03326",
      "Reference": "LLaVA-OneVision: Easy Visual Task Transfer\n",
      "Citations": "1692.0",
      "Authors": "Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, Chunyuan Li",
      "Abstract": "We present LLaVA-OneVision, a family of open large multimodal models (LMMs) developed by consolidating our insights into data, models, and visual representations in the LLaVA-NeXT blog series. Our experimental results demonstrate that LLaVA-OneVision is the first single model that can simultaneously push the performance boundaries of open LMMs in three important computer vision scenarios: single-image, multi-image, and video scenarios. Importantly, the design of LLaVAOneVision allows strong transfer learning across different modalities/scenarios, yielding new emerging capabilities. In particular, strong video understanding and cross-scenario capabilities are demonstrated through task transfer from images to\nvideos.",
      "Organization categorization": "Industry,Academia,Academia,Academia",
      "Country (of organization)": "China,Singapore,Hong Kong,Hong Kong",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "As shown in Table 4, LLaVA-OneVision (SI) consistently outperforms existing multi-image LMMs in all benchmarks. After additional tuning on multi-image and video data, LLaVA-OneVision shows a marked improvement over GPT-4V in specific areas, with significant margins. This highlights its strong performance in complex tasks such as multi-image reasoning, identifying differences, and understanding 3D environments",
      "Epochs": "1.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "Qwen2-72B",
      "Finetune compute (FLOP)": "1.6551985824e+22",
      "Finetune compute notes": "6 FLOP/parameter/token * 72000000000 parameters * 38314782000 tokens * 1 epochs = 1.6551985824e+22 FLOP",
      "Batch size": "256.0",
      "Batch size notes": "We use a global batch size of 512 for the 0.5B model, and 256 for the 7B and 72B models.",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "",
      "Accessibility notes": "Apache 2\n\nhttps://huggingface.co/lmms-lab/llava-onevision-qwen2-72b-ov-sft\n\nhttps://github.com/LLaVA-VL/LLaVA-NeXT",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GPT-4o (Aug 2024)",
      "Organization": "OpenAI",
      "Publication date": "2024-08-06",
      "Domain": "Multimodal,Language,Audio,Speech,Vision",
      "Task": "Chat,Image generation,Audio generation,Vision-language generation,Table tasks,Language modeling/generation,Question answering,Speech recognition (ASR),Speech-to-text",
      "Parameters": "",
      "Parameters notes": "Not known.\n\nInference costs in the API are 2x cheaper than GPT-4 Turbo",
      "Training compute (FLOP)": "",
      "Training compute notes": "Training compute estimated to be 3.8e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Speculative",
      "Link": "https://help.openai.com/en/articles/9624314-model-release-notes",
      "Reference": "Update to GPT-4o",
      "Citations": "",
      "Authors": "Aidan Clark, Alex Paino, Jacob Menick, Liam Fedus, Luke Metz, Clemens Winter, Lia Guy, Sam Schoenholz, Daniel Levy, Nitish Keskar, Alex Carney, Alex Paino, Ian Sohl, Qiming Yuan, Reimar Leike, Arka Dhar, Brydon Eastman, Mia Glaese, Ben Sokolowsky, Andrew Kondrich, Felipe Petroski Such, Henrique Ponde de Oliveira Pinto, Jiayi Weng, Randall Lin, Youlong Cheng, Nick Ryder, Lauren Itow, Barret Zoph, John Schulman, Mianna Chen, Adam Lerer, Adam P. Goucher, Adam Perelman, Akila Welihinda, Alec Radford, Alex Borzunov, Alex Carney, Alex Chow, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexi Christakis, Ali Kamali, Allison Moyer, Allison Tam, Amin Tootoonchian, Ananya Kumar, Andrej Karpathy, Andrey Mishchenko, Andrew Cann, Andrew Kondrich, Andrew Tulloch, Angela Jiang, Antoine Pelisse, Anuj Gosalia, Avi Nayak, Avital Oliver, Behrooz Ghorbani, Ben Leimberger, Ben Wang, Blake Samic, Brian Guarraci, Brydon Eastman, Camillo Lugaresi, Chak Li, Charlotte Barette, Chelsea Voss, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christopher Hesse, Colin Wei, Daniel Kappler, Daniel Levin, Daniel Levy, David Farhi, David Mely, David Sasaki, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Peterson, Eric Sigler, Eugene Brevdo, Farzad Khorasani, Francis Zhang, Gene Oden, Geoff Salmon, Hadi Salman, Haiming Bao, Heather Schmidt, Hongyu Ren, Hyung Won Chung, Ian Kivlichan, Ian O'Connell, Ian Osband, Ilya Kostrikov, Ingmar Kanitscheider, Jacob Coxon, James Crooks, James Lennon, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jiayi Weng, Jie Tang, Joanne Jang, Jonathan Ward, Jonathan McKay, Jong Wook Kim, Josh Gross, Josh Kaplan, Joy Jiao, Joyce Lee, Juntang Zhuang, Kai Fricke, Kavin Karthik, Kenny Hsu, Kiel Howe, Kyle Luther, Larry Kai, Lauren Itow, Leo Chen, Lia Guy, Lien Mamitsuka, Lilian Weng, Long Ouyang, Louis Feuvrier, Lukas Kondraciuk, Lukasz Kaiser, Lyric Doshi, Mada Aflak, Maddie Simens, Madeleine Thompson, Marat Dukhan, Marvin Zhang, Mateusz Litwin, Max Johnson, Mayank Gupta, Mia Glaese, Michael Janner, Michael Petrov, Michael Wu, Michelle Fradin, Michelle Pokrass, Miguel Oom Temudo de Castro, Mikhail Pavlov, Minal Khan, Mo Bavarian, Natalia Gimelshein, Natalie Staudacher, Nick Stathas, Nik Tezak, Nithanth Kudige, Noel Bundick, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivier Godement, Owen Campbell-Moore, Philip Pronin, Philippe Tillet, Rachel Lim, Rajan Troll, Randall Lin, Rapha gontijo lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Rob Honsby, Rohit Ramchandani, Rory Carmichael, Ruslan Nigmatullin, Ryan Cheu, Scott Gray, Sean Grove, Sean Metzger, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shuaiqi (Tony) Xia, Sonia Phene, Spencer Papay, Steve Coffey, Steve Lee, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tarun Gogineni, Ted Sanders, Thomas Cunninghman, Thomas Dimson, Thomas Raoux, Tianhao Zheng, Tina Kim, Todd Underwood, Tristan Heywood, Valerie Qi, Vinnie Monaco, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wojciech Zaremba, Yash Patil, Yilei, Qian, Yongjik Kim, Youlong Cheng, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, Yury Malkov",
      "Abstract": "We\u2019re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.\n\nGPT-4o (\u201co\u201d for \u201comni\u201d) is a step towards much more natural human-computer interaction\u2014it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement,Significant use",
      "Notability criteria notes": "Outperforms GPT-4 Turbo and other models on text and especially on multimodal benchmarks, such as MMLU, GPQA, HumanEval, MMMU, etc See Model Evaluations: https://openai.com/index/hello-gpt-4o/ \n\nGPT-4o is now the default model in ChatGPT, so it's one of the most widely used models.",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "GPT-4o (May 2024)",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "Definitely a new model, not a GPT-4 finetune",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "AFM-server",
      "Organization": "Apple",
      "Publication date": "2024-07-29",
      "Domain": "Language",
      "Task": "Language modeling/generation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "4.3e+24",
      "Training compute notes": "\"The AFM base models are dense decoder-only models that build on the\nTransformer architecture\"\n\n\"We train AFM-server from scratch for 6.3T tokens on 8192\nTPUv4 chips, using a sequence length of 4096 and a batch-size of 4096 sequences.\"\n\n\"For both models we perform continued pre-training at a sequence length of\n8192, with another 1T tokens from a mixture that upweights math and code,\nand down-weights the bulk web-crawl.\"\n\n\"The sustained model-flop-utilization (MFU) for this training run was approximately 52%.\"\n\nParameter count is not specified other than it being \"larger\" than 3 billion.\n\nCounting FLOP: Chinchilla scaling laws would suggest 7.3T / 20 = 365B parameters. \n\n365B parameters * 7.3T tokens * 6 ~= 1.6e25 FLOP.\n\nHowever, the attention to inference optimization in the technical report suggests a smaller size, even for this \"server\" model. One point of reference is Llama 3 70B being overtrained by a factor of 10. If this is true of AFM-server, the parameter count would be ~37B and training compute would be 1.6e24 FLOP.\n\nGPU-time: assume a wall-clock training time of 30 days based on the current trend value for notable models.\n\n8192 chips * 275e12 FLOP/s per chip * 0.52 utilization * 30 * 24 * 60 * 60 s ~= 3.0e24 FLOP\n\nThe geometric mean of these three estimates is 4.3e24 FLOP.",
      "Training dataset": "",
      "Training dataset size (gradients)": "7400000000000",
      "Dataset size notes": "Not explicitly mentioned, but I assume the 7.4T tokens do not involve multiple epochs.",
      "Confidence": "Likely",
      "Link": "https://machinelearning.apple.com/research/apple-intelligence-foundation-language-models",
      "Reference": "Apple Intelligence Foundation Language Models",
      "Citations": "",
      "Authors": "Andy Narayanan, Aonan Zhang, Bowen Zhang, Chen Chen, Chong Wang, Chung-Cheng Chiu, David Qiu, Deepak Gopinath, Dian Ang Yap, Dong Yin, Feng Nan, Floris Weers, Guoli Yin, Haoshuo Huang, Jianyu Wang, Jiarui Lu, John Peebles, Ke Ye, Mark Lee, Nan Du, Qibin Chen, Quentin Keunebroek, Ruoming Pang, Sam Wiseman, Syd Evans, Tao Lei, Tom Gunter, Vivek Rathod, Xiang Kong, Xianzhi Du, Yanghao Li, Yongqiang Wang, Yuan Gao, Zaid Ahmed, Zhaoyang Xu, Zhiyun Lu, Zirui Wang, Al Rashid, Albin Madappally Jose, Alec Doane, Alfredo Bencomo, Allison Vanderby, Andrew Hansen, Ankur Jain, Anupama Mann Anupama, Areeba\nKamal, Bugu Wu, Carolina Brum, Charlie Maalouf, Chinguun Erdenebileg,\nChris Dulhanty, Dominik Moritz, Doug Kang, Eduardo Jimenez, Evan Ladd,\nFangping Shi, Felix Bai, Frank Chu, Fred Hohman, Hadas Kotek, Hannah\nGillis Coleman, Jane Li, Jeffrey Bigham, Jeffery Cao, Jeff Lai, Jessica Cheung, Jiulong Shan, Joe Zhou, John Li, Jun Qin, Karanjeet Singh, Karla Vega, Ke Ye, Kelvin Zou, Laura Heckman, Lauren Gardiner, Margit Bowler, Mark Lee, Maria Cordell, Meng Cao, Nicole Hay, Nilesh Shahdadpuri, Otto Godwin, Pranay Dighe, Pushyami Rachapudi, Ramsey Tantawi, Roman Frigg, Sam Davarnia, Sanskruti Shah, Saptarshi Guha, Sasha Sirovica, Shen Ma, Shuang Ma, Simon Wang, Sulgi Kim, Suma Jayaram, Vaishaal Shankar, Varsha Paidi, Vivek Kumar, Xiang Kong, Xin Wang, Xin Zheng, Walker Cheng, Yael Shrager, Yang Ye, Yasu Tanaka, Yihao Guo, Yunsong Meng, Zhao Tang Luo, Zhi Ouyang, Zhiyun Lu, Alp Aygar, Alvin Wan, Andrew Walkingshaw, Andy Narayanan, Antonie Lin, Arsalan Farooq, Brent Ramerth, Chong Wang, Colorado Reed, Chris Bartels, Chris Chaney, David Riazati, Eric Liang Yang, Erin Feldman, Gabriel Hochstrasser, Guillaume Seguin, Guoli Yin, Irina Belousova, Jianyu Wang, Joris Pelemans, Karen Yang, Keivan Alizadeh Vahid, Liangliang Cao, Mahyar Najibi , Marco Zuliani, Max Horton, Minsik Cho, Nikhil Bhendawade, Patrick Dong, Piotr Maj, Pulkit Agrawal, Qi Shan, Qibin Chen, Qichen Fu, Regan Poston, Sam Xu, Shuangning Liu, Sushma Rao, Tashweena Heeramun, Thomas Merth, Uday Rayala, Victor Cui, Vivek Rangarajan Sridhar, Vivek Rathod, Wencong Zhang, Wenqi Zhang, Wentao Wu, Xiang Kong, Xingyu Zhou, Xinwen Liu, Yang Zhao, Yin Xia, Zhile Ren, Zhongzheng Ren",
      "Abstract": "We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Significant use",
      "Notability criteria notes": "Currently in beta access only, but will be integrated into millions or billions of iPhones.",
      "Epochs": "1.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "Google TPU v4",
      "Hardware quantity": "8192.0",
      "Hardware utilization (MFU)": "0.52",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "5493309.4800721435",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "18949752.5758905",
      "Batch size notes": "Main pretraining uses sequence length of 4096 tokens; 4096 sequences per batch. During the \"continued\" pre-training stage, sequence length is upped to 8192 while batch size remains 4096. During context lengthening, sequence length is upped to 32768 while \"the recipe is similar to that used for continued pre-training\" implies same batch size of 4096.\n\nWeighting batch sizes by number of tokens seen in each stage:\n\nexp((6.3T * ln(4096 * 4096) + 1T * ln(8192 * 4096) + 100B * ln(32768 * 4096))/ 7.4T) = 18,949,753",
      "Model accessibility": "Hosted access (no API)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "AFM-on-device",
      "Organization": "Apple",
      "Publication date": "2024-07-29",
      "Domain": "Language",
      "Task": "Language modeling/generation",
      "Parameters": "2730000000.0",
      "Parameters notes": "Table 1, sum of non-embedding and embedding parameters",
      "Training compute (FLOP)": "4.5126e+23",
      "Training compute notes": "Model was initialized from a pruned version of a 6.4B parameter model trained using the same recipe as AFM-server. Assuming \"same recipe\" involves training for the full 6.3T tokens, this implies 6 * 6.3T * 6.4B = 2.42e23 FLOP. \n\nThe pruning masks are learned by training over 188B tokens, which suggests 6 * 188B * 6.4B = 7.22e21 FLOPs.\n\nPretraining is then run over 6.3T tokens; however, labels are a convex combination of true labels and the predicted labels from the unpruned 6.4B model. Since this involves running the 6.3T tokens forward through both the 6.4B and the 2.73B model, but only calculating gradients for the smaller model, FLOPs here are equal to (6 * 6.3T * 2.73B) + (2 * 6.3T * 6.4B) = 1.84e23. \n\nFinally, there is a 1T \"continuation\" pretraining stage without distillation loss, for 6 * 1T * 2.73B = 1.64e22 FLOP, and a 100B context-lengthening stage for another 6 * 100B * 2.73B = 1.64e21 FLOP\n\nIn total: 2.42e23 + 7.22e21 + 1.84e23 + 1.64e22 + 1.64e21 = 4.51e23 FLOP",
      "Training dataset": "",
      "Training dataset size (gradients)": "7588000000000",
      "Dataset size notes": "Not explicitly mentioned, but I assume the 7.588T tokens do not involve multiple epochs.",
      "Confidence": "Confident",
      "Link": "https://machinelearning.apple.com/research/apple-intelligence-foundation-language-models",
      "Reference": "Apple Intelligence Foundation Language Models",
      "Citations": "",
      "Authors": "Andy Narayanan, Aonan Zhang, Bowen Zhang, Chen Chen, Chong Wang, Chung-Cheng Chiu, David Qiu, Deepak Gopinath, Dian Ang Yap, Dong Yin, Feng Nan, Floris Weers, Guoli Yin, Haoshuo Huang, Jianyu Wang, Jiarui Lu, John Peebles, Ke Ye, Mark Lee, Nan Du, Qibin Chen, Quentin Keunebroek, Ruoming Pang, Sam Wiseman, Syd Evans, Tao Lei, Tom Gunter, Vivek Rathod, Xiang Kong, Xianzhi Du, Yanghao Li, Yongqiang Wang, Yuan Gao, Zaid Ahmed, Zhaoyang Xu, Zhiyun Lu, Zirui Wang, Al Rashid, Albin Madappally Jose, Alec Doane, Alfredo Bencomo, Allison Vanderby, Andrew Hansen, Ankur Jain, Anupama Mann Anupama, Areeba\nKamal, Bugu Wu, Carolina Brum, Charlie Maalouf, Chinguun Erdenebileg,\nChris Dulhanty, Dominik Moritz, Doug Kang, Eduardo Jimenez, Evan Ladd,\nFangping Shi, Felix Bai, Frank Chu, Fred Hohman, Hadas Kotek, Hannah\nGillis Coleman, Jane Li, Jeffrey Bigham, Jeffery Cao, Jeff Lai, Jessica Cheung, Jiulong Shan, Joe Zhou, John Li, Jun Qin, Karanjeet Singh, Karla Vega, Ke Ye, Kelvin Zou, Laura Heckman, Lauren Gardiner, Margit Bowler, Mark Lee, Maria Cordell, Meng Cao, Nicole Hay, Nilesh Shahdadpuri, Otto Godwin, Pranay Dighe, Pushyami Rachapudi, Ramsey Tantawi, Roman Frigg, Sam Davarnia, Sanskruti Shah, Saptarshi Guha, Sasha Sirovica, Shen Ma, Shuang Ma, Simon Wang, Sulgi Kim, Suma Jayaram, Vaishaal Shankar, Varsha Paidi, Vivek Kumar, Xiang Kong, Xin Wang, Xin Zheng, Walker Cheng, Yael Shrager, Yang Ye, Yasu Tanaka, Yihao Guo, Yunsong Meng, Zhao Tang Luo, Zhi Ouyang, Zhiyun Lu, Alp Aygar, Alvin Wan, Andrew Walkingshaw, Andy Narayanan, Antonie Lin, Arsalan Farooq, Brent Ramerth, Chong Wang, Colorado Reed, Chris Bartels, Chris Chaney, David Riazati, Eric Liang Yang, Erin Feldman, Gabriel Hochstrasser, Guillaume Seguin, Guoli Yin, Irina Belousova, Jianyu Wang, Joris Pelemans, Karen Yang, Keivan Alizadeh Vahid, Liangliang Cao, Mahyar Najibi , Marco Zuliani, Max Horton, Minsik Cho, Nikhil Bhendawade, Patrick Dong, Piotr Maj, Pulkit Agrawal, Qi Shan, Qibin Chen, Qichen Fu, Regan Poston, Sam Xu, Shuangning Liu, Sushma Rao, Tashweena Heeramun, Thomas Merth, Uday Rayala, Victor Cui, Vivek Rangarajan Sridhar, Vivek Rathod, Wencong Zhang, Wenqi Zhang, Wentao Wu, Xiang Kong, Xingyu Zhou, Xinwen Liu, Yang Zhao, Yin Xia, Zhile Ren, Zhongzheng Ren",
      "Abstract": "We present foundation language models developed to power Apple Intelligence features, including a \u223c3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute [Apple, 2024b]. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Significant use",
      "Notability criteria notes": "Currently in beta access only, but will be integrated into millions or billions of iPhones.",
      "Epochs": "1.0",
      "Training time (hours)": "",
      "Training time notes": "Trained on \"one slice of 2048 TPUv5p chips\"; wall-time not given.",
      "Training hardware": "Google TPU v5p",
      "Hardware quantity": "2048.0",
      "Hardware utilization (MFU)": "0.52",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "2181166.9994404097",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "18949752.5758905",
      "Batch size notes": "Main pretraining uses sequence length of 4096 tokens; 4096 sequences per batch. During the \"continued\" pre-training stage, sequence length is upped to 8192 while batch size remains 4096. During context lengthening, sequence length is upped to 32768 while \"the recipe is similar to that used for continued pre-training\" implies same batch size of 4096.\n\nWeighting batch sizes by number of tokens seen in each stage:\n\nexp((6.3T * ln(4096 * 4096) + 1T * ln(8192 * 4096) + 100B * ln(32768 * 4096))/ 7.4T) = 18,949,753",
      "Model accessibility": "Hosted access (no API)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Mistral Large 2",
      "Organization": "Mistral AI",
      "Publication date": "2024-07-24",
      "Domain": "Language",
      "Task": "Language modeling/generation,Translation,Code generation",
      "Parameters": "123000000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "2.13e+25",
      "Training compute notes": "Details are sparse, but we can hazard a guess based on evidence about the training cluster they may have used, the scale up in compute they likely would have used relative to Mistral Large 1, and from the model's MMLU score. Extended reasoning given here: https://docs.google.com/document/d/1I2ZWBLFMpRZYcdMMUfKAGZFJrOJpduNDS9ZeVFIHnd8/edit?usp=sharing",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://mistral.ai/news/mistral-large-2407/",
      "Reference": "Top-tier reasoning for high-complexity tasks, for your most sophisticated needs.",
      "Citations": "",
      "Authors": "Albert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Alok Kothari, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Augustin Garreau, Austin Birky, Bam4d, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Carole Rambaud, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Diogo Costa, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gaspard Blanchet, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Henri Roussez, Hichem Sattouf, Ian Mack, Jean-Malo Delignon, Jessica Chudnovsky, Justus Murke, Kartik Khandelwal, Lawrence Stewart, Louis Martin, Louis Ternon, Lucile Saulnier, L\u00e9lio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Marjorie Janiewicz, Micka\u00ebl Seznec, Nicolas Schuhl, Niklas Muhs, Olivier de Garrigues, Patrick von Platen, Paul Jacob, Pauline Buche, Pavan Kumar Reddy, Perry Savas, Pierre Stock, Romain Sauvestre, Sagar Vaze, Sandeep Subramanian, Saurabh Garg, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibault Schueller, Thibaut Lavril, Thomas Wang, Th\u00e9ophile Gervet, Timoth\u00e9e Lacroix, Valera Nemychnikova, Wendy Shang, William El Sayed, William Marshall",
      "Abstract": "Today, we are announcing Mistral Large 2, the new generation of our flagship model. Compared to its predecessor, Mistral Large 2 is significantly more capable in code generation, mathematics, and reasoning. It also provides a much stronger multilingual support, and advanced function calling capabilities.",
      "Organization categorization": "Industry",
      "Country (of organization)": "France",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "likely high training cost since previous Mistral Large cost around 20 million",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "\"We are releasing Mistral Large 2 under the Mistral Research License, that allows usage and modification for research and non-commercial usages. For commercial usage of Mistral Large 2 requiring self-deployment, a Mistral Commercial License must be acquired by contacting us.\"",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Llama 3.1-405B",
      "Organization": "Meta AI",
      "Publication date": "2024-07-23",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering,Code generation,Mathematical reasoning",
      "Parameters": "405000000000.0",
      "Parameters notes": "405B",
      "Training compute (FLOP)": "3.8e+25",
      "Training compute notes": "Stated in paper.\n\nAlso, 6 * 405B * 15.6T training tokens = 3.8e25",
      "Training dataset": "Llama 3 dataset",
      "Training dataset size (gradients)": "15600000000000",
      "Dataset size notes": "15.6T tokens",
      "Confidence": "Confident",
      "Link": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/",
      "Reference": "The Llama 3 Herd of Models",
      "Citations": "",
      "Authors": "Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAlan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie\nSravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen\nGregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux,\nChaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang\nWu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle\nPintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino,\nDieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip\nRadenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire\nMialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,\nImanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert,\nJana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong,\nJenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe\nSpisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala,\nKartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer,\nKshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence\nChen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat,\nLuke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas,\nMathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar\nSingh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji,\nOlivier Duchenne, Onur \u00c7elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng,\nPrajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong,\nRagavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta\nRaileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor,\nRuan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun\nSonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan,\nShruti Bhosale, Shun Zhang, Simon Vandenhende, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin\nGururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas\nScialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta,\nVignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei\nChu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan,\nXinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen\nSong, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, and Zoe\nPapakipos.\n(core contributors)",
      "Abstract": "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement,Training cost",
      "Notability criteria notes": "High training compute, exceeds 4o and Claude 3.5 on some benchmarks:\n\nhttps://ai.meta.com/blog/meta-llama-3-1/ ",
      "Epochs": "1.0",
      "Training time (hours)": "2142.0",
      "Training time notes": "Trained on 30.84M GPU hours (https://huggingface.co/blog/llama31) and used \"up to 16K H100 GPU[s]\" so training took at least\n30.84M / 16k = 1927.5 hours or ~80 days. \n\nSection 3.3.4 gives reliability details over a 54 day period during training, for which they had \"higher than 90% effective training time\"\n1927.5 / 0.9 = 2142 hours\n\nProbably, full training time is somewhat longer, since it sounds like there were periods where not all 16k H100s were running.",
      "Training hardware": "NVIDIA H100 SXM5 80GB",
      "Hardware quantity": "16384.0",
      "Hardware utilization (MFU)": "0.4042",
      "Training compute cost (2023 USD)": "52885433.95402246",
      "Compute cost notes": "",
      "Training power draw (W)": "22622532.159299146",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "16000000.0",
      "Batch size notes": "",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Open (restricted use)",
      "Inference code accessibility": "Open access (restricted use)",
      "Accessibility notes": "Llama 3.1 model license:\n\nhttps://huggingface.co/meta-llama/Meta-Llama-3.1-8B/blob/main/LICENSE \n\nmust seek separate license if over 700m monthly users, acceptable use restrictions\n\ntraining code here: https://github.com/meta-llama/llama-recipes/blob/main/src/llama_recipes/utils/train_utils.py#L70 \n",
      "Numerical format": "BF16",
      "Frontier model": "True",
      "Hardware acquisition cost": "582299563.0446011",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "175966627.18329293",
      "Training compute cost (upfront)": "928433368.1802773"
    },
    {
      "Model": "GPT-4o mini",
      "Organization": "OpenAI",
      "Publication date": "2024-07-18",
      "Domain": "Language,Multimodal,Vision",
      "Task": "Chat,Language modeling/generation,Code generation,Visual question answering",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "Training compute estimated to be 7.36001e+24 from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing\n\n90% CI [3.23e+24, 2.05e+25]",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Speculative",
      "Link": "https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/",
      "Reference": "GPT-4o mini: advancing cost-efficient intelligence",
      "Citations": "",
      "Authors": "Pre-training leads\nAidan Clark, Alex Paino, Jacob Menick\n\nPost-training leads\nLiam Fedus, Luke Metz\n\nArchitecture leads\nClemens Winter, Lia Guy\n\nOptimization leads\nSam Schoenholz, Daniel Levy\n\nLong-context lead\nNitish Keskar\n\nPre-training Data leads\nAlex Carney, Alex Paino, Ian Sohl, Qiming Yuan\n\nTokenizer lead\nReimar Leike\n\nHuman data leads\nArka Dhar, Brydon Eastman, Mia Glaese\n\nEval lead\nBen Sokolowsky\n\nData flywheel lead\nAndrew Kondrich\n\nInference lead\nFelipe Petroski Such\n\nInference Productionization lead\nHenrique Ponde de Oliveira Pinto\n\nPost-training infrastructure leads\nJiayi Weng, Randall Lin, Youlong Cheng\n\nPre-training organization lead\nNick Ryder\n\nPre-training program lead\nLauren Itow\n\nPost-training organization leads\nBarret Zoph, John Schulman\n\nPost-training program lead\nMianna Chen\n\nCore contributors\nAdam Lerer, Adam P. Goucher, Adam Perelman, Akila Welihinda, Alec Radford, Alex Borzunov, Alex Carney, Alex Chow, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexi Christakis, Ali Kamali, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoonchian, Ananya Kumar, Andrej Karpathy, Andrey Mishchenko, Andrew Cann, Andrew Kondrich, Andrew Tulloch, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Avi Nayak, Avital Oliver, Behrooz Ghorbani, Ben Leimberger, Ben Wang, Beth Hoover, Blake Samic, Brian Guarraci, Brydon Eastman, Camillo Lugaresi, Chak Li, Charlotte Barette, Chelsea Voss, Chen Ding, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christine Choi, Christopher Hesse, Colin Wei, Daniel Kappler, Daniel Levin, Daniel Levy, David Farhi, David Mely, David Sasaki, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Peterson, Eric Sigler, Eugene Brevdo, Farzad Khorasani, Francis Zhang, Gene Oden, Geoff Salmon, Hadi Salman, Haiming Bao, Heather Schmidt, Hongyu Ren, Hyung Won Chung, Ian Kivlichan, Ian O'Connell, Ian Osband, Ibrahim Okuyucu, Ilya Kostrikov, Ingmar Kanitscheider, Jacob Coxon, James Crooks, James Lennon, Jane Park, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jiayi Weng, Jie Tang, Joanne Jang, Jonathan Ward, Jonathan McKay, Jong Wook Kim, Josh Gross, Josh Kaplan, Joy Jiao, Joyce Lee, Juntang Zhuang, Kai Fricke, Kavin Karthik, Kenny Hsu, Kiel Howe, Kyle Luther, Larry Kai, Lauren Itow, Leo Chen, Lia Guy, Lien Mamitsuka, Lilian Weng, Long Ouyang, Louis Feuvrier, Lukas Kondraciuk, Lukasz Kaiser, Lyric Doshi, Mada Aflak, Maddie Simens, Madeleine Thompson, Marat Dukhan, Marvin Zhang, Mateusz Litwin, Matthew Zeng, Max Johnson, Mayank Gupta, Mia Glaese, Michael Janner, Michael Petrov, Michael Wu, Michelle Fradin, Michelle Pokrass, Miguel Oom Temudo de Castro, Mikhail Pavlov, Minal Khan, Mo Bavarian, Murat Yesildal, Natalia Gimelshein, Natalie Staudacher, Nick Stathas, Nik Tezak, Nithanth Kudige, Noel Bundick, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivier Godement, Owen Campbell-Moore, Philip Pronin, Philippe Tillet, Rachel Lim, Rajan Troll, Randall Lin, Rapha gontijo lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Rob Honsby, Rohit Ramchandani, Rory Carmichael, Ruslan Nigmatullin, Ryan Cheu, Sara Culver, Scott Gray, Sean Grove, Sean Metzger, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shuaiqi (Tony) Xia, Sonia Phene, Spencer Papay, Steve Coffey, Steve Lee, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tarun Gogineni, Ted Sanders, Thomas Cunninghman, Thomas Dimson, Thomas Raoux, Tianhao Zheng, Christina Kim, Todd Underwood, Tristan Heywood, Valerie Qi, Vinnie Monaco, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wojciech Zaremba, Yash Patil, Yilei, Qian, Yongjik Kim, Youlong Cheng, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, Yury Malkov",
      "Abstract": "OpenAI is committed to making intelligence as broadly accessible as possible. Today, we're announcing GPT-4o mini, our most cost-efficient small model. We expect GPT-4o mini will significantly expand the range of applications built with AI by making intelligence much more affordable. GPT-4o mini scores 82% on MMLU and currently outperforms GPT-41 on chat preferences in LMSYS leaderboard(opens in a new window). It is priced at 15 cents per million input tokens and 60 cents per million output tokens, an order of magnitude more affordable than previous frontier models and more than 60% cheaper than GPT-3.5 Turbo.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Significant use",
      "Notability criteria notes": "No public breakdown of GPT-4o mini users, but as of late 2024, it is one of the few main models available in ChatGPT and OpenAI's cheapest model. OpenAI has hundreds of millions of users.",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Mathstral",
      "Organization": "Mistral AI",
      "Publication date": "2024-07-16",
      "Domain": "Mathematics,Language",
      "Task": "Mathematical reasoning,Language modeling/generation,Quantitative reasoning",
      "Parameters": "7000000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://huggingface.co/mistralai/Mathstral-7B-v0.1\nhttps://mistral.ai/news/mathstral",
      "Reference": "Mathstral-7B-v0.1 ",
      "Citations": "",
      "Authors": "Albert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Alok Kothari, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Augustin Garreau, Austin Birky, Bam4d, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Carole Rambaud, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gaspard Blanchet, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Henri Roussez, Hichem Sattouf, Ian Mack, Jean-Malo Delignon, Jessica Chudnovsky, Justus Murke, Kartik Khandelwal, Lawrence Stewart, Louis Martin, Louis Ternon, Lucile Saulnier, L\u00e9lio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Marjorie Janiewicz, Micka\u00ebl Seznec, Nicolas Schuhl, Niklas Muhs, Olivier de Garrigues, Patrick von Platen, Paul Jacob, Pauline Buche, Pavan Kumar Reddy, Perry Savas, Pierre Stock, Romain Sauvestre, Sagar Vaze, Sandeep Subramanian, Saurabh Garg, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibault Schueller, Thibaut Lavril, Thomas Wang, Th\u00e9ophile Gervet, Timoth\u00e9e Lacroix, Valera Nemychnikova, Wendy Shang, William El Sayed, William Marshall",
      "Abstract": "We're contributing Mathstral to the science community to bolster efforts in advanced mathematical problems requiring complex, multi-step logical reasoning. The Mathstral release is part of our broader effort to support academic projects\u2014it was produced in the context of our collaboration with Project Numina.\n\nAkin to Isaac Newton in his time, Mathstral stands on the shoulders of Mistral 7B and specializes in STEM subjects. It achieves state-of-the-art reasoning capacities in its size category across various industry-standard benchmarks. In particular, it achieves 56.6% on MATH and 63.47% on MMLU, with the following MMLU performance difference by subject between Mathstral 7B and Mistral 7B.",
      "Organization categorization": "Industry",
      "Country (of organization)": "France",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "SOTA on MathOdyssey and AMC 2023 benchmarks.",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "Mistral 7B",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Apache 2.0 but \"You need to agree to share your contact information to access this model\"\nhttps://huggingface.co/mistralai/Mathstral-7B-v0.1",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DeepL LLM",
      "Organization": "DeepL",
      "Publication date": "2024-07-16",
      "Domain": "Language",
      "Task": "Translation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://www.deepl.com/en/blog/next-gen-language-model",
      "Reference": "DeepL's next-gen LLM outperforms ChatGPT-4, Google, and Microsoft for translation quality",
      "Citations": "",
      "Authors": "DeepL",
      "Abstract": "Our next-generation (\u201cnext-gen\u201d) language model outperforms Google Translate, ChatGPT-4, and Microsoft for translation quality\nThe new LLM's translations require fewer edits, with Google needing 2x and ChatGPT-4 needing 3x more edits to achieve the same quality\nBuilt using our own groundbreaking, specialized LLM technology and proprietary training data, designed specifically for translation\nThe same enterprise-level security you\u2019re used to for Pro customers",
      "Organization categorization": "Industry",
      "Country (of organization)": "Germany",
      "Notability criteria": "Significant use",
      "Notability criteria notes": "DeepL has ~10M monthly users: https://www.forbes.com/sites/rashishrivastava/2023/08/08/deepl-is-trying-to-take-on-google-translate-and-chatgpt/#:~:text=DeepL%20boasts%20more%20than%2010,struggle%20with%20a%20language%20barrier.= ",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Hosted access (no API)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "SenseChat 5.5",
      "Organization": "SenseTime",
      "Publication date": "2024-07-06",
      "Domain": "Multimodal,Language,Vision",
      "Task": "Vision-language generation,Visual question answering,Language modeling/generation,Question answering,Chat,Quantitative reasoning",
      "Parameters": "600000000000.0",
      "Parameters notes": "https://en.tmtpost.com/post/7159076",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://www.sensetime.com/en/news-detail/51168278?categoryId=1072",
      "Reference": "SenseTime Unveils SenseNova 5.5 - a Complete and Comprehensive Upgrade",
      "Citations": "",
      "Authors": "SenseTime",
      "Abstract": "SenseTime, a strategic partner of the 2024 World Artificial Intelligence Conference & High-Level Meeting on Global AI Governance (WAIC 2024), held its AI Forum on \"AI+: Catalyzing Next-Gen Transformations\", where it unveiled the upgraded SenseNova 5.5 Large Model. The updates include SenseNova 5o, the first real-time multimodal model in China, which provides a new AI interaction model on par with GPT-4o\u2019s streaming interaction capabilities.",
      "Organization categorization": "Industry",
      "Country (of organization)": "Hong Kong",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Best score on SuperCLUE\u603b\u6392\u884c\u699c\uff082024\u5e7412\u6708\uff09- SuperCLUE general benchmak from Dec 2024 (https://www.superclueai.com/) in terms of \"liberal arts\" evaluation.",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Hosted access (no API)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Unreleased",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Ernie 4.0 Turbo",
      "Organization": "Baidu",
      "Publication date": "2024-06-28",
      "Domain": "Multimodal,Language,Vision",
      "Task": "Vision-language generation,Language modeling/generation,Question answering,Chat,Visual question answering",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "Unlikely to be >1e25 FLOP given ERNIE 4.5 was <1e25 FLOP.",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://www.reuters.com/technology/artificial-intelligence/baidu-launches-upgraded-ai-model-says-user-base-hits-300-mln-2024-06-28/",
      "Reference": "Baidu launches upgraded AI model, says Ernie Bot hits 300 mln users",
      "Citations": "",
      "Authors": "Baidu",
      "Abstract": "BEIJING, June 28 (Reuters) - Chinese search engine giant Baidu (9888.HK), opens new tab on Friday unveiled an upgraded version of its artificial intelligence (AI) model, Ernie 4.0 Turbo, as it seeks to maintain its position in China's competitive AI market.\nThe launch follows the October 2023 release of Ernie 4, which Baidu claimed rivaled OpenAI's GPT-4 in capabilities.\nThe new model will be accessible to the public via web and mobile app interfaces, with developers able to integrate the technology through Baidu's Qianfan AI platform, the company's Chief Technology Officer Wang Haifeng said at a corporate event.\nWang said its artificial intelligence chatbot Ernie Bot has reached 300 million users since its launch.\nBaidu on Friday also announced an upgrade to its PaddlePaddle AI ecosystem, which it said now supports 14.65 million developers and serves 370,000 businesses and institutions.\nOpenAI announced this week plans to block access to its API from China and other countries starting July 9. The decision affects numerous Chinese enterprises that have been using OpenAI's technology.\nIn response, Baidu, Alibaba (9988.HK), opens new tab and other domestic AI firms have launched initiatives to attract affected users, offering free migration services and incentives.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "Significant use",
      "Notability criteria notes": "300 million users",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ESM3 (98B)",
      "Organization": "EvolutionaryScale,University of California (UC) Berkeley",
      "Publication date": "2024-06-25",
      "Domain": "Biology",
      "Task": "Protein generation",
      "Parameters": "98500000000.0",
      "Parameters notes": "98.5 billion (Table S1)",
      "Training compute (FLOP)": "1.07e+24",
      "Training compute notes": "\"ESM3 at its largest scale was trained with 1.07\u00d710^24 FLOPs on 2.78 billion proteins and 771 billion unique tokens, and has 98 billion parameters.\"\n\nper Table 1, trained 98B model on 1.8T training tokens. 98 billion * 1800 billion * 6 = 1.06e24. Likely some rounding, so will go with developer's reported count.",
      "Training dataset": "ESM3 Dataset",
      "Training dataset size (gradients)": "771000000000",
      "Dataset size notes": " 771 billion tokens",
      "Confidence": "Confident",
      "Link": "https://www.evolutionaryscale.ai/blog/esm3-release ",
      "Reference": "ESM3: Simulating 500 million years of evolution with a language model",
      "Citations": "",
      "Authors": "Thomas Hayes, Roshan Rao, Halil Akin, Nicholas James Sofroniew, Deniz Oktay, Zeming Lin, Robert Verkuil, Vincent Quy Tran, Jonathan Deaton, Marius Wiggert, Rohil Badkundri, Irhum Shafkat, Jun Gong, Alexander Derry, Raul Santiago Molina, Neil Thomas, Yousuf Khan, Chetan Mishra, Carolyn Kim, Liam J Bartie, Patrick D Hsu, Tom Sercu, Salvatore Candido, Alexander Rives",
      "Abstract": "More than three billion years of evolution have\nproduced an image of biology encoded into the\nspace of natural proteins. Here we show that language models trained on tokens generated by evolution can act as evolutionary simulators to generate functional proteins that are far away from\nknown proteins. We present ESM3, a frontier\nmultimodal generative language model that reasons over the sequence, structure, and function\nof proteins. ESM3 can follow complex prompts\ncombining its modalities and is highly responsive\nto biological alignment. We have prompted ESM3\nto generate fluorescent proteins with a chain of\nthought. Among the generations that we synthesized, we found a bright fluorescent protein at far\ndistance (58% identity) from known fluorescent\nproteins. Similarly distant natural fluorescent proteins are separated by over five hundred million\nyears of evolution,\n\n(from paper preview: https://evolutionaryscale-public.s3.us-east-2.amazonaws.com/research/esm3.pdf )",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "Largest (in compute) biology and protein model to date, was able to discover novel green fluorescent proteins",
      "Epochs": "2.3",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "4194304.0",
      "Batch size notes": "Table S1",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "only small version released",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Cambrian-1-34B",
      "Organization": "New York University (NYU)",
      "Publication date": "2024-06-24",
      "Domain": "Multimodal,Vision,Language",
      "Task": "Image captioning,Visual question answering,Character recognition (OCR)",
      "Parameters": "34000000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Cambrian-Alignment,Cambrian-10M",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2406.16860",
      "Reference": "Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs\n",
      "Citations": "619.0",
      "Authors": "Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Ziteng Wang, Rob Fergus, Yann LeCun, Saining Xie",
      "Abstract": "We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a vision-centric approach. While stronger language models can enhance multimodal capabilities, the design choices for vision components are often insufficiently explored and disconnected from visual representation learning research. This gap hinders accurate sensory grounding in real-world scenarios. Our study uses LLMs and visual instruction tuning as an interface to evaluate various visual representations, offering new insights into different models and architectures -- self-supervised, strongly supervised, or combinations thereof -- based on experiments with over 20 vision encoders. We critically examine existing MLLM benchmarks, address the difficulties involved in consolidating and interpreting results from various tasks, and introduce a new vision-centric benchmark, CV-Bench. To further improve visual grounding, we propose the Spatial Vision Aggregator (SVA), a dynamic and spatially-aware connector that integrates high-resolution vision features with LLMs while reducing the number of tokens. Additionally, we discuss the curation of high-quality visual instruction-tuning data from publicly available sources, emphasizing the importance of data source balancing and distribution ratio. Collectively, Cambrian-1 not only achieves state-of-the-art performance but also serves as a comprehensive, open cookbook for instruction-tuned MLLMs. We provide model weights, code, supporting tools, datasets, and detailed instruction-tuning and evaluation recipes. We hope our release will inspire and accelerate advancements in multimodal systems and visual representation learning.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Cambrian-1 outperforms other open source models and achieves competitive performance on a number of benchmarks, compared to proprietary models such as GPT-4V, Gemini, and Grok-1.5. \n\nnot absolute SOTA",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "Our final Cambrian-1 models are trained in less than 4 days on a TPU-V4-512.",
      "Training hardware": "Google TPU v4",
      "Hardware quantity": "512.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "343599.54926952836",
      "Base model": "Yi-34B",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "",
      "Accessibility notes": "Apache 2.0\nhttps://huggingface.co/nyu-visionx/cambrian-34b\n\nApache 2.0\nhttps://github.com/cambrian-mllm/cambrian",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Claude 3.5 Sonnet",
      "Organization": "Anthropic",
      "Publication date": "2024-06-20",
      "Domain": "Multimodal,Language,Vision",
      "Task": "Chat,Image captioning,Code generation,Language modeling/generation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "2.700000000000001e+25",
      "Training compute notes": "Blog post by Dario Amodei includes some info on 3.5 Sonnet compute: https://darioamodei.com/on-deepseek-and-export-controls\n\"Claude 3.5 Sonnet is a mid-sized model that cost a few $10M's to train (I won't give an exact number). Also, 3.5 Sonnet was not trained in any way that involved a larger or more expensive model (contrary to some rumors).\"\n\nUsing assumptions about GPU pricing, this lets us estimate compute. https://docs.google.com/spreadsheets/d/1-p-ab6t6dkUM6T7GwnFp85ePTMpZMW7LFY7fW2t8POs/",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Speculative",
      "Link": "https://www-cdn.anthropic.com/fed9cc193a14b84131812372d8d5857f8f304c52/Model_Card_Claude_3_Addendum.pdf",
      "Reference": "Claude 3.5 Sonnet",
      "Citations": "",
      "Authors": "",
      "Abstract": "This addendum to our Claude 3 Model Card describes Claude 3.5 Sonnet, a new model which outperforms\nour previous most capable model, Claude 3 Opus, while operating faster and at a lower cost. Claude 3.5\nSonnet offers improved capabilities, including better coding and visual processing. Since it is an evolution of\nthe Claude 3 model family, we are providing an addendum rather than a new model card. We provide updated\nkey evaluations and results from our safety testing",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Significant use,SOTA improvement",
      "Notability criteria notes": "\"It also sets new performance standards in evaluations of graduate level science knowledge (GPQA) [1], general reasoning (MMLU) [2], and coding proficiency (HumanEval) [3].\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "25870993.12024943",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DeepSeek-Coder-V2 236B",
      "Organization": "DeepSeek",
      "Publication date": "2024-06-17",
      "Domain": "Language",
      "Task": "Code generation,Code autocompletion",
      "Parameters": "236000000000.0",
      "Parameters notes": "Mixture of experts model. 21B parameters activated per token.",
      "Training compute (FLOP)": "1.2852e+24",
      "Training compute notes": "Trained on a total of 10.2T tokens\n6NC: 6 * 10.2T * 21B active parameters = 1.285e24",
      "Training dataset": "GitHub,Common Crawl",
      "Training dataset size (gradients)": "3191000000000",
      "Dataset size notes": "\"In the pre-training phase, the dataset of DeepSeek-Coder-V2 is created with a composition of 60% source code, 10% math corpus, and 30% natural language corpus ... The source code consists of 1,170B code-related tokens sourced from GitHub and CommonCrawl... For the math corpus, we collect 221B math-related tokens sourced from CommonCrawl... In total, DeepSeek-Coder-V2 has been exposed to 10.2T training tokens, where 4.2 trillion tokens originate from the DeepSeek V2 dataset, while the remaining 6 trillion tokens come from the DeepSeek-Coder-V2 dataset\"\n\nTotal of 1.391T tokens in the new data.\n\nFrom the DeepSeek-V2 paper: \"our tokenized pretraining corpus contains 8.1T tokens\"\n\nSo some tokens are trained over for multiple epochs:\n- 6T * 0.6 / 1.17T = 3.1 epochs on the code corpus\n- 6T * 0.1 / 221B = 2.7 epochs on the math corpus\n- 6T * 0.3 / 8.1T = 0.22 epochs on the natural language corpus\n\nTotal unique tokens seen is likely 1.17T + 221B + (6T*0.3) = 3.191T",
      "Confidence": "Confident",
      "Link": "https://github.com/deepseek-ai/DeepSeek-Coder-V2",
      "Reference": "DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence",
      "Citations": "",
      "Authors": "Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Y. Wu, Yukun Li, Huazuo Gao, Shirong Ma, Wangding Zeng, Xiao Bi, Zihui Gu, Hanwei Xu, Damai Dai, Kai Dong, Liyue Zhang, Yishi Piao, Zhibin Gou, Zhenda Xie, Zhewen Hao, Bingxuan Wang, Junxiao Song, Deli Chen, Xin Xie, Kang Guan, Yuxiang You, Aixin Liu, Qiushi Du, Wenjun Gao, Xuan Lu, Qinyu Chen, Yaohui Wang, Chengqi Deng, Jiashi Li, Chenggang Zhao, Chong Ruan, Fuli Luo, Wenfeng Liang",
      "Abstract": "We present DeepSeek-Coder-V2, an open-source Mixture-of-Experts (MoE) code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks. Specifically, DeepSeek-Coder-V2 is further pre-trained from an intermediate checkpoint of DeepSeek-V2 with additional 6 trillion tokens. Through this continued pre-training, DeepSeek-Coder-V2 substantially enhances the coding and mathematical reasoning capabilities of DeepSeek-V2, while maintaining comparable performance in general language tasks. Compared to DeepSeekCoder-33B, DeepSeek-Coder-V2 demonstrates significant advancements in various aspects of code-related tasks, as well as reasoning and general capabilities. Additionally, DeepSeek-CoderV2 expands its support for programming languages from 86 to 338, while extending the context length from 16K to 128K. In standard benchmark evaluations, DeepSeek-Coder-V2 achieves superior performance compared to closed-source models such as GPT4-Turbo, Claude 3 Opus, and Gemini 1.5 Pro in coding and math benchmarks.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "New SOTA on Aider, AIME 2024, and Math Odyssey benchmarks (including against proprietary models such as Claude 3 Opus, GPT-4o and GPT-4 Turbo).\nNote that Figure 1 appears to show the new model getting SOTA for several other benchmarks, but omits results from GPT-4o which wins in most cases.",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "DeepSeek-V2 (MoE-236B)",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "36864000.0",
      "Batch size notes": "Most training is done at batch size of 36,864. They do long context training: \"In the first stage, we utilize a sequence length of 32K and a batch size of 1152 for 1000 steps. In the second stage, we train the model for an additional 1000 steps, employing a sequence length of 128K and a batch size of 288 sequences\" 128k*288 = 36,864,000",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "license has some harmful use restrictions: https://github.com/deepseek-ai/DeepSeek-Coder-V2/blob/main/LICENSE-MODEL \n\nno training code",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Nemotron-4 340B",
      "Organization": "NVIDIA",
      "Publication date": "2024-06-14",
      "Domain": "Language",
      "Task": "Language modeling/generation,Chat,Question answering",
      "Parameters": "340000000000.0",
      "Parameters notes": "340B",
      "Training compute (FLOP)": "1.8e+25",
      "Training compute notes": "9 trillion tokens for training\n6 * 340B * 9T = 1.8E25\n\nalternatively, can do a hardware estimate with a few extra steps:\n\nAccording to the technical report, Nemotron-4 340B was trained using up to 6144 H100 GPUs. Helpfully, they also report the model FLOP utilization (MFU), which was 41-42% (Table 2). This is the ratio of the actual output of their GPUs, in FLOP used for training, relative to their theoretical max of 989 teraFLOP/s per GPU. \nUnfortunately, the report omits the last ingredient, which is the duration of the training run. However, in Table 2 they report some relevant data that we can use to infer the training time. \nNemotron-4 was trained in several stages, but the largest stage used all 6144 GPUs with a batch size of 2304 and an iteration time (time per batch) of 8.0 seconds. This stage involved 7.6T tokens, so it makes up the majority of training. \nA batch size of 2304 means that each batch consists of 2304 sequences, and they report that the sequence length used for training was 4096 tokens. This means that each batch contained 4096 * 2304 = 9,437,184 tokens. \nSo, during this stage, it took 8 seconds to train the model on 9.4m tokens. Extrapolating to the entire 9T token dataset, this implies the training run would have taken 7,659,574 seconds, or 89 days. (it actually took longer because they didn't use all their GPUs for the whole run) \nMultiplying 7,659,574 seconds by 41% MFU, 989 peak teraFLOP/s for each H100, and 6144 H100s, we get ~1.9e25 FLOP. This is very close to our first estimate. \n",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "9000000000000",
      "Dataset size notes": "9T training tokens.\n\nThey first train on an 8T token dataset and then an additional 1T tokens, it's slightly unclear if that's more data or a partial second epoch\n\n6.75T words using 1 token = 0.75 words",
      "Confidence": "Confident",
      "Link": "https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/ ",
      "Reference": "NVIDIA Releases Open Synthetic Data Generation Pipeline for Training Large Language Models",
      "Citations": "",
      "Authors": "Bo Adler, Niket Agarwal, Ashwath Aithal, Dong H. Anh, Pallab Bhattacharya, Annika Brundyn, Jared Casper, Bryan Catanzaro, Sharon Clay, Jonathan Cohen, Sirshak Das, Ayush Dattagupta, Olivier Delalleau, Leon Derczynski, Yi Dong, Daniel Egert, Ellie Evans, Aleksander Ficek, Denys Fridman, Shaona Ghosh, Boris Ginsburg, Igor Gitman, Tomasz Grzegorzek, Robert Hero, Jining Huang, Vibhu Jawa, Joseph Jennings, Aastha Jhunjhunwala, John Kamalu, Sadaf Khan, Oleksii Kuchaiev, Patrick LeGresley, Hui Li, Jiwei Liu, Zihan Liu, Eileen Long, Ameya Sunil Mahabaleshwarkar, Somshubra Majumdar, James Maki, Miguel Martinez, Maer Rodrigues de Melo, Ivan Moshkov, Deepak Narayanan, Sean Narenthiran, Jesus Navarro, Phong Nguyen, Osvald Nitski, Vahid Noroozi, Guruprasad Nutheti, Christopher Parisien, Jupinder Parmar, Mostofa Patwary, Krzysztof Pawelec, Wei Ping, Shrimai Prabhumoye, Rajarshi Roy, Trisha Saar, Vasanth Rao Naik Sabavat, Sanjeev Satheesh, Jane Polak Scowcroft, Jason Sewall, Pavel Shamis, Gerald Shen, Mohammad Shoeybi, Dave Sizer, Misha Smelyanskiy, Felipe Soares, Makesh Narsimhan Sreedhar, Dan Su, Sandeep Subramanian, Shengyang Sun, Shubham Toshniwal, Hao Wang, Zhilin Wang, Jiaxuan You, Jiaqi Zeng, Jimmy Zhang, Jing Zhang, Vivienne Zhang, Yian Zhang, Chen Zhu",
      "Abstract": "We release the Nemotron-4 340B model family, including Nemotron-4-340B-Base, Nemotron-4-\n340B-Instruct, and Nemotron-4-340B-Reward. Our models are open access under the NVIDIA Open\nModel License Agreement, a permissive model license that allows distribution, modification, and use of\nthe models and its outputs. These models perform competitively to open access models on a wide range\nof evaluation benchmarks, and were sized to fit on a single DGX H100 with 8 GPUs when deployed in\nFP8 precision. We believe that the community can benefit from these models in various research studies\nand commercial applications, especially for generating synthetic data to train smaller language models.\nNotably, over 98% of data used in our model alignment process is synthetically generated, showcasing\nthe effectiveness of these models in generating synthetic data. To further support open research and\nfacilitate model development, we are also open-sourcing the synthetic data generation pipeline used in\nour model alignment process.\n\n(from technical report: https://d1qx31qr3h6wln.cloudfront.net/publications/Nemotron_4_340B_8T_0.pdf )",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "~2e25 FLOP, so high training cost, likely >5M",
      "Epochs": "",
      "Training time (hours)": "2200.0",
      "Training time notes": "see training compute notes, this is an inferred estimate",
      "Training hardware": "NVIDIA H100 SXM5 80GB",
      "Hardware quantity": "6144.0",
      "Hardware utilization (MFU)": "0.410675",
      "Training compute cost (2023 USD)": "21271017.96222655",
      "Compute cost notes": "",
      "Training power draw (W)": "8490820.682633882",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "9437184.0",
      "Batch size notes": "2304 * 4096",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Permissive commercial license: https://developer.download.nvidia.com/licenses/nvidia-open-model-license-agreement-june-2024.pdf ",
      "Numerical format": "BF16",
      "Frontier model": "True",
      "Hardware acquisition cost": "224570394.07617396",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "67898317.22964796",
      "Training compute cost (upfront)": "348799800.27266073"
    },
    {
      "Model": "OpenVLA",
      "Organization": "Stanford University,University of California (UC) Berkeley,Toyota Research Institute,Google DeepMind,Massachusetts Institute of Technology (MIT),Physical Intelligence",
      "Publication date": "2024-06-13",
      "Domain": "Robotics,Vision,Language",
      "Task": "Robotic manipulation",
      "Parameters": "7188100000.0",
      "Parameters notes": "Based on a Prismatic-7B VLM backbone, which itself is comprised of 600M parameter vision encoder (DinoV2 + SigLIP) plus Llama-2 7B. Table 1 indicates 7.1881 billion trainable parameters",
      "Training compute (FLOP)": "1.1e+23",
      "Training compute notes": "Majority of compute is from VLA pre-training embedded in Prismatic-7B and it's constituent models. \n\nThe fine-tuning compute used in this paper is \"64 A100 GPUs for 14 days, or a total of 21,500 A100-hours\"\n21500 * 3600 * 3.12e14 * 0.4 = 9.66e21\n\nPrismatic-7B training took \"less than 9 hours\" on 8 A100s: 9 * 3600 * 8 * 3.12e14 * 0.4 = 3.23e19\n\nAdd in the pre-trained components:\n- DinoV2 = 7.42e21, per our database\n- The SigLIP model in question is SoViT-400m/14 from the cited Alabdulmohsin et al., 2023) and \"is pretrained on 40 billion examples, which amounts to 9T GFLOPs and 230K TPUv3 core-hours\" = 9e21\n- Llama 2-7B = 8.4e22, per our database\n\nTotal\n9.66e21 + 3.23e19 + 7.42e21 + 9e21 + 8.4e22 = 1.10e23",
      "Training dataset": "Open X-Embodiment",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "\"OpenVLA consists of a pretrained visually-conditioned language model backbone that captures visual features at multiple granularities, fine-tuned on a large, diverse dataset of 970k robot manipulation trajectories from the Open-X Embodiment [1] dataset\"\nFiltered from 2M total in OpenX.",
      "Confidence": "Confident",
      "Link": "https://openvla.github.io/\nhttps://arxiv.org/abs/2406.09246",
      "Reference": "OpenVLA: An Open-Source Vision-Language-Action Mode",
      "Citations": "1300.0",
      "Authors": "Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, Chelsea Finn",
      "Abstract": "Large policies pretrained on a combination of Internet-scale vision-language data and diverse robot demonstrations have the potential to change how we teach robots new skills: rather than training new behaviors from scratch, we can fine-tune such vision-language-action (VLA) models to obtain robust, generalizable policies for visuomotor control. Yet, widespread adoption of VLAs for robotics has been challenging as 1) existing VLAs are largely closed and inaccessible to the public, and 2) prior work fails to explore methods for efficiently fine-tuning VLAs for new tasks, a key component for adoption. Addressing these challenges, we introduce OpenVLA, a 7B-parameter open-source VLA trained on a diverse collection of 970k real-world robot demonstrations. OpenVLA builds on a Llama 2 language model combined with a visual encoder that fuses pretrained features from DINOv2 and SigLIP. As a product of the added data diversity and new model components, OpenVLA demonstrates strong results for generalist manipulation, outperforming closed models such as RT-2-X (55B) by 16.5% in absolute task success rate across 29 tasks and multiple robot embodiments, with 7x fewer parameters. We further show that we can effectively fine-tune OpenVLA for new settings, with especially strong generalization results in multi-task environments involving multiple objects and strong language grounding abilities, and outperform expressive from-scratch imitation learning methods such as Diffusion Policy by 20.4%. We also explore compute efficiency; as a separate contribution, we show that OpenVLA can be fine-tuned on consumer GPUs via modern low-rank adaptation methods and served efficiently via quantization without a hit to downstream success rate. Finally, we release model checkpoints, fine-tuning notebooks, and our PyTorch codebase with built-in support for training VLAs at scale on Open X-Embodiment datasets.",
      "Organization categorization": "Academia,Academia,Industry,Industry,Academia,Industry",
      "Country (of organization)": "United States of America,United States of America,United States of America,United States of America,United Kingdom of Great Britain and Northern Ireland,United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"OpenVLA outperforms the 55B-parameter RT-2-X model [1, 7], the prior state-of-the-art VLA, by 16.5% absolute success rate across 29 evaluation tasks on the WidowX and Google Robot embodiments.\"\n\nTop10 recent paper from Sebastian Sartor 2025-05-14\n\nTable 4",
      "Epochs": "27.0",
      "Training time (hours)": "336.0",
      "Training time notes": "\"The final OpenVLA model is trained on a cluster of 64 A100 GPUs for 14 days\"\n14 days * 24 hr/day = 336 hours",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "64.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "50541.724821294745",
      "Base model": "Llama 2-7B",
      "Finetune compute (FLOP)": "9.66e+21",
      "Finetune compute notes": "\"64 A100 GPUs for 14 days, or a total of 21,500 A100-hours\"\n21500 * 3600 * 3.12e14 * 0.4 = 9.66e21",
      "Batch size": "2048.0",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "\"OpenVLA uses multiple pretrained model components: SigLIP [9] and DinoV2 [25] vision encoders and a Llama 2 [10] language model backbone. For all three models, weights are open, but not their training data or code. We release training data, code and model weights for reproducing OpenVLA on top of these components.\"\nAll published material is on an MIT license.\n\ntrain code: https://github.com/openvla/openvla/blob/main/scripts/pretrain.py ",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Llama-3.1-Nemotron-70B-Instruct",
      "Organization": "NVIDIA,Meta AI",
      "Publication date": "2024-06-12",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "7.929e+24",
      "Training compute notes": "Taken from Llama 3.1 70B as the finetuning compute is multiple orders of magnitude lower",
      "Training dataset": "Llama 3 dataset",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://catalog.ngc.nvidia.com/orgs/nim/teams/nvidia/containers/llama-3.1-nemotron-70b-instruct",
      "Reference": "https://www.semanticscholar.org/paper/HelpSteer2%3A-Open-source-dataset-for-training-reward-Wang-Dong/f590d8926dd12345a3bd22253461850f5ca4b3ed",
      "Citations": "",
      "Authors": "Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy Zhang, Makesh Narsimhan Sreedhar, Oleksii Kuchaiev",
      "Abstract": "High-quality preference datasets are essential for training reward models that can effectively guide large language models (LLMs) in generating high-quality responses aligned with human preferences. As LLMs become stronger and better aligned, permissively licensed preference datasets, such as Open Assistant, HH-RLHF, and HelpSteer need to be updated to remain effective for reward modeling. Methods that distil preference data from proprietary LLMs such as GPT-4 have restrictions on commercial usage imposed by model providers. To improve upon both generated responses and attribute labeling quality, we release HelpSteer2, a permissively licensed preference dataset (CC-BY-4.0). Using a powerful internal base model trained on HelpSteer2, we are able to achieve the SOTA score (92.0%) on Reward-Bench's primary dataset, outperforming currently listed open and proprietary models, as of June 12th, 2024. Notably, HelpSteer2 consists of only ten thousand response pairs, an order of magnitude fewer than existing preference datasets (e.g., HH-RLHF), which makes it highly efficient for training reward models. Our extensive experiments demonstrate that reward models trained with HelpSteer2 are effective in aligning LLMs. In particular, we propose SteerLM 2.0, a model alignment approach that can effectively make use of the rich multi-attribute score predicted by our reward models. HelpSteer2 is available at https://huggingface.co/datasets/nvidia/HelpSteer2 and code is available at https://github.com/NVIDIA/NeMo-Aligner",
      "Organization categorization": "Industry,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"As of 1 Oct 2024, Llama-3.1-Nemotron-70B-Instruct performs best on Arena Hard, AlpacaEval 2 LC (verified tab) and MT Bench (GPT-4-Turbo)\"",
      "Epochs": "",
      "Training time (hours)": "96.0",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "Llama 3.1-70B",
      "Finetune compute (FLOP)": "1.0259136e+20",
      "Finetune compute notes": "Llama 3.1  70B: 7.929e+24\nFT (see Appendix F): 32+64=96 hours on a single H100\nCompute: 96*60*60*989500000000000*0.3=102591360000000000000=1e20\n",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Your use of this model is governed by the NVIDIA Open Model License. Additional Information: Llama 3.1 Community License Agreement (branding restrictions + cap size of 700M MAu for commercial use). \n\nhttps://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct-HF",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Qwen2-72B",
      "Organization": "Alibaba",
      "Publication date": "2024-06-07",
      "Domain": "Language",
      "Task": "Chat,Language modeling/generation,Question answering",
      "Parameters": "72710000000.0",
      "Parameters notes": "72.71B parameters in total, of which 70.21B are non-embedding parameters",
      "Training compute (FLOP)": "3.02e+24",
      "Training compute notes": "72 billion params, 7 trillion tokens\n\n6 * 72 billion * 7 trillion ~= 3.02e24",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "7000000000000",
      "Dataset size notes": "\"All models were pre-trained on a high-quality, large-scale dataset comprising over 7 trillion tokens, covering a wide range of domains and languages.\"",
      "Confidence": "Confident",
      "Link": "https://qwenlm.github.io/blog/qwen2/ \nhttps://arxiv.org/abs/2407.10671 ",
      "Reference": "Hello Qwen2",
      "Citations": "",
      "Authors": "An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, Zhihao Fan",
      "Abstract": "After months of efforts, we are pleased to announce the evolution from Qwen1.5 to Qwen2. This time, we bring to you:\n- Pretrained and instruction-tuned models of 5 sizes, including Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B, Qwen2-57B-A14B, and Qwen2-72B;\n- Having been trained on data in 27 additional languages besides English and Chinese;\n- State-of-the-art performance in a large number of benchmark evaluations;\n- Significantly improved performance in coding and mathematics;\n- Extended context length support up to 128K tokens with Qwen2-7B-Instruct and Qwen2-72B-Instruct.\n\n(Technical report to follow)",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "SOTA claims are against open source models within a parameter class.\n\nPossibly high training cost, at 3e24 FLOP seems borderline.",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "not listed",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Apache 2.0",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ALLaM\u00a0adapted 70B",
      "Organization": "Saudi Data and Artificial Intelligence Authority",
      "Publication date": "2024-05-21",
      "Domain": "Language",
      "Task": "Language modeling/generation,Translation,Question answering",
      "Parameters": "70000000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "1.062e+24",
      "Training compute notes": "Llama 70B: 8.1e+23\nFinetune: 6*70000000000*600000000000=252000000000000000000000\nTotal: 1062000000000000000000000",
      "Training dataset": "",
      "Training dataset size (gradients)": "600000000000",
      "Dataset size notes": "\"For the ALLaM-70B model, we only train up to 600B tokens\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2407.15390",
      "Reference": "ALLaM: Large Language Models for Arabic and English\n",
      "Citations": "42.0",
      "Authors": "Saudi Data and Artificial Intelligence Authority",
      "Abstract": "We present ALLaM: Arabic Large Language Model, a series of large language models to support the ecosystem of Arabic Language Technologies (ALT). ALLaM is carefully trained considering the values of language alignment and knowledge transfer at scale. Our autoregressive decoder-only architecture models demonstrate how second-language acquisition via vocabulary expansion and pretraining on a mixture of Arabic and English text can steer a model towards a new language (Arabic) without any catastrophic forgetting in the original language (English). Furthermore, we highlight the effectiveness of using parallel/translated data to aid the process of knowledge alignment between languages. Finally, we show that extensive alignment with human preferences can significantly enhance the performance of a language model compared to models of a larger scale with lower quality alignment. ALLaM achieves state-of-the-art performance in various Arabic benchmarks, including MMLU Arabic, ACVA, and Arabic Exams. Our aligned models improve both in Arabic and English from their base aligned models.",
      "Organization categorization": "Industry,Government",
      "Country (of organization)": "Saudi Arabia",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "SOTA at Arabic MMLU benchmarks, all results in this paper: https://openreview.net/pdf?id=MscdsFVZrN",
      "Epochs": "1.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "Llama 2-70B",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "not yet released",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Octo-Base",
      "Organization": "University of California (UC) Berkeley,Stanford University,Carnegie Mellon University (CMU),DeepMind",
      "Publication date": "2024-05-20",
      "Domain": "Robotics",
      "Task": "Robotic manipulation",
      "Parameters": "93000000.0",
      "Parameters notes": "Source: https://arxiv.org/abs/2405.12213 ",
      "Training compute (FLOP)": "5.85e+20",
      "Training compute notes": "Training compute \n= peak FLOPs * utilization rate * training time\n~= 128 TPUs * 275e12 FLOPs / TPU * 0.33 * 14 hours * 3600 s / hour\n~= 585446400e12 FLOPs\n= 5.85e20 FLOPs,\nassuming utilization rate = 0.33.",
      "Training dataset": "Open X-Embodiment",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "Octo was pre-trained on 800k robot episodes from the Open X-Embodiment dataset. The authors describe this dataset as \"heterogeneous,\" so it's unclear how to compute the size of the dataset. (Source: https://arxiv.org/abs/2405.12213)",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2405.12213 ",
      "Reference": "Octo: An Open-Source Generalist Robot Policy",
      "Citations": "849.0",
      "Authors": "Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, Jianlan Luo, You Liang Tan, Lawrence Yunliang Chen, Pannag Sanketi, Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, Sergey Levine",
      "Abstract": "Large policies pretrained on diverse robot datasets have the potential to transform robotic learning: instead of training new policies from scratch, such generalist robot policies may be finetuned with only a little in-domain data, yet generalize broadly. However, to be widely applicable across a range of robotic learning scenarios, environments, and tasks, such policies need to handle diverse sensors and action spaces, accommodate a variety of commonly used robotic platforms, and finetune readily and efficiently to new domains. In this work, we aim to lay the groundwork for developing open-source, widely applicable, generalist policies for robotic manipulation. As a first step, we introduce Octo, a large transformer-based policy trained on 800k trajectories from the Open X-Embodiment dataset, the largest robot manipulation dataset to date. It can be instructed via language commands or goal images and can be effectively finetuned to robot setups with new sensory inputs and action spaces within a few hours on standard consumer GPUs. In experiments across 9 robotic platforms, we demonstrate that Octo serves as a versatile policy initialization that can be effectively finetuned to new observation and action spaces. We also perform detailed ablations of design decisions for the Octo model, from architecture to training data, to guide future research on building generalist robot models.",
      "Organization categorization": "Academia,Academia,Academia,Industry",
      "Country (of organization)": "United States of America,United States of America,United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Octo\u2019s ability to control multiple robots out-of-the-box to the best openly available generalist robot policy, RT-1-X. RT-1-X had success rates of 0.2, ~0.38, and 0.6 on WidowX, UR5, and RT-1 Robot. Octo-Base outperformed RT-1-X on all tasks, with success rates of approximately 0.5, 0.7, and 0.8 on WidowX, UR5, and RT-1 Robot, respectively. (Source: https://arxiv.org/abs/2405.12213)",
      "Epochs": "",
      "Training time (hours)": "14.0",
      "Training time notes": "\u201cWe trained two variants of our model: Octo-Small with a transformer backbone that mirrors the size of a ViT-S, andOcto-Base with a transformer backbone that mirrors the size of a ViT-B. The ViT-B was trained for 300k steps with a batch size of 2048 using a TPU v4-128 pod, which took 14 hours.\" (Source: https://arxiv.org/abs/2405.12213)\n",
      "Training hardware": "Google TPU v4",
      "Hardware quantity": "128.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "85966.86619350823",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT license\nhttps://huggingface.co/rail-berkeley/octo-base\n\nhttps://github.com/octo-models/octo",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GLM-4 (0520)",
      "Organization": "Z.ai (Zhipu AI)",
      "Publication date": "2024-05-20",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering,Code generation,Quantitative reasoning,Translation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "- \u201cthe GLM-4 models are pre-trained on ten trillions of tokens\u201d\n- I did not find any information about parameters or compute. Here they speculatively estimate GLM-4 to be 200B parameters (which seems plausible), though no source provided: https://lifearchitect.ai/models-table/\n- \u201cGLM-4 gets close to the state-of-the-art models (GPT-4-Turbo, Gemini 1.5 Pro, and Claude 3 Opus)\u201d  none of these models has parameters disclosed or compute estimation.\n\n6 FLOP / parameter / token * 10000000000000 tokens * 200000000000 parameters = 1.2e+25 FLOPs with \u201cLikely\u201d confidence (+/- 1 OOM)",
      "Training dataset": "",
      "Training dataset size (gradients)": "10000000000000",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2406.12793",
      "Reference": "ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools",
      "Citations": "",
      "Authors": "Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, Zihan Wang",
      "Abstract": "We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "Trained on 10T tokens with similar architecture to GPT-4, probably >$1M compute cost.",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "GLM-4 (0116)",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "Not listed",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "the GLM-4 API at\nhttps://bigmodel.cn\n\nrepo with models from the same series but not exactly this one: https://github.com/THUDM/GLM-4",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Yi-Large",
      "Organization": "01.AI",
      "Publication date": "2024-05-13",
      "Domain": "Language",
      "Task": "Chat,Language modeling/generation",
      "Parameters": "100000000000.0",
      "Parameters notes": "\"Yi-Large is a software over-the-air-driven closed-source large model with a parameter of over 100 billion tokens.\" from https://www.chinadaily.com.cn/a/202405/13/WS6641abd1a31082fc043c6ccd.html",
      "Training compute (FLOP)": "1.8e+24",
      "Training compute notes": "6ND = 6*100000000000*3000000000000=1.8e+24\n\n(speculative confidence because training dataset size is very uncertain)",
      "Training dataset": "",
      "Training dataset size (gradients)": "3000000000000",
      "Dataset size notes": "3T tokens for previous Yi models: \"Targeted as a bilingual language model and trained on 3T multilingual corpus, the Yi series models become one of the strongest LLM worldwide, showing promise in language understanding, commonsense reasoning, reading comprehension, and more.\"\n",
      "Confidence": "Speculative",
      "Link": "",
      "Reference": "",
      "Citations": "",
      "Authors": "Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, Zonghong Dai",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP8",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GPT-4o (May 2024)",
      "Organization": "OpenAI",
      "Publication date": "2024-05-13",
      "Domain": "Multimodal,Language,Audio,Speech,Vision",
      "Task": "Chat,Image generation,Audio generation,Vision-language generation,Table tasks,Language modeling/generation,Question answering,Speech recognition (ASR),Speech-to-text",
      "Parameters": "",
      "Parameters notes": "Not known.\n\nInference costs in the API are 2x cheaper than GPT-4 Turbo",
      "Training compute (FLOP)": "",
      "Training compute notes": "Training compute estimated to be 3.8e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Speculative",
      "Link": "https://openai.com/index/hello-gpt-4o/ \nhttps://openai.com/index/gpt-4o-system-card/",
      "Reference": "Hello GPT-4o",
      "Citations": "",
      "Authors": "Aidan Clark, Alex Paino, Jacob Menick, Liam Fedus, Luke Metz, Clemens Winter, Lia Guy, Sam Schoenholz, Daniel Levy, Nitish Keskar, Alex Carney, Alex Paino, Ian Sohl, Qiming Yuan, Reimar Leike, Arka Dhar, Brydon Eastman, Mia Glaese, Ben Sokolowsky, Andrew Kondrich, Felipe Petroski Such, Henrique Ponde de Oliveira Pinto, Jiayi Weng, Randall Lin, Youlong Cheng, Nick Ryder, Lauren Itow, Barret Zoph, John Schulman, Mianna Chen, Adam Lerer, Adam P. Goucher, Adam Perelman, Akila Welihinda, Alec Radford, Alex Borzunov, Alex Carney, Alex Chow, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexi Christakis, Ali Kamali, Allison Moyer, Allison Tam, Amin Tootoonchian, Ananya Kumar, Andrej Karpathy, Andrey Mishchenko, Andrew Cann, Andrew Kondrich, Andrew Tulloch, Angela Jiang, Antoine Pelisse, Anuj Gosalia, Avi Nayak, Avital Oliver, Behrooz Ghorbani, Ben Leimberger, Ben Wang, Blake Samic, Brian Guarraci, Brydon Eastman, Camillo Lugaresi, Chak Li, Charlotte Barette, Chelsea Voss, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christopher Hesse, Colin Wei, Daniel Kappler, Daniel Levin, Daniel Levy, David Farhi, David Mely, David Sasaki, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Peterson, Eric Sigler, Eugene Brevdo, Farzad Khorasani, Francis Zhang, Gene Oden, Geoff Salmon, Hadi Salman, Haiming Bao, Heather Schmidt, Hongyu Ren, Hyung Won Chung, Ian Kivlichan, Ian O'Connell, Ian Osband, Ilya Kostrikov, Ingmar Kanitscheider, Jacob Coxon, James Crooks, James Lennon, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jiayi Weng, Jie Tang, Joanne Jang, Jonathan Ward, Jonathan McKay, Jong Wook Kim, Josh Gross, Josh Kaplan, Joy Jiao, Joyce Lee, Juntang Zhuang, Kai Fricke, Kavin Karthik, Kenny Hsu, Kiel Howe, Kyle Luther, Larry Kai, Lauren Itow, Leo Chen, Lia Guy, Lien Mamitsuka, Lilian Weng, Long Ouyang, Louis Feuvrier, Lukas Kondraciuk, Lukasz Kaiser, Lyric Doshi, Mada Aflak, Maddie Simens, Madeleine Thompson, Marat Dukhan, Marvin Zhang, Mateusz Litwin, Max Johnson, Mayank Gupta, Mia Glaese, Michael Janner, Michael Petrov, Michael Wu, Michelle Fradin, Michelle Pokrass, Miguel Oom Temudo de Castro, Mikhail Pavlov, Minal Khan, Mo Bavarian, Natalia Gimelshein, Natalie Staudacher, Nick Stathas, Nik Tezak, Nithanth Kudige, Noel Bundick, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivier Godement, Owen Campbell-Moore, Philip Pronin, Philippe Tillet, Rachel Lim, Rajan Troll, Randall Lin, Rapha gontijo lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Rob Honsby, Rohit Ramchandani, Rory Carmichael, Ruslan Nigmatullin, Ryan Cheu, Scott Gray, Sean Grove, Sean Metzger, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shuaiqi (Tony) Xia, Sonia Phene, Spencer Papay, Steve Coffey, Steve Lee, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tarun Gogineni, Ted Sanders, Thomas Cunninghman, Thomas Dimson, Thomas Raoux, Tianhao Zheng, Tina Kim, Todd Underwood, Tristan Heywood, Valerie Qi, Vinnie Monaco, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wojciech Zaremba, Yash Patil, Yilei, Qian, Yongjik Kim, Youlong Cheng, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, Yury Malkov",
      "Abstract": "We\u2019re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.\n\nGPT-4o (\u201co\u201d for \u201comni\u201d) is a step towards much more natural human-computer interaction\u2014it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement,Significant use",
      "Notability criteria notes": "Outperforms GPT-4 Turbo and other models on text and especially on multimodal benchmarks, such as MMLU, GPQA, HumanEval, MMMU, etc See Model Evaluations: https://openai.com/index/hello-gpt-4o/ \n\nGPT-4o is now the default model in ChatGPT, so it's one of the most widely used models.",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "Definitely a new model, not a GPT-4 finetune",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "AlphaFold 3",
      "Organization": "Google DeepMind,Isomorphic Labs",
      "Publication date": "2024-05-08",
      "Domain": "Biology",
      "Task": "Protein folding prediction,Antibody property prediction,Protein-ligand contact prediction,RNA structure prediction,Protein interaction prediction",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "4.1405645e+22",
      "Training compute notes": "256 GPUs * 480 hours [see training time notes] * 3600 sec / hour * 312000000000000 FLOP / GPU / sec * 0.3 [assumed utilization] = 4.1405645e+22 FLOP",
      "Training dataset": "PDB (Protein Data Bank)",
      "Training dataset size (gradients)": "29491200000",
      "Dataset size notes": "from https://www.biorxiv.org/content/10.1101/2024.11.19.624167v2.full.pdf\n\"As a comparison AlphaFol3 trained a similar architecture for nearly 150k steps with a batch size of 256\"\n\nfrom supplementary materials \"The model is trained with a batch size of 256\"\n\n150000 steps * 256 sequences per batch * 384 tokens per batch [at initial training stage, Table 6, supplementary materials] = 14 745 600 000 tokens\n\nfrom supplementary materials Table 6 fine-tuning took exactly the same amount of GPU-hours -> the entire amount of training tokens is 14 745 600 000 * 2 = 29491200000\n",
      "Confidence": "Confident",
      "Link": "https://www.nature.com/articles/s41586-024-07487-w",
      "Reference": "Accurate structure prediction of biomolecular interactions with AlphaFold 3",
      "Citations": "",
      "Authors": "Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, Lindsay Willmore, Andrew J. Ballard, Joshua Bambrick, Sebastian W. Bodenstein, David A. Evans, Chia-Chun Hung, Michael O\u2019Neill, David Reiman, Kathryn Tunyasuvunakool, Zachary Wu, Akvil\u0117 \u017demgulyt\u0117, Eirini Arvaniti, Charles Beattie, Ottavia Bertolli, Alex Bridgland, Alexey Cherepanov, Miles Congreve, Alexander I. Cowen-Rivers, Andrew Cowie, Michael Figurnov, Fabian B. Fuchs, Hannah Gladman, Rishub Jain, Yousuf A. Khan, Caroline M. R. Low, Kuba Perlin, Anna Potapenko, Pascal Savy, Sukhdeep Singh, Adrian Stecula, Ashok Thillaisundaram, Catherine Tong, Sergei Yakneen, Ellen D. Zhong, Michal Zielinski, Augustin \u017d\u00eddek, Victor Bapst, Pushmeet Kohli, Max Jaderberg, Demis Hassabis, John M. Jumper",
      "Abstract": "The introduction of AlphaFold\u20092 has spurred a revolution in modelling the structure of proteins and their interactions, enabling a huge range of applications in protein modelling and design Here we describe our AlphaFold\u20093 model with a substantially updated diffusion-based architecture that is capable of predicting the joint structure of complexes including proteins, nucleic acids, small molecules, ions and modified residues. The new AlphaFold model demonstrates substantially improved accuracy over many previous specialized tools: far greater accuracy for protein\u2013ligand interactions compared with state-of-the-art docking tools, much higher accuracy for protein\u2013nucleic acid interactions compared with nucleic-acid-specific predictors and substantially higher antibody\u2013antigen prediction accuracy compared with AlphaFold-Multimer. Together, these results show that high-accuracy modelling across biomolecular space is possible within a single unified deep-learning framework.",
      "Organization categorization": "Industry,Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Figure 4 +\n\"we achieve a higher average performance than RoseTTAFold2NA and\nAIchemy_RNA27 (the best AI-based submission in CASP1518,31)\"\n\n\"Even so, AF3 greatly outperforms classical docking tools such as\nVina37,38 even while not using any structural inputs (Fisher\u2019s exact test,\nP = 2.27 \u00d7 10\u221213) and greatly outperforms all other true blind docking like RoseTTAFold All-Atom \"\n\nalso is on top of the CAMEO leaderboard https://cameo3d.org/",
      "Epochs": "1.0",
      "Training time (hours)": "480.0",
      "Training time notes": "supplementary materials Table 6\n20 days *24 hours = 480 hours",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "256.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "202329.0409413959",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Inference code CC-BY-NC-SA 4.0\nweights - permissions given after approval (only non-commercial use)\n\n\"AlphaFold 3 will be available as a non-commercial usage only server at https://www.alphafoldserver.com, with restrictions on allowed ligands and covalent modifications. Pseudocode describing the algorithms is available in the Supplementary Information. Code is not provided\"",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "VILA1.5-13B",
      "Organization": "NVIDIA,Massachusetts Institute of Technology (MIT)",
      "Publication date": "2024-05-03",
      "Domain": "Multimodal,Language,Vision,Video",
      "Task": "Chat,Visual question answering,Image captioning,Language modeling/generation,Question answering",
      "Parameters": "13493916736.0",
      "Parameters notes": "https://huggingface.co/Efficient-Large-Model/VILA1.5-13b/blob/main/llm/model.safetensors.index.json\nhttps://huggingface.co/Efficient-Large-Model/VILA1.5-13b/tree/main/vision_tower\nhttps://huggingface.co/Efficient-Large-Model/VILA1.5-13b/blob/main/mm_projector/model.safetensors\n\nllm: 13015864320\nvision_tower: 428225600\nmm_projector: 49826816\n\ntotal: 13493916736\n",
      "Training compute (FLOP)": "2.3003136e+21",
      "Training compute notes": "not directly reported (arXiv preprint is for VILA-13B), but assumed to be similar to VILA-13B (given similar size/architecture)",
      "Training dataset": "",
      "Training dataset size (gradients)": "32430000000",
      "Dataset size notes": "Table 2\nMMC4: 25M images with 576+122.5 tokens each\nCOYO: 25M images with 576+22.7 tokens each\n25M*(576+122.5+576+22.7)=32430000000",
      "Confidence": "Confident",
      "Link": "https://huggingface.co/Efficient-Large-Model/VILA1.5-13b\nhttps://github.com/NVlabs/VILA/tree/bbc609baf326b1b49b93450b48edc516db3737fc/scripts/v1_5/release/13b\nhttps://developer.nvidia.com/blog/visual-language-models-on-nvidia-hardware-with-vila/\n",
      "Reference": "VILA: On Pre-training for Visual Language Models",
      "Citations": "661.0",
      "Authors": "Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, Song Han",
      "Abstract": "Visual language models (VLMs) rapidly progressed with the recent success of large language models. There have been growing efforts on visual instruction tuning to extend the LLM with visual inputs, but lacks an in-depth study of the visual language pre-training process, where the model learns to perform joint modeling on both modalities. In this work, we examine the design options for VLM pre-training by augmenting LLM towards VLM through step-by-step controllable comparisons. We introduce three main findings: (1) freezing LLMs during pre-training can achieve decent zero-shot performance, but lack in-context learning capability, which requires unfreezing the LLM; (2) interleaved pre-training data is beneficial whereas image-text pairs alone are not optimal; (3) re-blending text-only instruction data to image-text data during instruction fine-tuning not only remedies the degradation of text-only tasks, but also boosts VLM task accuracy. With an enhanced pre-training recipe we build VILA, a Visual Language model family that consistently outperforms the state-of-the-art models, e.g., LLaVA-1.5, across main benchmarks without bells and whistles. Multi-modal pre-training also helps unveil appealing properties of VILA, including multi-image reasoning, enhanced in-context learning, and better world knowledge.\n",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "The paper reports SOTA benchmark results for VILA-13B not VILA1.5-13B",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "128.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "101175.78544917757",
      "Base model": "SigLIP 400M,Vicuna-13B-v1.5",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache 2.0 for code, CC by NC for weights\nhttps://github.com/NVlabs/VILA\n\nhttps://huggingface.co/Efficient-Large-Model/VILA1.5-13b",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GenCast",
      "Organization": "Google DeepMind",
      "Publication date": "2024-05-01",
      "Domain": "Earth science",
      "Task": "Weather forecasting",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "8.169984e+20",
      "Training compute notes": "The model was trained for 120 hours on 32 TPUv5. \nAssuming it was TPUv5e, bf16 precision:\n\n197000000000000 FLOP/s * 120 hours * 3600 s/hour * 32 instances * 0.3 [assumed utilization] = 8.169984e+20 FLOP",
      "Training dataset": "ERA5",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "Stage 1:  2 million training steps. batch size 32 \nStage 2: 64 000 further training steps.  batch size 32 ",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2312.15796",
      "Reference": "GenCast: Diffusion-based ensemble forecasting for medium-range weather",
      "Citations": "",
      "Authors": "Ilan Price, Alvaro Sanchez-Gonzalez, Ferran Alet, Tom R. Andersson, Andrew El-Kadi, Dominic Masters, Timo Ewalds, Jacklynn Stott, Shakir Mohamed, Peter Battaglia, Remi Lam, Matthew Willson",
      "Abstract": "Weather forecasts are fundamentally uncertain, so predicting the range of probable weather scenarios is crucial for important decisions, from warning the public about hazardous weather, to planning renewable energy use. Here, we introduce GenCast, a probabilistic weather model with greater skill and speed than the top operational medium-range weather forecast in the world, the European Centre for Medium-Range Forecasts (ECMWF)'s ensemble forecast, ENS. Unlike traditional approaches, which are based on numerical weather prediction (NWP), GenCast is a machine learning weather prediction (MLWP) method, trained on decades of reanalysis data. GenCast generates an ensemble of stochastic 15-day global forecasts, at 12-hour steps and 0.25 degree latitude-longitude resolution, for over 80 surface and atmospheric variables, in 8 minutes. It has greater skill than ENS on 97.4% of 1320 targets we evaluated, and better predicts extreme weather, tropical cyclones, and wind power production. This work helps open the next chapter in operational weather forecasting, where critical weather-dependent decisions are made with greater accuracy and efficiency.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "Stage 1: \"This training stage takes a little over 3.5 days using 32 TPUv5 instances.\"\nStage 2: \"This takes just under 1.5 days using 32 TPUv5 instances.\"\n5*24 = 120 hours",
      "Training hardware": "Google TPU v5e",
      "Hardware quantity": "32.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache 2.0 for training and inference code\nhttps://github.com/google-deepmind/graphcast\n\nCC BY-NC-SA 4.0 for weights",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Llama 3-70B",
      "Organization": "Meta AI",
      "Publication date": "2024-04-18",
      "Domain": "Language",
      "Task": "Chat,Language modeling/generation,Code generation",
      "Parameters": "70000000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "7.861e+24",
      "Training compute notes": "Arithmetic calculation:\n6 * 15T tokens * 70B parameters = 6.3e24\n\nGPU calculation:\nhttps://huggingface.co/meta-llama/Meta-Llama-3-70B indicates training took 6.4M GPU-hours\nWe also know their larger scale training runs for 405B were getting between 0.38-0.41 MFU. Presumably the 70B model gets at least 0.43 utilization (405B has to be split across two nodes, while 70B should fit on one).\n990 TFLOPS per GPU * 6.4 million GPU hours * 3600s * 0.43 = 9.808e24\n\nGeometric mean: sqrt(6.3e24 * 9.808e24) = 7.861e24",
      "Training dataset": "Llama 3 dataset",
      "Training dataset size (gradients)": "15000000000000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://ai.meta.com/blog/meta-llama-3/",
      "Reference": "Introducing Meta Llama 3: The most capable openly available LLM to date",
      "Citations": "",
      "Authors": "Aaditya Singh; Aaron Grattafiori; Abhimanyu Dubey; Abhinav Jauhri; Abhinav Pandey; Abhishek Kadian; Adam Kelsey; Adi Gangidi; Ahmad Al-Dahle; Amit Sangani; Ahuva Goldstand; Aiesha Letman; Ajay Menon; Akhil Mathur; Alan Schelten; Alex Vaughan; Amy Yang; Andrei Lupu; Andres Alvarado; Andrew Gallagher; Andrew Gu; Andrew Ho; Andrew Poulton; Andrew Ryan; Angela Fan; Ankit Ramchandani; Anthony Hartshorn; Archi Mitra; Archie Sravankumar; Artem Korenev; Arun Rao; Ashley Gabriel; Ashwin Bharambe; Assaf Eisenman; Aston Zhang; Ash JJhaveri; Aurelien Rodriguez; Austen Gregerson; Ava Spataru; Baptiste Roziere; Ben Maurer; Benjamin Leonhardi; Bernie Huang; Bhargavi Paranjape; Bing Liu; Binh Tang; Bobbie Chern; Brani Stojkovic; Brian Fuller; Catalina Mejia Arenas; Chao Zhou; Charlotte Caucheteux; Chaya Nayak; Ching-Hsiang Chu; Chloe Bi; Chris Cai; Chris Cox; Chris Marra; Chris McConnell; Christian Keller; Christoph Feichtenhofer; Christophe Touret; Chunyang Wu; Corinne Wong; Cristian Canton Ferrer; Damien Allonsius; Daniel Kreymer; Daniel Haziza; Daniel Li; Danielle Pintz; Danny Livshits; Danny Wyatt; David Adkins; David Esiobu; David Xu; Davide Testuggine; Delia David; Devi Parikh; Dhruv Choudhary; Dhruv Mahajan; Diana Liskovich; Diego Garcia-Olano; Diego Perino; Dieuwke Hupkes; Dingkang Wang; Dustin Holland; Egor Lakomkin; Elina Lobanova; Xiaoqing Ellen Tan; Emily Dinan; Eric Smith; Erik Brinkman; Esteban Arcaute; Filip Radenovic; Firat Ozgenel; Francesco Caggioni; Frank Seide; Frank Zhang; Gabriel Synnaeve; Gabriella Schwarz; Gabrielle Lee; Gada Badeer; Georgia Anderson; Graeme Nail; Gregoire Mialon; Guan Pang; Guillem Cucurell; Hailey Nguyen; Hamid Shojanazeri; Hannah Korevaar; Hannah Wang; Haroun Habeeb; Harrison Rudolph; Henry Aspegren; Hu Xu; Hugo Touvron; Iga Kozlowska; Igor Molybog; Igor Tufanov; Iliyan Zarov; Imanol Arrieta Ibarra; Irina-Elena Veliche; Isabel Kloumann; Ishan Misra; Ivan Evtimov; Jacob Xu; Jade Copet; Jake Weissman; Jan Geffert; Jana Vranes; Japhet Asher; Jason Park; Jay Mahadeokar; Jean-Baptiste Gaya; Jeet Shah; Jelmer van der Linde; Jennifer Chan; Jenny Hong; Jenya Lee; Jeremy Fu; Jeremy Teboul; Jianfeng Chi; Jianyu Huang; Jie Wang; Jiecao Yu; Joanna Bitton; Joe Spisak; Joelle Pineau; Jon Carvill; Jongsoo Park; Joseph Rocca; Joshua Johnstun; Junteng Jia; Kalyan Vasuden Alwala; Kam Hou U; Kate Plawiak; Kartikeya Upasani; Kaushik Veeraraghavan; Ke Li; Kenneth Heafield; Kevin Stone; Khalid El-Arini; Krithika Iyer; Kshitiz Malik; Kuenley Chiu; Kunal Bhalla; Kyle Huang; Lakshya Garg; Lauren Rantala-Yeary; Laurens van der Maaten; Lawrence Chen; Leandro Silva; Lee Bell; Lei Zhang; Liang Tan; Louis Martin; Lovish Madaan; Luca Wehrstedt; Lukas Blecher; Luke de Oliveira; Madeline Muzzi; Madian Khabsa; Manav Avlani; Mannat Singh; Manohar Paluri; Mark Zuckerberg; Marcin Kardas; Martynas Mankus; Mathew Oldham; Mathieu Rita; Matthew Lennie; Maya Pavlova; Meghan Keneally; Melanie Kambadur; Mihir Patel; Mikayel Samvelyan; Mike Clark; Mike Lewis; Min Si; Mitesh Kumar Singh; Mo Metanat; Mona Hassan; Naman Goyal; Narjes Torabi; Nicolas Usunier; Nikolay Bashlykov; Nikolay Bogoychev; Niladri Chatterji; Ning Dong; Oliver Aobo Yang; Olivier Duchenne; Onur Celebi; Parth Parekh; Patrick Alrassy; Paul Saab; Pavan Balaji; Pedro Rittner; Pengchuan Zhang; Pengwei Li; Petar Vasic; Peter Weng; Polina Zvyagina; Prajjwal Bhargava; Pratik Dubal; Praveen Krishnan; Punit Singh Koura; Qing He; Rachel Rodriguez; Ragavan Srinivasan; Rahul Mitra; Ramon Calderer; Raymond Li; Robert Stojnic; Roberta Raileanu; Robin Battey; Rocky Wang; Rohit Girdhar; Rohit Patel; Romain Sauvestre; Ronnie Polidoro; Roshan Sumbaly; Ross Taylor; Ruan Silva; Rui Hou; Rui Wang; Russ Howes; Ruty Rinott; Saghar Hosseini; Sai Jayesh Bondu; Samyak Datta; Sanjay Singh; Sara Chugh; Sargun Dhillon; Satadru Pan; Sean Bell; Sergey Edunov; Shaoliang Nie; Sharan Narang; Sharath Raparthy; Shaun Lindsay; Sheng Feng; Sheng Shen; Shenghao Lin; Shiva Shankar; Shruti Bhosale; Shun Zhang; Simon Vandenhende; Sinong Wang; Seohyun Sonia Kim; Soumya Batra; Sten Sootla; Steve Kehoe; Suchin Gururangan; Sumit Gupta; Sunny Virk; Sydney Borodinsky; Tamar Glaser; Tamar Herman; Tamara Best; Tara Fowler; Thomas Georgiou; Thomas Scialom; Tianhe Li; Todor Mihaylov; Tong Xiao; Ujjwal Karn; Vedanuj Goswami; Vibhor Gupta; Vignesh Ramanathan; Viktor Kerkez; Vinay Satish Kumar; Vincent Gonguet; Vish Vogeti; Vlad Poenaru; Vlad Tiberiu Mihailescu; Vladan Petrovic; Vladimir Ivanov; Wei Li; Weiwei Chu; Wenhan Xiong; Wenyin Fu; Wes Bouaziz; Whitney Meers; Will Constable; Xavier Martinet; Xiaojian Wu; Xinbo Gao; Xinfeng Xie; Xuchao Jia; Yaelle Goldschlag; Yann LeCun; Yashesh Gaur; Yasmine Babaei; Ye Qi; Yenda Li; Yi Wen; Yiwen Song; Youngjin Nam; Yuchen Hao; Yuchen Zhang; Yun Wang; Yuning Mao; Yuzi He; Zacharie Delpierre Coudert; Zachary DeVito; Zahra Hankir; Zhaoduo Wen; Zheng Yan; Zhengxing Chen; Zhenyu Yang; Zoe Papakipos",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Significant use",
      "Notability criteria notes": "Will almost certainly be very influential and widely used in the open access AI industry, as with the previous Llama generations.",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA H100 SXM5 80GB",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "not mentioned",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md\n\nLicense A custom commercial license is available at: https://llama.meta.com/llama3/license",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Reka Core",
      "Organization": "Reka AI",
      "Publication date": "2024-04-15",
      "Domain": "Multimodal,Language,Vision,Video,Speech",
      "Task": "Chat,Language modeling/generation,Image captioning,Code generation,Code autocompletion,Question answering,Visual question answering,Video description,Speech recognition (ASR),Speech-to-text,Quantitative reasoning",
      "Parameters": "67000000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "8.400010000000001e+24",
      "Training compute notes": "No direct information about Reka Core model (\"Reka Core has not finished training and is still improving.\")\n\nThe smaller dense model Reka Flash has 21B parameters and was trained on 5 trillion language tokens.\n\nThere is information about compute: \"Our setup comprises of clusters from a mixture of vendors with our peak compute being approximately\n2.5K H100s and 2.5K A100s.\"\n\nIf we assume 2 months of training with 2.5k H100s and 2.5k A100s at utilization 0.5 we get 8.4e24 FLOP (2500*9.9e14+2500*3.12e14)*60*60*24*60*0.5.",
      "Training dataset": "Wikipedia,Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/2404.12387",
      "Reference": "Reka Core, Flash, and Edge: A Series of Powerful\nMultimodal Language Models",
      "Citations": "",
      "Authors": "Aitor Ormazabal, Che Zheng, Cyprien de Masson d'Autume, Dani Yogatama, Deyu Fu, Donovan Ong, Eric Chen, Eugenie Lamprecht, Hai Pham, Isaac Ong, Kaloyan Aleksiev, Lei Li, Matthew Henderson, Max Bain, Mikel Artetxe, Nishant Relan, Piotr Padlewski, Qi Liu, Ren Chen, Samuel Phua, Yazheng Yang, Yi Tay, Yuqi Wang, Zhongkai Zhu, Zhihui Xie",
      "Abstract": "We introduce Reka Core, Flash, and Edge, a series of powerful multimodal language models trained from scratch by Reka. Reka models are able to process and reason with text, images, video, and audio inputs. This technical report discusses details of training some of these models and provides comprehensive evaluation results. We show that Reka Edge and Reka Flash are not only state-of-the-art but also outperform many much larger models, delivering outsized values for their respective compute class. Meanwhile, our most capable and largest model, Reka Core, approaches the best frontier models on both automatic evaluations and blind human evaluations. On image question answering benchmarks (e.g. MMMU, VQAv2), Core performs competitively to GPT4-V. Meanwhile, on multimodal chat, Core ranks as the second most preferred model under a blind third-party human evaluation setup, outperforming other models such as Claude 3 Opus. On text benchmarks, Core not only performs competitively to other frontier models on a set of well-established benchmarks (e.g. MMLU, GSM8K) but also outperforms GPT4-0613 on human evaluation. On video question answering (Perception-Test), Core outperforms Gemini Ultra. Models are shipped in production at this http URL . A showcase of non cherry picked qualitative examples can also be found at this http URL .",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA A100,NVIDIA H100 SXM5 80GB",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GPT-4 Turbo (Apr 2024)",
      "Organization": "OpenAI",
      "Publication date": "2024-04-09",
      "Domain": "Multimodal,Vision,Language,Image generation",
      "Task": "Chat,Language modeling/generation,Image generation,Speech synthesis,Table tasks,Visual question answering,Image captioning",
      "Parameters": "",
      "Parameters notes": "Not known. Maybe smaller/sparser than GPT-4.",
      "Training compute (FLOP)": "",
      "Training compute notes": "Training compute estimated to be 2.2e25 FLOP using benchmark imputation. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://x.com/OpenAI/status/1778574613813006610",
      "Reference": "Our new GPT-4 Turbo is now available to paid ChatGPT users. We\u2019ve improved capabilities in writing, math, logical reasoning, and coding",
      "Citations": "",
      "Authors": "",
      "Abstract": "Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:\n\nNew GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Improves upon GPT-4 Turbo from Nov 2023",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "GPT-4 Turbo (Nov 2023)",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ReALM",
      "Organization": "Apple",
      "Publication date": "2024-03-29",
      "Domain": "Language",
      "Task": "Named entity recognition (NER),Language modeling,Part-of-speech tagging",
      "Parameters": "3000000000.0",
      "Parameters notes": "Fine-tuned FLAN-T5 models ranging from 80M to 3B",
      "Training compute (FLOP)": "",
      "Training compute notes": "Fine-tuned from FLAN-T5",
      "Training dataset": "",
      "Training dataset size (gradients)": "134000000000",
      "Dataset size notes": "2300 training examples from conversation; 3900 synthetically generated training examples; 10100 training examples using context from a phone screen.",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2403.20329",
      "Reference": "ReALM: Reference Resolution As Language Modeling",
      "Citations": "",
      "Authors": "Joel Ruben Antony Moniz, Soundarya Krishnan, Melis Ozyildirim, Prathamesh Saraf, Halim Cagri Ates, Yuan Zhang, Hong Yu, Nidhi Rajshree",
      "Abstract": "Reference resolution is an important problem, one that is essential to understand and successfully handle context of different kinds. This context includes both previous turns and context that pertains to non-conversational entities, such as entities on the user's screen or those running in the background. While LLMs have been shown to be extremely powerful for a variety of tasks, their use in reference resolution, particularly for non-conversational entities, remains underutilized. This paper demonstrates how LLMs can be used to create an extremely effective system to resolve references of various types, by showing how reference resolution can be converted into a language modeling problem, despite involving forms of entities like those on screen that are not traditionally conducive to being reduced to a text-only modality. We demonstrate large improvements over an existing system with similar functionality across different types of references, with our smallest model obtaining absolute gains of over 5% for on-screen references. We also benchmark against GPT-3.5 and GPT-4, with our smallest model achieving performance comparable to that of GPT-4, and our larger models substantially outperforming it.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We show that ReaLM outperforms previous approaches, and performs roughly as well as the state-of-the-art LLM today, GPT-4, despite consisting of far fewer parameters.\"\n\n\"We also benchmark against GPT-3.5 and GPT-4, with our smallest model achieving performance comparable to that of GPT-4, and our larger models substantially outperforming it.\"\n\nI don't see any standard benchmarks that they would claim SOTA on",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "Flan-T5 11B",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "No training details given",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DBRX",
      "Organization": "Databricks",
      "Publication date": "2024-03-27",
      "Domain": "Language",
      "Task": "Chat,Code generation",
      "Parameters": "132000000000.0",
      "Parameters notes": "132B mixture of experts. 36B parameters active per inference",
      "Training compute (FLOP)": "2.6e+24",
      "Training compute notes": "Mixture of Experts (MoE)\n\n36 billion active params * 12 trillion tokens * 6 ~= 2.6e24\nhttps://www.wolframalpha.com/input?i=6+FLOP+*+36+billion+*+12+trillion\n\nalso, it was trained on 3072 NVIDIA H100s, but with an unclear timeframe (end-end process was three months, including evals and red-teaming).",
      "Training dataset": "",
      "Training dataset size (gradients)": "12000000000000",
      "Dataset size notes": "12T tokens is equivalent to 9T words. Though it includes code data, so not very literally 9T words",
      "Confidence": "Confident",
      "Link": "https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm",
      "Reference": "Introducing DBRX: A New State-of-the-Art Open LLM",
      "Citations": "",
      "Authors": "Mosaic Research Team",
      "Abstract": "Today, we are excited to introduce DBRX, an open, general-purpose LLM created by Databricks. Across a range of standard benchmarks, DBRX sets a new state-of-the-art for established open LLMs. Moreover, it provides the open community and enterprises building their own LLMs with capabilities that were previously limited to closed model APIs; according to our measurements, it surpasses GPT-3.5, and it is competitive with Gemini 1.0 Pro. It is an especially capable code model, surpassing specialized models like CodeLLaMA-70B on programming, in addition to its strength as a general-purpose LLM.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "",
      "Epochs": "1.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA H100 SXM5 80GB",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "license: https://www.databricks.com/legal/open-model-license\nconditions based on monthly users",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "MM1-30B",
      "Organization": "Apple",
      "Publication date": "2024-03-14",
      "Domain": "Multimodal,Language,Vision",
      "Task": "Chat,Image captioning,Visual question answering",
      "Parameters": "30000000000.0",
      "Parameters notes": "30B",
      "Training compute (FLOP)": "4.86e+23",
      "Training compute notes": "Pre-trained on ~2B image-text pairs and 2T tokens (Table 2). Each image is 144 tokens, so the images are ~300B tokens.\nThen additional multimodal training for 400B tokens, for a total of ~2.7T tokens.\n\nThis is the final training recipe: \"We initialize both the image encoder and the underlying LLM decoder weights for MM1 from in-house pre-trained models2. We then perform multimodal pre-training on the above data mix for 200k steps (approx. 400B tokens).\"\n\nCompute  = 6ND = 6 * 2.7 trillion * 30 billion = 4.86e23\n\nmaybe the size of the visual connector is relevant",
      "Training dataset": "Conceptual Captions (CC3M),Conceptual Captions 12M (CC12M),COYO-700M,Unspecified unreleased,OBELICS",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "at least 2T tokens",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2403.09611",
      "Reference": "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training",
      "Citations": "243.0",
      "Authors": "Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Ankur Jain, Hongyu H\u00e8, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Guoli Yin, Mark Lee, Zirui Wang, Ruoming Pang, Peter Grasch, Alexander Toshev, Yinfei Yang",
      "Abstract": "In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models up to 30B parameters, including both dense models and mixture-of-experts (MoE) variants, that are SOTA in pre-training metrics and achieve competitive performance after supervised fine-tuning on a range of established multimodal benchmarks. Thanks to large-scale pre-training, MM1 enjoys appealing properties such as enhanced in-context learning, and multi-image reasoning, enabling few-shot chain-of-thought prompting.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"In particular, the pretrained model MM1 is SOTA, performing better than Emu2 [105], Flamingo [3], and IDEFICS [47] on captioning and visual question answering (VQA) tasks in few-shot settings, both in small and large size regimes\"\n\nTable 4: outperforms Gemini and GPT-4V on VQA",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ManiGaussian",
      "Organization": "Tsinghua University,Nanyang Technological University,Carnegie Mellon University (CMU)",
      "Publication date": "2024-03-13",
      "Domain": "Robotics,Vision,Video",
      "Task": "Robotic manipulation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2403.08321\nhttps://guanxinglu.github.io/ManiGaussian/",
      "Reference": "ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation",
      "Citations": "106.0",
      "Authors": "Guanxing Lu, Shiyi Zhang, Ziwei Wang, Changliu Liu, Jiwen Lu, Yansong Tang",
      "Abstract": "Performing language-conditioned robotic manipulation tasks in unstructured environments is highly demanded for general intelligent robots. Conventional robotic manipulation methods usually learn semantic representation of the observation for action prediction, which ignores the scene-level spatiotemporal dynamics for human goal completion. In this paper, we propose a dynamic Gaussian Splatting method named ManiGaussian for multi-task robotic manipulation, which mines scene dynamics via future scene reconstruction. Specifically, we first formulate the dynamic Gaussian Splatting framework that infers the semantics propagation in the Gaussian embedding space, where the semantic representation is leveraged to predict the optimal robot action. Then, we build a Gaussian world model to parameterize the distribution in our dynamic Gaussian Splatting framework, which provides informative supervision in the interactive environment via future scene reconstruction. We evaluate our ManiGaussian on 10 RLBench tasks with 166 variations, and the results demonstrate our framework can outperform the state-of-the-art methods by 13.1\\% in average success rate. Project page: this https URL.",
      "Organization categorization": "Academia,Academia,Academia",
      "Country (of organization)": "China,Singapore,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "1 Introduction: \"We evaluate our ManiGaussian method on the RLBench dataset [27] with 10 tasks and 166 variants, where our method outperforms the state-of-the-art multi-task robotic manipulation methods by 13.1% in the average task success rate\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "\"All the compared methods are trained on two NVIDIA RTX 4090 GPUs for 100k iterations with a batch size of 2.\"\n\nhttps://github.com/GuanxingLu/ManiGaussian\nTraining: \"We train our ManiGaussian on two NVIDIA RTX 4090 GPUs for <2 days.\"\n\nPaper didn't specify precision, but GitHub code/weights might confirm - I gave a cursory look over the repo but didn't see anything obvious.\n\nAssume BF16 -> 3.3e14 FLOP/s/GPU \nAssume <2 days = <48 hr  ->  42 hr = 151200 s\nAssume 0.3 utilization\n0.3 * 2 GPU * 3.3e14 FLOP/s/GPU * 151200 s ~= 2.99e19 FLOP\n\n\n ",
      "Training hardware": "NVIDIA GeForce RTX 4090",
      "Hardware quantity": "2.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "1780.5016389530874",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "",
      "Accessibility notes": "MIT license\nhttps://github.com/GuanxingLu/ManiGaussian",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "3719.3976977276193",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Inflection-2.5",
      "Organization": "Inflection AI",
      "Publication date": "2024-03-07",
      "Domain": "Language",
      "Task": "Chat",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "8.000001e+24",
      "Training compute notes": "\"Inflection-1 used approximately 4% the training FLOPs of GPT-4 and, on average, performed at approximately 72% GPT-4 level on a diverse range of IQ-oriented tasks. Inflection-2.5, now powering Pi, achieves more than 94% the average performance of GPT-4 despite using only 40% the training FLOPs.\"\n\nThis is a weird one - we estimated GPT-4 at 2.1e25 FLOP (which could be off somewhat, or Inflection could believe a different number). 40% of that is ~8e24. \n",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://inflection.ai/inflection-2-5",
      "Reference": "Inflection-2.5: meet the world's best personal AI",
      "Citations": "",
      "Authors": "",
      "Abstract": "At Inflection, our mission is to create a personal AI for everyone. Last May, we released Pi\u2014a personal AI, designed to be empathetic, helpful, and safe. In November we announced a new major foundation model, Inflection-2, the second best LLM in the world at the time.\n\nNow we are adding IQ to Pi\u2019s exceptional EQ.\n\nWe are launching Inflection-2.5, our upgraded in-house model that is competitive with all the world's leading LLMs like GPT-4 and Gemini. It couples raw capability with our signature personality and unique empathetic fine-tuning. Inflection-2.5 is available to all Pi's users today, at pi.ai, on iOS, on Android, or our new desktop app.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Significant use",
      "Notability criteria notes": "one million daily users; six million monthly",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA H100 SXM5 80GB",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "11804593.33",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Hosted access (no API)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Claude 3 Sonnet",
      "Organization": "Anthropic",
      "Publication date": "2024-03-04",
      "Domain": "Multimodal,Language,Vision",
      "Task": "Chat,Image captioning,Code generation,Language modeling/generation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf",
      "Reference": "The Claude 3 Model Family: Opus, Sonnet, Haiku",
      "Citations": "",
      "Authors": "",
      "Abstract": "We introduce Claude 3, a new family of large multimodal models \u2013 Claude 3 Opus, our most capable offering, Claude 3 Sonnet, which provides a combination of skills and speed, and Claude 3 Haiku, our fastest and least expensive model. All new models have vision capabilities that enable them to process and analyze image data. The Claude 3 family demonstrates strong performance across benchmark evaluations and sets a new standard on\nmeasures of reasoning, math, and coding. Claude 3 Opus achieves state-of-the-art results on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. Claude 3 Haiku performs as well or better than Claude 2 [4] on most pure-text tasks, while Sonnet and Opus significantly outperform it. Additionally, these models exhibit improved fluency in non-English languages, making them more versatile for a global audience. In this report, we provide an in-depth analysis of our evaluations, focusing on core capabilities, safety, societal impacts, and the catastrophic risk assessments we committed to in our Responsible Scaling Policy [5].\n",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "Based on leaks, Claude 3 Opus and Sonnet probably cost >$1M to train.",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "Like its predecessors, Claude 3 models employ various training methods, such as unsupervised learning and Constitutional AI [6]. These models were trained using hardware from Amazon Web Services (AWS) and Google Cloud Platform (GCP)",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "Per https://time.com/6980000/anthropic/\n\"Claude 3 cost somewhere between $30 million and $300 million to train\"\nThis would seem to include all three versions.\n\nBallpark estimate, based on relative API costs:\nsqrt($30M * $300M) * (3 / (0.25 + 3 + 15)) = $15.6M\n(cost) * (Sonnet share of API cost)\n\nConvert to 2020 dollars: $12.9M",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Claude 3 Opus",
      "Organization": "Anthropic",
      "Publication date": "2024-03-04",
      "Domain": "Multimodal,Language,Vision",
      "Task": "Chat,Image captioning,Code generation,Language modeling/generation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "Training compute estimated to be 1.64e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Speculative",
      "Link": "https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf",
      "Reference": "The Claude 3 Model Family: Opus, Sonnet, Haiku",
      "Citations": "",
      "Authors": "",
      "Abstract": "We introduce Claude 3, a new family of large multimodal models \u2013 Claude 3 Opus, our most capable offering, Claude 3 Sonnet, which provides a combination of skills and speed, and Claude 3 Haiku, our fastest and least expensive model. All new models have vision capabilities that enable them to process and analyze image data. The Claude 3 family demonstrates strong performance across benchmark evaluations and sets a new standard on\nmeasures of reasoning, math, and coding. Claude 3 Opus achieves state-of-the-art results on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. Claude 3 Haiku performs as well or better than Claude 2 [4] on most pure-text tasks, while Sonnet and Opus significantly outperform it. Additionally, these models exhibit improved fluency in non-English languages, making them more versatile for a global audience. In this report, we provide an in-depth analysis of our evaluations, focusing on core capabilities, safety, societal impacts, and the catastrophic risk assessments we committed to in our Responsible Scaling Policy [5].\n",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement,Training cost",
      "Notability criteria notes": "Based on leaks, Claude 3 Opus and Sonnet probably cost >$1M to train.\n\n\"Claude 3 Opus achieves state-of-the-art results on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. \"\n\nTable 1, Table 3",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "Like its predecessors, Claude 3 models employ various training methods, such as unsupervised learning and Constitutional AI [6]. These models were trained using hardware from Amazon Web Services (AWS) and Google Cloud Platform (GCP)",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "Per https://time.com/6980000/anthropic/\n\"Claude 3 cost somewhere between $30 million and $300 million to train\"\nThis would seem to include all three versions.\n\nBallpark estimate, based on relative API costs:\nsqrt($30M * $300M) * (15 / (0.25 + 3 + 15)) = $78.0M\n(cost) * (Sonnet share of API cost)\n\nConvert to 2020 dollars: $64.7M",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Aramco Metabrain AI",
      "Organization": "Saudi Aramco",
      "Publication date": "2024-03-04",
      "Domain": "Language",
      "Task": "Language modeling/generation",
      "Parameters": "250000000000.0",
      "Parameters notes": "\"It has 250 billion parameters that are adjustable during training to generate outputs or make predictions.\"",
      "Training compute (FLOP)": "1.05e+25",
      "Training compute notes": "6*250B*7T=1.05e+25",
      "Training dataset": "",
      "Training dataset size (gradients)": "7000000000000",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://www.offshore-technology.com/news/saudi-aramco-unveils-industry-first-generative-ai-model/",
      "Reference": "Saudi Aramco unveils industry\u2019s first generative AI model",
      "Citations": "",
      "Authors": "Saudi Aramco",
      "Abstract": "",
      "Organization categorization": "Industry,Government",
      "Country (of organization)": "Saudi Arabia",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Mistral Large",
      "Organization": "Mistral AI",
      "Publication date": "2024-02-26",
      "Domain": "Language",
      "Task": "Chat",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "1.12e+25",
      "Training compute notes": "https://www.wsj.com/tech/ai/the-9-month-old-ai-startup-challenging-silicon-valleys-giants-ee2e4c48\n\nMistral spent <20 million euro (meaning approximately 20 million?) to train Mistral Large:\n\nhttps://x.com/EMostaque/status/1762152740938031484?s=20\n\"assuming this is on H100s with @Scaleway who are \u20ac1.9/hour => 10m H100 hours (c 30m A100 hrs), 3 months at 4k H100s :timer_clock:\" -Emad Mostaque\n\nAssuming bf16 or fp16, H100 SXM performance is 989 TFLOPS\nAt 1.9 euro per H100-hour and 30% utilization, spending 20M euro produces 1.12*10^25 FLOP.\nhttps://www.wolframalpha.com/input?i=20+million+%2F+%281.9%2Fhour%29+*+989+TFLOPS+*+0.30 ",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://mistral.ai/news/mistral-large/",
      "Reference": "Mistral Large, our new flagship model",
      "Citations": "",
      "Authors": "",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "France",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "~$20M training cost: https://www.wsj.com/tech/ai/the-9-month-old-ai-startup-challenging-silicon-valleys-giants-ee2e4c48\nhttps://x.com/EMostaque/status/1762152740938031484?s=20 ",
      "Epochs": "",
      "Training time (hours)": "2500.0",
      "Training time notes": "Speculation by Emad Mostaque: 20M euro spent at Scaleway (1.9 euro per H100-hour) would be around 3 months on 4000 H100s.",
      "Training hardware": "NVIDIA H100 SXM5 80GB",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "14110111.93",
      "Compute cost notes": "In February 2024, 20M EUR = 22M USD\nConverting to 2020 USD, this is 18.5M\nhttps://www.in2013dollars.com/us/inflation/2024?endYear=2020&amount=22000000",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "MegaScale (Production)",
      "Organization": "ByteDance,Peking University",
      "Publication date": "2024-02-23",
      "Domain": "Language",
      "Task": "Language modeling/generation",
      "Parameters": "530000000000.0",
      "Parameters notes": "Production run is stated to have \"hundreds of billions of parameters\". Since the authors also do a number of experiments with a 530B model, I speculate they've used 530B for the production model.",
      "Training compute (FLOP)": "3.9e+24",
      "Training compute notes": "Speculative. The model is stated to have trained for \"several weeks\". Assuming 530B parameters and \"several\" = 3, compute can be estimated from the 175B model's stated PFLOP/sec:\n2166.3 aggregate PFlops/sec * 3 weeks * 7 days/week * 24 hours/day * 3600 seconds/hour = 3.9e+24.\nAs an upper bound, say 8e+24. ",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "Speculative. Authors note production system was trained on \"multi-trillions of tokens\". This could refer to training for multiple epochs on the same 300B tokens used to train the 175B and 530B models outlined in more detail in the paper. Alternatively, it could refer to a larger dataset of perhaps 3-9 trillion tokens.",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/2402.15627",
      "Reference": "MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs",
      "Citations": "233.0",
      "Authors": "Ziheng Jiang, Haibin Lin, Yinmin Zhong, Qi Huang, Yangrui Chen, Zhi Zhang, Yanghua Peng, Xiang Li, Cong Xie, Shibiao Nong, Yulu Jia, Sun He, Hongmin Chen, Zhihao Bai, Qi Hou, Shipeng Yan, Ding Zhou, Yiyao Sheng, Zhuo Jiang, Haohan Xu, Haoran Wei, Zhang Zhang, Pengfei Nie, Leqi Zou, Sida Zhao, Liang Xiang, Zherui Liu, Zhe Li, Xiaoying Jia, Jianxi Ye, Xin Jin, Xin Liu",
      "Abstract": "We present the design, implementation and engineering experience in building and deploying MegaScale, a production system for training large language models (LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale brings unprecedented challenges to training efficiency and stability. We take a full-stack approach that co-designs the algorithmic and system components across model block and optimizer design, computation and communication overlapping, operator optimization, data pipeline, and network performance tuning. Maintaining high efficiency throughout the training process (i.e., stability) is an important consideration in production given the long extent of LLM training jobs. Many hard stability issues only emerge at large scale, and in-depth observability is the key to address them. We develop a set of diagnosis tools to monitor system components and events deep in the stack, identify root causes, and derive effective techniques to achieve fault tolerance and mitigate stragglers. MegaScale achieves 55.2% Model FLOPs Utilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the MFU by 1.34x compared to Megatron-LM. We share our operational experience in identifying and fixing failures and stragglers. We hope by articulating the problems and sharing our experience from a systems perspective, this work can inspire future LLM systems research.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "China,China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Improves SOTA in FLOP utilization for distributed LLM training by 1.34X.",
      "Epochs": "",
      "Training time (hours)": "504.0",
      "Training time notes": "Speculative. Authors state \"several weeks\". For analysis, I've assumed this means around 3 weeks.",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "12288.0",
      "Hardware utilization (MFU)": "0.48",
      "Training compute cost (2023 USD)": "2614019.245",
      "Compute cost notes": "",
      "Training power draw (W)": "9728028.18454986",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Code for MegaScale (also called veScale) training system are released under Apache Licence: https://github.com/volcengine/vescale\nThe model itself is unreleased.",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Stable Diffusion 3",
      "Organization": "Stability AI",
      "Publication date": "2024-02-22",
      "Domain": "Image generation",
      "Task": "Image generation,Text-to-image",
      "Parameters": "8000000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "5.0000000000000004e+22",
      "Training compute notes": "Finally, we performed\na scaling study of this combination up to a model size of\n8B parameters and 5 \u00d7 1022 training FLOPs.",
      "Training dataset": "ImageNet,Conceptual Captions 12M (CC12M)",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2403.03206\nhttps://stability.ai/news/stable-diffusion-3",
      "Reference": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
      "Citations": "",
      "Authors": "Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M\u00fcller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, Robin Rombach",
      "Abstract": "Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models, and we will make our experimental data, code, and model weights publicly available.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Our largest models outperform state-of-the art open models such as SDXL (Podell et al., 2023), SDXL-Turbo (Sauer et al., 2023), Pixart-\u03b1 (Chen et al., 2023), and closed-source\nmodels such as DALL-E 3 (Betker et al., 2023) both in quantitative evaluation (Ghosh et al., 2023) of prompt understanding and human preference ratings.:",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Apr 2014\nWe have partnered with Fireworks AI, the fastest and most reliable API platform in the market, to deliver Stable Diffusion 3 and Stable Diffusion 3 Turbo.\n\nIn keeping with our commitment to open generative AI, we aim to make the model weights available for self-hosting with a Stability AI Membership in the near future.",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Sora",
      "Organization": "OpenAI",
      "Publication date": "2024-02-15",
      "Domain": "Video,Vision",
      "Task": "Video generation,Text-to-video,Image-to-video,Video-to-video",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://openai.com/index/video-generation-models-as-world-simulators/",
      "Reference": "Video generation models as world simulators",
      "Citations": "",
      "Authors": "",
      "Abstract": "Sora is OpenAI\u2019s video generation model, designed to take text, image, and video inputs and generate a new video as an output. Users can create videos up to 1080p resolution (20 seconds max) in various formats, generate new content from text, or enhance, remix, and blend their own assets. Users will be able to explore the Featured and Recent feeds which showcase community creations and offer inspiration for new ideas. Sora builds on learnings from DALL\u00b7E and GPT models, and is designed to give people expanded tools for storytelling and creative expression. \n\nSora is a diffusion model, which generates a video by starting off with a base video that looks like static noise and gradually transforms it by removing the noise over many steps. By giving the model foresight of many frames at a time, we\u2019ve solved a challenging problem of making sure a subject stays the same even when it goes out of view temporarily. Similar to GPT models, Sora uses a transformer architecture, unlocking superior scaling performance. ",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "They don't report evaluation results on any standard benchmarks",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Hosted access (no API)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Gemini 1.5 Pro",
      "Organization": "Google DeepMind",
      "Publication date": "2024-02-15",
      "Domain": "Language,Multimodal",
      "Task": "Language modeling,Visual question answering",
      "Parameters": "",
      "Parameters notes": "MoE architecture",
      "Training compute (FLOP)": "",
      "Training compute notes": "Training compute imputed to be 1.58e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Speculative",
      "Link": "https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf",
      "Reference": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
      "Citations": "",
      "Authors": "Gemini Team",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Significant use",
      "Notability criteria notes": "Google DeepMind's current best public model, being used for their products.",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "Google TPU v4",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "API access: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Aya",
      "Organization": "Cohere for AI,Brown University,Cohere,Carnegie Mellon University (CMU),Massachusetts Institute of Technology (MIT)",
      "Publication date": "2024-02-12",
      "Domain": "Language",
      "Task": "Language modeling/generation,Chat,Translation",
      "Parameters": "13000000000.0",
      "Parameters notes": "13B  - fine tune of mT5 - from last page - model card ",
      "Training compute (FLOP)": "",
      "Training compute notes": "13B parameters, batch size = 256, sequence length = 1024 (for both input and output), 30K updates\n- aproximation 6ND = 6 * 13B * 2 * 1024 * 30K * 256= 1226833920000000000000 = 1.22683392e+21\n\"We finetune mT5 models using the Adafactor optimizer [Shazeer & Stern, 2018] with a learning rate of 3 \u00d7 10\u22124 and a batch size of 256. We find that using a smaller learning rate compared to 1 \u00d7 10\u22123 leads to a better downstream performance, which is potentially due to the diverse nature of our IFT mixture. Both input and target sequence length are set to 1024.\"\n\"We train all the models for 30,000 update steps with data packing enabled.16 This results in a training budget of 25M samples. \"",
      "Training dataset": "",
      "Training dataset size (gradients)": "1144220000000",
      "Dataset size notes": "at least 835 GB + size of ShareGPT-command + size of DataProvenance collection\nhttps://huggingface.co/CohereForAI/aya-101#data-sourcesxP3x  - 680GB - from \nhttps://huggingface.co/datasets/CohereForAI/xP3x\n\naya_dataset - 138MB - https://huggingface.co/datasets/CohereForAI/aya_dataset\n\naya collection - 155GB - https://huggingface.co/datasets/CohereForAI/aya_collection",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/2402.07827",
      "Reference": "Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model",
      "Citations": "309.0",
      "Authors": "Ahmet \u00dcst\u00fcn, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel D'souza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee, Julia Kreutzer, Sara Hooker",
      "Abstract": "Recent breakthroughs in large language models (LLMs) have centered around a handful of data-rich languages. What does it take to broaden access to breakthroughs beyond first-class citizen languages? Our work introduces Aya, a massively multilingual generative language model that follows instructions in 101 languages of which over 50% are considered as lower-resourced. Aya outperforms mT0 and BLOOMZ on the majority of tasks while covering double the number of languages. We introduce extensive new evaluation suites that broaden the state-of-art for multilingual eval across 99 languages -- including discriminative and generative tasks, human evaluation, and simulated win rates that cover both held-out tasks and in-distribution performance. Furthermore, we conduct detailed investigations on the optimal finetuning mixture composition, data pruning, as well as the toxicity, bias, and safety of our models. We open-source our instruction datasets and our model at this https://huggingface.co/CohereForAI/aya-101",
      "Organization categorization": "Industry,Academia,Industry,Academia,Academia",
      "Country (of organization)": "Canada,United States of America,Canada,United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "from abstract \"We introduce extensive new evaluation suites that broaden the state-of-art for multilingual eval across 99 language\"\n\nTables 5-7\nseems to be SOTA only among multilingual instruction-tuned open models, not absolute SOTA",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "Google TPU v4",
      "Hardware quantity": "128.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "86154.6850166532",
      "Base model": "mT5-XXL",
      "Finetune compute (FLOP)": "1.22683392e+21",
      "Finetune compute notes": "13B parameters, batch size = 256, sequence length = 1024 (for both input and output), 30K updates\n- aproximation 6ND = 6 * 13B * 2 * 1024 * 30K * 256= 1226833920000000000000 = 1.22683392e+21\n\"We finetune mT5 models using the Adafactor optimizer [Shazeer & Stern, 2018] with a learning rate of 3 \u00d7 10\u22124 and a batch size of 256. We find that using a smaller learning rate compared to 1 \u00d7 10\u22123 leads to a better downstream performance, which is potentially due to the diverse nature of our IFT mixture. Both input and target sequence length are set to 1024.\"\n\"We train all the models for 30,000 update steps with data packing enabled.16 This results in a training budget of 25M samples. \"",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Apache 2.0",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Qwen1.5-72B",
      "Organization": "Alibaba",
      "Publication date": "2024-02-04",
      "Domain": "Language",
      "Task": "Chat,Language modeling/generation,Quantitative reasoning,Code generation,Translation",
      "Parameters": "72000000000.0",
      "Parameters notes": "72B",
      "Training compute (FLOP)": "1.3e+24",
      "Training compute notes": "3T training tokens: https://github.com/QwenLM/Qwen2/issues/97 \n\n6 * 72 billion * 3 trillion = ~1.3e24",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "3000000000000",
      "Dataset size notes": "3 trillion tokens from this response https://github.com/QwenLM/Qwen2/issues/97",
      "Confidence": "Confident",
      "Link": "https://qwenlm.github.io/blog/qwen1.5/",
      "Reference": "Introducing Qwen1.5",
      "Citations": "",
      "Authors": "Qwen Team",
      "Abstract": "In recent months, our focus has been on developing a \u201cgood\u201d model while optimizing the developer experience. As we progress towards Qwen1.5, the next iteration in our Qwen series, this update arrives just before the Chinese New Year. With Qwen1.5, we are open-sourcing base and chat models across six sizes: 0.5B, 1.8B, 4B, 7B, 14B, and 72B. In line with tradition, we\u2019re also providing quantized models, including Int4 and Int8 GPTQ models, as well as AWQ and GGUF quantized models. To enhance the developer experience, we\u2019ve merged Qwen1.5\u2019s code into Hugging Face transformers, making it accessible with transformers>=4.37.0 without needing trust_remote_code.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "#1 in C-Eval (84.1, better than Qwen-72B. https://qwenlm.github.io/blog/qwen1.5/, https://cevalbenchmark.com/static/leaderboard.html)",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "not listed",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "restriction on >100m monthly users:\n\nhttps://huggingface.co/Qwen/Qwen1.5-72B/blob/main/LICENSE",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Qwen-VL-Max",
      "Organization": "Alibaba",
      "Publication date": "2024-01-25",
      "Domain": "Multimodal,Language,Vision",
      "Task": "Chat,Image captioning,Face recognition,Visual question answering",
      "Parameters": "7000000000.0",
      "Parameters notes": "Not stated. Qwen-VL (less capable, presumably smaller version) is 9.6B\n\nUpd: 7B parameters mentioned here\nhttps://github.com/QwenLM/Qwen-VL#qwen-vl-plus",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://qwenlm.github.io/blog/qwen-vl/",
      "Reference": "Introducing Qwen-VL",
      "Citations": "",
      "Authors": "",
      "Abstract": "Along with the rapid development of our large language model Qwen, we leveraged Qwen\u2019s capabilities and unified multimodal pretraining to address the limitations of multimodal models in generalization, and we opensourced multimodal model Qwen-VL in Sep. 2023. Recently, the Qwen-VL series has undergone a significant upgrade with the launch of two enhanced versions, Qwen-VL-Plus and Qwen-VL-Max. The key technical advancements in these versions include:\n\nSubstantially boost in image-related reasoning capabilities;\nConsiderable enhancement in recognizing, extracting, and analyzing details within images and texts contained therein;\nSupport for high-definition images with resolutions above one million pixels and images of various aspect ratios.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Notably, Qwen-VL-Max outperforms both GPT-4V from OpenAI and Gemini from Google in tasks on Chinese question answering and Chinese text comprehension\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "https://help.aliyun.com/zh/dashscope/developer-reference/tongyi-qianwen-vl-plus-api",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "AlphaGeometry",
      "Organization": "Google DeepMind,New York University (NYU)",
      "Publication date": "2024-01-17",
      "Domain": "Mathematics",
      "Task": "Geometry,Mathematical reasoning",
      "Parameters": "151000000.0",
      "Parameters notes": "\"Overall, the transformer has 151 million parameters, excluding embedding layers at its input and output heads.\"",
      "Training compute (FLOP)": "",
      "Training compute notes": "Training details. Don't think there's enough info for a FLOP estimate.\n\n\"Our customized tokenizer is trained with \u2018word\u2019 mode using\nSentencePiece36 and has a vocabulary size of 757. We limit the maximum context length to 1,024 tokens and use T5-style relative position embedding37. Sequence packing38,39 is also used because more\nthan 90% of our sequences are under 200 in length. During training, a\ndropout40 rate of 5% is applied pre-attention and post-dense. A 4\u2009\u00d7\u20094 slice of TPUv3 (ref.\u200941) is used as its hardware accelerator. For pretraining, we train the transformer with a batch size of 16 per core\nand a cosine learning-rate schedule that decays from 0.01 to 0.001\nin 10,000,000 steps. For fine-tuning, we maintain the final learning rate of 0.001 for another 1,000,000 steps\"",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "100m examples of theorem-proofs\n\n\"By using existing symbolic engines on a diverse set of random theorem premises, we extracted 100 million synthetic theorems and their\nproofs, many with more than 200 proof steps, four times longer than\nthe average proof length of olympiad theorems.\"",
      "Confidence": "Confident",
      "Link": "https://www.nature.com/articles/s41586-023-06747-5",
      "Reference": "Solving olympiad geometry without human demonstrations",
      "Citations": "574.0",
      "Authors": "Trieu H. Trinh, Yuhuai Wu, Quoc V. Le, He He, Thang Luong",
      "Abstract": "Proving mathematical theorems at the olympiad level represents a notable milestone in human-level automated reasoning1,2,3,4, owing to their reputed difficulty among the world\u2019s best talents in pre-university mathematics. Current machine-learning approaches, however, are not applicable to most mathematical domains owing to the high cost of translating human proofs into machine-verifiable format. The problem is even worse for geometry because of its unique translation challenges1,5, resulting in severe scarcity of training data. We propose AlphaGeometry, a theorem prover for Euclidean plane geometry that sidesteps the need for human demonstrations by synthesizing millions of theorems and proofs across different levels of complexity. AlphaGeometry is a neuro-symbolic system that uses a neural language model, trained from scratch on our large-scale synthetic data, to guide a symbolic deduction engine through infinite branching points in challenging problems. On a test set of 30 latest olympiad-level problems, AlphaGeometry solves 25, outperforming the previous best method that only solves ten problems and approaching the performance of an average International Mathematical Olympiad (IMO) gold medallist. Notably, AlphaGeometry produces human-readable proofs, solves all geometry problems in the IMO 2000 and 2015 under human expert evaluation and discovers a generalized version of a translated IMO theorem in 2004.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"On a test set of 30 latest olympiad-level problems, AlphaGeometry solves 25, outperforming the previous best method that only solves ten problems and approaching the performance of an average International Mathematical Olympiad (IMO) gold medallist.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache 2.0: https://github.com/google-deepmind/alphageometry \n\nData is synthetic so can be reproduced using open code",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Palmyra X 003",
      "Organization": "Writer",
      "Publication date": "2024-01-01",
      "Domain": "Language",
      "Task": "Language modeling/generation",
      "Parameters": "72000000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://writer.com/llms/palmyra-x-003/",
      "Reference": "Palmyra X 003 Instruct",
      "Citations": "",
      "Authors": "Writer",
      "Abstract": "Palmyra X 003, is a top-performing instruct model, built specifically for structured text completion rather than conversational use. Available in AI Studio no-code apps, the Writer Framework, and via API, it delivers precise, contextually accurate responses to instructions across diverse workflows. Ranked #3 on HELM at release, Palmyra X 003 remains a powerful option for enterprises needing reliable, instruct-focused AI.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Palmyra X 003 launched as the #3 model on Stanford\u2019s HELM benchmark and has since claimed the top spot in translation accuracy.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Kimi Explorer",
      "Organization": "Moonshot",
      "Publication date": "2024-01-01",
      "Domain": "Language",
      "Task": "Language modeling/generation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://www.53ai.com/news/LargeLanguageModel/2024101137012.html",
      "Reference": "",
      "Citations": "",
      "Authors": "",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Highest score at SuperCLUE benchmark for \"AISearch\" in term of \"Basic Capabilities https://www.superclueai.com/",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Hosted access (no API)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "CoRe",
      "Organization": "Tsinghua University",
      "Publication date": "2023-12-29",
      "Domain": "Mathematics,Language",
      "Task": "Quantitative reasoning,Language modeling/generation",
      "Parameters": "12400000000.0",
      "Parameters notes": "\"Since the default setting consists of two GPT-J (6B) and a DeBERTa-large (0.4B), we note our backbone as \u201cGPT-J 12B\u201d, which implies around 12.4 billion parameters in total. \"",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "GSM8K,ASDiv",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/2210.16257",
      "Reference": "Solving Math Word Problems via Cooperative Reasoning induced Language Models",
      "Citations": "104.0",
      "Authors": "Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Ruyi Gan, Jiaxing Zhang, Yujiu Yang",
      "Abstract": "Large-scale pre-trained language models (PLMs) bring new opportunities to challenging problems, especially those that need high-level intelligence, such as the math word problem (MWPs). However, directly applying existing PLMs to MWPs can fail as the generation process lacks sufficient supervision and thus lacks fast adaptivity as humans. We notice that human reasoning has a dual reasoning framework that consists of an immediate reaction system (system 1) and a delicate reasoning system (system 2), where the entire reasoning is determined by their interaction. This inspires us to develop a cooperative reasoning-induced PLM for solving MWPs, called Cooperative Reasoning (CoRe), resulting in a human-like reasoning architecture with system 1 as the generator and system 2 as the verifier. In our approach, the generator is responsible for generating reasoning paths, and the verifiers are used to supervise the evaluation in order to obtain reliable feedback for the generator. We evaluate our CoRe framework on several mathematical reasoning datasets and achieve decent improvement over state-of-the-art methods, up to 9.6% increase over best baselines.",
      "Organization categorization": "Academia",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "We evaluate our CoRe framework on several mathematical reasoning datasets and achieve decent improvement over state-of-the-art methods, up to 9.6% increase over best baselines.",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA A100 SXM4 40 GB",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "GPT-J-6B",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "Open access (non-commercial)",
      "Accessibility notes": "no clear license\nhttps://github.com/TianHongZXY/CoRe",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GQA-8-XXL",
      "Organization": "Google Research",
      "Publication date": "2023-12-23",
      "Domain": "Language",
      "Task": "Text summarization,Language modeling/generation,Translation",
      "Parameters": "11000000000.0",
      "Parameters notes": "same as base model",
      "Training compute (FLOP)": "3.4912896e+22",
      "Training compute notes": "3.3e+22 FLOP [base model] + 1.912896e+21 FLOP = 3.4912896e+22 FLOP",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2305.13245",
      "Reference": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints",
      "Citations": "",
      "Authors": "Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebr\u00f3n, Sumit Sanghai",
      "Abstract": "Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5% of original pre-training compute, and (2) introduce grouped-query attention (GQA), a generalization of multi-query attention which uses an intermediate (more than one, less than number of query heads) number of key-value heads. We show that uptrained GQA achieves quality close to multi-head attention with comparable speed to MQA.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "the paper introduced grouped-query attention",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "\"For \u03b1 = 0.05, training took approximately 600 TPUv3 chip-days\"\n= 14400 chip-hours",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "T5-11B",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "123000000000000 FLOP/GPU/sec * 14400 GPU-hours * 3600 sec / hour * 0.3 [assumed utilization] = 1.912896e+21 FLOP",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "",
      "Accessibility notes": "Apache 2.0\nhttps://github.com/google/flaxformer",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "nekomata-14b",
      "Organization": "rinna",
      "Publication date": "2023-12-21",
      "Domain": "Language",
      "Task": "Language generation",
      "Parameters": "14200000000.0",
      "Parameters notes": "Source: https://huggingface.co/rinna/nekomata-14b",
      "Training compute (FLOP)": "2.5562e+23",
      "Training compute notes": "Begins from Qwen-14B (2.5e23 FLOP). They continue pretraining on a mix of Japanese and English text for 66B tokens.\n(assuming 1 epoch, and using the C=6ND approximation)\n= # of active parameters / forward pass * # of tokens * 6 FLOPs / token\n~= 14.2e9 active parameters * 66e9 tokens * 6 FLOPs / token\n~= 5623.2e18 FLOPs\n~= 5.62e21 FLOPs\n\nIn total, 2.5562e23",
      "Training dataset": "Japanese CC-100,Japanese C4,The Pile,Wikipedia",
      "Training dataset size (gradients)": "66000000000",
      "Dataset size notes": "The 66B tokens on which the model was pre-trained came from a mixture of Japanese and English datasets. The authors didn\u2019t state the Japanese-to-English ratio of these tokens, so I decided to estimate the maximum size of the dataset. Since the model is a predictive language model, the dataset size is measured in number of words (https://docs.google.com/document/d/1XWLyMzcVfDv4eFQX3yPgM8MZ3_Q1phtIFz9GKv4_KaM/edit?tab=t.0#heading=h.or67a8q9faep).\n\nAccording to the source cited above, for text generation models, English has 0.75 words/token and Japanese has 1 word/token. Thus, the dataset attains maximum size when it contains only Japanese text. I don\u2019t think this assumption is unreasonable because the model is a Japanese language model. To that end,\nMaximum dataset size\n~= 66e9 tokens * 1 Japanese word / token\n= 6.6e10 words",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2404.01657",
      "Reference": "rinna/nekomata-14b",
      "Citations": "29.0",
      "Authors": "Tianyu Zhao, Akio Kaga, Kei Sawada",
      "Abstract": "We conduct continual pre-training of qwen-14b on 66B tokens from a mixture of Japanese and English datasets. The continual pre-training significantly improves the model's performance on Japanese tasks. It also enjoys the following great features provided by the original Qwen model.\n\n* The inclusive Qwen vocabulary (vocab size > 150k) enables the model to processs Japanese texts much more efficiently than the previously released youri series.\n* The model supports a maximum sequence length of 8192.\n\nThe name nekomata comes from the Japanese word \u732b\u53c8/\u306d\u3053\u307e\u305f/Nekomata, which is a kind of Japanese mythical creature (\u5996\u602a/\u3088\u3046\u304b\u3044/Youkai).",
      "Organization categorization": "Industry",
      "Country (of organization)": "Japan",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Nekomata-14b was released on December 21, 2023. According to rinna, on that date, the model was the best-performing one on the JSQuAD (2-shot) dataset, with an F1 score of 94.21, when evaluated using template v0.2 of Stability-AI/lm-evaluation-harness. When evaluated using template v0.3 on that same date, nekomata-14b was the best-performing model on the JCommonsenseQA (3-shot) dataset (with an accuracy of 92.23%), the MARC-ja (0-shot) dataset (with a balanced accuracy of 92.31%), and the jaqket-2 (1-shot) dataset (with an F1 score of 89.34). \n\nWhile the JSQuAD dataset alone is not a benchmark, it is part of the JGLUE benchmark. This benchmark should be recognized since it was introduced in a paper published by the LREC, which has 105 citations as of April 9, 2025 (source: https://aclanthology.org/2022.lrec-1.317/). I think nekomata-14b\u2019s performance on the JSQuAD dataset suffices to show that it is state of the art. I decided to use the evaluation with template v0.2 to determine notability because nekomata-14b was compared against dozens of other models here, unlike in the evaluation with template v0.3, where it was compared to only several models by Qwen and rinna themselves.",
      "Epochs": "",
      "Training time (hours)": "168.0",
      "Training time notes": "The pre-training job was completed within a timeframe of approximately 7 days (source: https://huggingface.co/rinna/nekomata-14b).",
      "Training hardware": "Amazon Trainium1",
      "Hardware quantity": "256.0",
      "Hardware utilization (MFU)": "0.171",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "Qwen-14B",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Tongyi Qianwen license: requires separate license if >100M MAUs\nhttps://huggingface.co/rinna/nekomata-14b",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Gemini Nano-2",
      "Organization": "Google DeepMind",
      "Publication date": "2023-12-19",
      "Domain": "Multimodal,Language,Vision,Audio",
      "Task": "Chat,Image captioning,Speech recognition (ASR)",
      "Parameters": "3250000000.0",
      "Parameters notes": "3.25B",
      "Training compute (FLOP)": "",
      "Training compute notes": "More tokens than Chinchilla-optimal:\n\n\"The number of tokens used to train the largest models were determined following the approach in Hoffmann et al. (2022). The smaller models are trained for significantly more tokens to improve performance for a given inference budget, similar to the approach advocated in Touvron et al. (2023a)\"\n\nChinchilla was 1.4T tokens for 70B params, so Chinchilla-optimal for 3.25B params would be ~1.4T/20 = 70B tokens.\n\nSo compute was significantly greater than 3.25B * 70B * 6, which is 1.4e21. \n\nTouvron et al. is the Llama 1 paper, in which a 6.7B model is trained for 1T tokens. Using the same ratio, a 3.25B model would be trained on ~500B tokens. 3.25 * 500B * 6 = 9.75e21. No guarantee that the exact ratio for Nano is close to Llama's, of course.",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2312.11805",
      "Reference": "Gemini: A Family of Highly Capable Multimodal Models",
      "Citations": "633.0",
      "Authors": "Gemini Team",
      "Abstract": "This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Significant use",
      "Notability criteria notes": "Significant use; deployed on Android phones such as the Pixel: https://store.google.com/intl/en/ideas/articles/pixel-feature-drop-december-2023/\n\n\"Despite their size, they show exceptionally strong performance on factuality,\ni.e. retrieval-related tasks, and significant performance on reasoning, STEM, coding, multimodal and multilingual tasks\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "Google TPU v5e",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "May be API access in the future. There is an Android API but it \"is under a closed early access preview program at this time\": https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Gemini Nano-1",
      "Organization": "Google DeepMind",
      "Publication date": "2023-12-19",
      "Domain": "Multimodal,Language,Vision,Audio",
      "Task": "Chat,Image captioning,Speech recognition (ASR)",
      "Parameters": "1800000000.0",
      "Parameters notes": "1.8B",
      "Training compute (FLOP)": "",
      "Training compute notes": "More tokens than Chinchilla-optimal:\n\n\"The number of tokens used to train the largest models were determined following the approach in Hoffmann et al. (2022). The smaller models are trained for significantly more tokens to improve performance for a given inference budget, similar to the approach advocated in Touvron et al. (2023a)\"",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2312.11805",
      "Reference": "Gemini: A Family of Highly Capable Multimodal Models",
      "Citations": "633.0",
      "Authors": "Gemini Team",
      "Abstract": "This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Significant use",
      "Notability criteria notes": "Significant use; deployed on Android phones such as the Pixel: https://store.google.com/intl/en/ideas/articles/pixel-feature-drop-december-2023/\n\n\"Despite their size, they show exceptionally strong performance on factuality,\ni.e. retrieval-related tasks, and significant performance on reasoning, STEM, coding, multimodal and multilingual tasks\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "Google TPU v5e",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "https://developer.android.com/ai/gemini-nano",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "FunSearch",
      "Organization": "Google DeepMind",
      "Publication date": "2023-12-14",
      "Domain": "Language,Search",
      "Task": "Code generation",
      "Parameters": "15000000000.0",
      "Parameters notes": "From the section called \"Pretrained LLM\": \"We use Codey, an LLM built on top of the PaLM2 model family... Because FunSearch relies on sampling from an LLM extensively, an important performance-defining tradeoff is between the quality of the samples and the inference speed of the LLM. In practice, we have chosen to work with a fast-inference model (rather than slower-inference, higher-quality)\"\n\nUnclear which PaLM2 model was used (of Gecko, Otter, Bison, and Unicorn); above quote indicates it was perhaps Otter or Bison, but not Unicorn. Exact parameter counts are not publicly disclosed for any of these models. In comparisons where FunSearch uses StarCoder-15B, Codey is an improvement but not obviously of an entirely different model class.\n\nI report the 15B parameters from StarCoder-15B, used as an open-source comparison",
      "Training compute (FLOP)": "3.87e+23",
      "Training compute notes": "Appendix A.5: \"Finding the full-sized symmetric admissible set I(15, 10) required the generation and analysis of approximately two million programs... To reproduce admissible set experiments done above (generating 2 million samples) one would have to use 15 instances of StarCoder-15B running on A100 40 GB GPU each and 5 CPU servers (each running 32 evaluators in parallel) for two days. We estimate that when running on Google Cloud, the price of an experiment is around $800 \u2013 $1400, and the energy usage around 250 \u2013 500 kWh; i.e., 0.5% of the energy used for training StarCoder\"\n\n15 GPUs * 7.80E+13 FLOP/GPU-sec * 2 days * 24 hours/day * 3600 sec/hour = 2.02e20 FLOP for the GPU servers\n\nWe should also add the compute used to train the PaLM2 variant used as the base LLM. Since we don't have any details about this model, I use the compute from StarCoder-15B (used as the open source comparison point): 3.87e+23 FLOP\n\nUnclear how to evaluate the compute from the CPU servers implementing the evolutionary algorithm, but this is very likely dwarfed by the pre-training compute for the LLM.",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "\"The experiments carried out in this paper do not require any data corpus other than the publicly available OR-Library bin packing benchmarks\"",
      "Confidence": "Speculative",
      "Link": "https://www.nature.com/articles/s41586-023-06924-6\nhttps://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/",
      "Reference": "Mathematical discoveries from program search with large language models",
      "Citations": "593.0",
      "Authors": "Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M. Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, Jordan S. Ellenberg, Pengming Wang, Omar Fawzi, Pushmeet Kohli, Alhussein Fawzi ",
      "Abstract": "Large language models (LLMs) have demonstrated tremendous capabilities in solving complex tasks, from quantitative reasoning to understanding natural language. However, LLMs sometimes suffer from confabulations (or hallucinations), which can result in them making plausible but incorrect statements1,2. This hinders the use of current large models in scientific discovery. Here we introduce FunSearch (short for searching in the function space), an evolutionary procedure based on pairing a pretrained LLM with a systematic evaluator. We demonstrate the effectiveness of this approach to surpass the best-known results in important problems, pushing the boundary of existing LLM-based approaches3. Applying FunSearch to a central problem in extremal combinatorics\u2014the cap set problem\u2014we discover new constructions of large cap sets going beyond the best-known ones, both in finite dimensional and asymptotic cases. This shows that it is possible to make discoveries for established open problems using LLMs. We showcase the generality of FunSearch by applying it to an algorithmic problem, online bin packing, finding new heuristics that improve on widely used baselines. In contrast to most computer search approaches, FunSearch searches for programs that describe how to solve a problem, rather than what the solution is. Beyond being an effective and scalable strategy, discovered programs tend to be more interpretable than raw solutions, enabling feedback loops between domain experts and FunSearch, and the deployment of such programs in real-world applications.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement,Historical significance",
      "Notability criteria notes": "Improved SOTA for the cap set problem. Can plausibly claim the first instance of a LLM system making a genuine and novel scientific contribution.",
      "Epochs": "",
      "Training time (hours)": "48.0",
      "Training time notes": "Appendix A.5: \"To reproduce admissible set experiments done above (generating 2 million samples) one would have to use 15 instances of StarCoder-15B running on A100 40 GB GPU each and 5 CPU servers (each running 32 evaluators in parallel) for two days\"",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "Appendix A.5: \"We estimate that when running on Google Cloud, the price of an experiment is around $800 \u2013 $1400, and the energy usage around 250 \u2013 500 kWh; i.e., 0.5% of the energy used for training StarCoder\" (in reference to a replication done using StarCoder-15B)\n\nEstimate (800+1400)/2 = $1100 at time of publication. CPI conversion to 2020 dollars: $929",
      "Training power draw (W)": "",
      "Base model": "PaLM 2",
      "Finetune compute (FLOP)": "0.0",
      "Finetune compute notes": "No finetuning",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Code to run FunSearch with an LLM of your choice is open source under Apache 2.0 (software) and CC-BY (all else). However, the actual LLM used in the main experiments is unknown and may or may not be one of the Codey models available via API access.\n\n(in other words code is available for the search tool but not for the model): https://github.com/google-deepmind/funsearch ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "CogAgent",
      "Organization": "Tsinghua University,Z.ai (Zhipu AI)",
      "Publication date": "2023-12-14",
      "Domain": "Vision,Language",
      "Task": "Instruction interpretation,Visual question answering",
      "Parameters": "18000000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "6.707e+22",
      "Training compute notes": "States 12.6 TFLOP per 1120x1120 image forward pass. Trained 60k steps with 4608 batch size, and then 10k with 1024 batch size.\n12.6 TFLOP * (60000*4608 + 10000*1024) = 3.76e21\n\nUses pretrained CogVLM as base (6.331e22 FLOP), along with EVA2-CLIP-L. EVA2-CLIP-L's FLOPs are potentially estimable, but based on details about EVA2-CLIP-g/14 (a larger model), they likely contribute negligibly to CogAgent.\n\nSum: 6.707e22",
      "Training dataset": "COYO-700M,LAION-2B,Common Crawl,Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2312.08914",
      "Reference": "CogAgent: A Visual Language Model for GUI Agents",
      "Citations": "578.0",
      "Authors": "Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong, Ming Ding, Jie Tang",
      "Abstract": "People are spending an enormous amount of time on digital devices through graphical user interfaces (GUIs), e.g., computer or smartphone screens. Large language models (LLMs) such as ChatGPT can assist people in tasks like writing emails, but struggle to understand and interact with GUIs, thus limiting their potential to increase automation levels. In this paper, we introduce CogAgent, an 18-billion-parameter visual language model (VLM) specializing in GUI understanding and navigation. By utilizing both low-resolution and high-resolution image encoders, CogAgent supports input at a resolution of 1120*1120, enabling it to recognize tiny page elements and text. As a generalist visual language model, CogAgent achieves the state of the art on five text-rich and four general VQA benchmarks, including VQAv2, OK-VQA, Text-VQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. CogAgent, using only screenshots as input, outperforms LLM-based methods that consume extracted HTML text on both PC and Android GUI navigation tasks -- Mind2Web and AITW, advancing the state of the art. The model and codes are available at this https URL .",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "China,China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "See Table 1",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "CogVLM-17B",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Code is Apache License 2.0; model is under a more restrictive custom licence which still allows commercial usage but which limits uses undermining Chinese national security and unity.\n\nfinetune code (this model is a finetune): https://github.com/THUDM/CogVLM/blob/main/finetune_demo/finetune_cogagent_demo.py ",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "VILA-13B",
      "Organization": "NVIDIA,Massachusetts Institute of Technology (MIT)",
      "Publication date": "2023-12-12",
      "Domain": "Multimodal,Language,Vision",
      "Task": "Chat,Visual question answering,Image captioning,Language modeling/generation,Question answering",
      "Parameters": "13350839296.0",
      "Parameters notes": "https://huggingface.co/Efficient-Large-Model/VILA-13b/tree/main?show_file_info=model.safetensors.index.json\n",
      "Training compute (FLOP)": "2.3003136e+21",
      "Training compute notes": "Appendix B: \"We perform training on 16 A100 GPU nodes, each node has 8 GPUs. The training hours for each stage of the 7B model are: projector initialization: 4 hours; visual language pre-training: 30 hours; visual instruction-tuning: 6 hours. The training corresponds to a total of 5.1k GPU hours. Most of the computation is spent on the pre-training stage.\"\n16 nodes * 8 GPU/node = 128 GPU\n128 GPU * (4 h + 30 h + 6 h) = 5120 GPU*h\n8 GPU/node -> A100 SMX4\n\nhttps://huggingface.co/Efficient-Large-Model/VILA-13b/tree/main?show_file_info=model.safetensors.index.json\n-> BF16\n\nAssume 80 GB variant (A100 SMX4 80 GB)\npk-BF16: 312000000000000 FLOP/s = 3.12e14 FLOP/s\n\nAssume 0.4 utilization (NVIDIA in-house)\n\n0.4 * 3.12e14 FLOP/s/GPU * (3600 s / 1 h) * 5120 GPU*h = 2.3003136e21 FLOP\n",
      "Training dataset": "",
      "Training dataset size (gradients)": "32430000000",
      "Dataset size notes": "Table 2\nMMC4: 25M images with 576+122.5 tokens each\nCOYO: 25M images with 576+22.7 tokens each\n25M*(576+122.5+576+22.7)=32430000000",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2312.07533\nhttps://huggingface.co/Efficient-Large-Model/VILA-13b",
      "Reference": "VILA: On Pre-training for Visual Language Models",
      "Citations": "661.0",
      "Authors": "Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, Song Han",
      "Abstract": "Visual language models (VLMs) rapidly progressed with the recent success of large language models. There have been growing efforts on visual instruction tuning to extend the LLM with visual inputs, but lacks an in-depth study of the visual language pre-training process, where the model learns to perform joint modeling on both modalities. In this work, we examine the design options for VLM pre-training by augmenting LLM towards VLM through step-by-step controllable comparisons. We introduce three main findings: (1) freezing LLMs during pre-training can achieve decent zero-shot performance, but lack in-context learning capability, which requires unfreezing the LLM; (2) interleaved pre-training data is beneficial whereas image-text pairs alone are not optimal; (3) re-blending text-only instruction data to image-text data during instruction fine-tuning not only remedies the degradation of text-only tasks, but also boosts VLM task accuracy. With an enhanced pre-training recipe we build VILA, a Visual Language model family that consistently outperforms the state-of-the-art models, e.g., LLaVA-1.5, across main benchmarks without bells and whistles. Multi-modal pre-training also helps unveil appealing properties of VILA, including multi-image reasoning, enhanced in-context learning, and better world knowledge.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Table5. Comparison with state-of-the-art methods on 12 visual-language benchmarks. Our models consistently outperform LLaVA-1.5 under a head-to-head comparison, using the same prompts and the same base LLM (Vicuna-1.5 is based on Llama-2), showing the effectiveness of visual-language pre-training",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA A100 SXM4 80 GB",
      "Hardware quantity": "128.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "101498.49533614099",
      "Base model": "Llama 2-13B,CLIP (ViT L/14@336px)",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "https://huggingface.co/Efficient-Large-Model/VILA-13b\n\ncode:\nApache 2.0\n\nweights:\nCC-BY-NC-SA-4.0",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "W.A.L.T",
      "Organization": "Stanford University,Google Research,Georgia Institute of Technology",
      "Publication date": "2023-12-11",
      "Domain": "Video",
      "Task": "Video generation,Text-to-video",
      "Parameters": "4719000000.0",
      "Parameters notes": "\"We train our base model at resolution 17 \u00d7 128 \u00d7 128 (3B parameters), and two 2\u00d7 cascaded super-resolution models for 17 \u00d7 128 \u00d7 224 \u2192 17 \u00d7 256 \u00d7 448 (L, 1.3B, p = 2) and 17 \u00d7 256 \u00d7 448 \u2192\n17 \u00d7 512 \u00d7 896 (L, 419M, p = 2) respectively\"\n\n3B + 1.3B + 419M = 4.719B",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "\"We train W.A.L.T for text-to-video jointly on text-image and text-video pairs (Sec. 4.2). We used a dataset of \u223c970M text-image pairs and \u223c89M text-video pairs from the public internet and internal sources. \"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2312.06662",
      "Reference": "Photorealistic Video Generation with Diffusion Models",
      "Citations": "",
      "Authors": "Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, Jos\u00e9 Lezama",
      "Abstract": "We present W.A.L.T, a transformer-based approach for photorealistic video generation via diffusion modeling. Our approach has two key design decisions. First, we use a causal encoder to jointly compress images and videos within a unified latent space, enabling training and generation across modalities. Second, for memory and training efficiency, we use a window attention architecture tailored for joint spatial and spatiotemporal generative modeling. Taken together these design decisions enable us to achieve state-of-the-art performance on established video (UCF-101 and Kinetics-600) and image (ImageNet) generation benchmarks without using classifier free guidance. Finally, we also train a cascade of three models for the task of text-to-video generation consisting of a base latent video diffusion model, and two video super-resolution diffusion models to generate videos of  resolution at  frames per second.",
      "Organization categorization": "Academia,Industry,Academia",
      "Country (of organization)": "United States of America,United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"these design decisions enable us to achieve state-of-the-art performance on established video (UCF-101 and Kinetics-600) and image (ImageNet) generation benchmarks without using classifier free guidance\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Mixtral 8x7B",
      "Organization": "Mistral AI",
      "Publication date": "2023-12-11",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering,Quantitative reasoning,Translation",
      "Parameters": "46700000000.0",
      "Parameters notes": "46.7B *sparse* params. 12.9B params used on average:\n\n\"Concretely, Mixtral has 46.7B total parameters but only uses 12.9B parameters per token. It, therefore, processes input and generates output at the same speed and for the same cost as a 12.9B model.\"",
      "Training compute (FLOP)": "7.74e+23",
      "Training compute notes": "Assuming the model was trained on ~1-10 trillions of tokens (same OOM as the models from the comparison in Figure 1. Llama 2 was trained on 2T tokens) + Mistral Small 3 was trained on 8T of tokens, we can estimate training compute with \"speculative\" confidence:\n\n6 FLOP / token / parameter * 12.9 * 10^9 active parameters * 10*10^12 tokens [speculatively] = 7.74e+23 FLOP",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Speculative",
      "Link": "https://mistral.ai/news/mixtral-of-experts/, https://arxiv.org/abs/2401.04088",
      "Reference": "Mixtral of experts: A high quality Sparse Mixture-of-Experts.",
      "Citations": "",
      "Authors": "Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L\u00e9lio Renard Lavaud, Louis Ternon, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Th\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, William El Sayed.",
      "Abstract": "Today, the team is proud to release Mixtral 8x7B, a high-quality sparse mixture of experts model (SMoE) with open weights. Licensed under Apache 2.0. Mixtral outperforms Llama 2 70B on most benchmarks with 6x faster inference. It is the strongest open-weight model with a permissive license and the best model overall regarding cost/performance trade-offs. In particular, it matches or outperforms GPT3.5 on most standard benchmarks.",
      "Organization categorization": "Industry",
      "Country (of organization)": "France",
      "Notability criteria": "Significant use",
      "Notability criteria notes": "Frequently downloaded: https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1\n\nProbably the best OS model by a big margin at time of release, e.g. #7 on Chatbot Arena, above Gemini Pro and Claude 2.1: https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard\n",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Apache 2.0",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "SeamlessM4T",
      "Organization": "Facebook,INRIA,University of California (UC) Berkeley",
      "Publication date": "2023-12-08",
      "Domain": "Speech,Language",
      "Task": "Translation,Speech synthesis,Speech recognition (ASR),Speech-to-text,Speech-to-speech",
      "Parameters": "2300000000.0",
      "Parameters notes": "2.3B",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "~5M hours of audio data (figure 2)",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2312.05187, https://huggingface.co/facebook/seamless-m4t-v2-large",
      "Reference": "Seamless: Multilingual Expressive and Streaming Speech Translation",
      "Citations": "222.0",
      "Authors": "Lo\u00efc Barrault, Yu-An Chung, Mariano Coria Meglioli, David Dale, Ning Dong, Mark Duppenthaler, Paul-Ambroise Duquenne, Brian Ellis, Hady Elsahar, Justin Haaheim, John Hoffman, Min-Jae Hwang, Hirofumi Inaguma, Christopher Klaiber, Ilia Kulikov, Pengwei Li, Daniel Licht, Jean Maillard, Ruslan Mavlyutov, Alice Rakotoarison, Kaushik Ram Sadagopan, Abinesh Ramakrishnan, Tuan Tran, Guillaume Wenzek, Yilin Yang, Ethan Ye, Ivan Evtimov, Pierre Fernandez, Cynthia Gao, Prangthip Hansanti, Elahe Kalbassi, Amanda Kallet, Artyom Kozhevnikov, Gabriel Mejia Gonzalez, Robin San Roman, Christophe Touret, Corinne Wong, Carleigh Wood, Bokai Yu, Pierre Andrews, Can Balioglu, Peng-Jen Chen, Marta R. Costa-juss\u00e0, Maha Elbayad, Hongyu Gong, Francisco Guzm\u00e1n, Kevin Heffernan, Somya Jain, Justine Kao, Ann Lee, Xutai Ma, Alex Mourachko, Benjamin Peloquin, Juan Pino, Sravya Popuri, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Anna Sun, Paden Tomasello, Changhan Wang, Jeff Wang, Skyler Wang, Mary Williamson",
      "Abstract": "Large-scale automatic speech translation systems today lack key features that help machine-mediated communication feel seamless when compared to human-to-human dialogue. In this work, we introduce a family of models that enable end-to-end expressive and multilingual translations in a streaming fashion. First, we contribute an improved version of the massively multilingual and multimodal SeamlessM4T model-SeamlessM4T v2. This newer model, incorporating an updated UnitY2 framework, was trained on more low-resource language data. SeamlessM4T v2 provides the foundation on which our next two models are initiated. SeamlessExpressive enables translation that preserves vocal styles and prosody. Compared to previous efforts in expressive speech research, our work addresses certain underexplored aspects of prosody, such as speech rate and pauses, while also preserving the style of one's voice. As for SeamlessStreaming, our model leverages the Efficient Monotonic Multihead Attention mechanism to generate low-latency target translations without waiting for complete source utterances. As the first of its kind, SeamlessStreaming enables simultaneous speech-to-speech/text translation for multiple source and target languages. To ensure that our models can be used safely and responsibly, we implemented the first known red-teaming effort for multimodal machine translation, a system for the detection and mitigation of added toxicity, a systematic evaluation of gender bias, and an inaudible localized watermarking mechanism designed to dampen the impact of deepfakes. Consequently, we bring major components from SeamlessExpressive and SeamlessStreaming together to form Seamless, the first publicly available system that unlocks expressive cross-lingual communication in real-time. The contributions to this work are publicly released and accessible at this https URL",
      "Organization categorization": "Industry,Academia,Academia",
      "Country (of organization)": "United States of America,France,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"As an improved version of SeamlessM4T, SeamlessM4T v2 delivers state-of-the-art semantic accuracy across different speech and text translation tasks while supporting nearly 100 languages as input speech or text\"\n\nTable 6",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "W2v-BERT",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "expanded from 1M hours data to 4.5M hours",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "looks like code is MIT licensed, model is CC 4.0?\nhttps://github.com/facebookresearch/seamless_communication?tab=readme-ov-file\ntrain code: https://github.com/facebookresearch/seamless_communication/blob/main/src/seamless_communication/cli/m4t/finetune/trainer.py",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Llama Guard",
      "Organization": "Meta AI",
      "Publication date": "2023-12-07",
      "Domain": "Language",
      "Task": "Chat",
      "Parameters": "7000000000.0",
      "Parameters notes": "7B",
      "Training compute (FLOP)": "1.6e+23",
      "Training compute notes": "1.7e17 finetune compute, plus Llama 2-13B pretrain compute (1.6e+23)",
      "Training dataset": "",
      "Training dataset size (gradients)": "4096000",
      "Dataset size notes": "14k prompt-response pairs. Based on training details it's trained on ~4M tokens, which is stated to be ~1 epoch:\n2 * 4096 * 500 = 4,096,000\n(batch size) * (sequence length) * (steps)",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2312.06674",
      "Reference": "Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations",
      "Citations": "723.0",
      "Authors": "Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Davide Testuggine, Madian Khabsa",
      "Abstract": "We introduce Llama Guard, an LLM-based input-output safeguard model geared towards Human-AI conversation use cases. Our model incorporates a safety risk taxonomy, a valuable tool for categorizing a specific set of safety risks found in LLM prompts (i.e., prompt classification). This taxonomy is also instrumental in classifying the responses generated by LLMs to these prompts, a process we refer to as response classification. For the purpose of both prompt and response classification, we have meticulously gathered a dataset of high quality. Llama Guard, a Llama2-7b model that is instruction-tuned on our collected dataset, albeit low in volume, demonstrates strong performance on existing benchmarks such as the OpenAI Moderation Evaluation dataset and ToxicChat, where its performance matches or exceeds that of currently available content moderation tools. Llama Guard functions as a language model, carrying out multi-class classification and generating binary decision scores. Furthermore, the instruction fine-tuning of Llama Guard allows for the customization of tasks and the adaptation of output formats. This feature enhances the model's capabilities, such as enabling the adjustment of taxonomy categories to align with specific use cases, and facilitating zero-shot or few-shot prompting with diverse taxonomies at the input. We are making Llama Guard model weights available and we encourage researchers to further develop and adapt them to meet the evolving needs of the community for AI safety.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Llama Guard, a Llama2-7b model that is instruction-tuned on our collected dataset, albeit low in volume, demonstrates strong performance on existing benchmarks such as the OpenAI Moderation Evaluation dataset and oxicChat, where its performance matches or exceeds that of currently available content moderation tools. \"\n\nTable 2",
      "Epochs": "1.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA A100 SXM4 80 GB",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "Llama 2-7B",
      "Finetune compute (FLOP)": "1.7e+17",
      "Finetune compute notes": "\"We train on a single machine with 8xA100 80GB GPUs using a batch size of 2, with sequence length of 4096, using model parallelism of 1 and a learning rate of 2 \u00d7 10\u22126. We train for 500 steps, which corresponds to \u223c1 epoch over our training set.\"\n\n6 * 2*4096*500 * 7 billion = 1.7e17",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Llama 2 license\nhttps://github.com/facebookresearch/PurpleLlama/tree/main/Llama-Guard",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Gemini 1.0 Ultra",
      "Organization": "Google DeepMind",
      "Publication date": "2023-12-06",
      "Domain": "Multimodal,Language,Vision",
      "Task": "Language modeling,Visual question answering,Chat,Translation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "5.0000000001e+25",
      "Training compute notes": "This number is an estimate based on limited evidence. In particular, we combine information about the performance of Gemini Ultra on various benchmarks compared to other models, and guesstimates about the hardware setup used for training to arrive at our estimate. Our reasoning and calculations are detailed in this Colab notebook.\nhttps://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Speculative",
      "Link": "https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf",
      "Reference": "Gemini: A Family of Highly Capable Multimodal Models",
      "Citations": "633.0",
      "Authors": "Gemini Team",
      "Abstract": "This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks \u2014 notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.\n",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement,Training cost",
      "Notability criteria notes": "\"Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks \u2014 notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined.\"\n\nTable 2",
      "Epochs": "",
      "Training time (hours)": "2400.0",
      "Training time notes": "Dylan Patel, author of SemiAnalysis, speculates that the training duration of Gemini may have been 100 days.",
      "Training hardware": "Google TPU v4",
      "Hardware quantity": "57000.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "30719419.93534952",
      "Compute cost notes": "",
      "Training power draw (W)": "38423900.11232233",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "API access: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "206082489.8134068",
      "Training compute cost (upfront)": "619525432.37549"
    },
    {
      "Model": "Gemini 1.0 Pro",
      "Organization": "Google DeepMind",
      "Publication date": "2023-12-06",
      "Domain": "Multimodal,Language,Vision",
      "Task": "Language modeling,Visual question answering,Chat,Translation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "Training compute estimated to be 1.8e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing \n\nOur reasoning and calculations for Gemini 1 Ultra are detailed in this Colab notebook.\nhttps://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c\n\n",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Speculative",
      "Link": "https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf",
      "Reference": "Gemini: A Family of Highly Capable Multimodal Models",
      "Citations": "633.0",
      "Authors": "Gemini Team",
      "Abstract": "This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks \u2014 notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.\n",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Significant use",
      "Notability criteria notes": "Default/free model on gemini.google.com\n\nFrom paper:\n\"Broadly, we find that the performance of Gemini Pro outperforms inference-optimized models such as GPT-3.5 and performs comparably with several of the most capable models available, and Gemini Ultra outperforms all current models. In this section, we examine some of these findings.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "Google TPU v4",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "API access: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Mamba-24M (SC09)",
      "Organization": "Carnegie Mellon University (CMU),Princeton University",
      "Publication date": "2023-12-01",
      "Domain": "Speech",
      "Task": "Audio generation,Speech synthesis,Text-to-speech (TTS)",
      "Parameters": "23400000.0",
      "Parameters notes": "Table 4",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "SC09",
      "Training dataset size (gradients)": "96672",
      "Dataset size notes": "Section 4.4.2: \"We largely follow the autoregressive training setup and generation protocol of Goel et al. (2022)\"\nIn which they model raw audio waveforms, such that each sample is a datapoint.\n\nSC09 is 5.3 hours long. 5.3h * 3600 sec/h * 16k samples/sec = 305,280,000 samples\n\nAppendix E.4.2: \"We used a learning rate of 0.002 and 200000 training steps at a batch size of 16... training went through 100 epochs\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2312.00752",
      "Reference": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "Citations": "5025.0",
      "Authors": "Albert Gu, Tri Dao",
      "Abstract": "Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5\u00d7 higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"SC09 is a benchmark speech generation dataset (Donahue, McAuley, and Puckette 2019; Warden 2018), consisting  of 1-second clips sampled at 16000 Hz of the digits \u201czero\u201d through \u201cnine\u201d with highly variable characteristics. We largely follow the autoregressive training setup and generation protocol of Goel et al. (2022).\nTable 4 shows automated metrics of the Mamba-UNet model compared to a variety of baselines from Goel et al. (2022): WaveNet (Oord et al. 2016), SampleRNN (Mehri et al. 2017), WaveGAN (Donahue, McAuley, and Puckette 2019), DiffWave (Z. Kong et al. 2021), and SaShiMi. A small Mamba model outperforms the state-of-the-art (and much larger) GAN- and diffusion- based models. A larger model parameter-matched to the baselines further improves on fidelity metrics dramatically.\"",
      "Epochs": "100.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Unreleased",
      "Accessibility notes": "no code or weights for 24M Mamaba model, but there are code and weights for other Mamba models:\nhttps://github.com/state-spaces/mamba",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Qwen-72B",
      "Organization": "Alibaba",
      "Publication date": "2023-11-30",
      "Domain": "Language",
      "Task": "Chat,Code generation,Language modeling/generation,Question answering",
      "Parameters": "72000000000.0",
      "Parameters notes": "72B",
      "Training compute (FLOP)": "1.3e+24",
      "Training compute notes": "72 billion params, 3 trillion tokens\n72b * 3T * 6 = 1.3e24",
      "Training dataset": "",
      "Training dataset size (gradients)": "3000000000000",
      "Dataset size notes": "Assuming not trained for multiple epochs.",
      "Confidence": "Confident",
      "Link": "https://huggingface.co/Qwen/Qwen-72B",
      "Reference": "",
      "Citations": "",
      "Authors": "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, Tianhang Zhu",
      "Abstract": "Qwen-72B is the 72B-parameter version of the large language model series, Qwen (abbr. Tongyi Qianwen), proposed by Alibaba Cloud. Qwen-72B is a Transformer-based large language model, which is pretrained on a large volume of data, including web texts, books, codes, etc. Additionally, based on the pretrained Qwen-72B, we release Qwen-72B-Chat, a large-model-based AI assistant, which is trained with alignment techniques.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "SOTA on several Chinese benchmarks, with highest average rating overall for Chinese benchmarks:\n\nhttps://opencompass.org.cn/leaderboard-llm\n\n\"significantly surpasses existing open-source models on multiple Chinese and English downstream evaluation tasks (including commonsense, reasoning, code, mathematics, etc.)\"\n\nI haven't found confirmations of it being absolute SOTA on any particular benchmarks, only among similarly sized models",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "4000000.0",
      "Batch size notes": "Table 1 https://arxiv.org/abs/2309.16609\n(this is uncertain because this table only lists sizes up to 14B. 72B was released after the paper)",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "up to 100m active users:\nhttps://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "PPLX-70B-Online",
      "Organization": "Perplexity",
      "Publication date": "2023-11-29",
      "Domain": "Language",
      "Task": "Question answering,Chat,Language modeling/generation",
      "Parameters": "70000000000.0",
      "Parameters notes": "70B",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://blog.perplexity.ai/blog/introducing-pplx-online-llms",
      "Reference": "Introducing PPLX Online LLMs ",
      "Citations": "",
      "Authors": "Lauren Yang, Kevin Hu, Aarash Heydari, Gradey Wang, Dmitry Pervukhin, Nikhil Thota, Alexandr Yarats, Max Morozov, Denis Yarats",
      "Abstract": "We\u2019re excited to share two new PPLX models: pplx-7b-online and pplx-70b-online! Our online models are focused on delivering helpful, up-to-date, and factual responses, and are publicly available via pplx-api, making it a first-of-its-kind API. pplx-7b-online and pplx-70b-online are also accessible via Perplexity Labs, our LLM playground.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Significant use",
      "Notability criteria notes": "Probably significant use: \"Perplexity, which has just 41 employees and is based out of a shared working space in San Francisco, has 10 million monthly active users, an impressive number for a young start-up.\" However, this includes everyone who uses Perplexity's app which also uses third party models like GPT-4.\n\nhttps://www.nytimes.com/2024/02/01/technology/perplexity-search-ai-google.html\n\n",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "Llama 2-70B",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "\"Fine-tuning: our PPLX models have been fine-tuned to effectively use snippets to inform their responses. Using our in-house data contractors, we carefully curate high quality, diverse, and large training sets in order to achieve high performance on various axes like helpfulness, factuality, and freshness. Our models are regularly fine-tuned to continually improve performance.\"",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "https://docs.perplexity.ai/home",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GNoME for crystal discovery",
      "Organization": "Google DeepMind",
      "Publication date": "2023-11-29",
      "Domain": "Materials science",
      "Task": "Crystal discovery",
      "Parameters": "16240000.0",
      "Parameters notes": "\"The pretrained potential has 16.24 million parameters.\"\nThis refers to the GNoME network, which is a \"Gaussian Network Model of Energy\" for predicting crystal potential energy of new crystals.",
      "Training compute (FLOP)": "",
      "Training compute notes": "Pretraining involved 36.43M steps:\n\"The learning rate was decreased to a new value of 2\u2009\u00d7\u200910\u22124 after approximately 23 million steps, to 5\u2009\u00d7\u200910\u22125 after a further approximately 11 million steps and then trained for a final 2.43 million steps. Training was performed on four TPU v3 chips.\"\n\n\"Inference on an A100 GPU on a 50-atom system takes approximately 14\u2009ms\"\n3.12e14 FLOP/s * 0.014 s = 4.368e12 FLOP per 50 atom system\n\nBatch size was 32. Multiply inference FLOPs by 3 to account for forward and backward passes during training.\n32 * 4.368e12 * 36.43 million * 3 = 1.53e22\n\nThis seems implausible \u2013 on 4 TPUv3 chips this would take\n(1.53e22 / (4 * 1.23e14)) / (3600 * 24) = 360 days",
      "Training dataset": "",
      "Training dataset size (gradients)": "69000",
      "Dataset size notes": "\"Initial models are trained on a snapshot of the Materials Project from 2018 of approximately 69,000 materials\"",
      "Confidence": "Likely",
      "Link": "https://www.nature.com/articles/s41586-023-06735-9",
      "Reference": "Scaling deep learning for materials discovery",
      "Citations": "",
      "Authors": "Amil Merchant, Simon Batzner, Samuel S. Schoenholz, Muratahan Aykol, Gowoon Cheon, Ekin Dogus Cubuk",
      "Abstract": "Novel functional materials enable fundamental breakthroughs across technological applications from clean energy to information processing. From microchips to batteries and photovoltaics, discovery of inorganic crystals has been bottlenecked by expensive trial-and-error approaches. Concurrently, deep-learning models for language, vision and biology have showcased emergent predictive capabilities with increasing data and computation. Here we show that graph networks trained at scale can reach unprecedented levels of generalization, improving the efficiency of materials discovery by an order of magnitude. Building on 48,000 stable crystals identified in continuing studies15,16,17, improved efficiency enables the discovery of 2.2 million structures below the current convex hull, many of which escaped previous human chemical intuition. Our work represents an order-of-magnitude expansion in stable materials known to humanity. Stable discoveries that are on the final convex hull will be made available to screen for technological applications, as we demonstrate for layered materials and solid-electrolyte candidates. Of the stable structures, 736 have already been independently experimentally realized. The scale and diversity of hundreds of millions of first-principles calculations also unlock modelling capabilities for downstream applications, leading in particular to highly accurate and robust learned interatomic potentials that can be used in condensed-phase molecular-dynamics simulations and high-fidelity zero-shot prediction of ionic conductivity.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "Economic impacts from development of commercially and socially valuable protein designs and materials",
      "Epochs": "1000.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "Google TPU v4",
      "Hardware quantity": "4.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "2696.8344078515975",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Inflection-2",
      "Organization": "Inflection AI",
      "Publication date": "2023-11-22",
      "Domain": "Language",
      "Task": "Language modeling,Language modeling/generation,Chat,Question answering",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "1.001e+25",
      "Training compute notes": "\"Inflection-2 was trained on 5,000 NVIDIA H100 GPUs in fp8 mixed precision for ~10\u00b2\u2075 FLOPs\"\n\n(the second 1 is there because of airtable being wonky, it's not a real sig fig)",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://inflection.ai/inflection-2",
      "Reference": "Inflection-2: The Next Step Up",
      "Citations": "",
      "Authors": "",
      "Abstract": "Today we are proud to announce that we have completed training of Inflection-2, the best model in the world for its compute class and the second most capable LLM in the world today. Our mission at Inflection is to create a personal AI for everyone. Just a few months ago, we announced Inflection-1 \u2014 a best-in-class language model that currently powers Pi. Our new model, Inflection-2, is substantially more capable than Inflection-1, demonstrating much improved factual knowledge, better stylistic control, and dramatically improved reasoning.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Significant use,Training cost",
      "Notability criteria notes": "Inflection-2 either already powers Pi or soon will: https://inflection.ai/inflection-2\n\nInflection has claimed that Pi has >1m users: https://x.com/inflectionAI/status/1699100179390210091?s=20",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA H100 SXM5 80GB",
      "Hardware quantity": "5000.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "13461144.182562498",
      "Compute cost notes": "",
      "Training power draw (W)": "6941464.657305156",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Hosted access (no API)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "via Pi, no API",
      "Numerical format": "FP8",
      "Frontier model": "True",
      "Hardware acquisition cost": "211771191.3641175",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": "285986159.97968376"
    },
    {
      "Model": "Claude 2.1",
      "Organization": "Anthropic",
      "Publication date": "2023-11-21",
      "Domain": "Language",
      "Task": "Language modeling,Chat,Language modeling/generation,Question answering",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://www.anthropic.com/index/claude-2-1",
      "Reference": "Introducing Claude 2.1",
      "Citations": "",
      "Authors": "",
      "Abstract": "Our latest model, Claude 2.1, is now available over API in our Console and is powering our claude.ai chat experience. Claude 2.1 delivers advancements in key capabilities for enterprises\u2014including an industry-leading 200K token context window, significant reductions in rates of model hallucination, system prompts and our new beta feature: tool use.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Significant use",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "Claude 2",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "AndesGPT",
      "Organization": "Oppo Mobile Telecommunications",
      "Publication date": "2023-11-16",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://www.oppo.com/en/newsroom/press/2023-oppo-developers-conference-odc23/",
      "Reference": "",
      "Citations": "",
      "Authors": "",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Highest score at SuperCLUE Safety benchmark. https://www.superclueai.com/",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Hosted access (no API)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Unreleased",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Nemotron-3-8B",
      "Organization": "NVIDIA",
      "Publication date": "2023-11-15",
      "Domain": "Language",
      "Task": "Chat,Language generation,Language modeling/generation,Translation,Code generation,Question answering",
      "Parameters": "8000000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "1.8e+23",
      "Training compute notes": "https://huggingface.co/nvidia/nemotron-3-8b-base-4k\n\n\"This model was trained on a dataset containing 3.8 Trillion tokens of text\"\n\n8 billion * 3.8 trillion * 6 = 1.8e23\n\nAlso, using the hardware method: \"1,024 A100s were used for 19 days to train the model.\"\n\n19*1024 * 312 trillion * 24 * 3600 * 0.3 = 1.57e23",
      "Training dataset": "Unspecified unreleased,Flan,P3 (Public Pool of Prompts)",
      "Training dataset size (gradients)": "3800000000000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://developer.nvidia.com/blog/nvidia-ai-foundation-models-build-custom-enterprise-chatbots-and-co-pilots-with-production-ready-llms/\n\nhttps://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/nemotron-3-8b-base-4k",
      "Reference": "NVIDIA AI Foundation Models: Build Custom Enterprise Chatbots and Co-Pilots with Production-Ready LLMs",
      "Citations": "",
      "Authors": "",
      "Abstract": "Large language models (LLMs) are revolutionizing data science, enabling advanced capabilities in natural language understanding, AI, and machine learning. Custom LLMs, tailored for domain-specific insights, are finding increased traction in enterprise applications.\n\nThe NVIDIA Nemotron-3 8B family of foundation models is a powerful new tool for building production-ready generative AI applications for the enterprise\u2013fostering innovations ranging from customer service AI chatbots to cutting-edge AI products.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"The Nemotron-3-8B-QA model offers state-of-the-art performance, achieving a zero-shot F1 score of 41.99% on the Natural Questions dataset. This metric measures how closely the generated answer resembles the truth in \u200cQA. \"",
      "Epochs": "",
      "Training time (hours)": "456.0",
      "Training time notes": "19 days",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "1024.0",
      "Hardware utilization (MFU)": "0.3473",
      "Training compute cost (2023 USD)": "214467.02013524104",
      "Compute cost notes": "",
      "Training power draw (W)": "812476.3359553809",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "25667824.0",
      "Batch size notes": "\"We pretrained the model with a sequence length of 8192 and a batch size of 3072, resulting in roughly 25 million tokens per batch\"",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "can't use to train other models:\n\nhttps://developer.download.nvidia.com/ai-foundation-models/nvidia-ai-foundation-models-license-10Nov2023.pdf",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Qwen-Audio-Chat",
      "Organization": "Alibaba",
      "Publication date": "2023-11-14",
      "Domain": "Language,Speech,Audio",
      "Task": "Audio question answering,Chat,Speech recognition (ASR),Translation,Transcription,Text classification,Question answering,Audio classification,Voice identification,Part-of-speech tagging,Speech-to-speech",
      "Parameters": "8460000000.0",
      "Parameters notes": "the model has two components - audio and language.\n670M + 7.7B = 8.46B\n\"The audio encoder is composed of 640M parameters\"\n\n\"Qwen-Audio incorporates a large language model as its foundational component.\nThe model is initialized using pre-trained weights derived from Qwen-7B (Bai et al., 2023a). Qwen-7B is a 32-layer Transformer decoder model with a hidden size of 4096, encompassing a total of 7.7B parameters.\"",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "not clear",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2311.07919",
      "Reference": "Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models",
      "Citations": "582.0",
      "Authors": "Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, Jingren Zhou",
      "Abstract": " Recently, instruction-following audio-language models have received broad attention for audio interaction with humans. However, the absence of pre-trained audio models capable of handling diverse audio types and tasks has hindered progress in this field. Consequently, most existing works have only been able to support a limited range of interaction capabilities. In this paper, we develop the Qwen-Audio model and address this limitation by scaling up audio-language pre-training to cover over 30 tasks and various audio types, such as human speech, natural sounds, music, and songs, to facilitate universal audio understanding abilities. However, directly co-training all tasks and datasets can lead to interference issues, as the textual labels associated with different datasets exhibit considerable variations due to differences in task focus, language, granularity of annotation, and text structure. To overcome the one-to-many interference, we carefully design a multi-task training framework by conditioning on a sequence of hierarchical tags to the decoder for encouraging knowledge sharing and avoiding interference through shared and specified tags respectively. Remarkably, Qwen-Audio achieves impressive performance across diverse benchmark tasks without requiring any task-specific fine-tuning, surpassing its counterparts. Building upon the capabilities of Qwen-Audio, we further develop Qwen-Audio-Chat, which allows for input from various audios and text inputs, enabling multi-turn dialogues and supporting various audio-central scenarios. ",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"A notable achievement of Qwen-Audio is its state-of-the-art performance on the test set of Aishell1, cochlscene, ClothoAQA, and VocalSound\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Open (restricted use)",
      "Inference code accessibility": "Open access (restricted use)",
      "Accessibility notes": "Qwen license:\n\nhttps://github.com/QwenLM/Qwen-Audio/blob/main/LICENSE\n\nhttps://huggingface.co/Qwen/Qwen-Audio\n\nseparate license required for companies with 100M+ MAU",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GraphCast",
      "Organization": "Google DeepMind",
      "Publication date": "2023-11-14",
      "Domain": "Earth science",
      "Task": "Weather forecasting",
      "Parameters": "",
      "Parameters notes": "Not mentioned in paper.",
      "Training compute (FLOP)": "2.1000000000000002e+22",
      "Training compute notes": "\"Training GraphCast took roughly four weeks on 32 Cloud TPU v4 devices using batch parallelism.\"\n\n4.6: \"we use bfloat16 floating point precision\"\n\n2.1e22 = 2.75E+14 FLOP/s * 32 * 60* 60 * 24 * 7 * 4",
      "Training dataset": "ERA5",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Speculative",
      "Link": "https://www.science.org/doi/epdf/10.1126/science.adi2336\n\nhttps://arxiv.org/abs/2212.12794",
      "Reference": "Learning skillful medium-range globalweather forecasting",
      "Citations": "",
      "Authors": "Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato, Ferran Alet, Suman Ravuri, Timo Ewalds, Zach Eaton-Rosen, Weihua Hu, Alexander Merose, Stephan Hoyer, George Holland, Oriol Vinyals, Jacklynn Stott, Alexander Pritzel, Shakir Mohamed, Peter Battaglia",
      "Abstract": "Global medium-range weather forecasting is critical to decision-making across many social and economic domains. Traditional numerical weather prediction uses increased compute resources to improve forecast accuracy but does not directly use historical weather data to improve the underlying model. Here, we introduce GraphCast, a machine learning\u2013based method trained directly from reanalysis data. It predicts hundreds of weather variables for the next 10 days at 0.25\u00b0 resolution globally in under 1 minute. GraphCast significantly outperforms the most accurate operational deterministic systems on 90% of 1380 verification targets, and its forecasts support better severe event prediction, including tropical cyclone tracking, atmospheric rivers, and extreme temperatures. GraphCast is a key advance in accurate and efficient weather forecasting and helps realize the promise of machine learning for modeling complex dynamical systems.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Our state-of-the-art model delivers 10-day weather predictions at unprecedented accuracy in under one minute\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache 2 for training and inference code\nhttps://github.com/google-deepmind/graphcast\n\nCC BY-NC-SA 4.0 for weights",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Volcano 13B",
      "Organization": "Korea University,Korea Advanced Institute of Science and Technology (KAIST),LG",
      "Publication date": "2023-11-13",
      "Domain": "Language,Multimodal,Vision",
      "Task": "Language modeling/generation,Visual question answering",
      "Parameters": "13000000000.0",
      "Parameters notes": "13B",
      "Training compute (FLOP)": "4.56e+22",
      "Training compute notes": "Base model is LLaVa-1.5 13B, which used 4.55e22 FLOP (mostly coming from Llama base)\n\n\"For this research, we used an NVIDIA A100-SXM4-80GB GPU and an AMD EPYC 7513 32-Core Processor running at 2.0778 GHz. Training\nVOLCANO 7B required 8 GPUs and took a total of 15 hours, while training VOLCANO 13B took 30 hours.\"\n3.12e14 * 8 * 30 * 3600 * 0.3 = 8.1e19 finetune compute",
      "Training dataset": "LAION,SBU,ShareGPT4V,Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "https://huggingface.co/datasets/kaist-ai/volcano-train\n\n558k image-text pairs, rest of dataset is ~1M examples of text data; length per sequence is not clear",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2311.07362",
      "Reference": "Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision",
      "Citations": "84.0",
      "Authors": "Seongyun Lee, Sue Hyun Park, Yongrae Jo, Minjoon Seo",
      "Abstract": "Large multimodal models (LMMs) suffer from multimodal hallucination, where they provide incorrect responses misaligned with the given visual information. Recent works have conjectured that one of the reasons behind multimodal hallucination might be due to the vision encoder failing to ground on the image properly. To mitigate this issue, we propose a novel approach that leverages self-feedback as visual cues. Building on this approach, we introduce Volcano, a multimodal self-feedback guided revision model. Volcano generates natural language feedback to its initial response based on the provided visual information and utilizes this feedback to self-revise its initial response. Volcano effectively reduces multimodal hallucination and achieves state-of-the-art on MMHal-Bench, POPE, and GAVIE. It also improves on general multimodal abilities and outperforms previous models on MM-Vet and MMBench. Through a qualitative analysis, we show that Volcano's feedback is properly grounded on the image than the initial response. This indicates that Volcano can provide itself with richer visual information, helping alleviate multimodal hallucination. We publicly release Volcano models of 7B and 13B sizes along with the data and code at this https URL.",
      "Organization categorization": "Academia,Academia,Industry",
      "Country (of organization)": "Korea (Republic of),Korea (Republic of),Korea (Republic of)",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Volcano effectively reduces multimodal hallucination and achieves state-of-the-art on MMHal-Bench, POPE, and GAVIE\" (hallucination benchmarks)",
      "Epochs": "1.0",
      "Training time (hours)": "30.0",
      "Training time notes": "",
      "Training hardware": "NVIDIA A100 SXM4 80 GB",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "LLaVA 1.5",
      "Finetune compute (FLOP)": "8.1e+19",
      "Finetune compute notes": "\"For this research, we used an NVIDIA A100- SXM4-80GB GPU and an AMD EPYC 7513 32-Core Processor running at 2.0778 GHz. Training VOLCANO 7B required 8 GPUs and took a total of 15 hours, while training VOLCANO 13B took 30 hours\"\n\n= 8 * 312 teraflops * 30 * 3600 * 0.4 utilization (assumed)\n= 8.1e19\n",
      "Batch size": "128.0",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "Open access (non-commercial)",
      "Accessibility notes": "dataset and weights are open, no license\n\nhttps://github.com/kaistAI/Volcano\n\nhttps://huggingface.co/kaist-ai/volcano-13b",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "SPHINX (Llama 2 13B)",
      "Organization": "Shanghai AI Lab,Chinese University of Hong Kong (CUHK),ShanghaiTech University",
      "Publication date": "2023-11-13",
      "Domain": "Vision,Language,Multimodal",
      "Task": "Visual question answering,Image captioning",
      "Parameters": "19900000000.0",
      "Parameters notes": "SPHINX + Llama 2 13B\nSPHINX component involves four vision encoders:\n- CLIP - ViT\n- CLIP - ConvNeXt V2 (89M to 659M params, depending on size)\n- DinoV2 - ViT (22M to 1.14B params, depending on size)\n- Q-former (188M params)\nAlso involves to projection networks\n\nHuggingface Hub model files appear to be 39.8GB. Assuming models are stored in fp16 there are 2 bytes per parameter, so 39.8 / 2 = 19.9B parameters.",
      "Training compute (FLOP)": "3.04e+22",
      "Training compute notes": "\"The pre-training time is around 125 hours on 32 A100 GPUs with a 7B\nlanguage model and about twice the time with a 13B language model... The fine-tuning takes about 38 hours with 16 A100 GPUs with a 13B\nlanguage model.\"\n\n((125*2 * 32) + (38 * 16)) * 3.12e14 * 3600 * 0.3 = 2.9e21\n\nComponent vision encoders were initialized from pre-trained:\n- CLIP ViT: 1.5e22 FLOPs for L/14@336\n- ConvNeXt V2: 6.8e21 FLOPs for largest\n- DinoV2: 7.42e+21 FLOPs for largest\n- Q-former: 1.2e21 FLOPs for largest\n\n(Based on full parameter count, SPHINX probably uses largest models)\n\nSum: 3.04e22 FLOPs",
      "Training dataset": "LAION-400M,LAION-COCO,RefinedWeb",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "\" For the joint training on both images and texts, we form each\nbatch with 640 image-text pairs from LAION-400M or LAION-COCO and 65, 536 text tokens from RefinedWeb\"\n\nAs per Figure 7 there was 20000 training steps",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2311.07575",
      "Reference": "SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models",
      "Citations": "",
      "Authors": "Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, Jiaming Han, Siyuan Huang, Yichi Zhang, Xuming He, Hongsheng Li, Yu Qiao",
      "Abstract": "We present SPHINX, a versatile multi-modal large language model (MLLM) with a joint mixing of model weights, tuning tasks, and visual embeddings. First, for stronger vision-language alignment, we unfreeze the large language model (LLM) during pre-training, and introduce a weight mix strategy between LLMs trained by real-world and synthetic data. By directly integrating the weights from two domains, the mixed LLM can efficiently incorporate diverse semantics with favorable robustness. Then, to enable multi-purpose capabilities, we mix a variety of tasks for joint visual instruction tuning, and design task-specific instructions to avoid inter-task conflict. In addition to the basic visual question answering, we include more challenging tasks such as region-level understanding, caption grounding, document layout detection, and human pose estimation, contributing to mutual enhancement over different scenarios. Additionally, we propose to extract comprehensive visual embeddings from various network architectures, pre-training paradigms, and information granularity, providing language models with more robust image representations. Based on our proposed joint mixing, SPHINX exhibits superior multi-modal understanding capabilities on a wide range of applications. On top of this, we further propose an efficient strategy aiming to better capture fine-grained appearances of high-resolution images. With a mixing of different scales and high-resolution sub-images, SPHINX attains exceptional visual parsing and reasoning performance on existing evaluation benchmarks. We hope our work may cast a light on the exploration of joint mixing in future MLLM research.",
      "Organization categorization": "Academia,Academia,Academia",
      "Country (of organization)": "China,Hong Kong,China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"as shown in Figure 2, SPHINX can achieve impressive fine-grained visual perception for high-resolution images, which exhibits state-of-the-art performance on extensive evaluation benchmarks, e.g., MMBench (Liu et al., 2023f), MME (Fu et al., 2023a), and POPE (Li et al., 2023e).\"",
      "Epochs": "",
      "Training time (hours)": "290.0",
      "Training time notes": "\"The pre-training time is around 125 hours on 32 A100 GPUs with a 7B\nlanguage model and about twice the time with a 13B language model.\"\n\" The fine-tuning takes about 38 hours with 16 A100 GPUs with a 13B\nlanguage model.\"",
      "Training hardware": "NVIDIA A100 SXM4 40 GB",
      "Hardware quantity": "32.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "239188.6875340231",
      "Compute cost notes": "",
      "Training power draw (W)": "25391.016357362594",
      "Base model": "Llama 2-13B",
      "Finetune compute (FLOP)": "4.00000000001e+21",
      "Finetune compute notes": "32 A100 * 312 TFLOPS/A100 * 290 hours * 40% utilization ~= 4e21 FLOP\nhttps://www.wolframalpha.com/input?i=250+hours+*+312+TFLOPS+*+32+*+0.4",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Open (restricted use)",
      "Inference code accessibility": "Open access (restricted use)",
      "Accessibility notes": "https://github.com/Alpha-VLLM/LLaMA2-Accessory\n\nlooks like same as LLama license\n\nfinetune code: https://github.com/Alpha-VLLM/LLaMA2-Accessory/tree/main/SPHINX ",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "328436.4147835633",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "RoFormer",
      "Organization": "Zhuiyi Technology",
      "Publication date": "2023-11-08",
      "Domain": "Language",
      "Task": "Text classification,Language modeling/generation",
      "Parameters": "110000000.0",
      "Parameters notes": "\"we use bert-base\" \nBERT base has 110M parameters from https://arxiv.org/pdf/1810.04805",
      "Training compute (FLOP)": "2.162688e+18",
      "Training compute notes": "6 FLOP/parameter/token * 110000000 parameters * 3276800000 tokens = 2162688000000000000 FLOP",
      "Training dataset": "BookCorpus (BooksCorpus, Toronto Book Corpus),Wikipedia",
      "Training dataset size (gradients)": "3276800000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2104.09864",
      "Reference": "RoFormer: Enhanced Transformer with Rotary Position Embedding",
      "Citations": "",
      "Authors": "Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, Yunfeng Liu",
      "Abstract": "Position encoding recently has shown effective in the transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding(RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: \\url{this https URL}.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "the paper introduced RoPE",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "\"All the experiments were run on two cloud servers with 4 x V100 GPUs.\"",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "4.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "2380.6728491831727",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "",
      "Accessibility notes": "Apache 2.0\nhttps://github.com/ZhuiyiTechnology/roformer \n(only Chinese roformer implementations, not BERT)\n\nRoFormer is already integrated into Huggingface:\nhttps://huggingface.co/docs/transformers/model_doc/roformer",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "MultiBand Diffusion",
      "Organization": "Meta AI,Hebrew University of Jerusalem,LORIA",
      "Publication date": "2023-11-08",
      "Domain": "Audio,Speech",
      "Task": "Audio generation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "2.6e+19",
      "Training compute notes": "\"It takes around 2 days on 4 Nvidia V100 with 16 GB to train one of the 4 models.\"\n\n125 tflop/s for V100 SXM (not clear which they used, could be PCI given small number - still same OOM thus confident)\n4 * 125 trillion * 2 * 24 * 3600 * 0.3 = 2.6e19",
      "Training dataset": "Common Voice,DNS,MTG-Jamendo,FSD50K,AudioSet",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "9096+2425+919+108+4989=17537 hours",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2308.02560",
      "Reference": "From Discrete Tokens to High-Fidelity Audio Using Multi-Band Diffusion",
      "Citations": "36.0",
      "Authors": "Robin San Roman, Yossi Adi, Antoine Deleforge, Romain Serizel, Gabriel Synnaeve, Alexandre D\u00e9fossez",
      "Abstract": "Deep generative models can generate high-fidelity audio conditioned on various types of representations (e.g., mel-spectrograms, Mel-frequency Cepstral Coefficients (MFCC)). Recently, such models have been used to synthesize audio waveforms conditioned on highly compressed representations. Although such methods produce impressive results, they are prone to generate audible artifacts when the conditioning is flawed or imperfect. An alternative modeling approach is to use diffusion models. However, these have mainly been used as speech vocoders (i.e., conditioned on mel-spectrograms) or generating relatively low sampling rate signals. In this work, we propose a high-fidelity multi-band diffusion-based framework that generates any type of audio modality (e.g., speech, music, environmental sounds) from low-bitrate discrete representations. At equal bit rate, the proposed approach outperforms state-of-the-art generative techniques in terms of perceptual quality. Training and, evaluation code, along with audio samples, are available on the facebookresearch/audiocraft Github page.",
      "Organization categorization": "Industry,Academia,Academia",
      "Country (of organization)": "United States of America,Israel,France",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"At equal bit rate, the proposed approach outperforms state-of-the-art generative techniques in terms of perceptual quality\"",
      "Epochs": "",
      "Training time (hours)": "48.0",
      "Training time notes": "around 2 days",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "22.81032329809286",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "training, inference, and models (MIT)\nhttps://github.com/facebookresearch/audiocraft/blob/main/docs/MBD.md ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "OmniVec",
      "Organization": "TensorTour",
      "Publication date": "2023-11-07",
      "Domain": "Multimodal,Vision,Speech,Language,Video,3D modeling",
      "Task": "Image classification,Speech recognition (ASR)",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "AudioSet,Something-Something v2 (SSv2),English Wikipedia,ImageNet-1k,SUN RGB-D,ModelNet40",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/2311.05709v1",
      "Reference": "OmniVec: Learning robust representations with cross modal sharing",
      "Citations": "81.0",
      "Authors": "Siddharth Srivastava, Gaurav Sharma",
      "Abstract": "Majority of research in learning based methods has been towards designing and training networks for specific tasks. However, many of the learning based tasks, across modalities, share commonalities and could be potentially tackled in a joint framework. We present an approach in such direction, to learn multiple tasks, in multiple modalities, with a unified architecture. The proposed network is composed of task specific encoders, a common trunk in the middle, followed by task specific prediction heads. We first pre-train it by self-supervised masked training, followed by sequential training for the different tasks. We train the network on all major modalities, e.g.\\ visual, audio, text and 3D, and report results on 22 diverse and challenging public benchmarks. We demonstrate empirically that, using a joint network to train across modalities leads to meaningful information sharing and this allows us to achieve state-of-the-art results on most of the benchmarks. We also show generalization of the trained network on cross-modal tasks as well as unseen datasets and tasks.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\" We surpass the state of the art on iNaturalist with a top-1 accuracy of 93.8%, compared to InternImage\u2019s 92.6%. On Places-365, we beat all competitors, achieving 61.6% accuracy versus InternImage\u2019s 61.2%\"\n\n\"We observe that we outperform all the competing methods on Moments in Time dataset \"\n\n\"on the ESC50 dataset. OmniVec outperforms competing methods, achieving an accuracy of 98.4%, significantly higher than the Audio Spectrogram Transformer (AST) at 85.7%.\"",
      "Epochs": "2000.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "BERT-Large",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "Appears to build on several models, like BERT and ViT (Table 1)",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "mPLUG-Owl2",
      "Organization": "Alibaba",
      "Publication date": "2023-11-07",
      "Domain": "Vision,Language,Multimodal",
      "Task": "Visual question answering,Image captioning,Language modeling/generation",
      "Parameters": "7120000000.0",
      "Parameters notes": "\"As depicted in Figure 2, our model, referred to as mPLUGOwl2, is composed of three main components: a fundamental vision encoder, a visual abstractor, and a language decoder. Specifically, we utilize ViT-L/14 as the\nvision encoder and LLaMA-2-7B [58] as the language decoder\"\nViT-L/14 has 123M parameters and Llama 2 7B has 7B parameters.",
      "Training compute (FLOP)": "",
      "Training compute notes": "ViT-L/14 and Llama 2-7b compute, plus 1.7e19 joint pretrain FLOP (6 * 400M * 7.1B) and 4e16 joint finetune FLOP. Everything is a negligible fraction except the Llama 2 compute.",
      "Training dataset": "Conceptual Captions (CC3M),Conceptual Captions 12M (CC12M),COCO,LAION,COYO-700M",
      "Training dataset size (gradients)": "182032793600",
      "Dataset size notes": "400 million image-text pairs at pre-training + 672k image-text pairs at instruction tuning (table 14) = 400672000 images\n\n+ 558k text instructions (table 14) ",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/2311.04257",
      "Reference": "mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration",
      "Citations": "",
      "Authors": "Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, Jingren Zhou",
      "Abstract": "Multi-modal Large Language Models (MLLMs) have demonstrated impressive instruction abilities across various open-ended tasks. However, previous methods primarily focus on enhancing multi-modal capabilities. In this work, we introduce a versatile multi-modal large language model, mPLUG-Owl2, which effectively leverages modality collaboration to improve performance in both text and multi-modal tasks. mPLUG-Owl2 utilizes a modularized network design, with the language decoder acting as a universal interface for managing different modalities. Specifically, mPLUG-Owl2 incorporates shared functional modules to facilitate modality collaboration and introduces a modality-adaptive module that preserves modality-specific features. Extensive experiments reveal that mPLUG-Owl2 is capable of generalizing both text tasks and multi-modal tasks and achieving state-of-the-art performances with a single generic model. Notably, mPLUG-Owl2 is the first MLLM model that demonstrates the modality collaboration phenomenon in both pure-text and multi-modal scenarios, setting a pioneering path in the development of future multi-modal foundation models.\n",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Extensive experiments illustrate the effectiveness and generalization abilities of mPLUG-Owl2, which achieves state-of-the-art performance on 8 classic vision-language benchmarks using a single generic model.\"\n\nFigure 1",
      "Epochs": "1.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "Llama 2-7B",
      "Finetune compute (FLOP)": "1.7000000001e+19",
      "Finetune compute notes": "https://www.wolframalpha.com/input?i=6+*+400+million+*+7.12+billion",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "",
      "Accessibility notes": "Apache 2\nhttps://github.com/X-PLUG/mPLUG-Owl/tree/main/mPLUG-Owl2\n\nhttps://huggingface.co/Mizukiluke/mplug_owl_2_1",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GPT-4 Turbo (Nov 2023)",
      "Organization": "OpenAI",
      "Publication date": "2023-11-06",
      "Domain": "Multimodal,Vision,Language,Image generation",
      "Task": "Chat,Language modeling/generation,Image generation,Speech synthesis,Table tasks,Visual question answering,Image captioning",
      "Parameters": "",
      "Parameters notes": "Not known. Maybe smaller/sparser than GPT-4.",
      "Training compute (FLOP)": "",
      "Training compute notes": "Training compute estimated to be 2.2e25 FLOP using benchmark imputation. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://openai.com/blog/new-models-and-developer-products-announced-at-devday",
      "Reference": "New models and developer products announced at DevDay",
      "Citations": "",
      "Authors": "",
      "Abstract": "Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:\n\nNew GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"More capable\" than GPT-4 according to OpenAI, with larger context window",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "CogVLM-17B",
      "Organization": "Tsinghua University,Z.ai (Zhipu AI),Beihang University",
      "Publication date": "2023-11-06",
      "Domain": "Multimodal,Vision,Language",
      "Task": "Image captioning,Visual question answering,Chat",
      "Parameters": "17000000000.0",
      "Parameters notes": "CogVLM-17B has 10 billion vision parameters and 7 billion language parameters. However, \"the total number of trainable parameters is 6.5B\".\n\n\"CogVLM model comprises four fundamental components: a vision transformer (ViT) encoder, an MLP adapter, a pretrained large language model (GPT), and a visual expert module.\"\n\nViT: EVA2-CLIP-E, last layer removed (5B params with last layer, non-trainable)\nMLP adapter: 2 layers, parameter count unavailable\nGPT: Vicuna1.5-7B (7B params)\nVisual expert module: parameter count unclear",
      "Training compute (FLOP)": "6.331e+22",
      "Training compute notes": "from table 8 on page 17\n\n230.1 FLOPS*days \nso \n10**15*24*3600*230.1= 1.988e22\n\nSince this training uses pretrained weights from EVA02-CLIP-E and Vicuna1.5-7B, we report the full number of FLOPs baked into the model.\n\nEVA02-CLIP-g/14 is stated to have taken 25 days to train 12B samples using 64 A100-40GB GPUs, implying: \n25 days * 24 hr/day * 3600 sec/hr * 64 GPU * 7.80E+13 FLOP/GPU-sec * 30% efficiency = 3.23e21\n\nEVA02-CLIP-E doesn't give a training time; it saw 1/4 as many samples as the g/14 model but has 4.27x more parameters; as a rough estimate, assume it took the same number of FLOPs to train.\n\nVicuna1.5-7B's training compute is ~entirely embedded in the base Llama-7b weights, which took 4.02e+22 FLOPs\n\nTotal: 1.988e22 + 3.23e21 + 4.02e22 = 6.331E22",
      "Training dataset": "VQAv2,LAION-2B,COYO-700M,OKVQA,TextVQA,OCR-VQA,ScienceQA,LLaVA-Instruct-150k,LRV-Instruction,LLaVAR,Flickr30K Entities,RefCOCO,Visual7W,VisualGenome,COCO,TextCaps",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "After filtering, about 1.5B image-text pairs are left for pretraining in stage one. Stage two of pretraining adds a visual grounding dataset of 40M images with generated noun bounding boxes. These are filtered from LAION-115M so that 75% of images contain at least two bounding boxes.\n\nTwo different kinds of finetuning are done, each using a number of datasets:\n- CogVLM-Chat: VQAv2 (11059040), OKVQA (70275), TextVQA (453360), OCRVQA (1002146), ScienceQA (21208), LLaVAInstruct (150000), LRV-Instruction (300000), LLaVAR (1633000)\n- CogVLM-Grounding: Flickr30K Entities (520000), RefCOCO (142209), Visual7W (889388), VisualGenome (1700000)\n\nAdditional experiments finetune using the training sets from COCO (413915 in train) and TextCaps (109765 in train)\n\nIn sum, pretraining and finetuning appear to contain 1,500,000,000 and 18,534,581 datapoints, respectively.",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2311.03079\nhttps://huggingface.co/THUDM/cogvlm-chat-hf\nhttps://github.com/THUDM/CogVLM\n",
      "Reference": "CogVLM: Visual Expert for Pretrained Language Models",
      "Citations": "698.0",
      "Authors": "Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, Jie Tang",
      "Abstract": "We introduce CogVLM, a powerful open-source visual language foundation model. Different from the popular shallow alignment method which maps image features into the input space of language model, CogVLM bridges the gap between the frozen pretrained language model and image encoder by a trainable visual expert module in the attention and FFN layers. As a result, CogVLM enables deep fusion of vision language features without sacrificing any performance on NLP tasks. CogVLM-17B achieves state-of-the-art performance on 10 classic cross-modal benchmarks, including NoCaps, Flicker30k captioning, RefCOCO, RefCOCO+, RefCOCOg, Visual7W, GQA, ScienceQA, VizWiz VQA and TDIUC, and ranks the 2nd on VQAv2, OKVQA, TextVQA, COCO captioning, etc., surpassing or matching PaLI-X 55B. Codes and checkpoints are available at this https URL. ",
      "Organization categorization": "Academia,Industry,Academia",
      "Country (of organization)": "China,China,China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"CogVLM-17B achieves state-of-the-art performance on 17 classic cross-modal benchmarks, including 1) image captioning datasets: NoCaps, Flicker30k, 2) VQA datasets: OKVQA, TextVQA, OCRVQA, ScienceQA, 3) LVLM benchmarks: MM-Vet, MMBench, SEED-Bench, LLaVABench, POPE, MMMU, MathVista, 4) visual grounding\ndatasets: RefCOCO, RefCOCO+, RefCOCOg, Visual7W. Codes and checkpoints are available at https://github.com/THUDM/CogVLM\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "Vicuna-7B v0",
      "Finetune compute (FLOP)": "2e+22",
      "Finetune compute notes": "Trained from Vicuna1.5-7B weights",
      "Batch size": "",
      "Batch size notes": "8192 in pretraining stage 1, 1024 in stage 2",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "code is Apache, model more restrictive, commercial allowed, subject to PRC laws and interests\n\ncode isn't training code",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "LLaVA 1.5",
      "Organization": "University of Wisconsin Madison,Microsoft Research",
      "Publication date": "2023-11-05",
      "Domain": "Multimodal,Language,Vision",
      "Task": "Chat,Question answering,Visual question answering",
      "Parameters": "13000000000.0",
      "Parameters notes": "from abstract \"Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~1 day on a single 8-A100 node. \"",
      "Training compute (FLOP)": "7.807e+22",
      "Training compute notes": "\"Due to the increased image input resolution to 336^2, the training of LLaVA-1.5 is \u223c2\u00d7 as long as LLaVA: \u223c6 hours of pretraining and \u223c20 hours of visual instruction tuning using 8\u00d7 A100s.\"\n26 * 3600 * 8 * 3.12e14 * 0.3 = 7.0e19\n\nFine-tuned from Vicuna-13B (itself finetuned from LLaMa-13B), which used 7.8e22 FLOPs\n\n7.0e19 + 7.8e22",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "1.2M text-image pairs from https://huggingface.co/liuhaotian/llava-v1.5-13b#training-dataset",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2310.03744,\n",
      "Reference": "Improved Baselines with Visual Instruction Tuning",
      "Citations": "4070.0",
      "Authors": "Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee",
      "Abstract": "Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this note, we show that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient. With simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with simple response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~1 day on a single 8-A100 node. We hope this can make state-of-the-art LMM research more accessible. Code and model will be publicly available. ",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "from abstract: \"we establish stronger baselines that achieve state-of-the-art across 11 benchmark\"",
      "Epochs": "1.0",
      "Training time (hours)": "24.0",
      "Training time notes": "from abstract \"Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~1 day on a single 8-A100 node. \"",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "8.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "6348.88507402357",
      "Base model": "Vicuna-13B v0",
      "Finetune compute (FLOP)": "7.008768e+19",
      "Finetune compute notes": "\"Due to the increased image input resolution to 336^2, the training of LLaVA-1.5 is \u223c2\u00d7 as long as LLaVA: \u223c6 hours of pretraining and \u223c20 hours of visual instruction tuning using 8\u00d7 A100s.\"\n\n26 * 3600 * 8 * 3.12e14 * 0.3 = 7.0e19",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Llama 2 license for weights\n\nhttps://huggingface.co/liuhaotian/llava-v1.5-13b\n\nApache 2.0 license for code:\nhttps://github.com/haotian-liu/LLaVA",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Grok-1",
      "Organization": "xAI",
      "Publication date": "2023-11-04",
      "Domain": "Language",
      "Task": "Language modeling,Chat",
      "Parameters": "314000000000.0",
      "Parameters notes": "\"314B parameter Mixture-of-Experts model with 25% of the weights active on a given token\". So effectively 78B parameters\n\nMixture of 8 experts: https://github.com/xai-org/grok-1",
      "Training compute (FLOP)": "2.90000000001e+24",
      "Training compute notes": "\"On these benchmarks, Grok-1 displayed strong results, surpassing all other models in its compute class, including ChatGPT-3.5 and Inflection-1. It is only surpassed by models that were trained with a significantly larger amount of training data and compute resources like GPT-4\"\n\nPer table, Grok-1 is surpassed by Palm 2, Claude 2, GPT-4, so it required less compute than these three models. Palm 2 was trained on 7e24 FLOP.\n\nGPT-3.5 is ~2.6e24. Inflection-1's compute is not public/known by us but Inflection says Inflection-1 compute was <= Palm-540B's (which was ~2.5e24). \n\nFor optimal training, our current working hypothesis is that you still need something like Chinchilla scaling on the total number of parameters in the model, even for MoE models, so optimal dataset size would be 20*310B tokens. With 25%*314B params active per forward pass, this would be around 3e24 FLOP.\nhttps://www.wolframalpha.com/input?i=20*310+billion+*+6+*+25%25+*+314+billion",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "6200000000000",
      "Dataset size notes": "(Speculative confidence, see compute notes)",
      "Confidence": "Likely",
      "Link": "https://x.ai/model-card/, https://x.ai/blog/grok-os",
      "Reference": "Announcing Grok",
      "Citations": "",
      "Authors": "",
      "Abstract": "Grok is an AI modeled after the Hitchhiker\u2019s Guide to the Galaxy, so intended to answer almost anything and, far harder, even suggest what questions to ask!\n\nGrok is designed to answer questions with a bit of wit and has a rebellious streak, so please don\u2019t use it if you hate humor!\n\nA unique and fundamental advantage of Grok is that it has real-time knowledge of the world via the \ud835\udd4f platform. It will also answer spicy questions that are rejected by most other AI systems.\n\nGrok is still a very early beta product \u2013 the best we could do with 2 months of training \u2013 so expect it to improve rapidly with each passing week with your help.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "\"On these benchmarks, Grok-1 displayed strong results, surpassing all other models in its compute class, including ChatGPT-3.5 and Inflection-1\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "not listed",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "apache 2.0\nhttps://github.com/xai-org/grok-1",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "RT-Trajectory",
      "Organization": "Google DeepMind,University of California San Diego,Stanford University",
      "Publication date": "2023-11-03",
      "Domain": "Robotics",
      "Task": "Robotic manipulation",
      "Parameters": "",
      "Parameters notes": "seems to be based on the RT-1 architecture (35M parameters) with some modifications (section 3.3)",
      "Training compute (FLOP)": "",
      "Training compute notes": "Given the architecture seems to use 35M parameters, it seems unlikely this is above 1e23 FLOP.",
      "Training dataset": "RT-1",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/2311.01977",
      "Reference": "RT-Trajectory: Robotic Task Generalization via Hindsight Trajectory Sketches",
      "Citations": "110.0",
      "Authors": "Jiayuan Gu, Sean Kirmani, Paul Wohlhart, Yao Lu, Montserrat Gonzalez Arenas, Kanishka Rao, Wenhao Yu, Chuyuan Fu, Keerthana Gopalakrishnan, Zhuo Xu, Priya Sundaresan, Peng Xu, Hao Su, Karol Hausman, Chelsea Finn, Quan Vuong, Ted Xiao",
      "Abstract": "Generalization remains one of the most important desiderata for robust robot learning systems. While recently proposed approaches show promise in generalization to novel objects, semantic concepts, or visual distribution shifts, generalization to new tasks remains challenging. For example, a language-conditioned policy trained on pick-and-place tasks will not be able to generalize to a folding task, even if the arm trajectory of folding is similar to pick-and-place. Our key insight is that this kind of generalization becomes feasible if we represent the task through rough trajectory sketches. We propose a policy conditioning method using such rough trajectory sketches, which we call RT-Trajectory, that is practical, easy to specify, and allows the policy to effectively perform new tasks that would otherwise be challenging to perform. We find that trajectory sketches strike a balance between being detailed enough to express low-level motion-centric guidance while being coarse enough to allow the learned policy to interpret the trajectory sketch in the context of situational visual observations. In addition, we show how trajectory sketches can provide a useful interface to communicate with robotic policies: they can be specified through simple human inputs like drawings or videos, or through automated methods such as modern image-generating or waypoint-generating methods. We evaluate RT-Trajectory at scale on a variety of real-world robotic tasks, and find that RT-Trajectory is able to perform a wider range of tasks compared to language-conditioned and goal-conditioned policies, when provided the same training data.",
      "Organization categorization": "Industry,Academia,Academia",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland,United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "from blog https://deepmind.google/discover/blog/shaping-the-future-of-advanced-robotics/\n\n\"When tested on 41 tasks unseen in the training data, an arm controlled by RT-Trajectory more than doubled the performance of existing state-of-the-art RT models: it achieved a task success rate of 63%, compared with 29% for RT-2.\"\n\nthe task suite is custom, not public",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "BLUUMI",
      "Organization": "University of Turku,Hugging Face",
      "Publication date": "2023-11-03",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "176000000000.0",
      "Parameters notes": "176 billion",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Parsebank,mC4,Common Crawl,Wikipedia",
      "Training dataset size (gradients)": "38000000000",
      "Dataset size notes": "38B tokens\n\"In total, the final pretraining dataset (including oversampling) consists of 38 billion tokens when processed with our Finnish tokenizer.\"",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2311.05640",
      "Reference": "FinGPT: Large Generative Models for a Small Language",
      "Citations": "63.0",
      "Authors": "Risto Luukkonen, Ville Komulainen, Jouni Luoma, Anni Eskelinen, Jenna Kanerva, Hanna-Mari Kupari, Filip Ginter, Veronika Laippala, Niklas Muennighoff, Aleksandra Piktus, Thomas Wang, Nouamane Tazi, Teven Le Scao, Thomas Wolf, Osma Suominen, Samuli Sairanen, Mikko Merioksa, Jyrki Heinonen, Aija Vahtola, Samuel Antao, Sampo Pyysalo",
      "Abstract": "Large language models (LLMs) excel in many tasks in NLP and beyond, but most open models have very limited coverage of smaller languages and LLM work tends to focus on languages where nearly unlimited data is available for pretraining. In this work, we study the challenges of creating LLMs for Finnish, a language spoken by less than 0.1% of the world population. We compile an extensive dataset of Finnish combining web crawls, news, social media and eBooks. We pursue two approaches to pretrain models: 1) we train seven monolingual models from scratch (186M to 13B parameters) dubbed FinGPT, 2) we continue the pretraining of the multilingual BLOOM model on a mix of its original training data and Finnish, resulting in a 176 billion parameter model we call BLUUMI. For model evaluation, we introduce FIN-bench, a version of BIG-bench with Finnish tasks. We also assess other model qualities such as toxicity and bias. Our models and tools are openly available at this https URL.",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "Finland,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "SOTA for Finnish: \"Our best monolingual model outperforms this result by over 10% points and the BLUUMI model by over 20% points, representing a substantial advance in the state of the art in the capability of generative models trained for Finnish.\"",
      "Epochs": "8.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "AMD Radeon Instinct MI250X",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "BLOOM-176B",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "They \"continued pretraining\" of BLOOM on Finnish data. Don't think they specify the number of tokens they trained BLOOM/BLUUMI on; for their smaller models it was 300b.",
      "Batch size": "4194304.0",
      "Batch size notes": "Table 5.",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "The BigScience RAIL License\nhttps://turkunlp.org/gpt3-finnish\nhttps://huggingface.co/TurkuNLP/bloom-finnish-176b\n\n\"The Responsible AI License allows users to take advantage of the model in a wide range of settings (including free use and redistribution) as long as they respect the specific use case restrictions outlined, which correspond to model applications the licensor deems ill-suited for the model or are likely to cause harm.\"",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Yi-34B",
      "Organization": "01.AI",
      "Publication date": "2023-11-02",
      "Domain": "Language",
      "Task": "Chat,Language modeling/generation,Translation,Code generation",
      "Parameters": "34000000000.0",
      "Parameters notes": "34b",
      "Training compute (FLOP)": "6.1e+23",
      "Training compute notes": "\"The dataset we use contains Chinese & English only. We used approximately 3T tokens\" sounds like this means it was trained on 3T tokens, not necessarily that the dataset contains 3T tokens?\n\nIf so, 34b * 3T * 6 = 6.1e23",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "3100000000000",
      "Dataset size notes": "\"language models pretrained from scratch on 3.1T highly-engineered large amount of data, and finetuned on a small but meticulously polished alignment data.\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2403.04652",
      "Reference": "Yi: Open Foundation Models by 01.AI",
      "Citations": "",
      "Authors": "Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, Zonghong Dai",
      "Abstract": "The Yi series models are large language models trained from scratch by developers at 01.AI.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "Significant use",
      "Notability criteria notes": "2nd most popular model on HuggingFace: https://decrypt.co/206195/new-open-source-ai-model-from-china-boasts-twice-the-capacity-of-chatgpt\n\nalso maybe the best open-source model, does better than Llama 2-70B on several benchmarks",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "128.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "101588.94792366635",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "not listed",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "apply for commercial license:\nno training code\nhttps://github.com/01-ai/Yi/blob/main/MODEL_LICENSE_AGREEMENT.txt\n\nthe model https://huggingface.co/01-ai/Yi-34B-Chat Apache 2.0\n\"If you create derivative works based on this model, please include the following attribution in your derivative works: ....\"",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Cohere Embed",
      "Organization": "Cohere",
      "Publication date": "2023-11-02",
      "Domain": "Language",
      "Task": "Semantic embedding,Retrieval-augmented generation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "https://docs.cohere.com/docs/environmental-impact\n\nEmbed v2 (older version) produced 6689.76 kg CO2 to train. Using the calculator Cohere links (https://mlco2.github.io/impact/) that's the equivalent of 80,000 TPUv3-hours in the \"us-west1\" region. That's 3.5e22 FLOP without considering utilization. However, I have no idea which region Cohere's GPUs are in (looks like CO2/energy can vary a lot by region), and they probably used a more recent GPU.",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "\"First, they have been trained on questions and answers from a large web crawl. When we presented our multilingual-v2.0 model last year, we had a collection of over 1.4 billion question-and-answer pairs from 100+ languages on basically every topic on the internet.\"\n\n\"Hence, the second stage involved measuring content quality. We used over 3 million search queries from search engines and retrieved the top-10 most similar documents for each query.\"",
      "Confidence": "Unknown",
      "Link": "https://txt.cohere.com/introducing-embed-v3/",
      "Reference": "Cohere Command & Embed on Amazon Bedrock",
      "Citations": "",
      "Authors": "Nils Reimers, Elliott Choi, Amr Kayid, Alekhya Nandula, Manoj Govindassamy, Abdullah Elkady",
      "Abstract": "We're excited to introduce Embed v3, our latest and most advanced embeddings model. Embed v3 offers state-of-the-art performance per trusted MTEB and BEIR benchmarks.",
      "Organization categorization": "Industry",
      "Country (of organization)": "Canada",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We are releasing new English and multilingual Embed versions with either 1024 or 384 dimensions. All models can be accessed via our APIs. As of October 2023, these models achieve state-of-the-art performance among 90+ models on the Massive Text Embedding Benchmark (MTEB) and state-of-the-art performance for zero-shot dense retrieval on BEIR.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Skywork-13B",
      "Organization": "Kunlun Inc.",
      "Publication date": "2023-10-30",
      "Domain": "Language",
      "Task": "Language modeling,Language modeling/generation,Translation",
      "Parameters": "13000000000.0",
      "Parameters notes": "13B",
      "Training compute (FLOP)": "2.5e+23",
      "Training compute notes": "\"Our Skywork-13B is trained on a cluster of 64 NVIDIA-HGX-A800 nodes, a total of 512 A800-80G SXM GPUs... The training process of Skywork-13B spanned a total of 39 days.\"\n\nThey note that \"we achieved a token throughput of 1873 per GPU per second and a model flops utilization (MFU) of 56.5%... \". \n\n\"MFU\" was coined in the Palm paper (https://arxiv.org/pdf/2204.02311.pdf) and only counts operations used to train the model, not all operations observed on the hardware. MFU is lower than traditionally measured utilization.\n\nUsing the 56.5% number, and a peak tensor performance of 623.8 TFLOPS for the A800, this suggests 512 * 623.8 TFLOPS * 39 days * 86400 seconds/day * 0.565 = 6.08e23 FLOP.\n\nBased on C=6ND, with 13B parameters and 3.2T tokens, we have C=6*(13B)*(3.2T)=2.5e23 FLOP.\n\nSince the reported MFU is quite high, and would imply a higher compute usage than 6ND, it seems they may have trained on mixed precision and with the GPUs not always operating in the 623.8 TFLOPS mode.",
      "Training dataset": "SkyPile",
      "Training dataset size (gradients)": "3200000000000",
      "Dataset size notes": "The full SkyPile dataset is 6 trillion tokens, roughly half English and half Chinese: (https://huggingface.co/Skywork/Skywork-13B-base).\n\nThe model is trained for the equivalent of 0.53 epochs on the full dataset, or 3.18 trillion unique tokens. This is around 2.78 trillion words, based on an average of 1 word/token for the Chinese portion and 0.75 word/token on the English portion.",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2310.19341",
      "Reference": "Skywork: A More Open Bilingual Foundation Model",
      "Citations": "119.0",
      "Authors": "Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei L\u00fc, Rui Hu, Chenxia Li, Liu Yang, Xilin Luo, Xuejie Wu, Lunan Liu, Wenjun Cheng, Peng Cheng, Jianhao Zhang, Xiaoyu Zhang, Lei Lin, Xiaokun Wang, Yutuan Ma, Chuanhai Dong, Yanqi Sun, Yifu Chen, Yongyi Peng, Xiaojuan Liang, Shuicheng Yan, Han Fang, Yahui Zhou",
      "Abstract": "In this technical report, we present Skywork-13B, a family of large language models (LLMs) trained on a corpus of over 3.2 trillion tokens drawn from both English and Chinese texts. This bilingual foundation model is the most extensively trained and openly published LLMs of comparable size to date. We introduce a two-stage training methodology using a segmented corpus, targeting general purpose training and then domain-specific enhancement training, respectively. We show that our model not only excels on popular benchmarks, but also achieves state of the art performance in Chinese language modeling on diverse domains. Furthermore, we propose a novel leakage detection method, demonstrating that test data contamination is a pressing issue warranting further investigation by the LLM community. To spur future research, we release Skywork-13B along with checkpoints obtained during intermediate stages of the training process. We are also releasing part of our SkyPile corpus, a collection of over 150 billion tokens of web text, which is the largest high quality open Chinese pre-training corpus to date. We hope Skywork-13B and our open corpus will serve as a valuable open-source resource to democratize access to high-quality LLMs.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We show that our model not only excels on popular benchmarks, but also achieves state of the art performance in Chinese language modeling on diverse domains\"\n\nthey compare the model only to other Chinese models, not absolute SOTA",
      "Epochs": "1.0",
      "Training time (hours)": "940.0",
      "Training time notes": "39 days",
      "Training hardware": "NVIDIA A800 PCIe 40 GB",
      "Hardware quantity": "512.0",
      "Hardware utilization (MFU)": "0.4637",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "253989.3377909499",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "16000000.0",
      "Batch size notes": "Table 3",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Open (restricted use)",
      "Inference code accessibility": "Open access (restricted use)",
      "Accessibility notes": "commercial but restrictive license: https://github.com/SkyworkAI/Skywork/blob/main/LICENSE\n\npart of the training data is open, but only 2.5%: \"In order to train Skywork-13B, we build SkyPile, a vast, high quality corpus comprising more than 6 trillion tokens. A segment of the corpus, comprising over 150 billion tokens of web text, has been open sourced to facilitate research and training on Chinese LLMs\"\n\ntraining code: https://github.com/SkyworkAI/Skywork/blob/main/train/train.py ",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "0.565",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ChatGLM3-6B",
      "Organization": "Z.ai (Zhipu AI)",
      "Publication date": "2023-10-27",
      "Domain": "Multimodal,Language,Vision",
      "Task": "Chat,Visual question answering,Code generation",
      "Parameters": "6000000000.0",
      "Parameters notes": "6B from https://arxiv.org/abs/2406.12793",
      "Training compute (FLOP)": "5.04e+22",
      "Training compute notes": "Highly speculative.\nAssume 1 epoch on 1.4T tokens.\n6 FLOP/token/param * 1.4T tokens * 6B params=50.4 * 10 ^(12+9) = 5.04*10^(22)",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "1400000000000",
      "Dataset size notes": "\"ChatGLM-6B was pre-trained on approximately one trillion tokens of Chinese and English corpus\"\n\"By further realizing more diverse training datasets, more sufficient training steps, and more optimized training strategies, ChatGLM3-6B topped 42 benchmarks across semantics, mathematics, reasoning, code, and knowledge.\"\nThe ChatGLM website states that the latest ChatGLM service is based on (and upgraded from) ChatGLM2, which was trained on 1.4T tokens. Assume that ChatGLM3 is trained on at least the same number of tokens.\nSources:\nhttps://chatglm.cn/\nhttps://github.com/THUDM/ChatGLM2-6B/blob/main/README_EN.md\nhttps://www.zhipuai.cn/en/news/76\n\nhere (https://github.com/Kwai-Kolors/Kolors/blob/master/imgs/Kolors_paper.pdf) they confirm the dataset size \"Consequently, in Kolors, we utilize the open-source ChatGLM3-6B-Base as text encoder, which has been pre-trained with over 1.4 trillion bilingual tokens, resulting in a robust capability for Chinese language understanding.\"",
      "Confidence": "Speculative",
      "Link": "https://www.zhipuai.cn/en/news/76\n\nhttps://huggingface.co/zai-org/chatglm3-6b",
      "Reference": "Zhipu AI launches third-generation base model",
      "Citations": "",
      "Authors": "Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, Zihan Wang",
      "Abstract": "On October 27, 2023, at the 2023 China Computer Conference (CNCC), Zhipu AI launched the fully self-developed third-generation large base model ChatGLM3 and related series of products.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Aiming at GPT-4V, ChatGLM3 has implemented iterative upgrades of several new functions this time, including:\n\nCogVLM with multi-modal understanding capabilities, looks at image semantics, and achieved SOTA on more than 10 international standard image and text evaluation data sets;\n\nnot absolute SOTA\n\"ChatGLM3-6B-Base has the strongest performance among pre-trained models under 10B\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "weights available with restricted license: https://huggingface.co/THUDM/chatglm-6b/blob/main/MODEL_LICENSE ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DiT-XL/2 + CADS",
      "Organization": "ETH Zurich,Disney Research",
      "Publication date": "2023-10-26",
      "Domain": "Image generation",
      "Task": "Image generation",
      "Parameters": "675000000.0",
      "Parameters notes": "original parameter count for DiT-XL/2",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "ImageNet",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2310.17347v2",
      "Reference": "CADS: Unleashing the Diversity of Diffusion Models through Condition-Annealed Sampling",
      "Citations": "",
      "Authors": "Seyedmorteza Sadat, Jakob Buhmann, Derek Bradley, Otmar Hilliges, Romann M. Weber",
      "Abstract": "While conditional diffusion models are known to have good coverage of the data distribution, they still face limitations in output diversity, particularly when sampled with a high classifier-free guidance scale for optimal image quality or when trained on small datasets. We attribute this problem to the role of the conditioning signal in inference and offer an improved sampling strategy for diffusion models that can increase generation diversity, especially at high guidance scales, with minimal loss of sample quality. Our sampling strategy anneals the conditioning signal by adding scheduled, monotonically decreasing Gaussian noise to the conditioning vector during inference to balance diversity and condition alignment. Our Condition-Annealed Diffusion Sampler (CADS) can be used with any pretrained model and sampling algorithm, and we show that it boosts the diversity of diffusion models in various conditional generation tasks. Further, using an existing pretrained diffusion model, CADS achieves a new state-of-the-art FID of 1.70 and 2.31 for class-conditional ImageNet generation at 256\u00d7256 and 512\u00d7512 respectively.",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "Switzerland,United States of America,Switzerland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Further, using an existing pretrained diffusion model, CADS achieves a new state-of-the-art FID of 1.70 and 2.31 for class-conditional ImageNet generation at 256\u00d7256 and 512\u00d7512 respectively\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "DiT-XL/2",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "CODEFUSION (Python)",
      "Organization": "Microsoft,Microsoft Research",
      "Publication date": "2023-10-26",
      "Domain": "Language",
      "Task": "Code generation",
      "Parameters": "75000000.0",
      "Parameters notes": "Table 1",
      "Training compute (FLOP)": "7.92e+18",
      "Training compute notes": "V100 performance: 125 teraFLOPS according to https://www.nvidia.com/en-us/data-center/v100/\n\n11 hours * 4 GPUs * 125 teraFLOPS/GPU * 0.40 utilization = 7.92e18 FLOP",
      "Training dataset": "",
      "Training dataset size (gradients)": "4390400",
      "Dataset size notes": "Section A3, Table 5: for python, 56k samples with an average length of 78.4 tokens",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2310.17680 (was withdrawn)\n\nhttps://www.microsoft.com/en-us/research/wp-content/uploads/2023/11/CodeFusion-Revised-CameraReady.pdf",
      "Reference": "CODEFUSION: A Pre-trained Diffusion Model for Code Generation",
      "Citations": "45.0",
      "Authors": "Mukul Singh, Jos\u00e9 Cambronero, Sumit Gulwani, Vu Le, Carina Negreanu, Gust Verbruggen",
      "Abstract": "Imagine a developer who can only change their last line of code, how often would they have to start writing a function from scratch before it is correct? Auto-regressive models for code generation from natural language have a similar limitation: they do not easily allow reconsidering earlier tokens generated. We introduce CodeFusion, a pre-trained diffusion code generation model that addresses this limitation by iteratively denoising a complete program conditioned on the encoded natural language. We evaluate CodeFusion on the task of natural language to code generation for Bash, Python, and Microsoft Excel conditional formatting (CF) rules. Experiments show that CodeFusion (75M parameters) performs on par with state-of-the-art auto-regressive systems (350M-175B parameters) in top-1 accuracy and outperforms them in top-3 and top-5 accuracy due to its better balance in diversity versus quality.",
      "Organization categorization": "Industry,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "See Table 1, SOTA in Python code generation\n\n\"We evaluate Python using CodeBERTScore (Zhou et al., 2023), which has been shown to be a high quality non-execution-based code matching metric.\"",
      "Epochs": "",
      "Training time (hours)": "11.0",
      "Training time notes": "\"The system used to run the experiments uses an Intel Core i7 processor (base at 1.8 GHz) along with 4 V100 GPU units, a 64-bit operating system, and 56 GB RAM. CODEFUSION took 8 hours to pre-train and 3 hours to fine-tune on average for each dataset.\"",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "4.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "8.542235671062665",
      "Compute cost notes": "",
      "Training power draw (W)": "2381.36215809478",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DALL\u00b7E 3",
      "Organization": "OpenAI",
      "Publication date": "2023-10-19",
      "Domain": "Image generation",
      "Task": "Image generation,Text-to-image",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://cdn.openai.com/papers/dall-e-3.pdf",
      "Reference": "Improving Image Generation with Better Captions",
      "Citations": "1448.0",
      "Authors": "James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao, Aditya Ramesh",
      "Abstract": "We show that prompt following abilities of text-to-image models can be substantially improved by training on highly descriptive generated image captions.\nExisting text-to-image models struggle to follow detailed image descriptions and often ignore words or confuse the meaning of prompts. We hypothesize that this issue stems from noisy and inaccurate image captions in the training dataset. We address this by training a bespoke image captioner and use it to recaption the training dataset. We then train several text-to-image models and find that training on these synthetic captions reliably improves prompt following ability. Finally, we use these findings to build DALL-E 3: a new text-to-image generation system, and benchmark its performance on an evaluation designed to measure prompt following, coherence, and aesthetics, finding that it compares favorably to competitors. We publish samples and code for these evaluations so that future research can continue optimizing this important aspect of text-to-image systems.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"we train DALL-E 3, a new state of the art text to image generator. \"\n\nSection 4.1",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "https://platform.openai.com/docs/models/dall-e",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "PaLI-3",
      "Organization": "Google DeepMind,Google Research,Google Cloud",
      "Publication date": "2023-10-17",
      "Domain": "Multimodal,Language,Vision",
      "Task": "Visual question answering,Character recognition (OCR),Image captioning",
      "Parameters": "5000000000.0",
      "Parameters notes": "5B",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased,WebLI,Conceptual Captions (CC3M),VQAv2",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "contrastive image pretraining at 224\u00b2 then high\u2011res 812\u20131064\u00b2",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2310.09199",
      "Reference": "PaLI-3 Vision Language Models: Smaller, Faster, Stronger",
      "Citations": "",
      "Authors": "Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov, Jialin Wu, Paul Voigtlaender, Basil Mustafa, Sebastian Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski, Daniel Salz, Xi Xiong, Daniel Vlasic, Filip Pavetic, Keran Rong, Tianli Yu, Daniel Keysers, Xiaohua Zhai, Radu Soricut",
      "Abstract": "This paper presents PaLI-3, a smaller, faster, and stronger vision language model (VLM) that compares favorably to similar models that are 10x larger. As part of arriving at this strong performance, we compare Vision Transformer (ViT) models pretrained using classification objectives to contrastively (SigLIP) pretrained ones. We find that, while slightly underperforming on standard image classification benchmarks, SigLIP-based PaLI shows superior performance across various multimodal benchmarks, especially on localization and visually-situated text understanding. We scale the SigLIP image encoder up to 2 billion parameters, and achieves a new state-of-the-art on multilingual cross-modal retrieval. We hope that PaLI-3, at only 5B parameters, rekindles research on fundamental pieces of complex VLMs, and could fuel a new generation of scaled-up models.",
      "Organization categorization": "Industry,Industry,Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland,United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Table 2, Table 5",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "UL2",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ERNIE 4.0",
      "Organization": "Baidu",
      "Publication date": "2023-10-17",
      "Domain": "Multimodal,Language,Video,Image generation",
      "Task": "Chat,Language modeling/generation,Video generation,Image generation",
      "Parameters": "",
      "Parameters notes": "\"similar architecture with 3.5 version\"  -interpreter dub at 01:25:08 https://www.youtube.com/watch?v=wYozcsavRuM",
      "Training compute (FLOP)": "",
      "Training compute notes": "Unlikely to be >1e25 FLOP, ERNIE 4.5 was <1e25 FLOP.\n\nmay be mentioned here https://www.youtube.com/watch?v=wYozcsavRuM",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://www.prnewswire.com/news-releases/baidu-launches-ernie-4-0-foundation-model-leading-a-new-wave-of-ai-native-applications-301958681.html",
      "Reference": "Baidu Launches ERNIE 4.0 Foundation Model, Leading a New Wave of AI-Native Applications",
      "Citations": "",
      "Authors": "",
      "Abstract": "Baidu, Inc. (NASDAQ: BIDU and HKEX: 9888), a leading AI company with strong Internet foundation, today hosted its annual flagship technology conference Baidu World 2023 in Beijing, marking the conference's return to an offline format after four years. With the theme \"Prompt the World,\" this year's Baidu World conference saw Baidu launch ERNIE 4.0, Baidu's next-generation and most powerful foundation model offering drastically enhanced core AI capabilities. Baidu also showcased some of its most popular applications, solutions, and products re-built around the company's state-of-the-art generative AI.                                                                               \n\nRobin Li, Co-founder, Chairman and CEO of Baidu, announced ERNIE 4.0 at Baidu World 2023\n\"ERNIE 4.0 has achieved a full upgrade with drastically improved performance in understanding, generation, reasoning, and memory,\" Robin Li, Co-founder, Chairman and CEO of Baidu, said at the event. \"These four core capabilities form the foundation of AI-native applications and have now unleashed unlimited opportunities for new innovations.\"\n\n",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "Significant use",
      "Notability criteria notes": "Likely SOTA for Mandarin? But very little info available.\n\nLots of users (https://www.cnn.com/2023/12/15/tech/gpt4-china-baidu-ernie-ai-comparison-intl-hnk/index.html):\n\n\"Baidu says ERNIE has racked up 70 million users. That\u2019s compared with 150 million users for ChatGPT, according to an estimate from Similarweb, a digital data and analytics company.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "\"ERNIE 4.0 is now accessible to invited users on ERNIE Bot, and the API will be available upon application to enterprise clients via Qianfan foundation model platform.\"",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "RT-2-X",
      "Organization": "Google DeepMind",
      "Publication date": "2023-10-13",
      "Domain": "Robotics",
      "Task": "Robotic manipulation",
      "Parameters": "55000000000.0",
      "Parameters notes": "55B",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Open X-Embodiment",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2310.08864",
      "Reference": "Open X-Embodiment: Robotic Learning Datasets and RT-X Models",
      "Citations": "745.0",
      "Authors": "Open X-Embodiment Collaboration",
      "Abstract": "Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models, with general pretrained backbones serving as a starting point for many applications. Can such a consolidation happen in robotics? Conventionally, robotic learning methods train a separate model for every application, every robot, and even every environment. Can we instead train generalist X-robot policy that can be adapted efficiently to new robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and models to make it possible to explore this possibility in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies. We assemble a dataset from 22 different robots collected through a collaboration between 21 institutions, demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data, which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms. More details can be found on the project website this https URL.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "SOTA is claimed on Google Robot - Custom/internal evaluation\n\n\"Emergent skills evaluation. To investigate the transfer\nof knowledge across robots, we conduct experiments with\nthe Google Robot, assessing the performance on tasks like\nthe ones shown in Fig. 5. These tasks involve objects and\nskills that are not present in the RT-2 dataset but occur in the\nBridge dataset [95] for a different robot (the WidowX robot).\nResults are shown in Table II, Emergent Skills Evaluation\ncolumn. Comparing rows (1) and (2), we find that RT-2-X\noutperforms RT-2 by \u223c 3\u00d7, suggesting that incorporating\ndata from other robots into the training improves the range\nof tasks that can be performed even by a robot that already\nhas large amounts of data available. Our results suggest that\nco-training with data from other platforms imbues the RT-2-\nX controller with additional skills for the platform that are\nnot present in that platform\u2019s original dataset.\"\n\nTop10 recent paper from Sebastian Sartor 2025-05-14",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "RT-2",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "\"RT-2-X is trained via\nco-fine-tuning (similarly to the original RT-2 [9]), with an approximately one to one split of the original VLM data\nand the robotics data mixture. N\"\n\nRT-2 is in turn a fine-tune of Pali-X 55B",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Ferret (13B)",
      "Organization": "Columbia University,Apple",
      "Publication date": "2023-10-11",
      "Domain": "Multimodal,Language,Vision",
      "Task": "Object recognition,Language modeling",
      "Parameters": "13000000000.0",
      "Parameters notes": "13B",
      "Training compute (FLOP)": "",
      "Training compute notes": "Fine-tuned from Vicuna-13B, which we don't have an estimate for. Finetuning cost is ~4e19.\n\n\"Training Details. We initialize the image encoder with CLIP-ViT-L/14@336p, the LLM with Vicuna, and the projection layer with LLaVA\u2019s first-stage weights, leaving the visual sampler randomly initialized. After the initialization, Ferret is trained on the aforementioned GRIT data for three epochs, optimized by Loshchilov & Hutter (2017) with a learning rate of 2e \u2212 5 and a batch size of 128. The training takes \u223c5/2.5 days on 8 A100 GPU for a Ferret-13B/7B.\"\n\n5 * 24 * 3600 * 0.3 utilization (assumption) * 312 TFLOP/s = 4.04e19",
      "Training dataset": "GRIT",
      "Training dataset size (gradients)": "172649572",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2310.07704",
      "Reference": "Ferret: Refer and Ground Anything Anywhere at Any Granularity",
      "Citations": "449.0",
      "Authors": "Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, Yinfei Yang",
      "Abstract": "We introduce Ferret, a new Multimodal Large Language Model (MLLM) capable of understanding spatial referring of any shape or granularity within an image and accurately grounding open-vocabulary descriptions. To unify referring and grounding in the LLM paradigm, Ferret employs a novel and powerful hybrid region representation that integrates discrete coordinates and continuous features jointly to represent a region in the image. To extract the continuous features of versatile regions, we propose a spatial-aware visual sampler, adept at handling varying sparsity across different shapes. Consequently, Ferret can accept diverse region inputs, such as points, bounding boxes, and free-form shapes. To bolster the desired capability of Ferret, we curate GRIT, a comprehensive refer-and-ground instruction tuning dataset including 1.1M samples that contain rich hierarchical spatial knowledge, with 95K hard negative data to promote model robustness. The resulting model not only achieves superior performance in classical referring and grounding tasks, but also greatly outperforms existing MLLMs in region-based and localization-demanded multimodal chatting. Our evaluations also reveal a significantly improved capability of describing image details and a remarkable alleviation in object hallucination.",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "claimed SOTA on a new benchmark \"To evaluate this new capability, we introduce Ferret-Bench, covering three new types of tasks: Referring Description, Referring Reasoning, and Grounding in Conversation. We benchmark existing MLLMs and observe that Ferret can outperform the best of them by 20.4% on average.\"",
      "Epochs": "3.0",
      "Training time (hours)": "120.0",
      "Training time notes": "\"The training takes \u223c5/2.5 days on 8 A100 GPU for a Ferret-13B/7B.\"",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "8.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "6352.420700106825",
      "Base model": "Vicuna-13B v0",
      "Finetune compute (FLOP)": "4.04e+19",
      "Finetune compute notes": "\"The training takes ~5 days on 8 A100 GPU for a Ferret-13B\"\n\n5 * 24 * 3600 * 0.3 utilization (assumption) * 312 TFLOP/s = 4.04e19",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "https://github.com/apple/ml-ferret?tab=License-1-ov-file#readme\nconfusingly, the license page in the repo is permissive and MIT-like, but the README says \"The data, and code is intended and licensed for research use only. They are also restricted to uses that follow the license agreement of LLaMA, Vicuna and GPT-4. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.\" \ntrain script: https://github.com/apple/ml-ferret/blob/main/experiments/ferret_13b_train.sh ",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "RoseTTAFold All-Atom (RFAA)",
      "Organization": "University of Washington,Seoul National University,University of Sheffield",
      "Publication date": "2023-10-09",
      "Domain": "Biology",
      "Task": "Protein folding prediction,Proteins",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "2.14e+20",
      "Training compute notes": "Supplementary material: \"This took 8 days on 8 NVIDIA A6000 GPUs.\"\n8 GPUS *3.87E+13 FLOPS/A6000 *60 * 60 * 24 * 8 days",
      "Training dataset": "PDB (Protein Data Bank),Cambridge Structural Dataset",
      "Training dataset size (gradients)": "63240960",
      "Dataset size notes": "Datasets:\n121,800 + 112,546 + 12,689 = 247,035\n\n\"All examples were cropped to have 256 tokens during the initial stages of training and 375 tokens during fine-tuning.\"\n\n247035 * 256 =63,240,960 tokens",
      "Confidence": "Confident",
      "Link": "https://www.biorxiv.org/content/10.1101/2023.10.09.561603v1",
      "Reference": "Generalized Biomolecular Modeling and Design with RoseTTAFold All-Atom",
      "Citations": "",
      "Authors": "Rohith Krishna, Jue Wang, Woody Ahern, Pascal Sturmfels, Preetham Venkatesh, Indrek Kalvet, Gyu Rie Lee, Felix S Morey-Burrows, Ivan Anishchenko, Ian R Humphreys, Ryan McHugh, Dionne Vafeados, Xinting Li, George A Sutherland, Andrew Hitchcock, C Neil Hunter, Minkyung Baek, Frank DiMaio, David Baker",
      "Abstract": "Although AlphaFold2 (AF2) and RoseTTAFold (RF) have transformed structural biology by enabling high-accuracy protein structure modeling, they are unable to model covalent modifications or interactions with small molecules and other non-protein molecules that can play key roles in biological function. Here, we describe RoseTTAFold All-Atom (RFAA), a deep network capable of modeling full biological assemblies containing proteins, nucleic acids, small molecules, metals, and covalent modifications given the sequences of the polymers and the atomic bonded geometry of the small molecules and covalent modifications. Following training on structures of full biological assemblies in the Protein Data Bank (PDB), RFAA has comparable protein structure prediction accuracy to AF2, excellent performance in CAMEO for flexible backbone small molecule docking, and reasonable prediction accuracy for protein covalent modifications and assemblies of proteins with multiple nucleic acid chains and small molecules which, to our knowledge, no existing method can model simultaneously. By fine-tuning on diffusive denoising tasks, we develop RFdiffusion All-Atom (RFdiffusionAA), which generates binding pockets by directly building protein structures around small molecules and other non-protein molecules. Starting from random distributions of amino acid residues surrounding target small molecules, we design and experimentally validate proteins that bind the cardiac disease therapeutic digoxigenin, the enzymatic cofactor heme, and optically active bilin molecules with potential for expanding the range of wavelengths captured by photosynthesis. We anticipate that RFAA and RFdiffusionAA will be widely useful for modeling and designing complex biomolecular systems.",
      "Organization categorization": "Academia,Academia,Academia",
      "Country (of organization)": "United States of America,Korea (Republic of),United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA RTX A6000",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT-like license. inference code + training hyperparams: https://github.com/baker-laboratory/RoseTTAFold-All-Atom?tab=License-1-ov-file#readme",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "FinGPT-13B",
      "Organization": "University of California Los Angeles (UCLA),Columbia University,New York University (NYU)",
      "Publication date": "2023-10-07",
      "Domain": "Language",
      "Task": "Named entity recognition (NER),Sentiment classification,Language modeling/generation,Financial management",
      "Parameters": "13000000000.0",
      "Parameters notes": "Finetunes using LoRA, so only trains 3.67 million parameters",
      "Training compute (FLOP)": "1.6e+23",
      "Training compute notes": "From Llama 2-13B",
      "Training dataset": "",
      "Training dataset size (gradients)": "76800",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2310.04793; https://github.com/AI4Finance-Foundation/FinGPT",
      "Reference": "FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets",
      "Citations": "85.0",
      "Authors": "Neng Wang, Hongyang Yang, Christina Dan Wang",
      "Abstract": "In the swiftly expanding domain of Natural Language Processing (NLP), the potential of GPT-based models for the financial sector is increasingly evident. However, the integration of these models with financial datasets presents challenges, notably in determining their adeptness and relevance. This paper introduces a distinctive approach anchored in the Instruction Tuning paradigm for open-source large language models, specifically adapted for financial contexts. Through this methodology, we capitalize on the interoperability of open-source models, ensuring a seamless and transparent integration. We begin by explaining the Instruction Tuning paradigm, highlighting its effectiveness for immediate integration. The paper presents a benchmarking scheme designed for end-to-end training and testing, employing a cost-effective progression. Firstly, we assess basic competencies and fundamental tasks, such as Named Entity Recognition (NER) and sentiment analysis to enhance specialization. Next, we delve into a comprehensive model, executing multi-task operations by amalgamating all instructional tunings to examine versatility. Finally, we explore the zero-shot capabilities by earmarking unseen tasks and incorporating novel datasets to understand adaptability in uncharted terrains. Such a paradigm fortifies the principles of openness and reproducibility, laying a robust foundation for future investigations in open-source financial large language models (FinLLMs).",
      "Organization categorization": "Academia,Academia,Academia",
      "Country (of organization)": "United States of America,United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "SOTA for financial sentiment analysis",
      "Epochs": "",
      "Training time (hours)": "17.25",
      "Training time notes": "https://github.com/AI4Finance-Foundation/FinGPT?tab=readme-ov-file",
      "Training hardware": "NVIDIA GeForce RTX 3090",
      "Hardware quantity": "1.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "Finetuning cost for FinGPT v3.3 given as $17.25 at github repo; paper notes cost to train a financial model using their methods are \"typically\" between $100 - $300",
      "Training power draw (W)": "381.79005907274245",
      "Base model": "Llama 2-13B",
      "Finetune compute (FLOP)": "6.532488e+17",
      "Finetune compute notes": "fine-tuned Llama 2 13B\n\nRTX 3090 for 17 hours, at a cost of $17\n\n35.5 trillion flops * 17 * 3600 * 0.3 = 6.532488e+17",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT license (though probably subject to Llama 2 license too)\nhttps://github.com/AI4Finance-Foundation/FinGPT/blob/master/LICENSE\n\ntrain code: https://github.com/AI4Finance-Foundation/FinGPT/blob/master/fingpt/FinGPT_Benchmark/train.sh ",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "1139.9904487660447",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "CTM (CIFAR-10)",
      "Organization": "Stanford University,Sony",
      "Publication date": "2023-10-01",
      "Domain": "Image generation",
      "Task": "Image generation,Text-to-image",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "Almost certainly <1e23 FLOP due to the small scale experiments.\n\n\"We use 4\u00d7V100 (16G) GPUs for CIFAR-10 experiment\"\n100K training iterations\n",
      "Training dataset": "CIFAR-10",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "100K training iterations / 60K images in the training dataset = 1.7 epochs",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/2310.02279v1",
      "Reference": "Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion",
      "Citations": "313.0",
      "Authors": "Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, Stefano Ermon",
      "Abstract": "Consistency Models (CM) (Song et al., 2023) accelerate score-based diffusion model sampling at the cost of sample quality but lack a natural way to trade-off quality for speed. To address this limitation, we propose Consistency Trajectory Model (CTM), a generalization encompassing CM and score-based models as special cases. CTM trains a single neural network that can -- in a single forward pass -- output scores (i.e., gradients of log-density) and enables unrestricted traversal between any initial and final time along the Probability Flow Ordinary Differential Equation (ODE) in a diffusion process. CTM enables the efficient combination of adversarial training and denoising score matching loss to enhance performance and achieves new state-of-the-art FIDs for single-step diffusion model sampling on CIFAR-10 (FID 1.73) and ImageNet at 64X64 resolution (FID 2.06). CTM also enables a new family of sampling schemes, both deterministic and stochastic, involving long jumps along the ODE solution trajectories. It consistently improves sample quality as computational budgets increase, avoiding the degradation seen in CM. Furthermore, CTM's access to the score accommodates all diffusion model inference techniques, including exact likelihood computation.",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "United States of America,Japan",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"CTM... achieves new state-of-the-art FIDs for single-step diffusion model sampling on CIFAR-10 (FID 1.73)\"",
      "Epochs": "1.7",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "4.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "2382.6883131695167",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "128.0",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT license\nhttps://github.com/Kim-Dongjun/ctm-cifar10\nhttps://drive.google.com/drive/folders/1ei4PLmTrAlj-j_yUfLqXSpI5OOIqDlgv",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Amazon Titan",
      "Organization": "Amazon",
      "Publication date": "2023-09-28",
      "Domain": "Language,Image generation",
      "Task": "Semantic search,Image generation,Language modeling/generation,Code generation,Chat,Text-to-image,Translation",
      "Parameters": "200000000000.0",
      "Parameters notes": "200B dense model\nhttps://importai.substack.com/p/import-ai-365-wmd-benchmark-amazon",
      "Training compute (FLOP)": "4.8e+24",
      "Training compute notes": "trained using NVIDIA NeMo: https://blogs.nvidia.com/blog/nemo-amazon-titan/\n\n13,760 NVIDIA A100 chips (using 1,720 P4d nodes). It took 48 days to train.\nfrom https://importai.substack.com/p/import-ai-365-wmd-benchmark-amazon\n\ncounting operations: 6*200000000000*4000000000000=4.8e+24\n\ngpu usage: 312000000000000(FLOP/s)*0.3*13760*1152*3600=5.3413281792e+24",
      "Training dataset": "",
      "Training dataset size (gradients)": "4000000000000",
      "Dataset size notes": "4T tokens of data, based on comments from amazon engineer James Hamilton at a 2024 talk: https://perspectives.mvdirona.com/2024/01/cidr-2024/\nAlso cited here:\nhttps://lifearchitect.ai/titan/",
      "Confidence": "Likely",
      "Link": "https://aws.amazon.com/bedrock/titan/",
      "Reference": "",
      "Citations": "",
      "Authors": "",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "1152.0",
      "Training time notes": "",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "13760.0",
      "Hardware utilization (MFU)": "0.2696",
      "Training compute cost (2023 USD)": "7933464.673729055",
      "Compute cost notes": "",
      "Training power draw (W)": "10929327.206416877",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "23965159.94750839",
      "Training compute cost (upfront)": "441735752.9808057"
    },
    {
      "Model": "Show-1",
      "Organization": "National University of Singapore",
      "Publication date": "2023-09-27",
      "Domain": "Video",
      "Task": "Video generation,Text-to-video",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "WebVid-10M",
      "Training dataset size (gradients)": "160358400000000",
      "Dataset size notes": "WebVid-10M\n10.7M video-caption pairs. 52K total video hours.",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/2309.15818",
      "Reference": "Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation",
      "Citations": "",
      "Authors": "David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, Mike Zheng Shou",
      "Abstract": "Significant advancements have been achieved in the realm of large-scale pre-trained text-to-video Diffusion Models (VDMs). However, previous methods either rely solely on pixel-based VDMs, which come with high computational costs, or on latent-based VDMs, which often struggle with precise text-video alignment. In this paper, we are the first to propose a hybrid model, dubbed as Show-1, which marries pixel-based and latent-based VDMs for text-to-video generation. Our model first uses pixel-based VDMs to produce a low-resolution video of strong text-video correlation. After that, we propose a novel expert translation method that employs the latent-based VDMs to further upsample the low-resolution video to high resolution. Compared to latent VDMs, Show-1 can produce high-quality videos of precise text-video alignment; Compared to pixel VDMs, Show-1 is much more efficient (GPU memory usage during inference is 15G vs 72G). We also validate our model on standard video generation benchmarks. Our code and model weights are publicly available at this https URL.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Singapore",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Our approach achieves state-of-the-art performance on standard benchmarks including UCF-101 and MSR-VTT.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open access (non-commercial)",
      "Accessibility notes": "https://github.com/showlab/Show-1 don't see training code\nAttribution-NonCommercial 4.0 International\n",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GPT-4V",
      "Organization": "OpenAI",
      "Publication date": "2023-09-25",
      "Domain": "Multimodal,Vision,Language",
      "Task": "Language modeling,Visual question answering",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://cdn.openai.com/papers/GPTV_System_Card.pdf",
      "Reference": "GPT-4V(ision) system card",
      "Citations": "",
      "Authors": "",
      "Abstract": "GPT-4 with vision (GPT-4V) enables users to instruct GPT-4 to analyze image inputs provided by the user, and is the latest capability we are making broadly available. Incorporating additional modalities (such as image inputs) into large language models (LLMs) is viewed by some as a key frontier in artificial intelligence research and development. Multimodal LLMs offer the possibility of expanding the impact of language-only systems with novel interfaces and capabilities, enabling them to solve new tasks and provide novel experiences for their users. In this system card, we analyze the safety properties of GPT-4V. Our work on safety for GPT-4V builds on the work done for GPT-4 and here we dive deeper into the evaluations, preparation, and mitigation work done specifically for image inputs.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Significant use",
      "Notability criteria notes": "Incorporated into ChatGPT",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "GPT-4 (Mar 2023)",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "AlphaMissense",
      "Organization": "Google DeepMind",
      "Publication date": "2023-09-22",
      "Domain": "Biology",
      "Task": "Protein pathogenicity prediction,Protein folding prediction,Proteins",
      "Parameters": "93000000.0",
      "Parameters notes": "\"The model architecture is similar to that of AlphaFold (21), with minor modifications\"\nReference is to the AlphaFold 2 paper; that model had 93 million parameters",
      "Training compute (FLOP)": "",
      "Training compute notes": "From supplementary materials: \"We independently trained three AlphaFold models and fine-tuned them independently on variants. We followed the training procedure described in (21), (only the \u201cInitial training\u201d stage) ... AF training is carried out for about 7e6 steps on single-chain structures ... Fine-tuning is carried out @until auROC of the evaluation set converges (about 350k samples, each training sample contains maximum 50 variants)\"\n\nTable S4 gives details. Total samples seen across the three pretraining models are (7.8M + 7.5M + 5.85M) = 21.15M\n\nEach sequence is cropped to 256 elements long, which suggests 5.4B tokens seen in training.",
      "Training dataset": "MGnify,UniRef90",
      "Training dataset size (gradients)": "2304000000",
      "Dataset size notes": "7800000 samples - size of training dataset (see Table S4 in supplementary materials)\n+1,345,605 variants for fine-tuning (but less could be used)  see Table S1\n\naround 9000000 samples is quite confident estimation",
      "Confidence": "Likely",
      "Link": "https://www.science.org/doi/10.1126/science.adg7492",
      "Reference": "Accurate proteome-wide missense variant effect prediction with AlphaMissense",
      "Citations": "1226.0",
      "Authors": "Jun Cheng, Guido Novati, Joshua Pan, Clare Bycroft, Akvile \u0307\u017demgulyte \u0307, Taylor Applebaum, Alexander Pritzel, Lai Hong Wong, Michal Zielinski, Tobias Sargeant, Rosalia G. Schneider,Andrew W. Senior, John Jumper, Demis Hassabis, Pushmeet Kohli,\u017diga Avsec",
      "Abstract": "The vast majority of missense variants observed in the human genome are of unknown clinical significance. We present AlphaMissense, an adaptation of AlphaFold fine-tuned on human and primate variant population frequency databases to predict missense variant pathogenicity. By combining structural context and evolutionary conservation, our model achieves state-of-the-art results across a wide range of genetic and experimental benchmarks, all without explicitly training on such data. The average pathogenicity score of genes is also predictive for their cell essentiality, capable of identifying short essential genes that existing statistical approaches are underpowered to detect. As a resource to the community, we provide a database of predictions for all possible human single amino acid substitutions and classify 89% of missense variants as either likely benign or likely pathogenic.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"By combining structural context and evolutionary conservation, our model achieves state-of-the-art results across a wide range of genetic and experimental benchmarks, all without explicitly training on such data.\" [Abstract]",
      "Epochs": "4.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "AlphaFold 2",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache for code. weights not released\nhttps://github.com/google-deepmind/alphamissense",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Robot Parkour",
      "Organization": "Shanghai Qi Zhi institute,Stanford University,Carnegie Mellon University (CMU),Tsinghua University",
      "Publication date": "2023-09-12",
      "Domain": "Robotics",
      "Task": "Animal (human/non-human) imitation",
      "Parameters": "500000.0",
      "Parameters notes": "Parkour policy details on page 8, table 11.",
      "Training compute (FLOP)": "",
      "Training compute notes": "The paper provides some details on the training time and hardware used:\n\nEach specialized skill policy (climbing, leaping, etc) was pre-trained with soft dynamics constraints for 12 hours using 1 Nvidia RTX 3090 GPU.\nThe skills were then fine-tuned with hard dynamics constraints for 6 hours each.\nThe final parkour policy distillation process used 4 computers with 1 RTX 3090 GPU each, training for an unspecified amount of time.\nSo the total training time was at least 12 + 6 x 5 = 42 hours for the initial skills, plus an additional unknown time for the distillation.\n\nThe hardware used was high-end Nvidia RTX 3090 GPUs, which at the time of paper writing would have been top of the line GPUs. Multiple GPUs were used in parallel during the distillation stage.",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2309.05665",
      "Reference": "Robot Parkour Learning",
      "Citations": "217.0",
      "Authors": "Ziwen Zhuang, Zipeng Fu, Jianren Wang, Christopher Atkeson, Soeren Schwertfeger, Chelsea Finn, Hang Zhao",
      "Abstract": "Parkour is a grand challenge for legged locomotion that requires robots to overcome various obstacles rapidly in complex environments. Existing methods can generate either diverse but blind locomotion skills or vision-based but specialized skills by using reference animal data or complex rewards. However, autonomous parkour requires robots to learn generalizable skills that are both vision-based and diverse to perceive and react to various scenarios. In this work, we propose a system for learning a single end-to-end vision-based parkour policy of diverse parkour skills using a simple reward without any reference motion data. We develop a reinforcement learning method inspired by direct collocation to generate parkour skills, including climbing over high obstacles, leaping over large gaps, crawling beneath low barriers, squeezing through thin slits, and running. We distill these skills into a single vision-based parkour policy and transfer it to a quadrupedal robot using its egocentric depth camera. We demonstrate that our system can empower two different low-cost robots to autonomously select and execute appropriate parkour skills to traverse challenging real-world environments.",
      "Organization categorization": "Academia,Academia,Academia",
      "Country (of organization)": "China,United States of America,United States of America,China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "I do not see any standard benchmarks that they are claiming SOTA on",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA GeForce RTX 3090",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT license for training and inference code:\nhttps://github.com/ZiwenZhuang/parkour",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Falcon-180B",
      "Organization": "Technology Innovation Institute",
      "Publication date": "2023-09-06",
      "Domain": "Language",
      "Task": "Language modeling,Language modeling/generation,Question answering",
      "Parameters": "180000000000.0",
      "Parameters notes": "\"Falcon 180B is a super-powerful language model with 180 billion parameters\"",
      "Training compute (FLOP)": "3.76e+24",
      "Training compute notes": "43,500 petaflop-days per Table 1 of the paper\n\n43500 * 1e15 * 24 * 3600 = 3.76e24\n\n\nC = 6ND = 6 FLOP/token/parameter * 3.5 trillion tokens * 180 billion parameters = 3.78*10^24 FLOP",
      "Training dataset": "RefinedWeb",
      "Training dataset size (gradients)": "3500000000000",
      "Dataset size notes": "3.5 trillion tokens * (~3 words per 4 tokens) ~= 2.625 trillion words",
      "Confidence": "Confident",
      "Link": "https://falconllm.tii.ae/falcon-180b.html; https://arxiv.org/abs/2311.16867",
      "Reference": "The Falcon Series of Open Language Models",
      "Citations": "598.0",
      "Authors": "Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, M\u00e9rouane Debbah, \u00c9tienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, Guilherme Penedo",
      "Abstract": "Falcon 180B is a super-powerful language model with 180 billion parameters, trained on 3.5 trillion tokens. It's currently at the top of the Hugging Face Leaderboard for pre-trained Open Large Language Models and is available for both research and commercial use.\n\nThis model performs exceptionally well in various tasks like reasoning, coding, proficiency, and knowledge tests, even beating competitors like Meta's LLaMA 2.\n\nAmong closed source models, it ranks just behind OpenAI's GPT 4, and performs on par with Google's PaLM 2 Large, which powers Bard, despite being half the size of the model.",
      "Organization categorization": "Government",
      "Country (of organization)": "United Arab Emirates",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "\"It's currently at the top of the Hugging Face Leaderboard for pre-trained Open Large Language Models and is available for both research and commercial use.\"\n\n\"This model performs exceptionally well in various tasks like reasoning, coding, proficiency, and knowledge tests, even beating competitors like Meta's LLaMA 2.\"",
      "Epochs": "1.0",
      "Training time (hours)": "4320.0",
      "Training time notes": "Stanford CRFM foundation model ecosystem graph data page https://crfm.stanford.edu/ecosystem-graphs/index.html?asset=Falcon-180B says 9 months, which is the maximum possible amount of time: training began sometime in 2023, and it was released in September. \n\nHowever, 6 months is more realistic. That is the length of the gap between Falcon 40B and Falcon 180B. Additionally, the amount of compute is specified in the paper, so there is only one degree of freedom in the uncertain values of training duration and hardware utilization rate. At six months, the utilization is unusually low, so the training was probably not longer than that.",
      "Training hardware": "NVIDIA A100 SXM4 40 GB",
      "Hardware quantity": "4096.0",
      "Hardware utilization (MFU)": "0.1892",
      "Training compute cost (2023 USD)": "10743500.868805483",
      "Compute cost notes": "From Hugging Face:\n\"Falcon-180B was trained on up to 4,096 A100 40GB GPUs, using a 3D parallelism strategy (TP=8, PP=8, DP=64) combined with ZeRO.\"\n\"Falcon-180B was trained on AWS SageMaker, on up to 4,096 A100 40GB GPUs in P4d instances.\"\nhttps://huggingface.co/tiiuae/falcon-180B\n\nUtilization must have been at least 12.5%, and they probably did not use the whole 4096 GPU cluster for 9 months, so it was probably higher. Lower bound estimate:\nhttps://www.wolframalpha.com/input?i=%286+FLOP+*+3.5+trillion+*+180+billion%29+%2F+%284096*312+teraFLOPS+*+9+months%29",
      "Training power draw (W)": "3254975.4289709153",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "4194304.0",
      "Batch size notes": "from paper (https://arxiv.org/pdf/2311.16867.pdf):\n\nBatch size 2048 (presumably sequences) per Table 16. Warmed up using smaller batches for first 100B tokens.\n\n\"All Falcon models are pretrained with a 2,048 sequence length\"\n\n2048*2048 = 4194304",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "\"Falcon 180b can be commercially used but under very restrictive conditions, excluding any \"hosting use\".\" https://huggingface.co/blog/falcon-180b",
      "Numerical format": "BF16",
      "Frontier model": "True",
      "Hardware acquisition cost": "44145767.83583505",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "26751806.453032624",
      "Training compute cost (upfront)": "131493433.44544913"
    },
    {
      "Model": "Swift",
      "Organization": "Intel Labs",
      "Publication date": "2023-08-30",
      "Domain": "Robotics",
      "Task": "Helicopter driving",
      "Parameters": "56804.0",
      "Parameters notes": "The control network is an MLP with input dimension 31, two hidden layers of size 128, and an output of dimension 4.\n(31+1)*128+(128+1)*128+(128+1)*4 = 21124\n\nGate detector is a 6 layer U-net with \n8*(3^3*3+1) + 16*(3^2*8+1) + 16*(3^2*16+1) + 16*(5^2*16+1) + 16*(7^2*16+1) + 16*(7^2*16+1) = 35680\n\n35680 + 21124 = 56804",
      "Training compute (FLOP)": "5.337e+16",
      "Training compute notes": "Policies are trained for a total of 1\u2009\u00d7\u2009108 environment interactions, which takes 50\u2009min on a workstation (i9 12900K, RTX 3090, 32\u2009GB RAM DDR5). Fine-tuning is performed for 2\u2009\u00d7\u2009107 environment interactions.\n\n35.58 TFLOPS * 50 min * 60 s/min * 0.50 utilization = 5.337*10^16 FLOP",
      "Training dataset": "",
      "Training dataset size (gradients)": "120000000",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://www.nature.com/articles/s41586-023-06419-4",
      "Reference": "Champion-level drone racing using deep reinforcement learning",
      "Citations": "101.0",
      "Authors": "Elia Kaufmann, Leonard Bauersfeld, Antonio Loquercio, Matthias M\u00fcller, Vladlen Koltun, Davide Scaramuzza ",
      "Abstract": "First-person view (FPV) drone racing is a televised sport in which professional competitors pilot high-speed aircraft through a 3D circuit. Each pilot sees the environment from the perspective of their drone by means of video streamed from an onboard camera. Reaching the level of professional pilots with an autonomous drone is challenging because the robot needs to fly at its physical limits while estimating its speed and location in the circuit exclusively from onboard sensors1. Here we introduce Swift, an autonomous system that can race physical vehicles at the level of the human world champions. The system combines deep reinforcement learning (RL) in simulation with data collected in the physical world. Swift competed against three human champions, including the world champions of two international leagues, in real-world head-to-head races. Swift won several races against each of the human champions and demonstrated the fastest recorded race time. This work represents a milestone for mobile robotics and machine intelligence2, which may inspire the deployment of hybrid learning-based solutions in other physical systems.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Our work marks the first time, to our knowledge, that an autonomous mobile robot achieved world-champion-level performance in a real-world competitive sport.\"",
      "Epochs": "",
      "Training time (hours)": "0.833",
      "Training time notes": "50 minutes (training details, page 8)",
      "Training hardware": "NVIDIA GeForce RTX 3090",
      "Hardware quantity": "1.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "382.113280348051",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "\"Pseudocode for Swift detailing the training process and algorithms can be found in the file \u2018pseudocode.zip\u2019 on Zenodo at https://doi.org/10.5281/zenodo.7955278. To safeguard against potential misuse, the full source code associated with this research will not be made publicly available.\"",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "1171.5580385820329",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Jais",
      "Organization": "Cerebras Systems,Mohamed bin Zayed University of Artificial Intelligence (MBZUAI),Inception G42",
      "Publication date": "2023-08-29",
      "Domain": "Language",
      "Task": "Language modeling,Chat,Translation,Language modeling/generation,Question answering",
      "Parameters": "13000000000.0",
      "Parameters notes": "\"With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic\"",
      "Training compute (FLOP)": "4.8946763e+22",
      "Training compute notes": "C = 6ND = 6 * 13 billion params * 395 billion tokens = 3.081e+22 FLOP\n\n7500000000000000 FLOP / chip / sec * 16 chips *  600 hours [see training time notes] * 3600 sec / hour * 0.3 [assumed utilization] = 7.776e+22 FLOP\n\nsqrt(3.081e+22*7.776e+22) = 4.8946763e+22 FLOP",
      "Training dataset": "Abu El-Khair,Aranews,ArabicText 2022,C4 Arabic,Arabic Wikipedia,ArabicNews 2020,Maktabah,United Nations Parallel Corpus,The Pile,Books3,arXiv,PubMed Central,WebText2,English Wikipedia,FreeLaw,PubMed Abstracts,DeepMind Mathematics,Project Gutenberg,BookCorpus2,EuroParl,PhilPapers,YouTube Subtitles,NIH Grant Abstracts,Enron Emails,GitHub",
      "Training dataset size (gradients)": "395000000000",
      "Dataset size notes": "395B tokens ~= 300B words",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2308.16149",
      "Reference": "Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models",
      "Citations": "57.0",
      "Authors": "Neha Sengupta, Sunil Kumar Sahu, Bokang Jia, Satheesh Katipomu, Haonan Li, Fajri Koto, Osama Mohammed Afzal, Samta Kamboj, Onkar Pandit, Rahul Pal, Lalit Pradhan, Zain Muhammad Mujahid, Massa Baali, Alham Fikri Aji, Zhengzhong Liu, Andy Hock, Andrew Feldman, Jonathan Lee, Andrew Jackson, Preslav Nakov, Timothy Baldwin, Eric Xing",
      "Abstract": "We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-centric open models of similar size, despite being trained on much less English data. We provide a detailed description of the training, the tuning, the safety alignment, and the evaluation of the models. We release two open versions of the model \u2014the foundation Jais model, and an instruction-tuned Jais-chat variant\u2014 with the aim of promoting research on Arabic LLMs.",
      "Organization categorization": "Industry,Academia,Industry",
      "Country (of organization)": "United States of America,United Arab Emirates,United Arab Emirates",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "SOTA at Arabic language tasks.",
      "Epochs": "1.0",
      "Training time (hours)": "600.0",
      "Training time notes": "2023 June 25 to July 18 = 25 days = 600 hours",
      "Training hardware": "Cerebras CS-2",
      "Hardware quantity": "16.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "3932160.0",
      "Batch size notes": "\"After packing, we used a global batch size of 1,920 sequences of 2,048 tokens each. \"",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "apache 2.0\nhttps://huggingface.co/inceptionai/jais-13b",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "29548566.830459684",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "PeptideBERT",
      "Organization": "Carnegie Mellon University (CMU)",
      "Publication date": "2023-08-28",
      "Domain": "Biology",
      "Task": "Proteins,Protein property prediction",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "4.9e+16",
      "Training compute notes": "\"Compute for fine-tuning ProtBERT: 1 NVidia GeForce GTX 1080Ti, 30 epochs, batch size 32, model trained for individual tasks with training time ranging from 58-116 minutes, assuming \nfrom Table 1 we have 244 minutes\n11.34e12 FLOPs and 0.3 utilization rate FLOP = 244 min * 60 sec/min * 11.34e12 FLOP/sec *0.3 = 4.9e16 FLOP,",
      "Training dataset": "",
      "Training dataset size (gradients)": "4160566",
      "Dataset size notes": "Pretraining:\n217,000,000 sequences \u00d7 100 residues = 2.17 \u00d7 10\u00b9\u2070 tokens\n\nFine-tuning sequences:\n9,316 + 29,892 + 17,185 = 56,393 sequences\n56,393 \u00d7 100 residues = 5.64 \u00d7 10\u2076 tokens\n\nTotal tokens:\n2.17 \u00d7 10\u00b9\u2070 + 5.64 \u00d7 10\u2076 \u2248 2.17 \u00d7 10\u00b9\u2070",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2309.03099",
      "Reference": "PeptideBERT: A language Model based on Transformers for Peptide Property Prediction",
      "Citations": "",
      "Authors": "Chakradhar Guntuboina, Adrita Das, Parisa Mollaei, Seongwon Kim, and Amir Barati Farimani",
      "Abstract": "Recent advances in Language Models have enabled the protein modeling community with a powerful tool since protein sequences can be represented as text. Specifically, by taking advantage of Transformers, sequence-to-property prediction will be amenable without the need for explicit structural data. In this work, inspired by recent progress in Large Language Models (LLMs), we introduce PeptideBERT, a protein language model for predicting three key properties of peptides (hemolysis, solubility, and non- fouling). The PeptideBert utilizes the ProtBERT pretrained transformer model with 12 attention heads and 12 hidden layers. We then finetuned the pretrained model for the three downstream tasks. Our model has achieved state of the art (SOTA) for predicting Hemolysis, which is a task for determining peptide\u2019s potential to induce red blood cell lysis. Our PeptideBert non-fouling model also achieved remarkable accuracy in predicting peptide\u2019s capacity to resist non-specific interactions. This model, trained predominantly on shorter sequences, benefits from the dataset where negative examples are largely associated with insoluble peptides. Codes, models, and data used in this study are freely available at: https://github.com/ChakradharG/PeptideBERT",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Our model has achieved state of the art (SOTA) for predicting Hemolysis, which is a task for determining peptide\u2019s potential to induce red blood cell lysis.\"",
      "Epochs": "30.0",
      "Training time (hours)": "4.067",
      "Training time notes": "244 minues from Table 1",
      "Training hardware": "NVIDIA GeForce GTX 1080 Ti",
      "Hardware quantity": "1.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "272.95021398005304",
      "Base model": "ProtBERT-UniRef",
      "Finetune compute (FLOP)": "4.980528e+16",
      "Finetune compute notes": "\"Compute for fine-tuning ProtBERT: 1 NVidia GeForce GTX 1080Ti, 30 epochs, batch size 32, model trained for individual tasks with training time ranging from 58-116 minutes, assuming \nfrom Table 1 we have 244 minutes\n11.34e12 FLOPs and 0.3 utilization rate FLOP = 244 min * 60 sec/min * 11.34e12 FLOP/sec *0.3 = 4.9e16 FLOP,",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT (models, training, inference): https://github.com/ChakradharG/PeptideBERT ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "219.42751936055186",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Qwen-VL",
      "Organization": "Alibaba",
      "Publication date": "2023-08-24",
      "Domain": "Multimodal,Language,Vision",
      "Task": "Image captioning,Chat,Question answering,Visual question answering",
      "Parameters": "9600000000.0",
      "Parameters notes": "9.6B total - Table 1",
      "Training compute (FLOP)": "",
      "Training compute notes": "Qwen-7B and ViT as base models, trained on 1.5B image-text pairs",
      "Training dataset": "",
      "Training dataset size (gradients)": "500000000000",
      "Dataset size notes": "1.4B text-image pairs",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2308.12966",
      "Reference": "Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond",
      "Citations": "1530.0",
      "Authors": "Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, Jingren Zhou",
      "Abstract": "We introduce the Qwen-VL series, a set of large-scale vision-language models designed to perceive and understand both text and images. Comprising Qwen-VL and Qwen-VL-Chat, these models exhibit remarkable performance in tasks like image captioning, question answering, visual localization, and flexible interaction. The evaluation covers a wide range of tasks including zero-shot captioning, visual or document visual question answering, and grounding. We demonstrate the Qwen-VL outperforms existing Large Vision Language Models (LVLMs). We present their architecture, training, capabilities, and performance, highlighting their contributions to advancing multimodal artificial intelligence. Code, demo and models are available at https://github.com/QwenLM/Qwen-VL.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"As the results shown, our Qwen-VL and Qwen-VL-Chat both achieve obviously better results compared to previous generalist models in terms of both two tasks. Specifically, on zero-shot image caption task, Qwen-VL achieves state-of-the-art performance (i.e., 85.8 CIDEr score) on the Flickr30K karpathy-test split, even outperforms previous generalist models with much more parameters (e.g., Flamingo-80B with 80B parameters).\"",
      "Epochs": "1.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "Qwen-7B",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "50k steps, 30k batch size (table 8)",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GGNN",
      "Organization": "Westlake University,Tsinghua University,Toyota Technological Institute at Chicago",
      "Publication date": "2023-08-05",
      "Domain": "Biology",
      "Task": "Proteins,Protein interaction prediction,Protein protein binding affinity prediction,Protein representation learning",
      "Parameters": "",
      "Parameters notes": "ESM-2 650M is used as the main PLM, they run ablations with versions up to 3B. Unclear how many parameters are are in the geometric graph neural network module.",
      "Training compute (FLOP)": "7.56e+21",
      "Training compute notes": "ESM-2 650M is very likely the majority of FLOPs, since they only used 2 A100s (ESM-2 650M used 512 V100s for 8 days). As such I'm reporting the compute from ESM-2 650M here only.",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://www.nature.com/articles/s42003-023-05133-1",
      "Reference": "Integration of pre-trained protein language models into geometric deep learning networks",
      "Citations": "43.0",
      "Authors": "Fang Wu, Lirong Wu, Dragomir Radev, Jinbo Xu and Stan Z. Li",
      "Abstract": "Geometric deep learning has recently achieved great success in non-Euclidean domains, and learning on 3D structures of large biomolecules is emerging as a distinct research area. However, its efficacy is largely constrained due to the limited quantity of structural data. Meanwhile, protein language models trained on substantial 1D sequences have shown burgeoning capabilities with scale in a broad range of applications. Several preceding studies consider combining these different protein modalities to promote the representation power of geometric neural networks but fail to present a comprehensive understanding of their benefits. In this work, we integrate the knowledge learned by well-trained protein language models into several state-of-the-art geometric networks and evaluate a variety of protein representation learning benchmarks, including protein-protein interface prediction, model quality assessment, protein-protein rigid-body docking, and binding affinity prediction. Our findings show an overall improvement of 20% over baselines. Strong evidence indicates that the incorporation of protein language models\u2019 knowledge enhances geometric networks\u2019 capacity by a significant margin and can be generalized to complex tasks.",
      "Organization categorization": "Academia,Academia,Academia",
      "Country (of organization)": "China,China,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"In this work, we integrate the knowledge learned by well-trained protein language models into several state-of-the-art geometric networks and evaluate a variety of protein representation learning benchmarks, including protein-protein interface prediction, model quality assessment, protein-protein rigid-body docking, and binding affinity prediction. Our findings show an overall improvement of 20% over baselines.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA A100 SXM4 80 GB",
      "Hardware quantity": "2.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "1590.4764725839211",
      "Base model": "ESM2-650M",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT license\nhttps://github.com/smiles724/GNN-Bottleneck",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "RT-2",
      "Organization": "Google DeepMind",
      "Publication date": "2023-07-28",
      "Domain": "Robotics",
      "Task": "Robotic manipulation",
      "Parameters": "55000000000.0",
      "Parameters notes": "\"We train two specific instantiations of RT-2 that leverage pre-trained VLMs: (1) RT-2-PaLI-X is built from 5B and 55B PaLI-X (Chen et al., 2023a), and (2) RT-2-PaLM-E is built from 12B PaLM-E (Driess et al., 2023).\"\n\n55B and 12B have similar overall performance",
      "Training compute (FLOP)": "",
      "Training compute notes": "\"\"For RT-2-PaLI-X-55B, we use learning rate 1e-3 and batch size 2048 and co-fine-tune the model for 80K gradient steps\"\nSequence length not stated",
      "Training dataset": "RT-1",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2307.15818",
      "Reference": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control",
      "Citations": "2095.0",
      "Authors": "Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alexander Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, Brianna Zitkovich",
      "Abstract": "We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. We refer to such category of models as vision-language-action models (VLA) and instantiate an example of such a model, which we call RT-2. Our extensive evaluation (6k evaluation trials) shows that our approach leads to performant robotic policies and enables RT-2 to obtain a range of emergent capabilities from Internet-scale training. This includes significantly improved generalization to novel objects, the ability to interpret commands not present in the robot training data (such as placing an object onto a particular number or icon), and the ability to perform rudimentary reasoning in response to user commands (such as picking up the smallest or largest object, or the one closest to another object). We further show that incorporating chain of thought reasoning allows RT-2 to perform multi-stage semantic reasoning, for example figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink).",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We compare our method to multiple state-of-the-art baselines that challenge different aspects of our method. All of the baselines use the exact same robotic data... Here, on average, both instantiations of RT-2 perform similarly, resulting in \u223c2x improvement over the next two baselines, RT-1 and MOO, and \u223c6x better than the other baselines\"\n\nTop10 recent paper from Sebastian Sartor 2025-05-14",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "PaLI-X",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "\"For RT-2-PaLI-X-55B, we use learning rate 1e-3 and batch size 2048 and co-fine-tune the model for 80K gradient steps\"\n\n",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "no model weights or training code releases are mentioned\nhttps://robotics-transformer2.github.io/",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "AudioLM",
      "Organization": "Google Research",
      "Publication date": "2023-07-26",
      "Domain": "Audio",
      "Task": "Audio generation",
      "Parameters": "1500000000.0",
      "Parameters notes": "\"We use identical decoder-only Transformers in\nall stages, with 12 layers, 16 attention heads, embedding\ndimension of 1024, feed-forward layer dimension of 4096\nand dropout of 0.1, together with T5-style relative positional\nembeddings [38], resulting in a model parameter size of\n0.3B per stage.\"\n\nThree stages (figure 2), and 300M per stage. Plus 600M parameters for w2v-BERT XL, so 1.5B total",
      "Training compute (FLOP)": "3.9e+18",
      "Training compute notes": "\"We train each stage on 16 TPUv4s with batch size of 256 for 1M steps.\"\n\nThat's for the 900M-param transformers\n\nIf there's 256 passes in each batch, then using 6ND that's 900m * 256m * 6 = 1.3e18. sanity check: 16 tpu4s is 4.4e15 FLOP/s. 1.3e18 FLOP / 4.4e15 FLOP/s is 295 seconds. adjusting for utilization it would be ~1000 seconds or 15 minutes? probably too short, so 1.3e18 seems too low.\n\nupd there are 3 stages -> 1.3e18*3 = 3.9e+18 (Speculative due to reasoning above)",
      "Training dataset": "LibriLight",
      "Training dataset size (gradients)": "135000000000",
      "Dataset size notes": "60k hours of English speech\n13680*60000 = 820800000 words\n\nhttps://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.sxcem9l5k3ce",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/2209.03143",
      "Reference": "AudioLM: a Language Modeling Approach to Audio Generation",
      "Citations": "798.0",
      "Authors": "Zal\u00e1n Borsos, Rapha\u00ebl Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, Neil Zeghidour",
      "Abstract": "We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Compared to other systems without text supervision, AudioLM achieves the highest sWUGGY scores across both splits. Similarly, it also attains the highest score in the sBLIMP metric, improving by 8% relative over the previous state-of-the-art (CPC-BERT [59]).",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "Google TPU v4",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Llama 2-70B",
      "Organization": "Meta AI",
      "Publication date": "2023-07-18",
      "Domain": "Language",
      "Task": "Language modeling,Language modeling/generation,Question answering",
      "Parameters": "70000000000.0",
      "Parameters notes": "Llama has been released in 7B, 13B, 34B, and 70B variants.",
      "Training compute (FLOP)": "8.1e+23",
      "Training compute notes": "\"Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB\" of which 1720320 GPU hours were used to train the 70B model.\n\n311.84 BF16 TFLOP/s * 1720320 hours * 0.40 utilization = 7.725e+23 FLOP.\n\nAlternatively: the model was trained for 1 epoch on 2 trillion tokens and has 70B parameters. C = 6ND = 6*70B*2T = 8.4e+23 FLOP.",
      "Training dataset": "Llama 2 dataset",
      "Training dataset size (gradients)": "2000000000000",
      "Dataset size notes": "[tokens]\n\n2 trillion tokens ~= 1.5 trillion words",
      "Confidence": "Confident",
      "Link": "https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/\nhttps://arxiv.org/abs/2307.09288",
      "Reference": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
      "Citations": "15053.0",
      "Authors": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom\n",
      "Abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance,Significant use,Highly cited,Training cost",
      "Notability criteria notes": "Model has been open-sourced and frequently downloaded. The paper claims that Llama 2 is the current best open-source chat model as of its release date.",
      "Epochs": "1.0",
      "Training time (hours)": "1728.0",
      "Training time notes": "Model was trained from January 2023 to July 2023, which is six months. However, the training run duration did not take up this whole period. According to a Meta employee interviewed by Epoch, Llama 2 34B and 70B were trained on different clusters, with overlapping training periods. Based on an estimate of 1000 GPUs, it would have taken 72 days.",
      "Training hardware": "NVIDIA A100 SXM4 80 GB",
      "Hardware quantity": "1000.0",
      "Hardware utilization (MFU)": "0.4191975017",
      "Training compute cost (2023 USD)": "1102561.194",
      "Compute cost notes": "A100 cost in 2023: $1.10/hour\nTraining time: 1720320 A100 GPU-hours\nInflation adjustment: $1.000 2020 = $1.145 2023",
      "Training power draw (W)": "795557.0703894598",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "4000000.0",
      "Batch size notes": "",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Llama 2 license. can't use outputs to train models.\n\nhttps://github.com/meta-llama/llama/blob/main/LICENSE\n\nhttps://huggingface.co/meta-llama/Llama-2-70b",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Llama 2-7B",
      "Organization": "Meta AI",
      "Publication date": "2023-07-18",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "7000000000.0",
      "Parameters notes": "Llama has been released in 7B, 13B, and 70B variants.",
      "Training compute (FLOP)": "8.4e+22",
      "Training compute notes": "Trained on 2 trillion tokens per Table 1. \n\nC = 6ND = 6 FLOP / token / parameter * 7B parameters * 2T tokens = 8.4e+22 FLOP.\n\nAlso, 7B model was trained on 184320 GPU-hours\n\n312 trillion * 184320 GPU-hours * 3600 sec/hour * 0.3 [utilization] = 6.21e22 FLOP",
      "Training dataset": "Llama 2 dataset",
      "Training dataset size (gradients)": "2000000000000",
      "Dataset size notes": "2 trillion tokens ~= 1.5T words",
      "Confidence": "Confident",
      "Link": "https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/",
      "Reference": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
      "Citations": "15053.0",
      "Authors": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom\n",
      "Abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance,Significant use,Highly cited",
      "Notability criteria notes": "Model has been open-sourced and frequently downloaded. The paper claims that Llama 2 is the current best open-source chat model as of its release date.",
      "Epochs": "1.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA A100 SXM4 80 GB",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "114259.38527188865",
      "Compute cost notes": "A100 cost in 2023: $1.10/hour\nTraining time: 184320 A100 GPU-hours\nInflation adjustment: $1.000 2020 = $1.145 2023\n\n184320 * 1.10 / 1.145 = $177,075",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "4000000.0",
      "Batch size notes": "",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Llama 2 license. can't use outputs to train models.\n\nhttps://github.com/meta-llama/llama/blob/main/LICENSE",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GPT3-2.7B (FlashAttention-2)",
      "Organization": "Stanford University,Princeton University",
      "Publication date": "2023-07-18",
      "Domain": "Language",
      "Task": "Language modeling/generation",
      "Parameters": "2700000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "8x A100 SXMs used, but no indication of dataset size, how long training took, or compute costs",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/pdf/2307.08691",
      "Reference": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning",
      "Citations": "2026.0",
      "Authors": "Tri Dao",
      "Abstract": "Scaling Transformers to longer sequence lengths has been a major problem in the last several years, promising to improve performance in language modeling and high-resolution image understanding, as well as to unlock new applications in code, audio, and video generation. The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length. FlashAttention exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup (2-4\\times compared to optimized baselines), with no approximation. However, FlashAttention is still not nearly as fast as optimized matrix-multiply (GEMM) operations, reaching only 25-40\\% of the theoretical maximum FLOPs/s. We observe that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We propose FlashAttention-2, with better work partitioning to address these issues. In particular, we (1) tweak the algorithm to reduce the number of non-matmul FLOPs (2) parallelize the attention computation, even for a single head, across different thread blocks to increase occupancy, and (3) within each thread block, distribute the work between warps to reduce communication through shared memory. These yield around 2\\times speedup compared to FlashAttention, reaching 50-73\\% of the theoretical maximum FLOPs/s on A100 and getting close to the efficiency of GEMM operations. We empirically validate that when used end-to-end to train GPT-style models, FlashAttention-2 reaches training speed of up to 225 TFLOPs/s per A100 GPU (72\\% model FLOPs utilization).",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "Highly cited,Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA A100 SXM4 80 GB",
      "Hardware quantity": "8.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "6364.456563115679",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "0.72",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Claude 2",
      "Organization": "Anthropic",
      "Publication date": "2023-07-11",
      "Domain": "Language",
      "Task": "Language modeling,Chat,Language modeling/generation,Question answering",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "3.866e+24",
      "Training compute notes": "https://colab.research.google.com/drive/1MdPuhS4Emaf23VXYZ-ooExDW-5GXZkw0#scrollTo=Ds0Q5X8aMnOY",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Speculative",
      "Link": "https://www.anthropic.com/index/claude-2, https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf",
      "Reference": "",
      "Citations": "",
      "Authors": "",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "4902644.123383027",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "xTrimoPGLM -100B",
      "Organization": "Tsinghua University,BioMap Research",
      "Publication date": "2023-07-06",
      "Domain": "Biology",
      "Task": "Proteins,Protein or nucleotide language model (pLM/nLM),Protein generation",
      "Parameters": "100000000000.0",
      "Parameters notes": "Abstract: \"training xTrimoPGLM at an unprecedented scale of 100 billion\nparameters and 1 trillion training tokens\"",
      "Training compute (FLOP)": "6.2e+23",
      "Training compute notes": "\"xTrimoPGLM-100B is trained on a cluster of 96 DGX-A100 GPU (8\u00d740G) servers in FP16 precision from January 18 to June 30, 2023. During this time, xTrimoPGLM-100B has consumed 1 trillion tokens from the dataset consisting of Uniref90 and ColAbFoldDB. As of the current date, xTrimoPGLM-100B continues its pre-training process to pass through as many tokens as possible\"\n\n6 * 100 billion params * 1T tokens = 6e23\n\n8*96 * 312 trillion * 163 days * 24 * 3600 * 0.3 ~= 1e24\n\ndirectly given in the paper (Table 9, or Table 4 in new version): 6.2E+23 ",
      "Training dataset": "UniRef50",
      "Training dataset size (gradients)": "275000000000",
      "Dataset size notes": "~24M protein sequences",
      "Confidence": "Confident",
      "Link": "https://www.biorxiv.org/content/10.1101/2023.07.05.547496v4",
      "Reference": "xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein",
      "Citations": "135.0",
      "Authors": "Bo Chen, Xingyi Cheng, Yangli-ao Geng, Shen Li, Xin Zeng, Boyan Wang, Jing Gong, Chiming Liu, Aohan Zeng, Yuxiao Dong, Jie Tang, Le Song",
      "Abstract": "Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. This paper proposes a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that xTrimoPGLM significantly outperforms other advanced baselines in diverse protein understanding tasks (13 out of 15 tasks across four categories) and generates novel protein sequences which are structurally similar to natural ones. Furthermore, using the same xTrimoPGLM framework, we train an antibody-specific model (xTrimoPGLM-Ab) using 1 billion parameters. This model set a new record in predicting antibody naturalness and structures, both essential to the field of antibody-based drug design, and demonstrated a significantly faster inference speed than AlphaFold2. These results highlight the substantial capability and versatility of xTrimoPGLM in understanding and generating protein sequences.",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "China,China",
      "Notability criteria": "SOTA improvement,Training cost",
      "Notability criteria notes": "\"Our extensive experiments reveal that xTrimoPGLM significantly outperforms other advanced baselines in diverse protein understanding tasks (13 out of 15 tasks across four categories)\"\n\n\"we propose xT-Fold, where building on the xTrimoPLGM-100B framework,\nmarks a significant advancement by achieving SOTA results for the PLM-based structure prediction model on benchmarks such as CAMEO and the latest CASP15\" (!) SOTA among PLM-based models",
      "Epochs": "",
      "Training time (hours)": "3912.0",
      "Training time notes": "163 days",
      "Training hardware": "NVIDIA A100 SXM4 40 GB",
      "Hardware quantity": "768.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "1823415.258",
      "Compute cost notes": "",
      "Training power draw (W)": "611151.12765533",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "2097152.0",
      "Batch size notes": "\"We employ batches of 2,048 sequences, each 1,024 tokens in length\"",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "8654562.294367751",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "InternLM",
      "Organization": "Shanghai AI Lab,SenseTime",
      "Publication date": "2023-07-06",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "104000000000.0",
      "Parameters notes": "\"We present InternLM, a multilingual foundational language model with 104B parameters\"",
      "Training compute (FLOP)": "9.984e+23",
      "Training compute notes": "6 * 104b * 1.6T = 9.984e23",
      "Training dataset": "",
      "Training dataset size (gradients)": "1600000000000",
      "Dataset size notes": "\"InternLM is pre-trained on a large corpora with 1.6T tokens\"",
      "Confidence": "Confident",
      "Link": "https://github.com/InternLM/InternLM-techreport/blob/main/InternLM.pdf",
      "Reference": "",
      "Citations": "",
      "Authors": "",
      "Abstract": "Pre-training a bilingual 100B Foundation model on data with over a trillion tokens, the model exhibits excellent performance in scenarios such as Chinese, English, and coding due to the appropriate data ratio. Based on the foundation model, the application of high-quality human annotated dialogue data combined with RLHF technology enables the InternLM large language model to respond to complex commands during human interaction, while also demonstrating responses in line with human morality and values.",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "China,Hong Kong",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "(from Google-translated page) \"In addition to using academic datasets to evaluate InternLM, we also use human examinations to assess its capabilities. InternLM can achieve good scores on examination benchmarks such as MMLU, AGIEval, C-Eval, and GAOKAO-bench that cover different languages and subjects, scoring higher than ChatGPT on multiple benchmarks\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "Training performance for the open-source InternLM-7B: https://github.com/InternLM/InternLM/blob/main/doc/en/train_performance.md",
      "Training hardware": "NVIDIA A100 SXM4 80 GB",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "1505257.378",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "not listed",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Though they released 7b and 20b models (https://github.com/InternLM/InternLM/tree/main/model_cards) 100b model is not found",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Pangu-Weather",
      "Organization": "Huawei",
      "Publication date": "2023-07-05",
      "Domain": "Earth science",
      "Task": "Weather forecasting",
      "Parameters": "256000000.0",
      "Parameters notes": "4*64 million = 256M params\n\n\"We trained four deep networks with lead times (the time difference\nbetween input and output) at 1\u2009h, 3\u2009h, 6\u2009h and 24\u2009h, respectively... \n\nThis modification increases the number of bias parameters by a factor of 527, with each 3D deep network containing approximately 64\u2009million parameters.\"",
      "Training compute (FLOP)": "3.98e+22",
      "Training compute notes": "\"Each of the four deep networks was trained for 100 epochs, and\neach of them takes approximately 16\u2009days on a cluster of 192 NVIDIA\nTesla-V100 GPUs.\"\n\n192 * 4 * 16 * 24 * 3600 * 125 teraflops * 0.3 utilization = 3.98e22",
      "Training dataset": "ERA5",
      "Training dataset size (gradients)": "24457821696000",
      "Dataset size notes": "\"We used a single point in time for both input and output. The time resolution\nof the ERA5 data is 1\u2009h; in the training subset (1979\u20132017), there were\nas many as 341,880 time points, the amount of training data in one\nepoch... We fed all included weather variables, including 13 layers of upper-air\nvariables and the surface variables\"\n\n341,880 is the number of hours in ~40 years. But there's lots of data for each hour.",
      "Confidence": "Confident",
      "Link": "https://www.nature.com/articles/s41586-023-06185-3, https://www.huaweicloud.com/intl/en-us/news/20230707180809498.html,\nhttps://www.huawei.com/en/news/2023/7/pangu-ai-model-nature-publish",
      "Reference": "Accurate medium-range global weather forecasting with 3D neural networks",
      "Citations": "197.0",
      "Authors": "Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, Qi Tian",
      "Abstract": "Weather forecasting is important for science and society. At present, the most accurate forecast system is the numerical weather prediction (NWP) method, which represents atmospheric states as discretized grids and numerically solves partial diferential equations that describe the transition between those states1 . However, this procedure is computationally expensive. Recently, artifcial-intelligence-based methods2 have shown potential in accelerating weather forecasting by orders of magnitude, but the forecast accuracy is still signifcantly lower than that of NWP methods. Here we introduce an artifcial-intelligence-based method for accurate, medium-range global weather forecasting. We show that three-dimensional deep networks equipped with\nEarth-specifc priors are efective at dealing with complex patterns in weather data, and that a hierarchical temporal aggregation strategy reduces accumulation errors in medium-range forecasting. Trained on 39\u2009years of global data, our program, Pangu-Weather, obtains stronger deterministic forecast results on reanalysis data in all tested variables when compared with the world\u2019s best NWP system, the operational integrated forecasting system of the European Centre for Medium-Range Weather Forecasts (ECMWF)3. Our method also works well with extreme weather forecasts and ensemble forecasts. When initialized with reanalysis data, the accuracy of tracking tropical cyclones is also higher than that of ECMWF-HRES.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"In meteorology, the Pangu Meteorology Model (or Pangu-Weather) is the first AI model to have surpassed state-of-the-art numerical weather prediction (NWP) methods in terms of accuracy. The prediction speed is also several orders of magnitude faster. In the past, predicting the trajectory of a typhoon over 10 days took 4 to 5 hours of simulation on a high-performance cluster of 3,000 servers. Now, the Pangu model can do it in 10 seconds on a single GPU of a single server, and with more accurate results.\"\n\nhttps://www.huaweicloud.com/intl/en-us/news/20230707180809498.html",
      "Epochs": "100.0",
      "Training time (hours)": "1536.0",
      "Training time notes": "4*16 = 64 days\n\"Each of the four deep networks was trained for 100 epochs, andeach of them takes approximately 16\u2009days on a cluster of 192 NVIDIA Tesla-V100 GPUs.\"\n",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "192.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "51279.017519054316",
      "Compute cost notes": "",
      "Training power draw (W)": "114593.38832967015",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "Possibly based on Pangu 3? Pangu-Weather is mentioned in the Pangu 3 announcement. But the architecture description doesn't seem to resemble Pangu 3. So it seems like Pangu-Weather is one of the higher-level models that can be attached to Pangu 3. \n\nhttps://www.huaweicloud.com/intl/en-us/news/20230707180809498.html \n",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open access (non-commercial)",
      "Accessibility notes": "Models and code here: https://github.com/198808xc/Pangu-Weather \n\nCommercial use forbidden",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Stable Diffusion XL (SDXL)",
      "Organization": "Stability AI",
      "Publication date": "2023-07-04",
      "Domain": "Image generation",
      "Task": "Image generation,Text-to-image",
      "Parameters": "3400000000.0",
      "Parameters notes": "\"...result in a model size of 2.6B parameters in the UNet, see Tab. 1. The text encoders have a total size of 817M parameters.\"",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/2307.01952",
      "Reference": "SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis",
      "Citations": "3712.0",
      "Authors": "Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, Robin Rombach",
      "Abstract": "We present SDXL, a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet backbone: The increase of model parameters is mainly due to more attention blocks and a larger cross-attention context as SDXL uses a second text encoder. We design multiple novel conditioning schemes and train SDXL on multiple aspect ratios. We also introduce a refinement model which is used to improve the visual fidelity of samples generated by SDXL using a post-hoc image-to-image technique. We demonstrate that SDXL shows drastically improved performance compared the previous versions of Stable Diffusion and achieves results competitive with those of black-box state-of-the-art image generators. In the spirit of promoting open research and fostering transparency in large model training and evaluation, we provide access to code and model weights at this https URL",
      "Organization categorization": "Industry",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Significant use",
      "Notability criteria notes": "Looks like this is now the main/flagship Stable Diffusion model",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "SDXL 0.9 Research License:\nhttps://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9\n\nMIT license for inference code, not sure if training code is here:\nhttps://github.com/Stability-AI/generative-models/tree/main",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "HyenaDNA",
      "Organization": "Stanford University,Harvard University,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),University of Montreal / Universit\u00e9 de Montr\u00e9al",
      "Publication date": "2023-06-27",
      "Domain": "Biology",
      "Task": "Protein or nucleotide language model (pLM/nLM)",
      "Parameters": "6600000.0",
      "Parameters notes": "Table A.1 shows details, largest experiment is on far right.",
      "Training compute (FLOP)": "1.811e+21",
      "Training compute notes": "8 Nvidia A100 (80GB) GPUs, ~4 weeks\n(4 * 7 * 24 * 3600) seconds * (8 * 3.12e14) FLOP/sec * 0.3 (utilization) = 1.811e21",
      "Training dataset": "Human Reference Genome (GRCh38/hg38)",
      "Training dataset size (gradients)": "2945000000",
      "Dataset size notes": "Human genome is ~3.2B nucleotide pairs, 14 and X are ~101M and 154M respectively. Largest run sees 2T tokens, which implies ~679 epochs.",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2306.15794",
      "Reference": "HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution",
      "Citations": "394.0",
      "Authors": "Eric Nguyen, Michael Poli, Marjan Faizi, Armin W. Thomas, Callum Birch Sykes, Michael Wornow, Aman Patel, Clayton Rabideau, Stefano Massaroli, Yoshua Bengio, Stefano Ermon, Stephen A. Baccus, Christopher R\u00e9",
      "Abstract": "Genomic (DNA) sequences encode an enormous amount of information for gene regulation and protein synthesis. Similar to natural language models, researchers have proposed foundation models in genomics to learn generalizable features from unlabeled genome data that can then be fine-tuned for downstream tasks such as identifying regulatory elements. Due to the quadratic scaling of attention, previous Transformer-based genomic models have used 512 to 4k tokens as context (<0.001% of the human genome), significantly limiting the modeling of long-range interactions in DNA. In addition, these methods rely on tokenizers or fixed k-mers to aggregate meaningful DNA units, losing single nucleotide resolution where subtle genetic variations can completely alter protein function via single nucleotide polymorphisms (SNPs). Recently, Hyena, a large language model based on implicit convolutions was shown to match attention in quality while allowing longer context lengths and lower time complexity. Leveraging Hyena's new long-range capabilities, we present HyenaDNA, a genomic foundation model pretrained on the human reference genome with context lengths of up to 1 million tokens at the single nucleotide-level - an up to 500x increase over previous dense attention-based models. HyenaDNA scales sub-quadratically in sequence length (training up to 160x faster than Transformer), uses single nucleotide tokens, and has full global context at each layer. We explore what longer context enables - including the first use of in-context learning in genomics. On fine-tuned benchmarks from the Nucleotide Transformer, HyenaDNA reaches state-of-the-art (SotA) on 12 of 18 datasets using a model with orders of magnitude less parameters and pretraining data. On the GenomicBenchmarks, HyenaDNA surpasses SotA on 7 of 8 datasets on average by +10 accuracy points.",
      "Organization categorization": "Academia,Academia,Academia,Academia",
      "Country (of organization)": "United States of America,United States of America,Canada,Canada",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"On fine-tuned benchmarks from the Nucleotide Transformer, HyenaDNA reaches state-of-the-art (SotA) on 12 of 18 datasets using a model with orders of magnitude less parameters and pretraining data.1 On the GenomicBenchmarks, HyenaDNA surpasses SotA on 7 of 8 datasets on average by +10 accuracy points, and by as much as +20 accuracy points on enhancer identification.\"",
      "Epochs": "679.117147708",
      "Training time (hours)": "672.0",
      "Training time notes": "\"For example, the largest model with context length 1M was trained on 2T tokens over 4 weeks.\"",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "8.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "5000.0",
      "Compute cost notes": "",
      "Training power draw (W)": "6367.433640556241",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "64000000.0",
      "Batch size notes": "Table A.1 indicates largest model saw sequence length of 1M, and that batch sizes range from 64-1024. In section 3.2: \"Our sequence length schedule starts at L1 = 64, then doubles the window at each stage while keeping the global batch size constant.\" I assume smallest batch size was used for largest sequence length.",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "BSD 3-Clause License for weights (prohibits others from using the name of the copyright holder)\nhttps://huggingface.co/LongSafari/hyenadna-large-1m-seqlen-hf\n\ntraining code: Apache 2.0\nhttps://github.com/HazyResearch/hyena-dna",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ERNIE 3.5",
      "Organization": "Baidu",
      "Publication date": "2023-06-27",
      "Domain": "Language",
      "Task": "Language modeling,Language modeling/generation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "http://research.baidu.com/Blog/index-view?id=185",
      "Reference": "Introducing ERNIE 3.5: Baidu\u2019s Knowledge-Enhanced Foundation Model Takes a Giant Leap Forward",
      "Citations": "",
      "Authors": "",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "SOTA scores on AGIEval and MMLU. See article in China Science Daily: https://mp.weixin.qq.com/s/QVdkmofRSTgjQ7UOFX7s1g",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Hosted access (no API)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "MusicGen",
      "Organization": "Meta AI",
      "Publication date": "2023-06-08",
      "Domain": "Audio",
      "Task": "Audio generation",
      "Parameters": "3359000000.0",
      "Parameters notes": "\"We train autoregressive transformer models at different sizes: 300M, 1.5B, 3.3B parameters\"\n\nUses EnCodec 32kHz (HF version has 59M params) for audio tokenization.",
      "Training compute (FLOP)": "",
      "Training compute notes": "We train the 300M, 1.5B and 3.3B parameter models, using respectively 32, 64 and 96 GPUs, with mixed precision.\n\nUnclear how many epochs used so FLOP calculation is not feasible.",
      "Training dataset": "ShutterStock and Pond5 music data collections",
      "Training dataset size (gradients)": "14284800000000",
      "Dataset size notes": "\"We train on 30-second audio crops sampled at random from the full track... We use 20K hours of licensed music\"\n\n20000 hours * 60 min/hour * 2 inputs/min = 2400000 input sequences\n\nEnCodec is run at 32kHz but after convolutions has a frame rate of 50 Hz, suggesting 2400000 * 30s * 50/s = 3,600,000,000 audio tokens.\n\nNot confident enough in this calculation to add to database.",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2306.05284",
      "Reference": "Simple and Controllable Music Generation",
      "Citations": "574.0",
      "Authors": "Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, Alexandre D\u00e9fossez",
      "Abstract": "We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen. Music samples, code, and models are available at this https URL.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "",
      "Accessibility notes": "Code is released under MIT, model weights are released under CC-BY-NC 4.0\n\nhttps://github.com/facebookresearch/audiocraft/blob/main/docs/MUSICGEN.md",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "LTM-1",
      "Organization": "Magic",
      "Publication date": "2023-06-06",
      "Domain": "Language",
      "Task": "Code generation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "Must be below 1e23 FLOP, as it's trained with a single A100.",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://magic.dev/blog/ltm-1",
      "Reference": "LTM-1: an LLM with a 5,000,000 token context window",
      "Citations": "",
      "Authors": "",
      "Abstract": "Magic\u2019s LTM-1 enables 50x larger context windows than transformers\nMagic's trained a Large Language Model (LLM) that\u2019s able to take in the gigantic amounts of context when generating suggestions. For our coding assistant, this means Magic can now see your entire repository of code.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Very long context window - 5M tokens\n\nno information about evaluations",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "PaLI-X",
      "Organization": "Google Research",
      "Publication date": "2023-05-29",
      "Domain": "Multimodal,Language,Vision,Video",
      "Task": "Image captioning,Video description,Character recognition (OCR),Visual question answering",
      "Parameters": "55000000000.0",
      "Parameters notes": "55B (table 1)",
      "Training compute (FLOP)": "5.6e+23",
      "Training compute notes": "\"we present PaLI-X [...] consisting of a pretrained large-capacity visual encoder (using [6] as the starting point) and a pretrained language-only encoder-decoder (using [7] as the starting point), further trained at-scale on a vision-and-language data mixture using a combination of self-supervision and full-supervision signals.\"\n\n\"Visual component Our visual backbone is scaled to 22B parameters, as introduced by [6], the largest dense ViT model to date.\" (elsewhere they specify this is ViT-22B)\nOriginal ViT-22B pre-training compute: 1.93248e+23 FLOP\n\n\"Overall model The encoder-decoder backbone is initialized from a variant of the UL2 [7] encoder-decoder model that uses 32B parameters. The architecture of this variant has 50 layers in both encoder and decoder (up from 32 layers in [7]), and is pretrained on a mixture of text data similar to [7].\" ([7] = UL2 paper)\nOriginal UL2 pre-training compute: 1.2e+23 FLOP\nBut that was for a 20B model. Assuming the same amount of text data, compute is roughly 32B/20B * 1.2e23 = 1.92e+23\n\nFurther training:]\n\n\"WebLI [5], consisting of roughly one billion images with alt-texts from the web and OCR annotations.\"'\n\n\"In addition to WebLI \u27e8image, text\u27e9 pairs, we introduce here Episodic WebLI data, where each episode corresponds to a set of such pairs. We aim to have each episode contain loosely related images (i.e., they are clustered according to their URL field), so as to encourage attention among examples in an \u201cepisode\u201d. We find this new dataset (with 75M episodes and around 400M images in total) important for developing the few-shot capabilities of the model.\"\n\n\"The pretraining mixture consists of the following data and objectives [...]\" - this seems to be the \"mixed-objective training\" referred to later\n\n\"Our model is trained in two stages. In stage 1, the visual encoder (after mixed-objective training) is kept frozen, while the rest of the parameters are trained on a total of 2.2B examples at the base resolution 224\u00d7224 (native to ViT-22B), using the entire mixture. In stage 2, it continues training using only the OCR-related objectives (pix2struct and split-ocr) plus the object detection objective; this is done in several substages, during which image resolution is gradually increased to 448\u00d7448, 672\u00d7672 and finally 756\u00d7756.\"\n\n\"256 tokens per image\"\n\n2.2B images * 256 tokens per image = 5.632e+11 tokens\n32B parameters for backward pass because the 22B visual encoder is frozen.\nSo stage 1 compute estimate is: 5.632e+11 * (55e9 * 2 forward + 32e9 * 6 backward) = 1.7e+23 FLOP\n\nUnsure how to estimate compute for stage 2 - will neglect that here.\n\nFine-tuning compute seems negligible - Table 20 implies millions of examples.\n\nSo total compute estimate:\n1.93248e+23 FLOP ViT-22B original\n1.92e+23 FLOP UL2 32B original\n1.7e+23 FLOP stage 1 training\n5.6e23 FLOP total",
      "Training dataset": "WebLI",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "1 billion images with alt texts in WebLI, 400m images in Episodic WebLI data",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2305.18565",
      "Reference": "PaLI-X: On Scaling up a Multilingual Vision and Language Model",
      "Citations": "248.0",
      "Authors": "Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, Siamak Shakeri, Mostafa Dehghani, Daniel Salz, Mario Lucic, Michael Tschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi, Bo Pang, Ceslee Montgomery, Paulina Pietrzyk, Marvin Ritter, AJ Piergiovanni, Matthias Minderer, Filip Pavetic, Austin Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas Beyer, Julien Amelot, Kenton Lee, Andreas Peter Steiner, Yang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu, Keran Rong, Alexander Kolesnikov, Mojtaba Seyedhosseini, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, Radu Soricut",
      "Abstract": "We present the training recipe and results of scaling up PaLI-X, a multilingual vision and language model, both in terms of size of the components and the breadth of its training task mixture. Our model achieves new levels of performance on a wide-range of varied and complex tasks, including multiple image-based captioning and question-answering tasks, image-based document understanding and few-shot (in-context) learning, as well as object detection, video question answering, and video captioning. PaLI-X advances the state-of-the-art on most vision-and-language benchmarks considered (25+ of them). Finally, we observe emerging capabilities, such as complex counting and multilingual object detection, tasks that are not explicitly in the training mix.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"PaLI-X advances the state-of-the-art on most vision-and-language benchmarks considered (25+ of them).\"\n\nTable 1, Table 2",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "UL2",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DPO on Pythia-2.8B",
      "Organization": "Stanford University,CZ Biohub Network",
      "Publication date": "2023-05-29",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering",
      "Parameters": "2800000000.0",
      "Parameters notes": "same as base model",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Anthropic HH",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2305.18290",
      "Reference": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
      "Citations": "",
      "Authors": "Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, Chelsea Finn",
      "Abstract": "While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "the paper introduced DPO",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "Pythia-2.8b",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "",
      "Accessibility notes": "\"DPO is relatively straightforward to implement; PyTorch code for the DPO loss is provided [in the paper]\"",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Goat-7B",
      "Organization": "National University of Singapore",
      "Publication date": "2023-05-23",
      "Domain": "Language",
      "Task": "Quantitative reasoning",
      "Parameters": "7000000000.0",
      "Parameters notes": "7B",
      "Training compute (FLOP)": "",
      "Training compute notes": "2.78e+22 for base LLaMA-7B",
      "Training dataset": "",
      "Training dataset size (gradients)": "43700000",
      "Dataset size notes": "Fine-tune dataset had 1 million question-answer pairs. likely ~10 tokens per pair?",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/2305.14201",
      "Reference": "Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks",
      "Citations": "98.0",
      "Authors": "Tiedong Liu, Bryan Kian Hsiang Low",
      "Abstract": "We introduce Goat, a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks. Fine-tuned on a synthetically generated dataset, Goat achieves state-of-the-art performance on BIG-bench arithmetic sub-task. In particular, the zero-shot Goat-7B matches or even surpasses the accuracy achieved by the few-shot PaLM-540B. Surprisingly, Goat can achieve near-perfect accuracy on large-number addition and subtraction through supervised fine-tuning only, which is almost impossible with previous pretrained language models, such as Bloom, OPT, GPT-NeoX, etc. We attribute Goat's exceptional performance to LLaMA's consistent tokenization of numbers. To tackle more challenging tasks like large-number multiplication and division, we propose an approach that classifies tasks based on their learnability, and subsequently decomposes unlearnable tasks, such as multi-digit multiplication and division, into a series of learnable tasks by leveraging basic arithmetic principles. We thoroughly examine the performance of our model, offering a comprehensive evaluation of the effectiveness of our proposed decomposition steps. Additionally, Goat-7B can be easily trained using LoRA on a 24GB VRAM GPU, facilitating reproducibility for other researchers. We release our model, dataset, and the Python script for dataset generation.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Singapore",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We introduce Goat, a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks. Fine-tuned on a synthetically generated dataset, Goat achieves state-ofthe-art performance on BIG-bench arithmetic sub-task.\"",
      "Epochs": "1.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA A10 PCIe",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "LLaMA-7B",
      "Finetune compute (FLOP)": "2.02e+18",
      "Finetune compute notes": "\"Goat-7B can be easily fine-tuned using LoRA on a 24GB VRAM GPU... In particular, the fine-tuning process for a specific arithmetic sub-task, such as 8-digit addition using 100K instances, takes only approximately 1.5 hours on an A10 GPU to achieve near-perfect accuracy\"\n\nInfo isn't very complete - no timeframe specified for the VRAM GPU, I'm not sure how many tokens are in the fine-tune dataset and they use LoRA. Maybe it's 15 A10-hours total (1M total instances)? But safe to assume it's a small fraction of Llama's pretrain compute.\n\n125 trillion (A10 FLOPs) * 15 * 3600 * 0.3 = 2.02e18",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "no license noted. perhaps LLaMA 1 license by default (non-comm). repo with finetune (i.e. training since this is a Llama finetune) code\nhttps://github.com/liutiedong/goat",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "CodeT5+",
      "Organization": "Salesforce",
      "Publication date": "2023-05-20",
      "Domain": "Language",
      "Task": "Code generation,Code autocompletion",
      "Parameters": "16000000000.0",
      "Parameters notes": "\"We implemented a family of CodeT5+ models, with model sizes ranging from 220M to 16B\"",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "51500000000",
      "Dataset size notes": "\"We use the CodeT5 tokenizer to tokenize the multilingual dataset, resulting in 51.5B tokens\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2305.07922",
      "Reference": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation",
      "Citations": "611.0",
      "Authors": "Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi D.Q. Bui, Junnan Li, Steven C.H. Hoi",
      "Abstract": "\"Large language models (LLMs) pretrained on vast source code have achieved prominent progress in code intelligence. However, existing code LLMs have two main limitations in terms of architecture and pretraining tasks. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks. The former paradigm is limited by inflexibility in applications while in the latter, the model is treated as a single system for all tasks, leading to suboptimal performance on a subset of tasks. Secondly, they often employ a limited set of pretraining objectives which might not be relevant to some downstream tasks and hence result in substantial performance degrade. To address these limitations, we propose ``CodeT5+'', a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of downstream code tasks. Such flexibility is enabled by our proposed mixture of pretraining objectives to mitigate the pretrain-finetune discrepancy. These objectives cover span denoising, contrastive learning, text-code matching, and causal LM pretraining tasks, on both unimodal and bimodal multilingual code corpora. Furthermore, we propose to initialize CodeT5+ with frozen off-the-shelf LLMs without training from scratch to efficiently scale up our models, and explore instruction-tuning to align with natural language instructions. We extensively evaluate CodeT5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. We observe state-of-the-art (SoTA) model performance on various code-related tasks, such as code generation and completion, math programming, and text-to-code retrieval tasks. Particularly, our instruction-tuned CodeT5+ 16B achieves new SoTA results on HumanEval code generation task against other open code LLMs.\"",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We extensively evaluate CodeT5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. We observe state-of-the-art (SoTA) model performance on various code-related tasks, such as code generation and completion, math programming, and text-to-code retrieval tasks\"\n\n\"our instruction-tuned CodeT5+ 16B achieves new SoTA results of\n35.0% pass@1 and 54.5% pass@10 on the HumanEval code generation task against other open code LLMs, even surpassing the OpenAI code-cushman-001 model\"\n\nnot SOTA overall",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "BSD 3-clause \nhttps://github.com/salesforce/CodeT5/blob/main/LICENSE.txt\n\nhttps://huggingface.co/Salesforce/codet5p-16b",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ONE-PEACE",
      "Organization": "Alibaba,Huazhong University of Science and Technology",
      "Publication date": "2023-05-18",
      "Domain": "Multimodal,Vision,Speech,Language",
      "Task": "Image classification,Speech recognition (ASR),Audio question answering,Audio classification,Semantic segmentation",
      "Parameters": "4000000000.0",
      "Parameters notes": "\"we propose ONE-PEACE, a model with 4B parameters\"",
      "Training compute (FLOP)": "1.8e+20",
      "Training compute notes": "4 billion params * 7.5 billion data * 6 = 1.8e20.\n\nsee training dataset size notes. this estimate required some more assumptions than usual.",
      "Training dataset": "LAION-2B,LAION-Audio-630K",
      "Training dataset size (gradients)": "490617000000",
      "Dataset size notes": "\"After these steps, we retain about 1.5 billion image-text pairs\"\n...\n\"We also perform simple cleaning on the data, which involves removing samples with text lengths less than 3 or greater than\n512, as well as texts containing non-English or emoji characters. Ultimately, we obtain about 2.4 million audio-text pairs, with a total duration of around 8,000 hours\"\n\n8000 hours = 480,000 minutes = ~109,440,000 words at 228 wpm\n\nhttps://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.3pbt0hfgv7pq\n\nTrained on 10 epochs for audio. For text, they train on \"200K steps with a batch size of 32768\" = 6,533,600,000\nAdding together, they train on ~ 7.5b data points on a dataset of 1.6b, for ~4.7 epochs on average.",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/2305.11172v1",
      "Reference": "ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities",
      "Citations": "151.0",
      "Authors": "Peng Wang, Shijie Wang, Junyang Lin, Shuai Bai, Xiaohuan Zhou, Jingren Zhou, Xinggang Wang, Chang Zhou",
      "Abstract": "In this work, we explore a scalable way for building a general representation model toward unlimited modalities. We release ONE-PEACE, a highly extensible model with 4B parameters that can seamlessly align and integrate representations across vision, audio, and language modalities. The architecture of ONE-PEACE comprises modality adapters, shared self-attention layers, and modality FFNs. This design allows for the easy extension of new modalities by adding adapters and FFNs, while also enabling multi-modal fusion through self-attention layers. To pretrain ONE-PEACE, we develop two modality-agnostic pretraining tasks, cross-modal aligning contrast and intra-modal denoising contrast, which align the semantic space of different modalities and capture fine-grained details within modalities concurrently. With the scaling-friendly architecture and pretraining tasks, ONE-PEACE has the potential to expand to unlimited modalities. Without using any vision or language pretrained model for initialization, ONE-PEACE achieves leading results on a wide range of uni-modal and multi-modal tasks, including image classification (ImageNet), semantic segmentation (ADE20K), audio-text retrieval (AudioCaps, Clotho), audio classification (ESC-50, FSD50K, VGGSound), audio question answering (AVQA), image-text retrieval (MSCOCO, Flickr30K), and visual grounding (RefCOCO/+/g). Code is available at this https URL.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "China,China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\" ONEPEACE achieves leading results in both uni-modal and multi-modal tasks, including image classification (89.8% accuracy on ImageNet w/o privately labeled data), semantic segmentation (63.0% mIoU on ADE20K), audio-text retrieval (outperforming previous SOTAs on AudioCaps and Clotho by a large margin), audio classification (91.8% zero-shot accuracy on ESC-50, 69.7% accuracy on FSD50K, 59.6% accuracy on VGGSound w/o visual information),\naudio question answering (86.2% accuracy on AVQA w/o visual information), image-text retrieval (84.1% I2T R@1 on MSCOCO and 97.6% I2T R@1 on Flickr30K w/o intermediate finetuning and ranking), and visual grounding (89.26%/83.23%/89.27% scores on RefCOCO/+/g test sets).\"",
      "Epochs": "4.7",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache 2.0, includes train code\nhttps://github.com/OFA-Sys/ONE-PEACE/tree/main/one_peace ",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "CoEdiT-xxl",
      "Organization": "University of Minnesota,Grammarly",
      "Publication date": "2023-05-17",
      "Domain": "Language",
      "Task": "Language generation",
      "Parameters": "11000000000.0",
      "Parameters notes": "11B",
      "Training compute (FLOP)": "",
      "Training compute notes": "finetuned from Flan-T5",
      "Training dataset": "",
      "Training dataset size (gradients)": "1093333.3333333333",
      "Dataset size notes": "82k pairs of sentences. Roughly 20 words per sentence based on examples but mean length could be higher due to outliers.\n40*82k = ~3,000,000",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2305.09857, https://huggingface.co/grammarly/coedit-large",
      "Reference": "CoEdIT: Text Editing by Task-Specific Instruction Tuning",
      "Citations": "72.0",
      "Authors": "Vipul Raheja, Dhruv Kumar, Ryan Koo, Dongyeop Kang",
      "Abstract": "We introduce COEDIT, a state-of-the-art text editing system for writing assistance. COEDIT takes instructions from the user specifying the attributes of the desired text, such as \"Make the sentence simpler\" or \"Write it in a more neutral style,\" and outputs the edited text. We present a large language model fine-tuned on a diverse collection of task-specific instructions for text editing (a total of 82K instructions). Our model (1) achieves state-of-the-art performance on various text editing benchmarks, (2) is competitive with publicly available largestsized LLMs trained on instructions while being \u223c60x smaller, (3) is capable of generalizing to unseen edit instructions, and (4) exhibits abilities to generalize to composite instructions containing different combinations of edit actions. Through extensive qualitative and quantitative analysis, we show that writers prefer the edits suggested by COEDIT, relative to other stateof-the-art text editing models1.",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We achieve state-of-the-art performance on multiple text editing tasks: grammatical error correction, text simplification, sentence fusion, iterative text editing, and three stylistic editing tasks (formality style transfer, neutralization, and paraphrasing).\"\n\nTable 2",
      "Epochs": "5.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "Flan-T5 11B",
      "Finetune compute (FLOP)": "1e+18",
      "Finetune compute notes": "\"We fine-tune different versions of pre-trained FLANT5 (Chung et al., 2022a) models on the COEDIT dataset. Specifically, we use FLANT5-L (770M parameters), FLANT5-XL (3B parameters), FLANT5-XXL (11B parameters) models.\"\n\n\"Each model is trained for 5 epochs with early stopping. All models were fine-tuned on A100 GPUs using Deepspeed\"\n\n6 * 5 epochs * 3 million words (rough estimate) * 11 billion = 9.9e17 ~= 1e18\n\n",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "Open access (non-commercial)",
      "Accessibility notes": "cc-by-nc (non commercial) for weights: https://huggingface.co/grammarly/coedit-large\ntraining code/data here with unclear licenses: https://github.com/vipulraheja/coedit",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Med-PaLM 2",
      "Organization": "Google Research,DeepMind",
      "Publication date": "2023-05-16",
      "Domain": "Medicine,Language",
      "Task": "Question answering",
      "Parameters": "340000000000.0",
      "Parameters notes": "from PaLM 2",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "MultiMedQA",
      "Training dataset size (gradients)": "16020450",
      "Dataset size notes": "Dataset Count Mixture ratio\nMedQA 10,178 37.5%\nMedMCQA 182,822 37.5%\nLiveQA 10 3.9%\nMedicationQA 9 3.5%\nHealthSearchQA 45 17.6%\n\nMedMCQA (https://proceedings.mlr.press/v174/pal22a/pal22a.pdf, Table 2) has on average 12.77+ 2.69+67.52 = 82.98 tokens per datapoint",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2305.09617",
      "Reference": "Towards Expert-Level Medical Question Answering with Large Language Models",
      "Citations": "",
      "Authors": "Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise Aguera y Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle Barral, Dale Webster, Greg S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, Vivek Natarajan",
      "Abstract": "Recent artificial intelligence (AI) systems have reached milestones in \"grand challenges\" ranging from Go to protein-folding. The capability to retrieve medical knowledge, reason over it, and answer medical questions comparably to physicians has long been viewed as one such grand challenge.\nLarge language models (LLMs) have catalyzed significant progress in medical question answering; Med-PaLM was the first model to exceed a \"passing\" score in US Medical Licensing Examination (USMLE) style questions with a score of 67.2% on the MedQA dataset. However, this and other prior work suggested significant room for improvement, especially when models' answers were compared to clinicians' answers. Here we present Med-PaLM 2, which bridges these gaps by leveraging a combination of base LLM improvements (PaLM 2), medical domain finetuning, and prompting strategies including a novel ensemble refinement approach.\nMed-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM by over 19% and setting a new state-of-the-art. We also observed performance approaching or exceeding state-of-the-art across MedMCQA, PubMedQA, and MMLU clinical topics datasets.\nWe performed detailed human evaluations on long-form questions along multiple axes relevant to clinical applications. In pairwise comparative ranking of 1066 consumer medical questions, physicians preferred Med-PaLM 2 answers to those produced by physicians on eight of nine axes pertaining to clinical utility (p < 0.001). We also observed significant improvements compared to Med-PaLM on every evaluation axis (p < 0.001) on newly introduced datasets of 240 long-form \"adversarial\" questions to probe LLM limitations.\nWhile further studies are necessary to validate the efficacy of these models in real-world settings, these results highlight rapid progress towards physician-level performance in medical question answering.",
      "Organization categorization": "Industry,Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "https://paperswithcode.com/sota/question-answering-on-medqa-usmle \n\n\"Med-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM by over 19% and\nsetting a new state-of-the-art. We also observed performance approaching or exceeding state-of-the-art\nacross MedMCQA, PubMedQA, and MMLU clinical topics datasets.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "PaLM 2",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "InstructBLIP",
      "Organization": "Salesforce Research,Hong Kong University of Science and Technology (HKUST),Nanyang Technological University",
      "Publication date": "2023-05-11",
      "Domain": "Multimodal,Language,Vision",
      "Task": "Visual question answering,Chat",
      "Parameters": "13000000000.0",
      "Parameters notes": "13B form 2.6",
      "Training compute (FLOP)": "1.94e+20",
      "Training compute notes": "\"All models are trained utilizing 16 Nvidia A100 (40G) GPUs and are completed within 1.5 days.\"\n16 * 3.12e14 * 1.5 * 24 * 3600 * 0.3 = 1.94e20",
      "Training dataset": "COCO,Web CapFilt,NoCaps,Flickr30K Entities,TextCaps,VQAv2,VizWiz,GQA,OKVQA,ScienceQA,OCR-VQA,TextVQA,LLaVA-Instruct-150k",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "\"All models are instruction-tuned with a maximum of 60K steps\"\n\n\"We employ a batch size of 192, 128, and 64 for the 3B, 7B, and 11/13B models, respectively. \"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2305.06500",
      "Reference": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning",
      "Citations": "2832.0",
      "Authors": "Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi",
      "Abstract": "Large-scale pre-training and instruction tuning have been successful at creating general-purpose language models with broad competence. However, building general-purpose vision-language models is challenging due to the rich input distributions and task diversity resulting from the additional visual input. Although vision-language pretraining has been widely studied, vision-language instruction tuning remains under-explored. In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pretrained BLIP-2 models. We gather 26 publicly available datasets, covering a wide variety of tasks and capabilities, and transform them into instruction tuning format. Additionally, we introduce an instruction-aware Query Transformer, which extracts informative features tailored to the given instruction. Trained on 13 held-in datasets, InstructBLIP attains state-of-the-art zero-shot performance across all 13 held-out datasets, substantially outperforming BLIP-2 and larger Flamingo models. Our models also lead to state-of-the-art performance when finetuned on individual downstream tasks (e.g., 90.7% accuracy on ScienceQA questions with image contexts). Furthermore, we qualitatively demonstrate the advantages of InstructBLIP over concurrent multimodal models. All InstructBLIP models are open-sourced at this https URL. ",
      "Organization categorization": "Industry,Academia,Academia",
      "Country (of organization)": "United States of America,Hong Kong,Singapore",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "from abstract - SOTA on ScienceQA\n\n\"InstructBLIP sets new state-of-the-art finetuning performance on ScienceQA (IMG), OCR-VQA, A-OKVQA, and is outperformed on OKVQA by PaLM-E [9] with 562B parameters\"",
      "Epochs": "",
      "Training time (hours)": "36.0",
      "Training time notes": "\"All models are trained utilizing 16 Nvidia A100 (40G) GPUs and are completed within 1.5 days.\"",
      "Training hardware": "NVIDIA A100 SXM4 40 GB",
      "Hardware quantity": "16.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "12748.20334097663",
      "Base model": "Vicuna-13B v0",
      "Finetune compute (FLOP)": "1.9408896e+20",
      "Finetune compute notes": "flops = (16) * (312 * 10**12) * (1.5* 24 * 3600) * (0.3) = 1.9e20\n(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)\n\n\"All models are trained utilizing 16 Nvidia A100 (40G) GPUs and are completed within 1.5 days.\"",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "Open access (non-commercial)",
      "Accessibility notes": "LlaMA/Vicuna license, non-comm:\nhttps://github.com/salesforce/LAVIS/tree/main/projects/instructblip\n\nresearch only:\nhttps://huggingface.co/Salesforce/instructblip-vicuna-13b",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "187709.2137963046",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "PaLM 2",
      "Organization": "Google",
      "Publication date": "2023-05-10",
      "Domain": "Language",
      "Task": "Language modeling,Language modeling/generation",
      "Parameters": "340000000000.0",
      "Parameters notes": "Model Architecture: \"PaLM-2 is a new state-of-the-art language model. We have small, medium, and large variants that use stacked layers based on the Transformer architecture, with varying parameters depending on model size. Further details of model size and architecture are withheld from external publication.\"\nHowever, the parameter count was leaked to CNBC: https://www.cnbc.com/2023/05/16/googles-palm-2-uses-nearly-five-times-more-text-data-than-predecessor.html",
      "Training compute (FLOP)": "7.34e+24",
      "Training compute notes": "Compute Requirements \"Not reported.\"\nPaper suggests heuristic of  C=6ND. Based on 340B parameters and 3.6T tokens, training compute would be around 7.3*10^24 FLOP.",
      "Training dataset": "",
      "Training dataset size (gradients)": "3600000000000",
      "Dataset size notes": "\"The pre-training corpus is significantly larger than the corpus used to train PaLM\" so greater than 6e+11. According to the leaked documents viewed by CNBC, the corpus was 3.6 trillion tokens or around 2.7*10^12 words.\n\nhttps://www.cnbc.com/2023/05/16/googles-palm-2-uses-nearly-five-times-more-text-data-than-predecessor.html",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2305.10403",
      "Reference": "PaLM 2 Technical Report",
      "Citations": "1734.0",
      "Authors": "Andrew M. Dai, David R. So, Dmitry Lepikhin, Jonathan H. Clark, Maxim Krikun, Melvin Johnson, Nan Du, Rohan Anil, Siamak Shakeri, Xavier Garcia, Yanping Huang, Yi Tay, Yong Cheng, Yonghui Wu, Yuanzhong Xu, Yujing Zhang, Zachary Nado, Bryan Richter, Alex Polozov, Andrew Nystrom, Fangxiaoyu Feng, Hanzhao Lin, Jacob Austin, Jacob Devlin, Kefan Xiao, Orhan Firat, Parker Riley, Steven Zheng, Yuhuai Wu, Zhongtao Liu, Jiahui Yu, Guy Gur-Ari, Weikang Zhou, Sneha Kudugunta, Sunipa Dev, Frederick Liu, Gustavo Hernandez Abrego, Kelvin Xu, Abe Ittycheriah, Daniel Sohn, John Nham, Le Hou, Siyuan Qiao, Pidong Wang, Zirui Wang, Laurent El Shafey, Hyeontaek Lim, Marcello Maggioni, Michael Isard, Paul Barham, Qiao Zhang, Tao Wang, Yash Katariya, Aurko Roy, Benjamin Lee, Brennan Saeta, Ce Zheng, Hadi Hashemi, Junwhan Ahn, Rajkumar Samuel, Steven Hand, Zhifeng Chen, Kiran Vodrahalli, Aakanksha Chowdhery, Ethan Dyer, Emanuel Taropa, Vlad Feinberg, James Bradbury, Reiner Pope, Wei Li, YaGuang Li, Eric Chu, Jeffrey Hui, Joshua Howland, Vlad Fienber, Aroma Mahendru, Michele Catasta, Vedant Misra, Kevin Robinson, Maysam Moussalem, Sebastian Ruder, Erica Moreira, Eric Ni, Paige Bailey, Lucas Gonzalez, Alexandre Passos, Slav Petrov, Gaurav Mishra, Mark Omernick, Ambrose Slone, Andrea Hu, Colin Cherry, Denny Zhou, Jan Botha, John Wieting, Joshua Maynez, Kathleen Kenealy, Kevin Brooks, Linting Xue, Markus Freitag, Martin Polacek, Pengcheng Yin, Sebastian Gehrmann, Xuezhi Wang, Kathy Meier-Hellstern, Christopher A. Choquette-Choo, Daniel Smilkov, Emily Reif, Alicia Parrish, Alex Castro Ros, Cl\u00e9ment Crepy, Dasha Valter, Jeremy Hurwitz, Katherine Lee, Mark D\u00edaz, Marie Pellat, Matthew Jagielski, Renee Shelby, Shachi Dave",
      "Abstract": "We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM (Chowdhery et al., 2022). PaLM 2 is a Transformer-based model trained using a mixture of objectives similar to UL2 (Tay et al., 2023). Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to PaLM. This improved efficiency enables broader deployment while also allowing the model to respond faster, for a more natural pace of interaction. PaLM 2 demonstrates robust reasoning capabilities exemplified by large improvements over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable performance on a suite of responsible AI evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities. Overall, PaLM 2 achieves state-of-the-art performance across a diverse set of tasks and capabilities.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement,Training cost,Significant use,Highly cited",
      "Notability criteria notes": "Significant use: Gmail and Google Docs have millions of users.\nSOTA performance: Table 5\n\"At I/O today, we announced over 25 new products and features powered by PaLM 2. That means that PaLM 2 is bringing the latest in advanced AI capabilities directly into our products and to people \u2014 including consumers, developers, and enterprises of all sizes around the world. Here are some examples:\nPaLM 2\u2019s improved multilingual capabilities are allowing us to expand Bard to new languages, starting today. Plus, it\u2019s powering our recently announced coding update.\nWorkspace features to help you write in Gmail and Google Docs, and help you organize in Google Sheets are all tapping into the capabilities of PaLM 2 at a speed that helps people get work done better, and faster.\"\nhttps://blog.google/technology/ai/google-palm-2-ai-large-language-model/",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "Google TPU v4",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "5014266.983518871",
      "Compute cost notes": "PaLM 2 was trained on TPU v4 according to the model card (pages 91-92)",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "not listed",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "StarCoder",
      "Organization": "Hugging Face,ServiceNow,Northeastern University,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),Carnegie Mellon University (CMU),Johns Hopkins University,Leipzig University,ScaDS.AI,Queen Mary University of London,Roblox,Sea AI Lab,Technion - Israel Institute of Technology,Monash University,CSIRO,Data61,McGill University,Saama,University of British Columbia (UBC),Massachusetts Institute of Technology (MIT),Technical University of Munich,IBM,University of Vermont,UnfoldML,SAP,University of Notre Dame,Columbia University,New York University (NYU),University of Allahabad,Discover Dollar,Toloka,Telefonica,Stanford University,Weizmann Institute of Science,Alan Turing Institute,Wellesley College,EleutherAI,Forschungszentrum Julich",
      "Publication date": "2023-05-09",
      "Domain": "Language",
      "Task": "Code generation",
      "Parameters": "15500000000.0",
      "Parameters notes": "\"We trained a 15.5B parameter model\"",
      "Training compute (FLOP)": "8.46e+22",
      "Training compute notes": "FLOP reported here, 8.46e22\nhttps://huggingface.co/bigcode/starcoder\n\n\n\"We trained our model on a GPU cluster with 512 A100 80 GB GPUs... Based on the total number of GPU hours that training took (320,256) and an average power usage of 280W per GPU... The fine-tuned model adds 3.5% of training time\"\n\n320256 * 312 tFLOP/s * 3600 * 1.035 * 0.3 (utilization assumption) = 1.12e23",
      "Training dataset": "The Stack",
      "Training dataset size (gradients)": "203750000000",
      "Dataset size notes": "\"StarCoderBase is trained on 1 trillion tokens sourced from The Stack\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2305.06161",
      "Reference": "StarCoder: may the source be with you!",
      "Citations": "1023.0",
      "Authors": "Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo\u00e3o Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu\u00f1oz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries",
      "Abstract": "\"The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.\"",
      "Organization categorization": "Industry,Industry,Academia,Academia,Academia,Academia,Academia,Academia,Industry,Academia,Academia,Government,Government,Academia,Academia,Academia,Academia,Industry,Academia,Industry,Academia,Academia,Academia,Academia,Industry,Industry,Industry,Academia,Academia,Government,Academia,Research collective,Government",
      "Country (of organization)": "United States of America,United States of America,United States of America,Canada,United States of America,United States of America,Germany,Germany,United Kingdom of Great Britain and Northern Ireland,United States of America,Singapore,Israel,Australia,Australia,Australia,Canada,United States of America,Canada,United States of America,Germany,United States of America,United States of America,Sweden,Germany,United States of America,United States of America,United States of America,India,India,Netherlands,Spain,United States of America,Israel,United Kingdom of Great Britain and Northern Ireland,United States of America,United States of America,Germany",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python\"\n\n\"StarCoder substantially outperforms all other models on data science problems from the DS-1000 benchmark. Moreover, this is true across every kind of data science library.\"",
      "Epochs": "1.0",
      "Training time (hours)": "625.5",
      "Training time notes": "625.5 hours = 320256 /512\n512 GPUs from \"We trained our model on a GPU cluster with 512 A100 80 GB GPUs \"\n\n320256 GPU hours from \"Based on the total number of GPU hours that training took (320,256)\"\ncitations from sections 5.6 and 5.7",
      "Training hardware": "NVIDIA A100 SXM4 80 GB",
      "Hardware quantity": "512.0",
      "Hardware utilization (MFU)": "0.2272360999",
      "Training compute cost (2023 USD)": "212217.65075330864",
      "Compute cost notes": "",
      "Training power draw (W)": "407960.6765621668",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "4000000.0",
      "Batch size notes": "\"The model was trained for 250k iterations, with a batch size of 4M tokens, for a total of one trillion tokens.\"",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "some restrictions\n\nhttps://huggingface.co/spaces/bigcode/bigcode-model-license-agreement\n\ndata is The Stack, which has multiple licenses\nhttps://huggingface.co/datasets/bigcode/the-stack-dedup ",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ImageBind",
      "Organization": "Meta AI",
      "Publication date": "2023-05-09",
      "Domain": "Multimodal,Vision,Audio,Language,Image generation,Speech",
      "Task": "Image classification,Speech recognition (ASR),Image generation,Language modeling/generation",
      "Parameters": "932000000.0",
      "Parameters notes": "used ViT-Huge 630M as an image/video encoder and OpenCLIP-302m as text encoder",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "SUN RGB-D,LLVIP,Ego4D,AudioSet",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2305.05665, https://github.com/facebookresearch/ImageBind",
      "Reference": "IMAGEBIND: One Embedding Space To Bind Them All",
      "Citations": "1281.0",
      "Authors": "Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, Ishan Misra",
      "Abstract": "We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications 'out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate vision models for visual and non-visual tasks.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models\"\n\nTable 2: they don't report absolute SOTA on any of the benchmarks",
      "Epochs": "64.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA V100,NVIDIA A100",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "ViT-Huge/14",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "Open access (non-commercial)",
      "Accessibility notes": "Creative commons non-commercial\nmodels and code in this repo: https://github.com/facebookresearch/ImageBind/blob/main/README.md \ntrain code: https://github.com/facebookresearch/ImageBind/blob/main/imagebind/models/imagebind_model.py ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Agile Soccer Robot",
      "Organization": "Google DeepMind",
      "Publication date": "2023-04-26",
      "Domain": "Robotics",
      "Task": "Animal (human/non-human) imitation,Sports",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "3140000000",
      "Dataset size notes": "\". The get-up teacher learns to get up relatively quickly and trained in total for approximately 2.4 \u00b7 10^8 environment steps,\nequivalent to approximately 70 days of simulation time, or 14 hours of wall-clock time. The soccer\nteacher was trained for 2 \u00b7 10^9 environment steps, which took 158 hours of training, equivalent to\napproximately 580 days of simulated match\"",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/2304.13653",
      "Reference": "Learning Agile Soccer Skills for a Bipedal Robot with Deep Reinforcement Learning",
      "Citations": "216.0",
      "Authors": "Tuomas Haarnoja, Ben Moran, Guy Lever, Sandy H. Huang, Dhruva Tirumala, Markus Wulfmeier, Jan Humplik, Saran Tunyasuvunakool, Noah Y. Siegel, Roland Hafner, Michael Bloesch, Kristian Hartikainen, Arunkumar Byravan, Leonard Hasenclever, Yuval Tassa, Fereshteh Sadeghi, Nathan Batchelor, Federico Casarini, Stefano Saliceti, Charles Game, Neil Sreendra, Kushal Patel, Marlon Gwira, Andrea Huber, Nicole Hurley, Francesco Nori, Raia Hadsell, Nicolas Heess",
      "Abstract": "We investigate whether Deep Reinforcement Learning (Deep RL) is able to synthesize sophisticated and safe movement skills for a low-cost, miniature humanoid robot that can be composed into complex behavioral strategies in dynamic environments. We used Deep RL to train a humanoid robot with 20 actuated joints to play a simplified one-versus-one (1v1) soccer game. We first trained individual skills in isolation and then composed those skills end-to-end in a self-play setting. The resulting policy exhibits robust and dynamic movement skills such as rapid fall recovery, walking, turning, kicking and more; and transitions between them in a smooth, stable, and efficient manner - well beyond what is intuitively expected from the robot. The agents also developed a basic strategic understanding of the game, and learned, for instance, to anticipate ball movements and to block opponent shots. The full range of behaviors emerged from a small set of simple rewards. Our agents were trained in simulation and transferred to real robots zero-shot. We found that a combination of sufficiently high-frequency control, targeted dynamics randomization, and perturbations during training in simulation enabled good-quality transfer, despite significant unmodeled effects and variations across robot instances. Although the robots are inherently fragile, minor hardware modifications together with basic regularization of the behavior during training led the robots to learn safe and effective movements while still performing in a dynamic and agile way. Indeed, even though the agents were optimized for scoring, in experiments they walked 156% faster, took 63% less time to get up, and kicked 24% faster than a scripted baseline, while efficiently combining the skills to achieve the longer term objectives. Examples of the emergent behaviors and full 1v1 matches are available on the supplementary website.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Likely the best bipedal soccer AI, since it's DeepMind, and related work section just discusses results involving specific soccer skills and quadruped robots:\n\n\"Whether bipedal or quadrupedal, navigation represents only a fraction of animal and human capabilities. Motivated by this observation, there is a growing interest in whole body control, i.e. tasks in which the whole body is used in flexible ways to interact with the environment. Examples include climbing (Rudin et al., 2022a), getting-up from the ground (Ma et al., 2023), catching objects (Ma et al., 2023), and mobile manipulation with legs (Cheng et al., 2023). Recently, reinforcement learning has been applied to learn simple soccer skills, including goalkeeping (Huang et al., 2022), ball manipulation on diverse terrains (Bohez et al., 2022; Ji et al., 2023), and shooting (Ji et al.,\n2022). These works focus on a narrower set of skills than the 1v1 soccer game, and the quadrupedal platform is inherently more stable and therefore presents an easier learning challenge.\"\n\nI do not see any standard benchmarks that they are claiming SOTA on",
      "Epochs": "",
      "Training time (hours)": "240.0",
      "Training time notes": "14+158+68 hours:\n\"Training the get-up and soccer teachers took 14 and 158 hours (6.5 days), respectively, and distillation and self-play\ntook 68 hours (see Appendix B for details)\"",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Unreleased",
      "Accessibility notes": "only video demos here\nhttps://sites.google.com/view/op3-soccer",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "LLaVA",
      "Organization": "University of Wisconsin Madison,Microsoft Research,Columbia University",
      "Publication date": "2023-04-17",
      "Domain": "Multimodal,Vision,Language",
      "Task": "Chat,Question answering,Visual question answering",
      "Parameters": "13000000000.0",
      "Parameters notes": "13B",
      "Training compute (FLOP)": "7.8049e+22",
      "Training compute notes": "8 * 3.12e14 * (18 * 3600) * 0.3 = 4.9e19\nnum gpus * peak flops * time *assumed utilization rate \n\"We train all models with 8\u00d7 A100s. Pretraining on CC-595K completes within 4 hours. Finetuning on Instruct-158K completes within 10 hours. Finetuning on ScienceQA completes within 4 hours.\" so 18 hours of time in total.\n\nHowever, they use Vicuna as their LLM backbone, which used 7.8e22 FLOPs in training. Total FLOPs are then 4.9e19 + 7.8e22 = 7.8049e22",
      "Training dataset": "Conceptual Captions (CC3M)",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "595K + 158K = 753K image text pairs\n\"This results in around 595K image-text pairs\"\n\"We collect 158K unique language-image instruction-following samples in total, including 58K in conversations, 23K in detailed description, and 77k in complex reasoning, respectively. \"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2304.08485",
      "Reference": "Visual Instruction Tuning",
      "Citations": "7255.0",
      "Authors": "Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee",
      "Abstract": "Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available. ",
      "Organization categorization": "Academia,Industry,Academia",
      "Country (of organization)": "United States of America,United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%.",
      "Epochs": "",
      "Training time (hours)": "10.0",
      "Training time notes": "\"We train all models with 8\u00d7 A100s. Pretraining on CC-595K completes within 4 hours. Finetuning on Instruct-158K completes within 10 hours. Finetuning on ScienceQA completes within 4 hours.\"",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "8.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "42.46267260692187",
      "Compute cost notes": "",
      "Training power draw (W)": "6377.509314719864",
      "Base model": "Vicuna-13B v0",
      "Finetune compute (FLOP)": "4.9e+19",
      "Finetune compute notes": "8 * 3.12e14 * (18 * 3600) * 0.3 = 4.9e19\nnum gpus * peak flops * time *assumed utilization rate \n\"We train all models with 8\u00d7 A100s. Pretraining on CC-595K completes within 4 hours. Finetuning on Instruct-158K completes within 10 hours. Finetuning on ScienceQA completes within 4 hours.\" so 18 hours of time in total.",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "apache 2.0\n\ntrain repo: https://github.com/haotian-liu/LLaVA?tab=readme-ov-file#train \n\nmodel: https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md ",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DINOv2",
      "Organization": "Facebook AI Research,INRIA",
      "Publication date": "2023-04-14",
      "Domain": "Vision",
      "Task": "Image representation,Image classification",
      "Parameters": "1140000000.0",
      "Parameters notes": "1.14B from https://huggingface.co/facebook/dinov2-giant",
      "Training compute (FLOP)": "7.41851136e+21",
      "Training compute notes": "table 14\n\n22016 * 3600 * 312 * 10 ** 12 * 3/10 = 7.41851136e+21\ngpu hours in seconds * flops of A100 * assumed utilization  rate",
      "Training dataset": "LVD142M",
      "Training dataset size (gradients)": "36380002816",
      "Dataset size notes": "new dataset  - named LVD142M Table 15",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2304.07193",
      "Reference": "DINOv2: Learning Robust Visual Features without Supervision",
      "Citations": "5830.0",
      "Authors": "Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herv\u00e9 Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, Piotr Bojanowski",
      "Abstract": "The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels. ",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,France,France",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Our family of models drastically improves over the previous state of the art in self-supervised learning and reaches performance comparable with weakly-supervised features.\"\n\n\"Because most SSL methods were developped using ImageNet-1k validation performance\nas a debugging signal, we also report the top-1 accuracy on ImageNet-ReaL and ImageNet-V2.\"\n\n\"For iNaturalist 2018, iNaturalist 2021, and Places205, we train a linear\nclassifier with data augmentations as in Sec. 7.1 We report top-1 accuracy for those three datasets in Table 7.\"\n\n\" We see that amongst self-supervised approaches, our model clearly sets a new state of the art\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA A100 SXM4 40 GB",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "10203.605181058361",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "apache 2.0\n\ntraining code and weights here: https://github.com/facebookresearch/dinov2 ",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Incoder-6.7B",
      "Organization": "Facebook AI Research,University of Washington,University of California (UC) Berkeley,Carnegie Mellon University (CMU),Toyota Technological Institute at Chicago",
      "Publication date": "2023-04-09",
      "Domain": "Language",
      "Task": "Code generation",
      "Parameters": "6700000000.0",
      "Parameters notes": "6.7B",
      "Training compute (FLOP)": "3.00001e+21",
      "Training compute notes": "per table 5, required 3 zettaflop (3e21) to train.\n\nalso, \"INCODER-6.7B was trained on 248 V100 GPUs for 24 days\"\n\nhardware method: 125 trillion * 248 * 24 * 24 * 3600 * 0.3 = 2e22. suggests their utilization was quite low, or 24 days was just calendar time.\n",
      "Training dataset": "",
      "Training dataset size (gradients)": "52000000000",
      "Dataset size notes": "216 GB: \"Our final pre-training corpus contains a total of 159 GB of code, 52 GB of it\nin Python, and a total of 57 GB of content from StackOverflow\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2204.05999",
      "Reference": "InCoder: A Generative Model for Code Infilling and Synthesis",
      "Citations": "791.0",
      "Authors": "Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, Mike Lewis",
      "Abstract": "Code is seldom written in a single left-to-right pass and is instead repeatedly edited and refined. We introduce InCoder, a unified generative model that can perform program synthesis (via left-to-right generation) as well as editing (via infilling). InCoder is trained to generate code files from a large corpus of permissively licensed code, where regions of code have been randomly masked and moved to the end of each file, allowing code infilling with bidirectional context. Our model is the first generative model that is able to directly perform zero-shot code infilling, which we evaluate on challenging tasks such as type inference, comment generation, and variable re-naming. We find that the ability to condition on bidirectional context substantially improves performance on these tasks, while still performing comparably on standard program synthesis benchmarks in comparison to left-to-right only models pretrained at similar scale. The InCoder models and code are publicly released. this https URL",
      "Organization categorization": "Industry,Academia,Academia,Academia,Academia",
      "Country (of organization)": "United States of America,France,United States of America,United States of America,United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Zero-shot infilling with bidirectional context substantially outperforms approaches based on left-to-right-only models, and on several tasks obtains performance comparable to state-of-the-art models fine-tuned on the tasks\"\n\nI don't see on which benchmarks they claim absolute SOTA",
      "Epochs": "1.0",
      "Training time (hours)": "576.0",
      "Training time notes": "24",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "3129.0771365574788",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open access (non-commercial)",
      "Accessibility notes": "CC-BY-NC 4.0 (non commercial)\n\ndata is open: \"To train our models, we collect a corpus of (1) public code with permissive, non-copyleft, opensource licenses from GitHub and GitLab and (2) StackOverflow questions, answers, and comments.\"\n\ninference code, not training code in this repo: https://github.com/dpfried/incoder/blob/main/README.md ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Segment Anything Model",
      "Organization": "Meta AI",
      "Publication date": "2023-04-05",
      "Domain": "Vision",
      "Task": "Image segmentation",
      "Parameters": "636000000.0",
      "Parameters notes": "From Facebook website: https://segment-anything.com/\n\"How big is the model? The image encoder has 632M parameters.\nThe prompt encoder and mask decoder have 4M parameters.\"",
      "Training compute (FLOP)": "7.8e+21",
      "Training compute notes": "\"SAM was trained on 256 A100 GPUS for 68 hours. We acknowledge the environmental impact and cost of training\nlarge scale models. The environmental impact of training the released SAM model is approximately 6963 kWh\"\n\n68*256 A100-hours = \n17408 hours * 3600 * 312 trillion * 0.4 (utilization assumption for image models)\n= 7.82e21\n\nmax A100 power is 400W. 6,963,000 watt-hours / 400 watts = 17407.5 hours (so they probably just calculated backwards from power rating, and this doesn't give any info on utilization)",
      "Training dataset": "Segment Anything 1B",
      "Training dataset size (gradients)": "1100000000",
      "Dataset size notes": "\"SA-1B contains 11M diverse, high-resolution, licensed, and privacy protecting images and 1.1B high-quality segmentation masks.\"\nsegmentation mask is a map that identifies segments in an image",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2304.02643",
      "Reference": "Segment Anything",
      "Citations": "10842.0",
      "Authors": "Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00e1r, Ross Girshick",
      "Abstract": "We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at this https URL to foster research into foundation models for computer vision.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,Historical significance",
      "Notability criteria notes": "",
      "Epochs": "2.0",
      "Training time (hours)": "68.0",
      "Training time notes": "\"SAM was trained on 256 A100 GPUS for 68 hours\"",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "256.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "15888.411228475235",
      "Compute cost notes": "",
      "Training power draw (W)": "204134.84223782964",
      "Base model": "ViT-Huge/14",
      "Finetune compute (FLOP)": "7.8e+21",
      "Finetune compute notes": "see Training Compute notes",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Apache 2.0 license\ndon't see pretrain code in the repo, could be wrong\n\nhttps://github.com/facebookresearch/segment-anything",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "BloombergGPT",
      "Organization": "Bloomberg,Johns Hopkins University",
      "Publication date": "2023-03-30",
      "Domain": "Language",
      "Task": "Language modeling,Language modeling/generation,Question answering,Financial management,Text classification",
      "Parameters": "50558868480.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "2.36e+23",
      "Training compute notes": "2.36e23 per Table 4\n\n(using our usual hardware method, 512 A100s over 53 days would be 512 * 312 teraFLOP/s * 53 * 24 * 3600 * 0.3 = 2.19e23)",
      "Training dataset": "",
      "Training dataset size (gradients)": "569000000000",
      "Dataset size notes": "708.9 billion tokens. At 0.75 English words per token, that's 532B words",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2303.17564",
      "Reference": "BloombergGPT: A Large Language Model for Finance",
      "Citations": "1112.0",
      "Authors": "Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, Gideon Mann",
      "Abstract": "The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. Additionally, we explain our modeling choices, training process, and evaluation methodology. We release Training Chronicles (Appendix C) detailing our experience in training BloombergGPT.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks.\"",
      "Epochs": "0.8",
      "Training time (hours)": "1270.0",
      "Training time notes": "\"~53 days\"",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "512.0",
      "Hardware utilization (MFU)": "0.246",
      "Training compute cost (2023 USD)": "369586.1352802876",
      "Compute cost notes": "",
      "Training power draw (W)": "408324.2395754059",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "4200000.0",
      "Batch size notes": "\"in the first 7,200 steps, we use a batch size of 1,024 (2.1M tokens), then switch to a batch size of 2,048 (4.2M tokens) for the remainder of training.\"",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "0.327",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "VideoMAE V2",
      "Organization": "Nanjing University,Shenzhen Institute of Advanced Technology,Shanghai AI Lab",
      "Publication date": "2023-03-29",
      "Domain": "Video",
      "Task": "Action recognition",
      "Parameters": "1000000000.0",
      "Parameters notes": "1B",
      "Training compute (FLOP)": "9.7e+21",
      "Training compute notes": "finetuned on ViT-g (smaller than ViT-G with 1B params)\n\n\"It takes more than two weeks to pre-train a ViT-g model with VideoMAE\non 64 A100 GPUs\"\n\n64 * 312 trillion * 2 * 7 * 24 * 3600 * 0.4 (utilization assumption) = 9.7e21",
      "Training dataset": "",
      "Training dataset size (gradients)": "1243350000",
      "Dataset size notes": "1.35 million video clips. Not sure about average length (34 seconds, but that's only reported for Instagram portion).\n\n\"In total, there are around 1.35M clips in our mixed dataset and\nthis is the largest dataset ever used for video masked autoencoding.",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2303.16727v2",
      "Reference": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
      "Citations": "519.0",
      "Authors": "Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, Yu Qiao",
      "Abstract": "Scale is the primary factor for building a powerful foundation model that could well generalize to a variety of downstream tasks. However, it is still challenging to train video foundation models with billions of parameters. This paper shows that video masked autoencoder (VideoMAE) is a scalable and general self-supervised pre-trainer for building video foundation models. We scale the VideoMAE in both model and data with a core design. Specifically, we present a dual masking strategy for efficient pre-training, with an encoder operating on a subset of video tokens and a decoder processing another subset of video tokens. Although VideoMAE is very efficient due to high masking ratio in encoder, masking decoder can still further reduce the overall computational cost. This enables the efficient pre-training of billion-level models in video. We also use a progressive training paradigm that involves an initial pre-training on a diverse multi-sourced unlabeled dataset, followed by a post-pre-training on a mixed labeled dataset. Finally, we successfully train a video ViT model with a billion parameters, which achieves a new state-of-the-art performance on the datasets of Kinetics (90.0% on K400 and 89.9% on K600) and Something-Something (68.7% on V1 and 77.0% on V2). In addition, we extensively verify the pre-trained video ViT models on a variety of downstream tasks, demonstrating its effectiveness as a general video representation learner. The code and model is available at \\url{this https URL}.",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "China,China,China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Finally, we successfully train a video ViT model with a billion parameters, which achieves a new state-of-the-art performance on the datasets of Kinetics (90.0% on K400 and 89.9% on K600) and Something-Something (68.7% on V1 and 77.0% on V2).\"",
      "Epochs": "1200.0",
      "Training time (hours)": "336.0",
      "Training time notes": "2 weeks",
      "Training hardware": "NVIDIA A100 SXM4 80 GB",
      "Hardware quantity": "64.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "18339.96928068276",
      "Compute cost notes": "",
      "Training power draw (W)": "51041.66660009344",
      "Base model": "ViT-G/14",
      "Finetune compute (FLOP)": "9.7e+21",
      "Finetune compute notes": "finetuned on ViT-g (smaller than ViT-G with 1B params)\n\n\"It takes more than two weeks to pre-train a ViT-g model with VideoMAE\non 64 A100 GPUs\"\n\n64 * 312 trillion * 2 * 7 * 24 * 3600 * 0.4 (utilization assumption) = 9.7e21\n",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT\nhttps://github.com/OpenGVLab/VideoMAEv2/blob/master/LICENSE",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "SigLIP 400M",
      "Organization": "Google DeepMind",
      "Publication date": "2023-03-27",
      "Domain": "Vision",
      "Task": "Image classification,Image embedding",
      "Parameters": "400000000.0",
      "Parameters notes": "Table 3",
      "Training compute (FLOP)": "4.9467301e+21",
      "Training compute notes": "Operation Counting: \n6ND = 6 FLOP / token / parameter*400*10^6 parameters * 6705000000000 tokens [see Dataset size notes] = 1.6092e+22 FLOP\n\nHardware:\n275000000000000 FLOP/s/GPU * 32 GPUs * 120 hours * 3600 sec / hour * 0.4 = 1.52064e+21 FLOPs\n\ngeometric mean (1.6092e+22, 1.52064e+21) = 4.9467301e+21",
      "Training dataset": "WebLI",
      "Training dataset size (gradients)": "6705000000000",
      "Dataset size notes": "\"B/16 ViT for image embeddings and B-sized transformer for text embeddings. The input images are resized to 224\u00d7224 resolution.\"\n\n\"SigLIP performs best at batch size 32 k [image-text pairs]\" \n\n729 patches (table 3)\n\n\"a maximum of 16 text tokens are kept\"\n\n9B examples * (16 text tokens + 729 image tokens) = 6.705e+12 total training tokens",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2303.15343",
      "Reference": "Sigmoid Loss for Language Image Pre-Training",
      "Citations": "",
      "Authors": "Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, Lucas Beyer",
      "Abstract": "We propose a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP). Unlike standard contrastive learning with softmax normalization, the sigmoid loss operates solely on image-text pairs and does not require a global view of the pairwise similarities for normalization. The sigmoid loss simultaneously allows further scaling up the batch size, while also performing better at smaller batch sizes. Combined with Locked-image Tuning, with only four TPUv4 chips, we train a SigLiT model that achieves 84.5% ImageNet zero-shot accuracy in two days. The disentanglement of the batch size from the loss further allows us to study the impact of examples vs pairs and negative to positive ratio. Finally, we push the batch size to the extreme, up to one million, and find that the benefits of growing batch size quickly diminish, with a more reasonable batch size of 32k being sufficient. We release our models at this https URL and hope our research motivates further explorations in improving the quality and efficiency of language-image pre-training.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Significant use",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "120.0",
      "Training time notes": "5 days = 120 hours",
      "Training hardware": "Google TPU v4",
      "Hardware quantity": "32.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "21693.674492506372",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "32000.0",
      "Batch size notes": "From Table 1: SigLit and SigLIP results",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache2 license\n\nhttps://github.com/google-research/big_vision\n\nhttps://colab.research.google.com/github/google-research/big_vision/blob/main/big_vision/configs/proj/image_text/SigLIP_demo.ipynb\n\ncode release is still pending as \"TODO\"",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Firefly",
      "Organization": "Adobe",
      "Publication date": "2023-03-21",
      "Domain": "Image generation",
      "Task": "Image generation,Text-to-image",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Adobe Stock",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://news.adobe.com/news/news-details/2023/Adobe-Unveils-Firefly-a-Family-of-new-Creative-Generative-AI/default.aspx",
      "Reference": "Adobe Unveils Firefly, a Family of new Creative Generative AI",
      "Citations": "",
      "Authors": "",
      "Abstract": "Today, Adobe (Nasdaq:ADBE) introduced Adobe Firefly, a new family of creative generative AI models, first focused on the generation of images and text effects. Adobe Firefly will bring even more precision, power, speed and ease directly into Creative Cloud, Document Cloud, Experience Cloud and Adobe Express workflows where content is created and modified. Adobe Firefly will be part of a series of new Adobe Sensei generative AI services across Adobe\u2019s clouds.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Significant use",
      "Notability criteria notes": "Integrated into Photoshop. Users generate >200m images within a few months of release:\n\nhttps://venturebeat.com/ai/adobe-stock-creators-arent-happy-with-firefly-the-companys-commercially-safe-gen-ai-tool/\n\nAs of October 2024, users have generated 13B images since March 2023. Paid users get 100 generations per month (and can continue at a slower rate after that). Assuming an average of 100 monthly generations per user, that's around 6.7M monthly average users across 19.5 months.",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Hosted access (no API)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Unreleased",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "PanGu-\u03a3",
      "Organization": "Huawei Noah's Ark Lab",
      "Publication date": "2023-03-20",
      "Domain": "Language",
      "Task": "Code generation,Language modeling,Translation,Question answering",
      "Parameters": "1085000000000.0",
      "Parameters notes": "\"In this work, we present PanGu-\u03a3 , a large language model with sparse architecture containing 1.085 trillion parameters.\"",
      "Training compute (FLOP)": "4.67e+23",
      "Training compute notes": "It has sparse architecture, so we can't use C=6ND.\n\"We develop PanGu-\u03a3 model under the framework of MindSpore and train it on a cluster with only 512 Ascend 910 AI Accelerators with 329 billion tokens over 100 days.\"\n100 days * 512 processors * 320 teraFLOPS/processor * 33% utilization = 4.67e+23 FLOP\nhttps://www.wolframalpha.com/input?i=100+days+*+512+*+320+terahertz+*+0.33",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "329000000000",
      "Dataset size notes": "329B tokens ~= 247B words",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2303.10845",
      "Reference": "PanGu-\u03a3: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing",
      "Citations": "78.0",
      "Authors": "Xiaozhe Ren, Pingyi Zhou, Xinfan Meng, Xinjing Huang, Yadao Wang, Weichao Wang, Pengfei Li, Xiaoda Zhang, Alexander Podolskiy, Grigory Arshinov, Andrey Bout, Irina Piontkovskaya, Jiansheng Wei, Xin Jiang, Teng Su, Qun Liu, Jun Yao",
      "Abstract": "The scaling of large language models has greatly improved natural language understanding, generation, and reasoning. In this work, we develop a system that trained a trillion-parameter language model on a cluster of Ascend 910 AI processors and MindSpore framework, and present the language model with 1.085T parameters named PanGu-{\\Sigma}. With parameter inherent from PanGu-{\\alpha}, we extend the dense Transformer model to sparse one with Random Routed Experts (RRE), and efficiently train the model over 329B tokens by using Expert Computation and Storage Separation(ECSS). This resulted in a 6.3x increase in training throughput through heterogeneous computing. Our experimental findings show that PanGu-{\\Sigma} provides state-of-the-art performance in zero-shot learning of various Chinese NLP downstream tasks. Moreover, it demonstrates strong abilities when fine-tuned in application data of open-domain dialogue, question answering, machine translation and code generation.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Our experimental findings show that PanGu-{\\Sigma} provides state-of-the-art performance in zero-shot learning of various Chinese NLP downstream tasks.\"\n\n\"The results on WMT20 translation task. PanGu-\u03a3 outperforms previous Chinese-English SOTA pre-trained\nlarge model with a large margin.\"\n\n\" The PanGu-\u03a3 outperforms the current state-of-the-art model PanGu-Coder by 1.4 point on the pass@1 for MBPP tasks\"\n\n\"Table 5: Zero-shot results of Chinese downstream tasks. Compared to ERNIE 3.0 Titan, PanGu-\u03a3 surpassed on 11 out of 16 datasets, with an average score of 3.96 points higher on all datasets.\"",
      "Epochs": "1.0",
      "Training time (hours)": "2400.0",
      "Training time notes": "We develop PanGu-\u03a3 model under the framework of MindSpore 5\nand train it on a cluster with only 512 Ascend 910 AI Accelerators [28] with 329 billion tokens over 100 days.",
      "Training hardware": "Huawei Ascend 910",
      "Hardware quantity": "512.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "316521.76523003745",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "524288.0",
      "Batch size notes": "\"We train PanGu-\u03a3 with global batch size of 512 with sequence length of 1024 for each sample\"",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Gen-2",
      "Organization": "Runway",
      "Publication date": "2023-03-20",
      "Domain": "Video",
      "Task": "Video generation,Text-to-video,Image-to-video,Video-to-video",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://research.runwayml.com/gen2",
      "Reference": "Gen-2: Generate novel videos with text, images or video clips",
      "Citations": "",
      "Authors": "Gen-2 authors",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Website claims SOTA improvement over Stable Diffusion and Text2Live, paper forthcoming\n\nThey don't report evaluations on standard benchmarks",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "LEP-AD",
      "Organization": "King Abdullah University of Science and Technology (KAUST),Karolinska Institute",
      "Publication date": "2023-03-15",
      "Domain": "Biology",
      "Task": "Proteins,Protein interaction prediction,Drug discovery,Protein-ligand binding affinity prediction",
      "Parameters": "3007381000.0",
      "Parameters notes": "Uses ESM-2 3B. Table 2 gives details on the non-ESM layers. The GCN appears to have about 3.31M parameters and the linear layers should have 771k and 3.3M, respectively. So total is ~3.007B",
      "Training compute (FLOP)": "",
      "Training compute notes": "No indication of the training used here. ESM-2 3B used 3e22.",
      "Training dataset": "",
      "Training dataset size (gradients)": "1244420",
      "Dataset size notes": "Largest dataset appears to be STITCH, at 1244420 drug-target pairs.",
      "Confidence": "Confident",
      "Link": "https://www.biorxiv.org/content/10.1101/2023.03.14.532563v1.full.pdf",
      "Reference": "LEP-AD: Language Embedding of Proteins and Attention to Drugs predicts Drug Target Interactions",
      "Citations": "1.0",
      "Authors": "Anuj Daga, Sumeer Ahmad Khan, David Gomez Cabrero, Robert Hoehndorf, Narsis A. Kiani, Jesper Tegner",
      "Abstract": "Predicting drug-target interactions is a tremendous challenge for drug development and lead optimization. Recent advances include training algorithms to learn drug-target interactions from data and molecular simulations. Here we utilize Evolutionary Scale Modeling (ESM-2) models to establish a Transformer protein language model for drug-target interaction predictions. Our architecture, LEPAD, combines pre-trained ESM-2 and Transformer-GCN models predicting binding affinity values. We report new best-in-class state-of-the-art results compared to competing methods such as SimBoost, DeepCPI, Attention-DTA, GraphDTA, and more using multiple datasets, including Davis, KIBA, DTC, Metz, ToxCast, and STITCH. Finally, we find that a pre-trained model with embedding of proteins (the LED-AD) outperforms a model using an explicit alpha-fold 3D representation of proteins (e.g., LEP-AD supervised by Alphafold). The LEP-AD model\nscales favorably in performance with the size of training data. Code available at https://github.com/adaga06/LEP-AD",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "Saudi Arabia,Sweden",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Tables 3-5\n\"We report new best-in-class state-of-the-art results compared\nto competing methods such as SimBoost, DeepCPI, Attention-DTA, GraphDTA, and more using multiple datasets, including Davis, KIBA, DTC, Metz, ToxCast, and STITCH. Finally, we find that a pre-trained model with embedding of proteins (the LED-AD) outperforms a model using an explicit alpha-fold 3D representation of proteins (e.g., LEP-AD supervised by Alphafold)\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "ESM2-3B",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "Open access (non-commercial)",
      "Accessibility notes": "https://github.com/adaga06/LEP-AD unclear license",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GPT-4 (Mar 2023)",
      "Organization": "OpenAI",
      "Publication date": "2023-03-15",
      "Domain": "Multimodal,Language,Vision",
      "Task": "Language modeling,Language modeling/generation,Question answering,Visual question answering",
      "Parameters": "1800000000000.0",
      "Parameters notes": "Rumored to be 1.8T parameter MoE with 280B activated on the forward pass, per https://www.semianalysis.com/p/gpt-4-architecture-infrastructure. Other sources estimate 1.76T with 220B per forward pass https://web.archive.org/web/20230712123915/https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/",
      "Training compute (FLOP)": "2.1e+25",
      "Training compute notes": "90% CI: 8.2E+24 to 4.4E+25\n\nNOTE: this is a rough estimate based on public information, much less information than most other systems in the database.\n\nCalculation and confidence intervals here: https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9?usp=sharing",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "5416666666666.667",
      "Dataset size notes": "Speculative. Reported secondhand by online sources such as Semianalysis, but not verified by OpenAI. If total number of tokens seen was 13T, text was repeated for 2 epochs, and text was the majority of tokens, then dataset size roughly is 13T*0.75/2 = 4.9T words.\n\nNote this examines only the text dataset, since GPT-4 was first and foremost a language model. However, the vision component had its own vision dataset, which we believe accounted for a much smaller part of the compute budget.",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2303.08774",
      "Reference": "GPT-4 Technical Report",
      "Citations": "20421.0",
      "Authors": "OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Sim\u00f3n Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain et al. (181 additional authors not shown)",
      "Abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,SOTA improvement,Training cost",
      "Notability criteria notes": "See the paper, p.1: \"On a suite of traditional NLP benchmarks, GPT-4 outperforms both previous large language models and most state-of-the-art systems (which often have benchmark-specific training or hand-engineering).\"\n\n\"On the MMLU benchmark [35, 36], an English-language suite of multiple-choice questions covering 57 subjects, GPT-4 not only outperforms existing models by a considerable margin in English, but also demonstrates strong performance in other languages\"",
      "Epochs": "2.0",
      "Training time (hours)": "2280.0",
      "Training time notes": "(Speculative) SemiAnalysis conjectures that GPT-4 training took 90-100 days with utilization of 32-36%.",
      "Training hardware": "NVIDIA A100 SXM4 40 GB",
      "Hardware quantity": "25000.0",
      "Hardware utilization (MFU)": "0.34",
      "Training compute cost (2023 USD)": "37334304.976625234",
      "Compute cost notes": "",
      "Training power draw (W)": "19944368.12599428",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "60000000.0",
      "Batch size notes": "not listed",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "305562116.88382745",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "81392748.68255694",
      "Training compute cost (upfront)": "806190976.8020828"
    },
    {
      "Model": "GPT-4 (Jun 2023)",
      "Organization": "OpenAI",
      "Publication date": "2023-03-15",
      "Domain": "Multimodal,Language,Vision",
      "Task": "Language modeling,Language modeling/generation,Question answering,Visual question answering",
      "Parameters": "1800000000000.0",
      "Parameters notes": "Rumored to be 1.8T parameter MoE with 280B activated on the forward pass, per https://www.semianalysis.com/p/gpt-4-architecture-infrastructure. Other sources estimate 1.76T with 220B per forward pass https://web.archive.org/web/20230712123915/https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/",
      "Training compute (FLOP)": "2.1e+25",
      "Training compute notes": "90% CI: 8.2E+24 to 4.4E+25\n\nNOTE: this is a rough estimate based on public information, much less information than most other systems in the database.\n\nCalculation and confidence intervals here: https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9?usp=sharing",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "5416666666666.667",
      "Dataset size notes": "Speculative. Reported secondhand by online sources such as Semianalysis, but not verified by OpenAI. If total number of tokens seen was 13T, text was repeated for 2 epochs, and text was the majority of tokens, then dataset size roughly is 13T*0.75/2 = 4.9T words.\n\nNote this examines only the text dataset, since GPT-4 was first and foremost a language model. However, the vision component had its own vision dataset, which we believe accounted for a much smaller part of the compute budget.",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2303.08774",
      "Reference": "GPT-4 Technical Report",
      "Citations": "20421.0",
      "Authors": "OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Sim\u00f3n Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain et al. (181 additional authors not shown)",
      "Abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,SOTA improvement,Training cost",
      "Notability criteria notes": "See the paper, p.1: \"On a suite of traditional NLP benchmarks, GPT-4 outperforms both previous large language models and most state-of-the-art systems (which often have benchmark-specific training or hand-engineering).\"\n\n\"On the MMLU benchmark [35, 36], an English-language suite of multiple-choice questions covering 57 subjects, GPT-4 not only outperforms existing models by a considerable margin in English, but also demonstrates strong performance in other languages\"",
      "Epochs": "2.0",
      "Training time (hours)": "2280.0",
      "Training time notes": "(Speculative) SemiAnalysis conjectures that GPT-4 training took 90-100 days with utilization of 32-36%.",
      "Training hardware": "NVIDIA A100 SXM4 40 GB",
      "Hardware quantity": "25000.0",
      "Hardware utilization (MFU)": "0.34",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "19944368.12599428",
      "Base model": "GPT-4 (Mar 2023)",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "60000000.0",
      "Batch size notes": "MoE so i hesitate but 7.5 mill? https://www.reddit.com/r/mlscaling/comments/14wcy7m/gpt4s_details_are_leaked/#:~:text=There%20is%20millions%20of%20rows,get%20the%20real%20batch%20size.",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "305562116.88382745",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Falcon-40B",
      "Organization": "Technology Innovation Institute",
      "Publication date": "2023-03-15",
      "Domain": "Language",
      "Task": "Language modeling,Language modeling/generation,Question answering",
      "Parameters": "40000000000.0",
      "Parameters notes": "Model comes in 7B and 40B variants.",
      "Training compute (FLOP)": "2.4e+23",
      "Training compute notes": "C = 6ND = 6 * 40B * 1000B = 2.4e+23 FLOP (assuming one epoch)\n\nTable 1 from https://arxiv.org/pdf/2311.16867 Falcon paper\n\n2,800 petaflop-days * 1e15 * 24 * 3600 = 2.4192e+23 FLOPs",
      "Training dataset": "RefinedWeb",
      "Training dataset size (gradients)": "1000000000000",
      "Dataset size notes": "1000B tokens ~= 750B words",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2311.16867; https://www.tii.ae/news/abu-dhabi-based-technology-innovation-institute-introduces-falcon-llm-foundational-large",
      "Reference": "Abu Dhabi-based Technology Innovation Institute Introduces Falcon LLM: Foundational Large Language Model (LLM) outperforms GPT-3 with 40 Billion Parameters",
      "Citations": "0.0",
      "Authors": "",
      "Abstract": "",
      "Organization categorization": "Government",
      "Country (of organization)": "United Arab Emirates",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "1440.0",
      "Training time notes": "\"Falcon-40B was trained on AWS SageMaker, on 384 A100 40GB GPUs in P4d instances.\"\n\"Training started in December 2022 and took two months.\"",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "384.0",
      "Hardware utilization (MFU)": "0.3864",
      "Training compute cost (2023 USD)": "319783.157242365",
      "Compute cost notes": "",
      "Training power draw (W)": "306345.49441527214",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "2359296.0",
      "Batch size notes": "Batch size 1152 (presumably sequences) per Table 16. Warmed up using smaller batches for first 100B tokens.\n\n\"All Falcon models are pretrained with a 2,048 sequence length\"\n\nhttps://arxiv.org/pdf/2311.16867.pdf\n",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "apache 2.0",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Claude",
      "Organization": "Anthropic",
      "Publication date": "2023-03-14",
      "Domain": "Language",
      "Task": "Language modeling,Chat,Language modeling/generation,Question answering",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://www.anthropic.com/index/introducing-claude",
      "Reference": "Introducing Claude",
      "Citations": "",
      "Authors": "",
      "Abstract": "Claude is a next-generation AI assistant based on Anthropic\u2019s research into training helpful, honest, and harmless AI systems. Accessible through chat interface and API in our developer console, Claude is capable of a wide variety of conversational and text processing tasks while maintaining a high degree of reliability and predictability.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance,SOTA improvement",
      "Notability criteria notes": "no standard benchmarks are cited",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "PaLM-E",
      "Organization": "Google,TU Berlin",
      "Publication date": "2023-03-06",
      "Domain": "Robotics,Vision,Language",
      "Task": "Visual question answering,Robotic manipulation,Image captioning,Language generation",
      "Parameters": "562000000000.0",
      "Parameters notes": "562B",
      "Training compute (FLOP)": "",
      "Training compute notes": "Based on Palm-540B and ViT-22B and then trained on robotics data.\n\n",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2303.03378",
      "Reference": "PaLM-E: An Embodied Multimodal Language Model",
      "Citations": "2191.0",
      "Authors": "Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, Pete Florence",
      "Abstract": "Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,Germany",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist\nwith state-of-the-art performance on OK-VQA, and retains\ngeneralist language capabilities with increasing scale.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "PaLM (540B)",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "Based on Palm-540B and ViT 22B. No compute details given.\n\n\"We scale PaLM-E up to 562B parameters, integrating the 540B PaLM (Chowdhery et al., 2022) LLM and the 22B Vision Transformer (ViT) (Dehghani et al., 2023) into, to our knowledge, the largest vision-language model currently reported.\"",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "AudioGen",
      "Organization": "Meta AI,Hebrew University of Jerusalem",
      "Publication date": "2023-03-05",
      "Domain": "Audio",
      "Task": "Audio generation",
      "Parameters": "1000000000.0",
      "Parameters notes": "\"We trained two sets of ALMs, one with 285M parameters (base) and the other with 1B parameters (large).\"",
      "Training compute (FLOP)": "9.5e+21",
      "Training compute notes": "\"the large model was trained on 128 A100 GPUs for 200k steps (\u223c1 week)\"\nA100s are 312 teraflop/s\n128 * 312 trillion * 7 * 24 * 3600 * 0.3 (utilization assumption) = 7.2e21\n\nText encoding uses T5-Large, which used 2.3e21 FLOP in pre-training per Flan paper: https://arxiv.org/abs/2210.11416 ",
      "Training dataset": "AudioSet,AudioCaps",
      "Training dataset size (gradients)": "230400000000",
      "Dataset size notes": "\"Overall we are left with \u223c4k hours for training data.\"\nmix of speech and other sounds\n\nTraining the audio autoencoder uses reconstruction loss on sequence of raw audio samples. Audio files are in 16kHz, so\n16k * 4k * 3600 = 230.4B samples\n\nAudio language modelling operates on tokens; \"each second of audio is represented by 500 tokens\". \n4k * 3600 * 500 = 7.2B tokens",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2209.15352",
      "Reference": "AudioGen: Textually Guided Audio Generation",
      "Citations": "388.0",
      "Authors": "Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre D\u00e9fossez, Jade Copet, Devi Parikh, Yaniv Taigman, Yossi Adi",
      "Abstract": "We tackle the problem of generating audio samples conditioned on descriptive text captions. In this work, we propose AaudioGen, an auto-regressive generative model that generates audio samples conditioned on text inputs. AudioGen operates on a learnt discrete audio representation. The task of text-to-audio generation poses multiple challenges. Due to the way audio travels through a medium, differentiating ``objects'' can be a difficult task (e.g., separating multiple people simultaneously speaking). This is further complicated by real-world recording conditions (e.g., background noise, reverberation, etc.). Scarce text annotations impose another constraint, limiting the ability to scale models. Finally, modeling high-fidelity audio requires encoding audio at high sampling rate, leading to extremely long sequences. To alleviate the aforementioned challenges we propose an augmentation technique that mixes different audio samples, driving the model to internally learn to separate multiple sources. We curated 10 datasets containing different types of audio and text annotations to handle the scarcity of text-audio data points. For faster inference, we explore the use of multi-stream modeling, allowing the use of shorter sequences while maintaining a similar bitrate and perceptual quality. We apply classifier-free guidance to improve adherence to text. Comparing to the evaluated baselines, AudioGen outperforms over both objective and subjective metrics. Finally, we explore the ability of the proposed method to generate audio continuation conditionally and unconditionally. Samples: this https URL",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,Israel",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We propose a state-of-the-art auto-regressive audio generation model conditioned on textual descriptions or audio prompts, as evaluated with objective and subjective (human listeners) scores.\"",
      "Epochs": "",
      "Training time (hours)": "168.0",
      "Training time notes": "1 week",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "9429.74091062958",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT license, but non-commercial for weights: https://github.com/facebookresearch/audiocraft/blob/main/LICENSE_weights\n\ntraining info: https://github.com/facebookresearch/audiocraft/blob/main/docs/AUDIOGEN.md ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DiT-XL/2",
      "Organization": "New York University (NYU),University of California (UC) Berkeley",
      "Publication date": "2023-03-02",
      "Domain": "Image generation",
      "Task": "Image generation",
      "Parameters": "675000000.0",
      "Parameters notes": "675M\n\nTable 4: \"Parameter and flop counts exclude the VAE model which contains 84M parameters across the encoder and decoder.\"\n675e6 not including VAE (likely frozen), 759e6 including VAE",
      "Training compute (FLOP)": "6e+20",
      "Training compute notes": "~6e20, based on eyeballing Figure 9. It's between 1e11 and 1e12 gigaflop (1 gigaflop = 1e9 flop), and about 80% of the way towards 1e12 on a log scale. 10^0.8 is about 6. \n\n3M iterations with a batch size of 256.\n\n\"Compute. We implement all models in JAX [1] and train\nthem using TPU-v3 pods. DiT-XL/2, our most computeintensive model, trains at roughly 5.7 iterations/second on a\nTPU v3-256 pod with a global batch size of 256\"\n256*123000000000000 FLOPs/s * 800000 training steps / 5.7 iterations/second * 0.3 = 1.3258105e+21",
      "Training dataset": "ImageNet",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "didn't state which ImageNet set",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2212.09748",
      "Reference": "Scalable Diffusion Models with Transformers",
      "Citations": "4096.0",
      "Authors": "William Peebles, Saining Xie",
      "Abstract": "We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops -- through increased transformer depth/width or increased number of input tokens -- consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512x512 and 256x256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"our largest DiT-XL/2 models outperform all prior diffusion models on the classconditional ImageNet 512\u00d7512 and 256\u00d7256 benchmarks,\nachieving a state-of-the-art FID of 2.27 on the latter.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "111048.19613085664",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "Stable Diffusion (LDM-KL-8-G)",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "",
      "Accessibility notes": "CC-BY-NC\nhttps://github.com/facebookresearch/DiT?tab=readme-ov-file\n\nhttps://www.wpeebles.com/DiT.html",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "LLaMA-65B",
      "Organization": "Meta AI",
      "Publication date": "2023-02-24",
      "Domain": "Language",
      "Task": "Language modeling,Code generation",
      "Parameters": "65200000000.0",
      "Parameters notes": "Model card, table 1: https://github.com/facebookresearch/llama/blob/53011c3d7946dadb8274a4c5c7586ab54edf792d/MODEL_CARD.md",
      "Training compute (FLOP)": "5.5e+23",
      "Training compute notes": "1.4e12 tokens * 6.52e10 parameters * 6 FLOP/token/parameter = 5.5e23 FLOP",
      "Training dataset": "CCNet,GitHub,Wikipedia,books,arXiv,Stack Exchange",
      "Training dataset size (gradients)": "1400000000000",
      "Dataset size notes": "Table 1 indicates that 1.4T tokens involved sampling sub-datasets at more or less than one epoch. Correcting for this:\n\n(1.1 epoch * 3.3TB) + (1.06 epoch * 0.783TB) + ... = 1.4T tokens\n5.24 epoch-TBs = 1.4T tokens\n5.24 epoch-TB * 1000 GB/TB * 200M token/GB = 1.4T tokens\n1.05T epoch*token = 1.4T tokens\n1 epoch = 1.34T tokens\n",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2302.13971",
      "Reference": "LLaMA: Open and Efficient Foundation Language Models",
      "Citations": "17466.0",
      "Authors": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample",
      "Abstract": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance,Highly cited",
      "Notability criteria notes": "Widely-used foundation model that has been adapted for others such as Alpaca.",
      "Epochs": "1.09",
      "Training time (hours)": "500.0",
      "Training time notes": "\"When training a 65B-parameter model, our code processes around 380 tokens/sec/GPU on 2048 A100 GPU with 80GB of RAM. This means that training over our dataset containing 1.4T tokens takes approximately 21 days.\"",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "2048.0",
      "Hardware utilization (MFU)": "0.4746",
      "Training compute cost (2023 USD)": "578026.3043",
      "Compute cost notes": "1023384 processor-hours on A100 GPUs. May 2023 cost rate is $1.36/GPU-hour on Azure ML cloud. https://azure.microsoft.com/en-us/pricing/details/machine-learning/ \nAccording to https://www.bls.gov/data/inflation_calculator.htm, $1.18 in May 2023 = $1.00 in January 2020.\n$1391674 / 1.18 = $1179385 in 2020 USD.",
      "Training power draw (W)": "1634534.091472035",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "4000000.0",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "\"we are releasing our model under a noncommercial license focused on research use cases\" https://ai.meta.com/blog/large-language-model-llama-meta-ai/",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "BASIC-L + Lion",
      "Organization": "Google,University of California Los Angeles (UCLA)",
      "Publication date": "2023-02-13",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "3070000000.0",
      "Parameters notes": "parameter count of original BASIC-L",
      "Training compute (FLOP)": "",
      "Training compute notes": "This model is BASIC-L retrained with a different optimizer, Lion. Lion seems more compute-efficient, so we should expect compute to be less than BASIC-L.",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2302.06675v4",
      "Reference": "Symbolic Discovery of Optimization Algorithms",
      "Citations": "509.0",
      "Authors": "Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, Quoc V. Le",
      "Abstract": "We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, Lion (EvoLved Sign Momentum). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of ViT by up to 2% on ImageNet and saves up to 5x the pre-training compute on JFT. On vision-language contrastive learning, we achieve 88.3% zero-shot and 91.1% fine-tuning accuracy on ImageNet, surpassing the previous best results by 2% and 0.1%, respectively. On diffusion models, Lion outperforms Adam by achieving a better FID score and reducing the training compute by up to 2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion exhibits a similar or better performance compared to Adam. Our analysis of Lion reveals that its performance gain grows with the training batch size. It also requires a smaller learning rate than Adam due to the larger norm of the update produced by the sign function. Additionally, we examine the limitations of Lion and identify scenarios where its improvements are small or not statistically significant. Lion is also successfully deployed in production systems such as Google search ads CTR model.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"On vision-language contrastive learning, we achieve 88.3% zero-shot and 91.1% fine-tuning accuracy on ImageNet, surpassing the previous best results by 2% and 0.1%, respectively\"\n\n\"On image classification, Lion boosts the accuracy of ViT by up to 2% on ImageNet and saves up to 5x the pre-training compute on JFT.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "",
      "Accessibility notes": "Apache 2.0\nhttps://github.com/google/automl/tree/master/lion",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ViT-22B",
      "Organization": "Google",
      "Publication date": "2023-02-10",
      "Domain": "Vision",
      "Task": "Object detection,Image classification",
      "Parameters": "21743000000.0",
      "Parameters notes": "21.743B, Table 1",
      "Training compute (FLOP)": "1.93248e+23",
      "Training compute notes": "\"ViT-22B was trained using 256 visual tokens per image, where each token represents a 14 \u00d7 14 patch extracted from 224 \u00d7 224 sized images. ViT-22B is trained for 177k steps with batch size of 65k: approximately 3 epochs\"\n\n\"ViT-22B was trained on 1024 TPU V4 chips for 177K steps\"\n\n\"Using these techniques, ViT-22B processes 1.15k tokens per second per core during training (forward and backward pass) on TPUv4 [...] ViT-22B\u2019s model flops utilization (MFU) is 54.9%\"\n\n256 * 177k * 65k = 2.945T tokens\n\nSo training time is 2.945T tokens / (1.15k * 2 * 1024) tokens/s = 1.25M seconds = 347.4 hours\n\nSo 1024 TPUv4 chips for 1.25M seconds at 54.9% MFU:\n1024 * 2.75e14 * 1.25M * 0.549 = 1.93248e23",
      "Training dataset": "JFT-4B",
      "Training dataset size (gradients)": "4000000000",
      "Dataset size notes": "\"Dataset. ViT-22B is trained on a version of JFT (Sun et al., 2017), extended to around 4B images (Zhai et al.,\n2022a). These images have been semi-automatically annotated with a class-hierarchy of 30k labels\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2302.05442v1",
      "Reference": "Scaling Vision Transformers to 22 Billion Parameters",
      "Citations": "757.0",
      "Authors": "Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin F. Elsayed, Aravindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Patrick Collier, Alexey Gritsenko, Vighnesh Birodkar, Cristina Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Paveti\u0107, Dustin Tran, Thomas Kipf, Mario Lu\u010di\u0107, Xiaohua Zhai, Daniel Keysers, Jeremiah Harmsen, Neil Houlsby",
      "Abstract": "The scaling of Transformers has driven breakthrough capabilities for language models. At present, the largest large language models (LLMs) contain upwards of 100B parameters. Vision Transformers (ViT) have introduced the same architecture to image and video modelling, but these have not yet been successfully scaled to nearly the same degree; the largest dense ViT contains 4B parameters (Chen et al., 2022). We present a recipe for highly efficient and stable training of a 22B-parameter ViT (ViT-22B) and perform a wide variety of experiments on the resulting model. When evaluated on downstream tasks (often with a lightweight linear model on frozen features), ViT-22B demonstrates increasing performance with scale. We further observe other interesting benefits of scale, including an improved tradeoff between fairness and performance, state-of-the-art alignment to human visual perception in terms of shape/texture bias, and improved robustness. ViT-22B demonstrates the potential for \"LLM-like\" scaling in vision, and provides key steps towards getting there.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"The largest ViT-22B sets the new SOTA on the challenging ObjectNet test set\"",
      "Epochs": "2.9",
      "Training time (hours)": "347.4",
      "Training time notes": "\"Using these techniques, ViT-22B processes 1.15k tokens per second per core during training (forward and backward pass)\"\nFrom model card we know they trained with 1024 TPUv4 chips, and there are 2 cores per chip. Total number of tokens was 177K steps * 65k images/step * 256 tokens/image = 2.945T tokens\n\nSo training time is 2.945T tokens / (1.15k * 2 * 1024) tokens/s = 1.25M seconds = 347.4 hours",
      "Training hardware": "Google TPU v4",
      "Hardware quantity": "1024.0",
      "Hardware utilization (MFU)": "0.549",
      "Training compute cost (2023 USD)": "285555.57016183576",
      "Compute cost notes": "",
      "Training power draw (W)": "694893.6029178143",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "don't see it here: https://github.com/google-research/vision_transformer?tab=readme-ov-file#available-vit-models ",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ProteinDT",
      "Organization": "University of California (UC) Berkeley,California Institute of Technology,University of Toronto,University of Wisconsin Madison,Texas A&M,NVIDIA,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms)",
      "Publication date": "2023-02-09",
      "Domain": "Biology,Language",
      "Task": "Proteins,Protein representation learning,Protein generation,Protein design",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "UniProtKB",
      "Training dataset size (gradients)": "132303900",
      "Dataset size notes": "Total amino acids: 197,000,000 residues\n\nFinal calculation: 1.97 \u00d7 10\u2078 datapoints\n\nValue = 197,000,000 = 1.97e8",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2302.04611",
      "Reference": "A Text-guided Protein Design Framework",
      "Citations": "",
      "Authors": "Shengchao Liu, Yanjing Li, Zhuoxinran Li, Anthony Gitter, Yutao Zhu, Jiarui Lu, Zhao Xu, Weili Nie, Arvind Ramanathan, Chaowei Xiao, Jian Tang, Hongyu Guo, and Anima Anandkumar",
      "Abstract": "Current AI-assisted protein design mainly utilizes protein sequential and structural information. Meanwhile, there exists tremendous knowledge curated by humans in the text format describing proteins\u2019 high-level functionalities. Yet, whether the incorporation of such text data can help protein design tasks has not been explored. To bridge this gap, we propose ProteinDT, a multi-modal framework that leverages textual descriptions for protein design. ProteinDT consists of three subsequent steps: ProteinCLAP which aligns the representation of two modalities, a facilitator that generates the protein representation from the text modality, and a decoder that creates the protein sequences from the representation. To train ProteinDT, we construct a large dataset, SwissProtCLAP, with 441K text and protein pairs. We quantitatively verify the effectiveness of ProteinDT on three challenging tasks: (1) over 90% accuracy for text-guided protein generation; (2) best hit ratio on 10 zero-shot text-guided protein editing tasks; (3) superior performance on four out of six protein property prediction benchmarks.",
      "Organization categorization": "Academia,Academia,Academia,Academia,Academia,Industry,Academia",
      "Country (of organization)": "United States of America,United States of America,Canada,United States of America,United States of America,United States of America,Canada",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Compared to six state-of-the-art protein sequence representation methods, ProteinDT can obtain consistently superior performance on four of six benchmark tasks.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "SciBERT",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "",
      "Accessibility notes": "MIT license\nhttps://github.com/chao1224/ProteinDT",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Gen-1",
      "Organization": "Runway",
      "Publication date": "2023-02-06",
      "Domain": "Video",
      "Task": "Video generation,Text-to-video",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/2302.03011",
      "Reference": "Structure and Content-Guided Video Synthesis with Diffusion Models",
      "Citations": "656.0",
      "Authors": "Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, Anastasis Germanidis",
      "Abstract": "Text-guided generative diffusion models unlock powerful image creation and editing tools. While these have been extended to video generation, current approaches that edit the content of existing footage while retaining structure require expensive re-training for every input or rely on error-prone propagation of image edits across frames. In this work, we present a structure and content-guided video diffusion model that edits videos based on visual or textual descriptions of the desired output. Conflicts between user-provided content edits and structure representations occur due to insufficient disentanglement between the two aspects. As a solution, we show that training on monocular depth estimates with varying levels of detail provides control over structure and content fidelity. Our model is trained jointly on images and videos which also exposes explicit control of temporal consistency through a novel guidance method. Our experiments demonstrate a wide variety of successes; fine-grained control over output characteristics, customization based on a few reference images, and a strong user preference towards results by our model.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "I don't see on which standard benchmarks they claim SOTA results",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "BLIP-2 (Q-Former)",
      "Organization": "Salesforce Research",
      "Publication date": "2023-01-30",
      "Domain": "Vision,Language",
      "Task": "Visual question answering,Image captioning",
      "Parameters": "1480000000.0",
      "Parameters notes": "Q-Former has 188M params. The BLIP-2 system overall has \"54x fewer trainable parameters\" than Flamingo80B.",
      "Training compute (FLOP)": "1.20000000001e+21",
      "Training compute notes": "https://www.wolframalpha.com/input?i=312+teraFLOPS+*+16+*+200+hours+*+0.33",
      "Training dataset": "COCO,LAION-400M,Conceptual Captions (CC3M),Conceptual Captions 12M (CC12M),VisualGenome,SBU",
      "Training dataset size (gradients)": "2322000000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2301.12597",
      "Reference": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
      "Citations": "6512.0",
      "Authors": "Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi",
      "Abstract": "The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"BLIP-2 achieves state-of-the-art performance on various vision-language tasks\"\n\n\"our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters\"",
      "Epochs": "",
      "Training time (hours)": "200.0",
      "Training time notes": "\"For example, using\na single 16-A100(40G) machine, our largest model with\nViT-g and FlanT5-XXL requires less than 6 days for the first\nstage and less than 3 days for the second stage.\"\n9 days = 216 hours",
      "Training hardware": "NVIDIA A100 SXM4 40 GB",
      "Hardware quantity": "16.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "1960.8225382725811",
      "Compute cost notes": "",
      "Training power draw (W)": "12776.908953102382",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "https://github.com/salesforce/LAVIS/tree/main/projects/blip2 includes training and inference code\n\nmodels here: https://huggingface.co/models?other=blip-2 ",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "201843.66394344394",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DDPM-IP (CelebA)",
      "Organization": "Utrecht University",
      "Publication date": "2023-01-27",
      "Domain": "Image generation",
      "Task": "Image generation,Text-to-image",
      "Parameters": "295000000.0",
      "Parameters notes": "295M for CelebA model, per Table 9",
      "Training compute (FLOP)": "3.5e+20",
      "Training compute notes": "\"We use Pytorch 1.8 (Paszke et al., 2019) and trained all the models on different NVIDIA Tesla V100s (16G memory). In\nmore detail, we use 2 GPUs to train the models on CIFAR10 for 2 days, and 4 GPUs to train the models on ImageNet 32\u00d732\nfor 34 days. For LSUN tower 64\u00d764, CelebA 64\u00d764 and FFHQ 128\u00d7128, we used 16 GPUs to train the models for 3 days,\n5 days and 4 days, respectively\"\n\n5*16 V100-days for CelebA.\n\n5 * 16 * 24 * 3600 * 125 teraflops * 0.4 ~= 3.5e20",
      "Training dataset": "CelebA",
      "Training dataset size (gradients)": "831488000",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2301.11706v3",
      "Reference": "Input Perturbation Reduces Exposure Bias in Diffusion Models",
      "Citations": "87.0",
      "Authors": "Mang Ning, Enver Sangineto, Angelo Porrello, Simone Calderara, Rita Cucchiara",
      "Abstract": "Denoising Diffusion Probabilistic Models have shown an impressive generation quality, although their long sampling chain leads to high computational costs. In this paper, we observe that a long sampling chain also leads to an error accumulation phenomenon, which is similar to the exposure bias problem in autoregressive text generation. Specifically, we note that there is a discrepancy between training and testing, since the former is conditioned on the ground truth samples, while the latter is conditioned on the previously generated results. To alleviate this problem, we propose a very simple but effective training regularization, consisting in perturbing the ground truth samples to simulate the inference time prediction errors. We empirically show that, without affecting the recall and precision, the proposed input perturbation leads to a significant improvement in the sample quality while reducing both the training and the inference times. For instance, on CelebA 64\u00d764, we achieve a new state-of-the-art FID score of 1.27, while saving 37.5% of the training time. The code is publicly available at this https URL",
      "Organization categorization": "Academia",
      "Country (of organization)": "Netherlands",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"For instance, on CelebA 64\u00d764, we achieve a new state-of-theart FID score of 1.27, while saving 37.5% of the training time\"",
      "Epochs": "681.0",
      "Training time (hours)": "120.0",
      "Training time notes": "5 days",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "390.4861317667304",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "",
      "Accessibility notes": "MIT license",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "MusicLM",
      "Organization": "Google",
      "Publication date": "2023-01-26",
      "Domain": "Audio",
      "Task": "Audio generation",
      "Parameters": "860000000.0",
      "Parameters notes": "\"We use decoder-only Transformers for modeling the semantic stage and the acoustic stages of AudioLM. The models\nshare the same architecture, composed of 24 layers, 16 attention heads, an embedding dimension of 1024, feed-forward\nlayers of dimensionality 4096, dropout of 0.1, and relative\npositional embeddings (Raffel et al., 2020), resulting in\n430M parameters per stage.\"\n\n\"stage\" seems to mean semantic + acoustic, so 860M total",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Free Music Archive",
      "Training dataset size (gradients)": "",
      "Dataset size notes": ">280k hours",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2301.11325",
      "Reference": "MusicLM: Generating Music From Text",
      "Citations": "590.0",
      "Authors": "Andrea Agostinelli, Timo I. Denk, Zal\u00e1n Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, Matt Sharifi, Neil Zeghidour, Christian Frank",
      "Abstract": "We introduce MusicLM, a model generating high-fidelity music from text descriptions such as \"a calming violin melody backed by a distorted guitar riff\". MusicLM casts the process of conditional music generation as a hierarchical sequence-to-sequence modeling task, and it generates music at 24 kHz that remains consistent over several minutes. Our experiments show that MusicLM outperforms previous systems both in audio quality and adherence to the text description. Moreover, we demonstrate that MusicLM can be conditioned on both text and a melody in that it can transform whistled and hummed melodies according to the style described in a text caption. To support future research, we publicly release MusicCaps, a dataset composed of 5.5k music-text pairs, with rich text descriptions provided by human experts.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We demonstrate that our method outperforms baselines on MusicCaps, a hand-curated, high-quality dataset of 5.5k music-text pairs prepared by musicians.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "W2v-BERT",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "also MuLan and SoundStream",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Ankh_large",
      "Organization": "Technical University of Munich,Columbia University",
      "Publication date": "2023-01-16",
      "Domain": "Biology",
      "Task": "Protein generation,Proteins,Protein or nucleotide language model (pLM/nLM),Protein contact and distance prediction,Protein classification,Protein localization prediction,Protein fold classification",
      "Parameters": "1900000000.0",
      "Parameters notes": "Figure 1 indicates 1.15B parameters, but both the huggingface model and a replication (https://huggingface.co/ElnaggarLab/ankh-large and https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1.full.pdf) indicate 1.9B parameters.\nNotebook for counting params: https://colab.research.google.com/drive/1EGI5_vDl4pOBUukJexMHQR16BFKJe4a5?usp=sharing",
      "Training compute (FLOP)": "6.5e+21",
      "Training compute notes": "Table 9 from here: https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1.full.pdf\n\nCan also be manually estimated based on the details in Table 11 and 4.6.1 Exp 4. 14B residues * 68 epochs = 952B tokens seen in forward passes. However, only 20% of tokens are masked as individual targets; other tokens in consecutive spans are collapsed into single-token targets to reduce computations. For masking rate of 20%, the average sequence will have 36% as many targets as input tokens under this strategy. This is the relevant number of backward passes:\n(2 * 952B * 19B) + (4 * 952B * 0.36 * 19B) = 6.22e22\n\n36% figure verified here: https://colab.research.google.com/drive/1ETsmp_KRMK8kIRA5kdfcO9QiPK28cBQ6?usp=sharing ",
      "Training dataset": "UniRef50",
      "Training dataset size (gradients)": "14000000000",
      "Dataset size notes": "Pretrained over UniRef50; 45M proteins and 14B amino acids, per Table 2\n\n952B tokens from Table 9 at:\nhttps://www.biorxiv.org/content/10.1101/2023.07.05.547496v1\n(This is total tokens over multiple epochs)",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2301.06568",
      "Reference": "Ankh: Optimized Protein Language Model Unlocks General-Purpose Modelling",
      "Citations": "143.0",
      "Authors": "Ahmed Elnaggar, Hazem Essam, Wafaa Salah-Eldin, Walid Moustafa, Mohamed Elkerdawy, Charlotte Rochereau, Burkhard Rost",
      "Abstract": "As opposed to scaling-up protein language models (PLMs), we seek improving performance via protein-specific optimization. Although the proportionality between the language model size and the richness of its learned representations is validated, we prioritize accessibility and pursue a path of data-efficient, cost-reduced, and knowledge-guided optimization. Through over twenty experiments ranging from masking, architecture, and pre-training data, we derive insights from protein-specific experimentation into building a model that interprets the language of life, optimally. We present Ankh, the first general-purpose PLM trained on Google\u2019s TPU-v4 surpassing the state-of-the-art performance with fewer parameters (<10% for pre-training, <7% for inference, and <30% for the embedding dimension). We provide a representative range of structure and function benchmarks where Ankh excels. We further provide a protein variant generation analysis on High-N and One-N input data scales where Ankh succeeds in learning protein evolutionary conservation-mutation trends and introducing functional diversity while retaining key structural-functional characteristics. We dedicate our work to promoting accessibility to research innovation via attainable resources.",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "Germany,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"On average, Ankh improved the PLM SOTA performance by 4.8%\"\n\nTable 1",
      "Epochs": "68.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "Google TPU v4",
      "Hardware quantity": "64.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "4802.398249072418",
      "Compute cost notes": "",
      "Training power draw (W)": "43455.036357563025",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "524288.0",
      "Batch size notes": "Table 11",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open access (non-commercial)",
      "Accessibility notes": "cc non-commercial:\nhttps://github.com/agemagician/Ankh/blob/main/LICENSE.md\n\ncc-by-nc for weigths:\nhttps://huggingface.co/ElnaggarLab/ankh-large\n",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Nucleotide Transformer",
      "Organization": "NVIDIA,Technical University of Munich,InstaDeep",
      "Publication date": "2023-01-15",
      "Domain": "Biology",
      "Task": "Protein or nucleotide language model (pLM/nLM),Nucleotide generation",
      "Parameters": "2500000000.0",
      "Parameters notes": "\"We built four distinct foundation language models of different sizes, ranging from 500M up to 2.5B parameters\"",
      "Training compute (FLOP)": "8.08e+21",
      "Training compute notes": "\"Training the largest parameter model required a total of 128 GPUs across 16 compute nodes for 28 days\"\n\nIn repo, they default to jnp.float32, but recommend fp16 or bp16 for activations in the docstring. JAX defaults to TF32 so they should be utilizing tensor cores.\n\nAssuming 1.56e14 FLOP/s for 32-bit calculations and 0.3 utilization rate\n\nEstimate: 1.56e14 FLOP/s * 128 GPUs * 28 days * 24 h/day * 3600 s/h * 0.3 utilization rate = 1.45e22\n\nOr, with 6ND:\n\"the model processed a total of 300B tokens during training\"\n6 * 300B * 2.5B = 4.5e21\n\nGeometric mean: sqrt(1.45e22 * 4.5e21) = 8.08e21",
      "Training dataset": "Human Reference Genome (GRCh38/hg38),1000 Genomes Project",
      "Training dataset size (gradients)": "300000000000",
      "Dataset size notes": "Largest dataset is the 1000 Genome dataset, with 3202 genomes for a total of 20.5 trillion nucleotides. However, for each dataset training was run until the model saw 300B tokens.",
      "Confidence": "Likely",
      "Link": "https://www.biorxiv.org/content/10.1101/2023.01.11.523679v1.full.pdf",
      "Reference": "The Nucleotide Transformer: Building and Evaluating Robust\nFoundation Models for Human Genomics",
      "Citations": "22.0",
      "Authors": "Hugo Dalla-Torre, Liam Gonzalez, Javier Mendoza Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, Evan Trop, Hassan Sirelkhatim, Guillaume Richard, Marcin Skwark, Karim Beguir,\nMarie Lopez, Thomas Pierrot",
      "Abstract": "Closing the gap between measurable genetic information and observable traits is a longstanding challenge in genomics. Yet, the prediction of molecular phenotypes from DNA sequences alone remains limited and inaccurate, often driven by the scarcity of annotated data and the inability to transfer learnings between prediction tasks. Here, we present an extensive study of foundation models pre-trained on DNA sequences, named the Nucleotide Transformer, integrating information from 3,202 diverse human genomes, as well as 850 genomes from a wide range of species, including model and non-model organisms. These transformer models yield transferable, context-specific representations of nucleotide sequences, which allow for accurate molecular phenotype prediction even in low-data settings. We show that the representations alone match or outperform specialized methods on 11 of 18 prediction tasks, and up to 15 after fine-tuning. Despite no supervision, the transformer models learnt to focus attention on key genomic elements, including those that regulate gene expression, such as enhancers. Lastly, we demonstrate that utilizing model representations alone can improve the prioritization of functional genetic variants. The training and application of foundational models in genomics explored in this study provide a widely applicable stepping stone to bridge the gap of accurate molecular phenotype prediction from DNA sequence alone.\n",
      "Organization categorization": "Industry,Academia,Industry",
      "Country (of organization)": "United States of America,Germany,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We show that the representations alone match or outperform specialized methods on 11 of 18 prediction tasks, and up to 15 after fine-tuning.\"",
      "Epochs": "1.0",
      "Training time (hours)": "672.0",
      "Training time notes": "\"Training the largest parameter model required a total of 128 GPUs across 16 compute nodes for 28 days\"",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "128.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "51064.70621394041",
      "Compute cost notes": "",
      "Training power draw (W)": "102249.42137571001",
      "Base model": "",
      "Finetune compute (FLOP)": "9.35e+17",
      "Finetune compute notes": "\"All fine-tuning runs were performed on a single node with eight A100 GPUs. [...] On average, a fine-tuning run lasted 20 minutes for the 500M parameter models, and 50 minutes for the 2.5B parameter models.\"\n\nEstimate: 78e12 FLOP/s * 8 GPUs * 50 min * 60 seconds * 0.5 utilization rate = 9.36e17",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Attribution-NonCommercial-ShareAlike 4.0 International license\nhttps://github.com/instadeepai/nucleotide-transformer\n\nIn this repository, you will find the following:\n\nInference code for our models\nPre-trained weights for all 9 NT models and 2 SegmentNT models\nInstructions for using the code and pre-trained models",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DreamerV3",
      "Organization": "DeepMind,University of Toronto",
      "Publication date": "2023-01-10",
      "Domain": "Games",
      "Task": "Open ended play",
      "Parameters": "200000000.0",
      "Parameters notes": "Table B1",
      "Training compute (FLOP)": "2.2032e+20",
      "Training compute notes": "16 environment instances, each with 1 V100 running for 17 days (table A1) - it's not entirely clear if the GPU days already account for multiple environment instances.\n\nAssuming no:\nCompute: 17*24*60*60*125000000000000*0.3=5.508e+19\nAssuming yes:\nCompute: 17*24*60*60*16*125000000000000*0.3=8.8128e+20\n\nGeometric mean: 220320000000000000000",
      "Training dataset": "",
      "Training dataset size (gradients)": "1600000000",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2301.04104v1",
      "Reference": "Mastering Diverse Domains through World Models",
      "Citations": "834.0",
      "Authors": "Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, Timothy Lillicrap ",
      "Abstract": "General intelligence requires solving tasks across many domains. Current reinforcement learning algorithms carry this potential but are held back by the resources and knowledge required to tune them for new tasks. We present DreamerV3, a general and scalable algorithm based on world models that outperforms previous approaches across a wide range of domains with fixed hyperparameters. These domains include continuous and discrete actions, visual and low-dimensional inputs, 2D and 3D worlds, different data budgets, reward frequencies, and reward scales. We observe favorable scaling properties of DreamerV3, with larger models directly translating to higher data-efficiency and final performance. Applied out of the box, DreamerV3 is the first algorithm to collect diamonds in Minecraft from scratch without human data or curricula, a long-standing challenge in artificial intelligence. Our general algorithm makes reinforcement learning broadly applicable and allows scaling to hard decision making problems",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland,Canada",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Using the same hyperparameters across all domains, DreamerV3 outperforms specialized model-free and model-based algorithms in a wide range of benchmarks and data-efficiency regimes. Applied out of the box, DreamerV3 also learns to obtain diamonds in the popular video game Minecraft from scratch given sparse rewards, a long-standing challenge in artificial intelligence for which previous approaches required human data or domain-specific heuristics.",
      "Epochs": "",
      "Training time (hours)": "6528.0",
      "Training time notes": "",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "16.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "9586.950671364422",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "",
      "Accessibility notes": "Apache 2.0\nhttps://github.com/danijar/dreamerv3",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "VALL-E",
      "Organization": "Microsoft",
      "Publication date": "2023-01-05",
      "Domain": "Audio,Speech",
      "Task": "Speech synthesis,Text-to-speech (TTS)",
      "Parameters": "353000000.0",
      "Parameters notes": "\"Both the AR model and the NAR model have the same transformer architecture with 12\nlayers, 16 attention heads, an embedding dimension of 1024, a feed-forward layer dimension of 4096, and a dropout of 0.1\"\n\nBen's script says that's 353M parameters, using n_block 12, d_model 1024, d_ff 4096, encoder only False\n\nhttps://github.com/bencottier/ml-parameter-count/blob/main/parameter_count.py",
      "Training compute (FLOP)": "1.01e+19",
      "Training compute notes": "\"The models are trained using 16 NVIDIA TESLA V100 32GB GPUs with a batch size of 6k acoustic\ntokens per GPU for 800k steps\"\n\n353M * 800k * 6k * 6 = 1.01e19\n\n16 V100s is 2080 teraFLOP or 2e15 FLOP so 1e19 would take 1.5 hours at 100% utilization or ~5 hours at 30%. Is that plausible?",
      "Training dataset": "LibriLight",
      "Training dataset size (gradients)": "76800000000",
      "Dataset size notes": "60k hours\n~13,680 words/hour * 60,000 = 820800000 words\nhttps://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.3pbt0hfgv7pq",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/2301.02111",
      "Reference": "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers",
      "Citations": "998.0",
      "Authors": "Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, Furu Wei",
      "Abstract": "We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called Vall-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. Vall-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. Experiment results show that Vall-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find Vall-E could preserve the speaker's emotion and acoustic environment of the acoustic prompt in synthesis. See this https URL for demos of our work.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"VALL-E significantly outperforms the state-of-the-art zero-shot TTS system [Casanova et al., 2022b] in terms of speech naturalness and speaker similarity, with +0.12 comparative mean option score (CMOS) and +0.93 similarity mean option score (SMOS) improvement on LibriSpeech\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "11.40575196486363",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Unreleased",
      "Accessibility notes": "onlly demos https://www.microsoft.com/en-us/research/project/vall-e-x/vall-e/",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Hybrid H3-2.7B",
      "Organization": "Stanford University,University at Buffalo",
      "Publication date": "2022-12-28",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering",
      "Parameters": "2700000000.0",
      "Parameters notes": "2.7B",
      "Training compute (FLOP)": "6.48e+21",
      "Training compute notes": "6 FLOP/token/parameter * 400000000000 training tokens * 2700000000 parameters = 6.48e+21 FLOP\n\n___________________\nin the algorithmic progress paper the estimation was 8.49 \u00d7 10^20 based on the assumption of WT-103 dataset and 509 epochs",
      "Training dataset": "The Pile",
      "Training dataset size (gradients)": "400000000000",
      "Dataset size notes": "\"We train hybrid models at sizes 125M, 355M, 1.3B, and 2.7B on the Pile [21] for 400B tokens\"",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2212.14052",
      "Reference": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
      "Citations": "548.0",
      "Authors": "Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, Christopher R\u00e9",
      "Abstract": "State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2\u00d7 speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4\u00d7 faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Results table shows SOTA performance for some benchmarks\n\nnot absolute SOTA, only among same size models",
      "Epochs": "509.02",
      "Training time (hours)": "",
      "Training time notes": "\"All models were trained on either a single 16xA100-40GB node or a cluster of 8xA100-80GB nodes.\"",
      "Training hardware": "NVIDIA A100 SXM4 80 GB",
      "Hardware quantity": "8.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "6393.151008587547",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "1048576.0",
      "Batch size notes": "512 * 2048 was for 1.3B, but probably same",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "apache 2.0\nrepo (weights and inference only): https://github.com/HazyResearch/H3/blob/main/README.md ",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "CaLM",
      "Organization": "University of Oxford",
      "Publication date": "2022-12-19",
      "Domain": "Biology",
      "Task": "Protein or nucleotide language model (pLM/nLM),Protein embedding,Protein property prediction,Protein localization prediction",
      "Parameters": "86000000.0",
      "Parameters notes": "\"We trained a large language model with 86M parameters\"",
      "Training compute (FLOP)": "2.9e+19",
      "Training compute notes": "\"4 NVIDIA Quadro RTX4000 GPUs for 40 days\"\n\nCalculation assuming FP32, utilization 30%:\n= (40 * 24 * 3600) s * 7.1e12 FLOP/s * 0.3 * 4 GPU = 2.999808e+19\n\nalternative calculation:\n\"Gradients were accumulated to an effective batch size of 1,000 examples, or approximately 256,000 tokens. \"\n\"(66,000 gradient steps, 14 full epochs)\"\n\n256000*66000*14*86000000*6=1.220567e+20",
      "Training dataset": "European Nucleotide Archive (ENA)",
      "Training dataset size (gradients)": "2523746560",
      "Dataset size notes": "\"a dataset of 9M non-redundant and diverse cDNA sequences identified from whole-genome sequencing\"\n\n\"Gradients were accumulated to an effective batch size of 1,000 examples, or approximately 256,000 tokens. \"\n\n9000000*256000/1000=2304000000 tokens",
      "Confidence": "Likely",
      "Link": "https://www.biorxiv.org/content/10.1101/2022.12.15.519894v1.full.pdf",
      "Reference": "Codon language embeddings provide strong signals for protein engineering",
      "Citations": "34.0",
      "Authors": "Carlos Outeiral, Charlotte M. Deane",
      "Abstract": "Protein representations from deep language models have yielded state-of-the-art performance across many tasks in computational protein engineering. In recent years, progress has primarily focused on parameter count, with recent models\u2019 capacities surpassing the size of the very datasets they were trained on. Here, we propose an alternative direction. We show that large language models trained on codons, instead of amino acid sequences, provide high-quality representations that outperform comparable state-of-the-art models across a variety of tasks.\nIn some tasks, like species recognition, prediction of protein and transcript abundance, or melting point estimation, we show that a language model trained on codons outperforms every other published protein language model, including some that contain over 50 times more parameters. These results suggest that, in addition to commonly studied scale and model complexity, the information content of biological data provides an orthogonal direction to improve the power of machine learning in biology.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "not absolute SOTA, SOTA among similar size models\n\n\"We show that large language models trained on codons, instead of amino acid sequences, provide high-quality representations that outperform comparable state-of-the-art models across a variety of tasks. In some tasks, like species recognition, prediction of protein and transcript abundance, or melting point estimation, we show that a language model trained on codons outperforms every other published protein language model, including some that contain over 50 times more parameters\" [Abstract]",
      "Epochs": "14.0",
      "Training time (hours)": "960.0",
      "Training time notes": "\"The model reported in this work was trained on 4 NVIDIA Quadro\nRTX4000 GPUs for 40 days (66,000 gradient steps, 14 full epochs)\"",
      "Training hardware": "NVIDIA Quadro RTX 4000",
      "Hardware quantity": "4.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "1278.886496016633",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "BSD-3-Clause license\nhttps://github.com/oxpig/CaLM",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "2102.1501690037267",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "RT-1",
      "Organization": "Google",
      "Publication date": "2022-12-13",
      "Domain": "Robotics",
      "Task": "Robotic manipulation",
      "Parameters": "35000000.0",
      "Parameters notes": "\"we also limit the size of the model compared to\nthe original publication, which was 1.2B parameters (resulting in on robot inference time of 1.9s),\nto be of similar size to RT-1 (37M parameters for Gato vs. 35M for RT-1\"\n\n16M params for image tokenizer, 19M for the transformer",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "RT-1",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2212.06817",
      "Reference": "RT-1: Robotics Transformer for Real-World Control at Scale",
      "Citations": "1684.0",
      "Authors": "Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, Brianna Zitkovich",
      "Abstract": "By transferring knowledge from large, diverse, task-agnostic datasets, modern machine learning models can solve specific downstream tasks either zero-shot or with small task-specific datasets to a high level of performance. While this capability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the difficulty of collecting real-world robotic data. We argue that one of the keys to the success of such general robotic models lies with open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the diverse, robotic data. In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties. We verify our conclusions in a study of different model classes and their ability to generalize as a function of the data size, model size, and data diversity based on a large-scale data collection on real robots performing real-world tasks. The project's website and videos can be found at this http URL",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "I do not see any standard benchmarks that they are claiming SOTA on, only internal evaluations\n\n\"we evaluate RT-1 with a set of mobile manipulators from Everyday Robots in three environments: two real office kitchens and a training environment modelled off these real kitchens.\"\n\"Across each category, we find that RT-1 outperforms the prior\nmodels significantly. On seen tasks, RT-1 is able to perform 97% of the more than 200 instructions successfully, which is 25% more than BC-Z and 32% more than Gato. On unseen tasks, RT-1\nshows it is capable of generalizing to novel instructions, performing 76% of the never-before-seen instructions, 24% more than the next best baseline\"\n\nTop10 recent paper from Sebastian Sartor 2025-05-14",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "weights and code here, apache 2.0:\nhttps://github.com/google-research/robotics_transformer\n\ndata here, also apache: \nhttps://github.com/google-deepmind/open_x_embodiment",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "TranceptEve",
      "Organization": "University of Oxford,Harvard Medical School",
      "Publication date": "2022-12-10",
      "Domain": "Biology",
      "Task": "Proteins,Protein pathogenicity prediction",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "ProteinGym",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1",
      "Reference": "TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction",
      "Citations": "",
      "Authors": "Pascal Notin, Lood Van Niekerk, Aaron W Kollasch, Daniel Ritter, Yarin Gal, Debora S. Marks",
      "Abstract": "Modeling the fitness landscape of protein sequences has historically relied on training models on family-specific sets of homologous sequences called Multiple Sequence Alignments. Many proteins are however difficult to align or have shallow alignments which limits the potential scope of alignment-based methods. Not subject to these limitations, large protein language models trained on non-aligned sequences across protein families have achieved increasingly high predictive performance \u2013 but have not yet fully bridged the gap with their alignment-based counterparts. In this work, we introduce TranceptEVE \u2013 a hybrid method between family-specific and family-agnostic models that seeks to build on the relative strengths from each approach. Our method gracefully adapts to the depth of the alignment, fully relying on its autoregressive transformer when dealing with shallow alignments and leaning more heavily on the family-specifc models for proteins with deeper alignments. Besides its broader application scope, it achieves state-of-the-art performance for mutation effects prediction, both in terms of correlation with experimental assays and with clinical annotations from ClinVar.",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Besides its broader application scope, it achieves state-of- the-art performance for mutation effects prediction, both in terms of correlation with experimental assays and with clinical annotations from ClinVar.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "Tranception",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Unreleased",
      "Accessibility notes": "https://www.pascalnotin.com/publication/trancepteve/",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Vega v2",
      "Organization": "Wuhan University,JD Explore Academy,Shanghai AI Lab,Nanyang Technological University,Washington University in St Louis,Chongqing University of Posts and Telecommunications,University of Sydney",
      "Publication date": "2022-12-04",
      "Domain": "Language",
      "Task": "Language modeling,Question answering,Word sense disambiguation",
      "Parameters": "6000000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "7.76e+22",
      "Training compute notes": "Pretraining took 1 month on 320 A100 (Section 3.1)\n720*60*60*320*312000000000000*0.3=7.7635584e+22",
      "Training dataset": "",
      "Training dataset size (gradients)": "6439999999",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2212.01853",
      "Reference": "Toward Efficient Language Model Pretraining and Downstream Adaptation via Self-Evolution: A Case Study on SuperGLUE",
      "Citations": "39.0",
      "Authors": "Qihuang Zhong, Liang Ding, Yibing Zhan, Yu Qiao, Yonggang Wen, Li Shen, Juhua Liu, Baosheng Yu, Bo Du, Yixin Chen, Xinbo Gao, Chunyan Miao, Xiaoou Tang, Dacheng Tao",
      "Abstract": "This technical report briefly describes our JDExplore d-team's Vega v2 submission on the SuperGLUE leaderboard. SuperGLUE is more challenging than the widely used general language understanding evaluation (GLUE) benchmark, containing eight difficult language understanding tasks, including question answering, natural language inference, word sense disambiguation, coreference resolution, and reasoning. [Method] Instead of arbitrarily increasing the size of a pretrained language model (PLM), our aim is to 1) fully extract knowledge from the input pretraining data given a certain parameter budget, e.g., 6B, and 2) effectively transfer this knowledge to downstream tasks. To achieve goal 1), we propose self-evolution learning for PLMs to wisely predict the informative tokens that should be masked, and supervise the masked language modeling (MLM) process with rectified smooth labels. For goal 2), we leverage the prompt transfer technique to improve the low-resource tasks by transferring the knowledge from the foundation model and related downstream tasks to the target task. [Results] According to our submission record (Oct. 2022), with our optimized pretraining and fine-tuning strategies, our 6B Vega method achieved new state-of-the-art performance on 4/8 tasks, sitting atop the SuperGLUE leaderboard on Oct. 8, 2022, with an average score of 91.3.",
      "Organization categorization": "Academia,Academia,Academia,Academia,Academia,Academia",
      "Country (of organization)": "China,China,Singapore,United States of America,China,Australia",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Highest score at SuperGLUE leaderboard version 2.0 in terms of RTE (Recognizing Textual Entailment-Accuracy)",
      "Epochs": "",
      "Training time (hours)": "720.0",
      "Training time notes": "",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "320.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "255862.7534697245",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Unreleased",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DeepNash",
      "Organization": "DeepMind",
      "Publication date": "2022-12-01",
      "Domain": "Games",
      "Task": "Stratego",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "\"The final agent was trained using 768 MXU\u2019s (matrix multiplication unit) for Learners and\n256 MXU\u2019s for Actors (using 256 TPU\u2019s in total).\"\nSome more details in Table S1 (in supplementary materials)",
      "Training dataset": "",
      "Training dataset size (gradients)": "2109703680000",
      "Dataset size notes": "768 * 7.21M trajectories? (Table S1)\n\n768 * 7.21M = 5,537,280,000\n\nhttps://www.science.org/doi/suppl/10.1126/science.add4679/suppl_file/science.add4679_sm.pdf",
      "Confidence": "Unknown",
      "Link": "https://www.science.org/stoken/author-tokens/ST-887/full",
      "Reference": "Mastering the game of Stratego with model-free multiagent reinforcement learning",
      "Citations": "245.0",
      "Authors": "Julien Perolat, Bart de Vylder, Daniel Hennes, Eugene Tarassov, Florian Strub, Vincent de Boer, Paul Muller, Jerome T. Connor, Neil Burch, Thomas Anthony, Stephen McAleer, Romuald Elie, Sarah H. Cen, Zhe Wang, Audrunas Gruslys, Aleksandra Malysheva, Mina Khan, Sherjil Ozair, Finbarr Timbers, Toby Pohlen, Tom Eccles, Mark Rowland, Marc Lanctot, Jean-Baptiste Lespiau, Bilal Piot, Shayegan Omidshafiei, Edward Lockhart, Laurent Sifre, Nathalie Beauguerlange, Remi Munos, David Silver, Satinder Singh, Demis Hassabis, Karl Tuyls",
      "Abstract": "We introduce DeepNash, an autonomous agent that plays the imperfect information game Stratego at a human expert level. Stratego is one of the few iconic board games that artificial intelligence (AI) has not yet mastered. It is a game characterized by a twin challenge: It requires long-term strategic thinking as in chess, but it also requires dealing with imperfect information as in poker. The technique underpinning DeepNash uses a game-theoretic, model-free deep reinforcement learning method, without search, that learns to master Stratego through self-play from scratch. DeepNash beat existing state-of-the-art AI methods in Stratego and achieved a year-to-date (2022) and all-time top-three ranking on the Gravon games platform, competing with human expert players.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "DeepNash beat existing state-of-the-art AI methods in Stratego and achieved a year-to-date (2022) and all-time top-three ranking on the Gravon games platform, competing with human expert players.",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "the training code used to be here but not anymore\n\nhttps://github.com/google-deepmind/open_spiel/tree/master/open_spiel/python/algorithms/rnad",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GPT-3.5 Turbo",
      "Organization": "OpenAI",
      "Publication date": "2022-11-30",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering,Chat",
      "Parameters": "20000000000.0",
      "Parameters notes": "20B parameters according to Table 1 in Microsoft's CODEFUSION paper: https://arxiv.org/pdf/2310.17680.pdf",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Speculative",
      "Link": "https://platform.openai.com/docs/models/gpt-3.5-turbo",
      "Reference": "A fast, inexpensive model for simple tasks",
      "Citations": "",
      "Authors": "John Schulman, Barret Zoph, Christina Kim, Jacob Hilton, Jacob Menick, Jiayi Weng, Juan Felipe Ceron Uribe, Liam Fedus, Luke Metz, Michael Pokorny, Rapha Gontijo Lopes, Shengjia Zhao, Arun Vijayvergiya, Eric Sigler, Adam Perelman, Chelsea Voss, Mike Heaton, Joel Parish, Dave Cummings, Rajeev Nayak, Valerie Balcom, David Schnurr, Tomer Kaftan, Chris Hallacy, Nicholas Turley, Noah Deutsch, Vik Goel, Jonathan Ward, Aris Konstantinidis, Wojciech Zaremba, Long Ouyang, Leonard Bogdonoff, Joshua Gross, David Medina, Sarah Yoo, Teddy Lee, Ryan Lowe, Dan Mossing, Joost Huizinga, Roger Jiang, Carroll Wainwright, Diogo Almeida, Steph Lin, Marvin Zhang, Kai Xiao, Katarina Slama, Steven Bills, Alex Gray, Jan Leike, Jakub Pachocki, Phil Tillet, Shantanu Jain, Greg Brockman, Nick Ryder, Alex Paino, Qiming Yuan, Clemens Winter, Ben Wang, Mo Bavarian, Igor Babuschkin, Szymon Sidor, Ingmar Kanitscheider, Mikhail Pavlov, Matthias Plappert, Nik Tezak, Heewoo Jun, William Zhuk, Vitchyr Pong, Lukasz Kaiser, Jerry Tworek, Andrew Carr, Lilian Weng, Sandhini Agarwal, Karl Cobbe, Vineet Kosaraju, Alethea Power, Stanislas Polu, Jesse Han, Raul Puri, Shawn Jain, Benjamin Chess, Christian Gibson, Oleg Boiko, Emy Parparita, Amin Tootoonchian, Kyle Kosic, Christopher Hesse",
      "Abstract": "GPT-3.5 Turbo models can understand and generate natural language or code and have been optimized for chat using the Chat Completions API but work well for non-chat tasks as well. As of July 2024, use gpt-4o-mini in place of GPT-3.5 Turbo, as it is cheaper, more capable, multimodal, and just as fast. GPT-3.5 Turbo is still available for use in the API.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance,Significant use",
      "Notability criteria notes": "https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/\n\nwas the default free model in ChatGPT, so likely one of the most popular models in existence",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "available on API: https://platform.openai.com/docs/models/gpt-3-5-turbo ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GPT-3.5 (davinci-002)\n",
      "Organization": "OpenAI",
      "Publication date": "2022-11-28",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "",
      "Parameters notes": "Parameter count may be 175B based on OpenAI's statements that text-davinci-003 is in the GPT-3.5 series of models. It was also stated to be 175B in the Microsoft CODEFUSION paper, but the paper was reportedly retracted because the authors did not know the parameter count.",
      "Training compute (FLOP)": "2.578e+24",
      "Training compute notes": "https://colab.research.google.com/drive/1QSxa8YCWjEBQU7mrXLhw6TP1VX5oqgdW#scrollTo=Gt6Z6oZ26clI",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Speculative",
      "Link": "https://platform.openai.com/docs/models/gpt-3-5",
      "Reference": "",
      "Citations": "",
      "Authors": "",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance,Significant use,SOTA improvement,Training cost",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA A100 SXM4 40 GB",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "4805631.250163697",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DiT-XL/2 + Discriminator Guidance",
      "Organization": "Korea Advanced Institute of Science and Technology (KAIST),NAVER",
      "Publication date": "2022-11-28",
      "Domain": "Image generation",
      "Task": "Image generation,Text-to-image",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "This is a finetune of DiT-XL/2, so its compute won't be much higher.",
      "Training dataset": "",
      "Training dataset size (gradients)": "327978752",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/2211.17091v4",
      "Reference": "Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models",
      "Citations": "101.0",
      "Authors": "Dongjun Kim, Yeongmin Kim, Se Jung Kwon, Wanmo Kang, Il-Chul Moon",
      "Abstract": "The proposed method, Discriminator Guidance, aims to improve sample generation of pre-trained diffusion models. The approach introduces a discriminator that gives explicit supervision to a denoising sample path whether it is realistic or not. Unlike GANs, our approach does not require joint training of score and discriminator networks. Instead, we train the discriminator after score training, making discriminator training stable and fast to converge. In sample generation, we add an auxiliary term to the pre-trained score to deceive the discriminator. This term corrects the model score to the data score at the optimal discriminator, which implies that the discriminator helps better score estimation in a complementary way. Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66). We release the code at this https URL.",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "Korea (Republic of),Korea (Republic of)",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66)\"",
      "Epochs": "10.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "DiT-XL/2",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "",
      "Accessibility notes": "Attribution-NonCommercial-ShareAlike 4.0 International\nhttps://github.com/alsdudrla10/DG?tab=readme-ov-file\n\nI cannot see checkpoints / model weights in the repo",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Discriminator Guidance",
      "Organization": "Korea Advanced Institute of Science and Technology (KAIST),NAVER",
      "Publication date": "2022-11-28",
      "Domain": "Image generation",
      "Task": "Image generation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "2.1570000001e+20",
      "Training compute notes": "481 hours * 312 TFLOPS (A100) * 40% utilization",
      "Training dataset": "",
      "Training dataset size (gradients)": "327978752",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2211.17091v4",
      "Reference": "Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models",
      "Citations": "101.0",
      "Authors": "Dongjun Kim, Yeongmin Kim, Se Jung Kwon, Wanmo Kang, Il-Chul Moon",
      "Abstract": "The proposed method, Discriminator Guidance, aims to improve sample generation of pre-trained diffusion models. The approach introduces a discriminator that gives explicit supervision to a denoising sample path whether it is realistic or not. Unlike GANs, our approach does not require joint training of score and discriminator networks. Instead, we train the discriminator after score training, making discriminator training stable and fast to converge. In sample generation, we add an auxiliary term to the pre-trained score to deceive the discriminator. This term corrects the model score to the data score at the optimal discriminator, which implies that the discriminator helps better score estimation in a complementary way. Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66).",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "Korea (Republic of),Korea (Republic of)",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Using our algorithm, we achive state-of-the-art results on ImageNet 256x256 with FID 1.83 and recall 0.64, similar to the validation data's FID (1.68) and recall (0.66).\"\nhttps://paperswithcode.com/paper/refining-generative-process-with",
      "Epochs": "10.0",
      "Training time (hours)": "481.0",
      "Training time notes": "Table 6",
      "Training hardware": "NVIDIA A100 PCIe",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "337.8811363173893",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "Open access (non-commercial)",
      "Accessibility notes": "https://github.com/alsdudrla10/DG\nAttribution-NonCommercial-ShareAlike 4.0 International\n\ncheckpoints and train code here: https://github.com/alsdudrla10/DG/blob/main/README.md ",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ALM 1.0",
      "Organization": "Beijing Academy of Artificial Intelligence / BAAI",
      "Publication date": "2022-11-28",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "335000000.0",
      "Parameters notes": "335M parameters: https://github.com/FlagAI-Open/FlagAI/blob/master/examples/ALM/README.md",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "ArabicText 2022",
      "Training dataset size (gradients)": "22696572400",
      "Dataset size notes": "",
      "Confidence": "Speculative",
      "Link": "https://github.com/FlagAI-Open/FlagAI/blob/master/examples/ALM/README.md",
      "Reference": "ALM 1.0",
      "Citations": "",
      "Authors": "",
      "Abstract": "The Arabic Language Model (ALM) 1.0 is a pretrained language model based on autoregressive blank infilling . Below shows the count of model parameters in detail.\n\nALM-1.0 uses the largest open-source Arabic text dataset ArabicText 2022. You can check ArabicText 2022 for more information.",
      "Organization categorization": "Academia",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "SOTA results on Arabic-language benchmark ALUE.",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Hosted access (no API)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "It seems to have only inference and finetuning codes, no weights or pretraining code\n\nhttps://github.com/FlagAI-Open/FlagAI/tree/master/examples/ALM",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "CICERO",
      "Organization": "Meta AI",
      "Publication date": "2022-11-22",
      "Domain": "Games",
      "Task": "Diplomacy",
      "Parameters": "",
      "Parameters notes": "\"We took R2C2 (22) as our base model \u2013 a 2.7B parameter Transformer-based (23) encoder-decoder model pre-trained on text from the Internet using a BART de-noising objective (24).\"",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "WebDiplomacy",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "\"We obtained a dataset of 125,261 games of Diplomacy played online at webDiplomacy.net. Of these, 40,408 games contained dialogue, with a total of 12,901,662 messages exchanged between players. Player accounts were de-identified and automated redaction of personally identifiable information (PII) was performed by webDiplomacy. We refer to this dataset hereafter as WebDiplomacy .\"",
      "Confidence": "Unknown",
      "Link": "https://www.science.org/doi/10.1126/science.ade9097",
      "Reference": "Human-level play in the game of Diplomacy by combining language models with strategic reasoning",
      "Citations": "459.0",
      "Authors": "Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, Athul Paul Jacob, Mojtaba Komeili, Karthik Konath, Minae Kwon, Adam Lerer, Mike Lewis, Alexander H. Miller, Sasha Mitts, Adithya Renduchintala, Stephen Roller, Dirk Rowe, Weiyan Shi, Joe Spisak, Alexander Wei, David Wu, Hugh Zhang, Markus Zijlstra",
      "Abstract": "The game Diplomacy has been a major challenge for artificial intelligence (AI). Unlike other competitive games that AI has recently mastered, such as chess, Go, and poker, Diplomacy cannot be solved purely through self-play; it requires the development of an agent to understand other players\u2019 motivations and perspectives and to use natural language to negotiate complex shared plans. The Meta Fundamental AI Research Diplomacy Team (FAIR) et al. developed an agent that is able to play the full natural language form of the game and demonstrates performance well above the human average in an online Diplomacy league. The present work has far-reaching implications for the development of cooperative AI and language models for communication with people, even when interactions involve a mixture of aligned and competing interests. \u2014YS",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "creative commons (non comm) for model weights, MIT for code\nhttps://github.com/facebookresearch/diplomacy_cicero?tab=readme-ov-file#license-for-model-weights",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "AR-LDM",
      "Organization": "Alibaba,University of Waterloo,Vector Institute",
      "Publication date": "2022-11-20",
      "Domain": "Image generation",
      "Task": "Text-to-image",
      "Parameters": "1500000000.0",
      "Parameters notes": "Table 1",
      "Training compute (FLOP)": "5.1e+20",
      "Training compute notes": "8 NVIDIA A100 GPUs for 8 days",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "PororoSV, FlintstonesSV and VIST. All storytelling datasets, sizes would be possible to look up.",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2211.10950",
      "Reference": "Synthesizing Coherent Story with Auto-Regressive Latent Diffusion Models",
      "Citations": "78.0",
      "Authors": "Xichen Pan, Pengda Qin, Yuhong Li, Hui Xue, Wenhu Chen",
      "Abstract": "Conditioned diffusion models have demonstrated state-of-the-art text-to-image synthesis capacity. Recently, most works focus on synthesizing independent images; While for real-world applications, it is common and necessary to generate a series of coherent images for story-stelling. In this work, we mainly focus on story visualization and continuation tasks and propose AR-LDM, a latent diffusion model auto-regressively conditioned on history captions and generated images. Moreover, AR-LDM can generalize to new characters through adaptation. To our best knowledge, this is the first work successfully leveraging diffusion models for coherent visual story synthesizing. Quantitative results show that AR-LDM achieves SoTA FID scores on PororoSV, FlintstonesSV, and the newly introduced challenging dataset VIST containing natural images. Large-scale human evaluations show that AR-LDM has superior performance in terms of quality, relevance, and consistency.",
      "Organization categorization": "Industry,Academia,Academia",
      "Country (of organization)": "China,Canada,Canada",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "The first latent diffusion model for coherent visual story synthesizing.\n\"Quantitative results show that AR-LDM achieves SoTA FID scores on PororoSV, FlintstonesSV, and the newly introduced challenging dataset VIST containing natural images\"",
      "Epochs": "50.0",
      "Training time (hours)": "194.0",
      "Training time notes": "8 NVIDIA A100 GPUs for 8 days",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "745.8360575556148",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "Stable Diffusion (LDM-KL-8-G)",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "Open access (non-commercial)",
      "Accessibility notes": "no weights, no license. training and inference code here:\nhttps://github.com/xichenpan/ARLDM",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Fusion in Encoder",
      "Organization": "Samsung",
      "Publication date": "2022-11-18",
      "Domain": "Language",
      "Task": "Question answering,Language modeling/generation",
      "Parameters": "330000000.0",
      "Parameters notes": "330M",
      "Training compute (FLOP)": "1.3e+20",
      "Training compute notes": "\"The experiments were run on 8x80GB Nvidia A100s with 800GB RAM and 4x32-core CPUs, and each experiment took around 1 day for NQ and 2 days for TriviaQA with large models. Inference was run on the same system, and took 2 minutes.\"\n\n2 days * 24 * 3600 * 8 * 312 teraflop/s * 0.3 utilization = 1.3e20",
      "Training dataset": "TriviaQA",
      "Training dataset size (gradients)": "960000",
      "Dataset size notes": "79k per table 11 (probably number of question-answer pairs)",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2211.10147",
      "Reference": "FiE: Building a Global Probability Space by Leveraging Early Fusion in Encoder for Open-Domain Question Answering",
      "Citations": "11.0",
      "Authors": "Akhil Kedia, Mohd Abbas Zaidi, Haejun Lee",
      "Abstract": "Generative models have recently started to outperform extractive models in Open Domain Question Answering, largely by leveraging their decoder to attend over multiple encoded passages and combining their information. However, generative models tend to be larger than extractive models due to the need for a decoder, run slower during inference due to auto-regressive decoder beam search, and their generated output often suffers from hallucinations. We propose to extend transformer encoders with the ability to fuse information from multiple passages, using global representation to provide cross-sample attention over all tokens across samples. Furthermore, we propose an alternative answer span probability calculation to better aggregate answer scores in the global space of all samples. Using our proposed method, we outperform the current state-of-the-art method by 2.5 Exact Match score on the Natural Question dataset while using only 25% of parameters and 35% of the latency during inference, and 4.4 Exact Match on WebQuestions dataset. When coupled with synthetic data augmentation, we outperform larger models on the TriviaQA dataset as well. The latency and parameter savings of our method make it particularly attractive for open-domain question answering, as these models are often compute-intensive.",
      "Organization categorization": "Industry",
      "Country (of organization)": "Korea (Republic of)",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Using our proposed method, we outperform the current state-of-the-art method by 2.5 Exact Match score on the Natural Question dataset while using only 25% of parameters and 35% of the latency during inference, and 4.4 Exact Match on WebQuestions dataset\"",
      "Epochs": "",
      "Training time (hours)": "48.0",
      "Training time notes": "2 days",
      "Training hardware": "NVIDIA A100 SXM4 80 GB",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "233.0630322095263",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "\"The source code is based on the original implementation of FiD (Izacard and Grave, 2021b), which can be found at their Github. The modeling for the fused Electra model was implemented using\nHuggingFace (Wolf et al., 2020) by modifying the ElectraModel.",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Galactica",
      "Organization": "Meta AI",
      "Publication date": "2022-11-16",
      "Domain": "Language,Biology",
      "Task": "Language modeling,Question answering,Mathematical reasoning,Medical diagnosis,Language modeling/generation",
      "Parameters": "120000000000.0",
      "Parameters notes": "\"The largest 120B model we train runs on a single NVIDIA A100 node\"",
      "Training compute (FLOP)": "3.24e+23",
      "Training compute notes": "Authors state the model is trained on 450b tokens. Using 6 FLOP/token/parameter, this is 6*120b*450b = 3.24e23",
      "Training dataset": "Galactica Corpus",
      "Training dataset size (gradients)": "106000000000",
      "Dataset size notes": "\"Total dataset size = 106 billion tokens\"",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2211.09085",
      "Reference": "Galactica: A Large Language Model for Science",
      "Citations": "917.0",
      "Authors": "Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic",
      "Abstract": "Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH\"\n\n\"On reasoning tasks, Galactica beats existing language models on benchmarks such as MMLU and MATH (Hendrycks et al., 2020, 2021). With our reasoning token approach, we outperform Chinchilla on mathematical MMLU with an average score of 41.3% versus 35.7% (Hoffmann et al., 2022). Our 120B model achieves a score of 20.4% versus PaLM 540B\u2019s 8.8% on MATH (Chowdhery et al., 2022; Lewkowycz et al., 2022). The 30B model also beats PaLM 540B on this task with 18 times less parameters.\"\n\n\"Galactica also performs well in downstream scientific tasks, and we set a new state-of-the-art on several downstream tasks such as PubMedQA (77.6%) and MedMCQA dev (52.9%)\"",
      "Epochs": "4.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA A100 SXM4 80 GB",
      "Hardware quantity": "128.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "591076.8943544837",
      "Compute cost notes": "",
      "Training power draw (W)": "102386.13451047534",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "2000000.0",
      "Batch size notes": "Table 1: batch size 2M, warmup 1.1B (out of 450B tokens)",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "cc-by-nc (non-commercial): https://huggingface.co/facebook/galactica-120b \n\nrepo but no training code: https://github.com/paperswithcode/galai/blob/main/README.md ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "EVA-01",
      "Organization": "Beijing Academy of Artificial Intelligence / BAAI,Huazhong University of Science and Technology,Zhejiang University (ZJU),Beijing Institute of Technology",
      "Publication date": "2022-11-14",
      "Domain": "Vision",
      "Task": "Image classification,Object detection,Semantic segmentation,Video classification",
      "Parameters": "1011000000.0",
      "Parameters notes": "1011M from table 3",
      "Training compute (FLOP)": "1.501e+22",
      "Training compute notes": "flops = (128) * (3.12e14) * (14.5 * 24 * 3600) * (0.3) = 1.501e22\n(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)\n\nfrom Table 3, time and num gpus, GPU model is on page 4 (A100), precision is fp16 and likely utilizes tensor cores",
      "Training dataset": "ImageNet21k,COCO,Conceptual Captions 12M (CC12M),Conceptual Captions (CC3M)",
      "Training dataset size (gradients)": "7577600000",
      "Dataset size notes": "from table 3: 29.6M images",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2211.07636",
      "Reference": "EVA: Exploring the Limits of Masked Visual Representation Learning at Scale",
      "Citations": "888.0",
      "Authors": "Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, Yue Cao",
      "Abstract": "We launch EVA, a vision-centric foundation model to explore the limits of visual representation at scale using only publicly accessible data. EVA is a vanilla ViT pre-trained to reconstruct the masked out image-text aligned vision features conditioned on visible image patches. Via this pretext task, we can efficiently scale up EVA to one billion parameters, and sets new records on a broad range of representative vision downstream tasks, such as image recognition, video action recognition, object detection, instance segmentation and semantic segmentation without heavy supervised training. Moreover, we observe quantitative changes in scaling EVA result in qualitative changes in transfer learning performance that are not present in other models. For instance, EVA takes a great leap in the challenging large vocabulary instance segmentation task: our model achieves almost the same state-of-the-art performance on LVISv1.0 dataset with over a thousand categories and COCO dataset with only eighty categories. Beyond a pure vision encoder, EVA can also serve as a vision-centric, multi-modal pivot to connect images and text. We find initializing the vision tower of a giant CLIP from EVA can greatly stabilize the training and outperform the training from scratch counterpart with much fewer samples and less compute, providing a new direction for scaling up and accelerating the costly training of multi-modal foundation models. To facilitate future research, we release all the code and billion-scale model.",
      "Organization categorization": "Academia,Academia,Academia,Academia",
      "Country (of organization)": "China,China,China,China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "from abstract 'Via this pretext task, we can efficiently scale up EVA to one billion parameters, and sets new records on a broad range of representative vision downstream tasks, such as image recognition, video action recognition, object detection, instance segmentation and semantic segmentation without heavy supervised training.'\n\n\"EVA takes a great leap in the challenging large vocabulary instance segmentation task: our model achieves almost the same state-of-the-art performance on LVISv1.0 dataset with over a thousand categories and COCO dataset with only eighty categories.\"\n\nTables 6-9",
      "Epochs": "150.0",
      "Training time (hours)": "348.0",
      "Training time notes": "from Table 3 14.5 days = 348 hours",
      "Training hardware": "NVIDIA A100 SXM4 40 GB",
      "Hardware quantity": "128.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "29374.46909439904",
      "Compute cost notes": "",
      "Training power draw (W)": "102390.69476171424",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT: https://github.com/baaivision/EVA pretrain code: https://github.com/baaivision/EVA/tree/master/EVA-01/eva ",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "1706642.1708924389",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "AltCLIP_M9",
      "Organization": "Beijing Academy of Artificial Intelligence / BAAI",
      "Publication date": "2022-11-12",
      "Domain": "Multimodal,Language,Vision",
      "Task": "Language modeling/generation,Chat,Visual question answering,Image generation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Conceptual Captions (CC3M),LAION-400M,TSL2019,OPUS,WuDao Corpora,LAION-2B",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/2211.06679",
      "Reference": "AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities",
      "Citations": "104.0",
      "Authors": "Zhongzhi Chen, Guang Liu, Bo-Wen Zhang, Fulong Ye, Qinghong Yang, Ledell Wu",
      "Abstract": "In this work, we present a conceptually simple and effective method to train a strong bilingual/multilingual multimodal representation model. Starting from the pre-trained multimodal representation model CLIP released by OpenAI, we altered its text encoder with a pre-trained multilingual text encoder XLM-R, and aligned both languages and image representations by a two-stage training schema consisting of teacher learning and contrastive learning. We validate our method through evaluations of a wide range of tasks. We set new state-of-the-art performances on a bunch of tasks including ImageNet-CN, Flicker30k-CN, COCO-CN and XTD. Further, we obtain very close performances with CLIP on almost all tasks, suggesting that one can simply alter the text encoder in CLIP for extended capabilities such as multilingual understanding. Our models and code are available at this https URL.",
      "Organization categorization": "Academia",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We set new state-of-the-art performances on a bunch of tasks including ImageNet-CN, Flicker30kCN, COCO-CN and XTD\"",
      "Epochs": "10.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "CLIP (ViT L/14@336px)",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "https://github.com/FlagAI-Open/FlagAI/blob/master/examples/AltCLIP/altclip_ft_bmtrain.py Apache-2.0 license",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "InternImage",
      "Organization": "Shanghai AI Lab,Tsinghua University,Nanjing University,SenseTime,Chinese University of Hong Kong (CUHK)",
      "Publication date": "2022-11-10",
      "Domain": "Vision",
      "Task": "Image classification,Object detection,Image segmentation",
      "Parameters": "1080000000.0",
      "Parameters notes": "1.08B, table 1",
      "Training compute (FLOP)": "2.408e+21",
      "Training compute notes": "InternImage-H is pre-trained on a 427 million joint dataset of public Laion-400M [61], YFCC-15M [62], and CC12M [63] for 30 epochs, and then fine-tuned the model on ImageNet-1K for 20 epochs. ImageNet-1K has 1,281,167 images.\n\nTable 2 says InternImage-H uses 188 GFLOP per forward pass at 224 resolution, and 1478 GFLOP at 640\n\nTable 7 indicates training InternImage-H was done at a scale of \"224/640\" so presumably there was pretraining at 224x224 resolution and then some fine-tuning at 640x640. It's not clear how much training was done at each resolution, but typically this is a small fraction of total training (e.g. Noisy Student finds it sufficient to train for 350 epochs at smaller resolution, and then fine-tune at the higher resolution for 1.5 epochs). We'll ignore the additional FLOPs from high resolution training.\n\nTotal training FLOPs:\n188e9 FLOP/image * (427M images * 30 epochs) + (1.281M images * 20 epochs) * 3 (additional FLOPs for backward pass) = 2.408e21",
      "Training dataset": "LAION-400M,Conceptual Captions 12M (CC12M),ImageNet-1k",
      "Training dataset size (gradients)": "83692000000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2211.05778",
      "Reference": "InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions",
      "Citations": "939.0",
      "Authors": "Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, Xiaogang Wang, Yu Qiao",
      "Abstract": "Compared to the great progress of large-scale vision transformers (ViTs) in recent years, large-scale models based on convolutional neural networks (CNNs) are still in an early state. This work presents a new large-scale CNN-based foundation model, termed InternImage, which can obtain the gain from increasing parameters and training data like ViTs. Different from the recent CNNs that focus on large dense kernels, InternImage takes deformable convolution as the core operator, so that our model not only has the large effective receptive field required for downstream tasks such as detection and segmentation, but also has the adaptive spatial aggregation conditioned by input and task information. As a result, the proposed InternImage reduces the strict inductive bias of traditional CNNs and makes it possible to learn stronger and more robust patterns with large-scale parameters from massive data like ViTs. The effectiveness of our model is proven on challenging benchmarks including ImageNet, COCO, and ADE20K. It is worth mentioning that InternImage-H achieved a new record 65.4 mAP on COCO test-dev and 62.9 mIoU on ADE20K, outperforming current leading CNNs and ViTs. The code will be released at this https URL.",
      "Organization categorization": "Academia,Academia,Academia,Industry,Academia",
      "Country (of organization)": "China,China,China,Hong Kong,Hong Kong",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"InternImage-H achieved a new record 65.4 mAP on COCO test-dev and 62.9 mIoU on ADE20K, outperforming current leading CNNs and ViTs\"",
      "Epochs": "30.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "https://github.com/OpenGVLab/InternImage\nMIT license\n\nhttps://huggingface.co/OpenGVLab/internimage_h_jointto22k_384",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "mT0-13B",
      "Organization": "Hugging Face,BigScience",
      "Publication date": "2022-11-03",
      "Domain": "Language",
      "Task": "Translation,Language modeling/generation",
      "Parameters": "13000000000.0",
      "Parameters notes": "13B",
      "Training compute (FLOP)": "",
      "Training compute notes": "fine-tuned from mT5\n\n1.37e22 fine-tune compute",
      "Training dataset": "xP3",
      "Training dataset size (gradients)": "20000000000",
      "Dataset size notes": "per https://huggingface.co/datasets/bigscience/xP3, 94,941,936 KB or 94GB \n\nif approx 200M words per GB, that's ~20B words (rougher estimate because it's multilingual)\n\nhttps://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.ieihc08p8dn0",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2211.01786",
      "Reference": "Crosslingual Generalization through Multitask Finetuning",
      "Citations": "242.0",
      "Authors": "Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, Colin Raffel",
      "Abstract": "Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at this https URL.",
      "Organization categorization": "Industry,Research collective",
      "Country (of organization)": "United States of America,France",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results.\"\n\nFigure 4, zero-shot SOTA",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "mT5-XXL",
      "Finetune compute (FLOP)": "1.01e+21",
      "Finetune compute notes": "\"We finetune the models for an additional 13 billion tokens with loss only being computed on target tokens...\nFor finetuning mT5, we follow the same procedure as described above for BLOOM, except that inputs are fed into the encoder and thus are not space-separated from targets.\"\n\n13B * 13B * 6 = 1.01e21",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "apache 2.0",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Mogrifier RLSTM (WT2)",
      "Organization": "DeepMind",
      "Publication date": "2022-11-03",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "35000000.0",
      "Parameters notes": "Table 1",
      "Training compute (FLOP)": "1.4e+17",
      "Training compute notes": "6ND = 6*35000000*2666667*250 = 1.4000002e+17",
      "Training dataset": "WikiText-2",
      "Training dataset size (gradients)": "2666667",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2211.01848",
      "Reference": "Circling Back to Recurrent Models of Language",
      "Citations": "0.0",
      "Authors": "G\u00e1bor Melis",
      "Abstract": "Just because some purely recurrent models suffer from being hard to optimize and inefficient on today's hardware, they are not necessarily bad models of language. We demonstrate this by the extent to which these models can still be improved by a combination of a slightly better recurrent cell, architecture, objective, as well as optimization. In the process, we establish a new state of the art for language modelling on small datasets and on Enwik8 with dynamic evaluation.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"On top of these improvements, the RLSTM outperformed the LSTM by a small margin, and we established a new state of the art on both datasets\"",
      "Epochs": "250.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "eDiff-I",
      "Organization": "NVIDIA",
      "Publication date": "2022-11-02",
      "Domain": "Image generation",
      "Task": "Image generation,Text-to-image",
      "Parameters": "9100000000.0",
      "Parameters notes": "9.1B for config D, Table 1",
      "Training compute (FLOP)": "5.46e+19",
      "Training compute notes": "6ND = 6*9100000000*1000000000=5.46e+19 (likely, might change because of several epochs / dataset division)\n\n\"The base model was trained using 256 NVIDIA A100 GPUs, while the two super-resolution models were trained with 128 NVIDIA A100 GPUs each\" \nno info on duration",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "1556275200000",
      "Dataset size notes": "\"The final dataset to train our model contains about one billion text-image pairs\"",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2211.01324",
      "Reference": "eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers",
      "Citations": "974.0",
      "Authors": "Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, Ming-Yu Liu",
      "Abstract": "Large-scale diffusion-based generative models have led to breakthroughs in text-conditioned high-resolution image synthesis. Starting from random noise, such text-to-image diffusion models gradually synthesize images in an iterative fashion while conditioning on text prompts. We find that their synthesis behavior qualitatively changes throughout this process: Early in sampling, generation strongly relies on the text prompt to generate text-aligned content, while later, the text conditioning is almost entirely ignored. This suggests that sharing model parameters throughout the entire generation process may not be ideal. Therefore, in contrast to existing works, we propose to train an ensemble of text-to-image diffusion models specialized for different synthesis stages. To maintain training efficiency, we initially train a single model, which is then split into specialized models that are trained for the specific stages of the iterative generation process. Our ensemble of diffusion models, called eDiff-I, results in improved text alignment while maintaining the same inference computation cost and preserving high visual quality, outperforming previous large-scale text-to-image diffusion models on the standard benchmark. In addition, we train our model to exploit a variety of embeddings for conditioning, including the T5 text, CLIP text, and CLIP image embeddings. We show that these different embeddings lead to different behaviors. Notably, the CLIP image embedding allows an intuitive way of transferring the style of a reference image to the target text-to-image output. Lastly, we show a technique that enables eDiff-I's \"paint-with-words\" capability. A user can select the word in the input text and paint it in a canvas to control the output, which is very handy for crafting the desired image in mind. The project page is available at this https URL",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "SOTA zero-shot FID on COCO 2014, Table 1\n\nMay be significantly used, via Nvidia Picasso: https://www.nvidia.com/en-us/gpu-cloud/picasso/",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Unreleased",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "EnCodec",
      "Organization": "Meta AI",
      "Publication date": "2022-10-24",
      "Domain": "Audio",
      "Task": "Audio generation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "\"We train all models for 300 epochs, with one epoch being 2,000 updates with the Adam optimizer with a batch size of 64 examples of 1 second each, a learning rate of 3 \u00b7 10\u22124 , \u03b21 = 0.5, and \u03b22 = 0.9. All the models are traind using 8 A100 GPUs\"",
      "Training dataset": "DNS,Common Voice,AudioSet,FSD50K,Jamendo",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "~17k hours total, per Table A.1",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/2210.13438, ",
      "Reference": "High Fidelity Neural Audio Compression",
      "Citations": "970.0",
      "Authors": "Alexandre D\u00e9fossez, Jade Copet, Gabriel Synnaeve, Yossi Adi",
      "Abstract": "We introduce a state-of-the-art real-time, high-fidelity, audio codec leveraging neural networks. It consists in a streaming encoder-decoder architecture with quantized latent space trained in an end-to-end fashion. We simplify and speed-up the training by using a single multiscale spectrogram adversary that efficiently reduces artifacts and produce high-quality samples. We introduce a novel loss balancer mechanism to stabilize training: the weight of a loss now defines the fraction of the overall gradient it should represent, thus decoupling the choice of this hyper-parameter from the typical scale of the loss. Finally, we study how lightweight Transformer models can be used to further compress the obtained representation by up to 40%, while staying faster than real time. We provide a detailed description of the key design choices of the proposed model including: training objective, architectural changes and a study of various perceptual loss functions. We present an extensive subjective evaluation (MUSHRA tests) together with an ablation study for a range of bandwidths and audio domains, including speech, noisy-reverberant speech, and music. Our approach is superior to the baselines methods across all evaluated settings, considering both 24 kHz monophonic and 48 kHz stereophonic audio. Code and models are available at this http URL.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\" Finally, our best model, EnCodec, reaches state-of-the-art scores for speech and for\nmusic at 1.5, 3, 6, 12 kbps at 24 kHz, and at 6, 12, and 24 kbps for 48 kHz with stereo channels.\"",
      "Epochs": "300.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT for repo in general, non commercial weights. Dataset is in repo.\nhttps://github.com/facebookresearch/audiocraft/blob/main/docs/ENCODEC.md",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "U-PaLM (540B)",
      "Organization": "Google",
      "Publication date": "2022-10-20",
      "Domain": "Language",
      "Task": "Language generation,Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning",
      "Parameters": "540000000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "2.53e+24",
      "Training compute notes": "\"The total number of extra tokens we train on for the 540B\nmodel is approximately 1.3 Billion which constitutes 0.16% extra computation... Training an U-PaLM 540B model only consumes 512 TPUv4 chips and finishes in about 5 days which is considered to be lightweight.\"\n\noriginal PaLM was 2.527e+24. adding 0.16% is ~2.53e24",
      "Training dataset": "",
      "Training dataset size (gradients)": "1300000000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2210.11399",
      "Reference": "Transcending Scaling Laws with 0.1% Extra Compute",
      "Citations": "73.0",
      "Authors": "Yi Tay, Jason Wei, Hyung Won Chung, Vinh Q. Tran, David R. So, Siamak Shakeri, Xavier Garcia, Huaixiu Steven Zheng, Jinfeng Rao, Aakanksha Chowdhery, Denny Zhou, Donald Metzler, Slav Petrov, Neil Houlsby, Quoc V. Le, Mostafa Dehghani",
      "Abstract": "Scaling language models improves performance but comes with significant computational costs. This paper proposes UL2R, a method that substantially improves existing language models and their scaling curves with a relatively tiny amount of extra compute. The key idea is to continue training a state-of-the-art large language model (e.g., PaLM) on a few more steps with UL2's mixture-of-denoiser objective. We show that, with almost negligible extra computational costs and no new sources of data, we are able to substantially improve the scaling properties of large language models on downstream metrics. In this paper, we continue training PaLM with UL2R, introducing a new set of models at 8B, 62B, and 540B scale which we call U-PaLM. Impressively, at 540B scale, we show an approximately 2x computational savings rate where U-PaLM achieves the same performance as the final PaLM 540B model at around half its computational budget (i.e., saving \u223c4.4 million TPUv4 hours). We further show that this improved scaling curve leads to 'emergent abilities' on challenging BIG-Bench tasks -- for instance, U-PaLM does much better than PaLM on some tasks or demonstrates better quality at much smaller scale (62B as opposed to 540B). Overall, we show that U-PaLM outperforms PaLM on many few-shot setups, i.e., English NLP tasks (e.g., commonsense reasoning, question answering), reasoning tasks with chain-of-thought (e.g., GSM8K), multilingual tasks (MGSM, TydiQA), MMLU and challenging BIG-Bench tasks. Finally, we provide qualitative examples showing the new capabilities of U-PaLM for single and multi-span infilling.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Figure 3\n\"We show that U-PaLM 540B outperforms PaLM 540B on 21 out of 26 tasks. Given that PaLM is the SOTA language model on these tasks, this makes U-PaLM the new state-of-the-art on these tasks.\"\n\nperformance improvement equivalent to 2x training efficiency: \"Impressively, at 540B scale, we show an approximately 2x computational savings rate where U-PaLM achieves the same performance as the final PaLM 540B model at around half its computational budget \"",
      "Epochs": "",
      "Training time (hours)": "120.0",
      "Training time notes": "5 days",
      "Training hardware": "Google TPU v4",
      "Hardware quantity": "512.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "348322.23114529514",
      "Base model": "PaLM (540B)",
      "Finetune compute (FLOP)": "4e+21",
      "Finetune compute notes": "\"The total number of extra tokens we train on for the 540B\nmodel is approximately 1.3 Billion which constitutes 0.16% extra computation... Training an U-PaLM 540B model only consumes 512 TPUv4 chips and finishes in about 5 days which is considered to be lightweight.\"\n\nPaLM was 2.5e24\n0.16% of that is 4e21",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "LMSI-Palm",
      "Organization": "Google,University of Illinois Urbana-Champaign (UIUC)",
      "Publication date": "2022-10-20",
      "Domain": "Language",
      "Task": "Language generation,Language modeling/generation,Question answering,Mathematical reasoning",
      "Parameters": "540000000000.0",
      "Parameters notes": "540B",
      "Training compute (FLOP)": "",
      "Training compute notes": "(fine-tuned from Palm-540B, which was 2.52e24)",
      "Training dataset": "GSM8K",
      "Training dataset size (gradients)": "1920000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2210.11610",
      "Reference": "Large Language Models Can Self-Improve",
      "Citations": "749.0",
      "Authors": "Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han",
      "Abstract": "Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate \"high-confidence\" rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and 63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that fine-tuning on reasoning is critical for self-improvement.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Table 3\n\"We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and 63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "PaLM (540B)",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "\"To reduce the training burden, we sample 5k examples from the non-football and football partition of the DROP dataset, and sample 5k examples from ANLI-A2 and ANLI-A3. For each dataset, we fine-tune the model for 10k steps with a learning rate of 5e\u22125\nand a batch size of 32.\" Not sure about sequence length",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Flan-PaLM 540B",
      "Organization": "Google",
      "Publication date": "2022-10-20",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering,Mathematical reasoning",
      "Parameters": "540000000000.0",
      "Parameters notes": "540B",
      "Training compute (FLOP)": "2.540000000001e+24",
      "Training compute notes": "0.2% greater than Palm 540B, which used 2.5e24",
      "Training dataset": "Flan",
      "Training dataset size (gradients)": "1400000000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2210.11416",
      "Reference": "Scaling Instruction-Finetuned Language Models",
      "Citations": "3771.0",
      "Authors": "Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, Jason Wei",
      "Abstract": "Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": ">1k cites\n\n\"Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU.\"",
      "Epochs": "",
      "Training time (hours)": "37.0",
      "Training time notes": "\"we only use 0.2% of the pre-training compute to instruction-finetune Flan-PaLM 540B (approximately 512 v4 TPU chips for 37 hours)\"",
      "Training hardware": "Google TPU v4",
      "Hardware quantity": "512.0",
      "Hardware utilization (MFU)": "0.1892",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "348322.23114529514",
      "Base model": "PaLM (540B)",
      "Finetune compute (FLOP)": "5.6e+21",
      "Finetune compute notes": "5.6e21 per Table 2\n\n\"we only use 0.2% of the pre-training compute to instruction-finetune Flan-PaLM 540B (approximately 512 v4 TPU chips for 37 hours)\"\n\n512 * 37 * 3600 * 275 teraflops * 0.3 = 5.6e21 (so 30% utilization was correct)",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GenSLM",
      "Organization": "University of Chicago,NVIDIA,Harvard University,Cerebras Systems,Technical University of Munich,California Institute of Technology",
      "Publication date": "2022-10-11",
      "Domain": "Biology",
      "Task": "Protein or nucleotide language model (pLM/nLM)",
      "Parameters": "25000000000.0",
      "Parameters notes": "See Table 3",
      "Training compute (FLOP)": "1.42e+21",
      "Training compute notes": "See Table 3\nOverall ZettaFlops 1.42",
      "Training dataset": "SARS-CoV-2 genome dataset,BV-BRC",
      "Training dataset size (gradients)": "225280000000",
      "Dataset size notes": "110,000,000 sequences * 512 tokens/sequence = 56,320,000,000 tokens (5.6e10)",
      "Confidence": "Confident",
      "Link": "https://www.biorxiv.org/content/biorxiv/early/2022/10/11/2022.10.10.511571.full.pdf",
      "Reference": "GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics",
      "Citations": "114.0",
      "Authors": "Maxim Zvyagin, Alexander Brace, Kyle Hippe, Yuntian Deng, Bin Zhang, Cindy Orozco Bohorquez, Austin Clyde, Bharat Kale, Danilo Perez-Rivera, Heng Ma, Carla M. Mann, Michael Irvin, J. Gregory Pauloski, Logan Ward, Valerie Hayot, Murali Emani, Sam Foreman, Zhen Xie, Diangen Lin, Maulik Shukla, Weili Nie, Josh Romero, Christian Dallago, Arash Vahdat, Chaowei Xiao, Thomas Gibbs, Ian Foster, James J. Davis, Michael E. Papka, Thomas Brettin, Rick Stevens, Anima Anandkumar, Venkatram Vishwanath, Arvind Ramanathan",
      "Abstract": "Our work seeks to transform how new and emergent variants of pandemic causing viruses, specially SARS-CoV-2, are identified and classified. By adapting large language models (LLMs) for genomic data, we build genome-scale language models (GenSLMs) which can learn the evolutionary landscape of SARS-CoV-2 genomes. By pretraining on over 10 million prokaryotic gene sequences, and then finetuning a SARS-CoV-2 specific model on 1.5 million genomes, we show that GenSLM can accurately and rapidly identify variants of concern. Thus, to our knowledge, GenSLM represents one of the first whole genome scale foundation models which can generalize to other prediction tasks. We demonstrate the scaling of GenSLMs on both GPU-based supercomputers and AI-hardware accelerators, achieving over 1.54 zettaflops in training runs. We present initial scientific insights gleaned from examining GenSLMs in tracking the evolutionary dynamics of SARS-CoV-2, noting that its full potential on large biological data is yet to be realized.",
      "Organization categorization": "Academia,Industry,Academia,Industry,Academia,Academia",
      "Country (of organization)": "United States of America,United States of America,United States of America,United States of America,Germany,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Together, these capabilities go beyond state-of-the-art techniques\nfor global-scale whole genome surveillance of pandemic-causing\nviruses and address a critical infrastructure need for the global\npublic health organization\" - SOTA improvement on very specific task\n\nI haven't found standard benchmarks SOTA claims",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT license\nhttps://github.com/ramanathanlab/genslm",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Diplodocus",
      "Organization": "Meta AI,Massachusetts Institute of Technology (MIT)",
      "Publication date": "2022-10-11",
      "Domain": "Games",
      "Task": "Diplomacy",
      "Parameters": "",
      "Parameters notes": "may be estimated from https://github.com/facebookresearch/diplomacy_cicero?tab=readme-ov-file",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "\"we train the architecture described in Appendix F on a dataset of roughly 46000 online Diplomacy games provided by webdiplomacy.net.\"\nthen self-play training",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/2210.05492",
      "Reference": "Mastering the Game of No-Press Diplomacy via Human-Regularized Reinforcement Learning and Planning",
      "Citations": "54.0",
      "Authors": "Anton Bakhtin, David J Wu, Adam Lerer, Jonathan Gray, Athul Paul Jacob, Gabriele Farina, Alexander H Miller, Noam Brown",
      "Abstract": "No-press Diplomacy is a complex strategy game involving both cooperation and competition that has served as a benchmark for multi-agent AI research. While self-play reinforcement learning has resulted in numerous successes in purely adversarial games like chess, Go, and poker, self-play alone is insufficient for achieving optimal performance in domains involving cooperation with humans. We address this shortcoming by first introducing a planning algorithm we call DiL-piKL that regularizes a reward-maximizing policy toward a human imitation-learned policy. We prove that this is a no-regret learning algorithm under a modified utility function. We then show that DiL-piKL can be extended into a self-play reinforcement learning algorithm we call RL-DiL-piKL that provides a model of human play while simultaneously training an agent that responds well to this human model. We used RL-DiL-piKL to train an agent we name Diplodocus. In a 200-game no-press Diplomacy tournament involving 62 human participants spanning skill levels from beginner to expert, two Diplodocus agents both achieved a higher average score than all other participants who played more than two games, and ranked first and third according to an Elo ratings model.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "SOTA Improvement in no-press Diplomacy\n\"In a 200-game no-press Diplomacy tournament involving 62 human participants spanning skill levels from beginner to expert, two Diplodocus agents both achieved a higher average score than all other participants who played more than two games, and ranked first and third according to an Elo ratings model. \"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "creative commons (non comm) for model weights, MIT for code\nhttps://github.com/facebookresearch/diplomacy_cicero?tab=readme-ov-file#license-for-model-weights",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Phenaki",
      "Organization": "University College London (UCL),University of Michigan,Google Brain",
      "Publication date": "2022-10-05",
      "Domain": "Video",
      "Task": "Video generation,Text-to-video",
      "Parameters": "1800000000.0",
      "Parameters notes": "Unless specified otherwise, we train a 1.8B parameter Phenaki model on a corpus of \u223c15M textvideo pairs at 8 FPS mixed with \u223c50M text-images plus \u223c400M pairs of LAION-400M [41] (more\ndetails in Appendix B.3). The model used in the visualisations in this paper was trained for 1 million\nsteps at a batch size of 512, which took less than 5 days. In this setup 80% of the training data came\nfrom the video dataset and each image dataset contributed 10%.",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "LAION-400M,Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "Unless specified otherwise, we train a 1.8B parameter Phenaki model on a corpus of \u223c15M textvideo pairs at 8 FPS mixed with \u223c50M text-images plus \u223c400M pairs of LAION-400M [41] (more\ndetails in Appendix B.3). The model used in the visualisations in this paper was trained for 1 million\nsteps at a batch size of 512, which took less than 5 days. In this setup 80% of the training data came\nfrom the video dataset and each image dataset contributed 10%.",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/2210.02399",
      "Reference": "Phenaki: Variable Length Video Generation From Open Domain Textual Description",
      "Citations": "289.0",
      "Authors": "Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, Dumitru Erhan",
      "Abstract": "We present Phenaki, a model capable of realistic video synthesis, given a sequence of textual prompts. Generating videos from text is particularly challenging due to the computational cost, limited quantities of high quality text-video data and variable length of videos. To address these issues, we introduce a new model for learning video representation which compresses the video to a small representation of discrete tokens. This tokenizer uses causal attention in time, which allows it to work with variable-length videos. To generate video tokens from text we are using a bidirectional masked transformer conditioned on pre-computed text tokens. The generated video tokens are subsequently de-tokenized to create the actual video. To address data issues, we demonstrate how joint training on a large corpus of image-text pairs as well as a smaller number of video-text examples can result in generalization beyond what is available in the video datasets. Compared to the previous video generation methods, Phenaki can generate arbitrary long videos conditioned on a sequence of prompts (i.e. time variable text or a story) in open domain. To the best of our knowledge, this is the first time a paper studies generating videos from time variable prompts. In addition, compared to the per-frame baselines, the proposed video encoder-decoder computes fewer tokens per video but results in better spatio-temporal consistency.",
      "Organization categorization": "Academia,Academia,Industry",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland,United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"To the best of our knowledge, this is the first time a paper studies generating videos from time variable prompts\"\n\nThey don't claim any absolute SOTA results",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Unreleased",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "AlphaTensor",
      "Organization": "DeepMind",
      "Publication date": "2022-10-05",
      "Domain": "Other,Games,Mathematics",
      "Task": "",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "7.1414784e+20",
      "Training compute notes": "Compute: 0.3 [assumed utilization] * (64 cores / 2 cores per chip )*123000000000000 FLOP / TPU v3 chip / sec * 7 days * 24 hours / day * 3600 sec / hour = 7.1414784e+20 FLOP",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "\"We create a dataset containing 5 million such tensor-factorization pairs.\"",
      "Confidence": "Confident",
      "Link": "https://www.nature.com/articles/s41586-022-05172-4",
      "Reference": "Discovering faster matrix multiplication algorithms with reinforcement learning",
      "Citations": "",
      "Authors": "Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Francisco J. R. Ruiz, Julian Schrittwieser, Grzegorz Swirszcz, David Silver, Demis Hassabis & Pushmeet Kohli",
      "Abstract": "Improving the efficiency of algorithms for fundamental computations can have a widespread impact, as it can affect the overall speed of a large amount of computations. Matrix multiplication is one such primitive task, occurring in many systems\u2014from neural networks to scientific computing routines. The automatic discovery of algorithms using machine learning offers the prospect of reaching beyond human intuition and outperforming the current best human-designed algorithms. However, automating the algorithm discovery procedure is intricate, as the space of possible algorithms is enormous. Here we report a deep reinforcement learning approach based on AlphaZero1 for discovering efficient and provably correct algorithms for the multiplication of arbitrary matrices. Our agent, AlphaTensor, is trained to play a single-player game where the objective is finding tensor decompositions within a finite factor space. AlphaTensor discovered algorithms that outperform the state-of-the-art complexity for many matrix sizes. Particularly relevant is the case of 4\u2009\u00d7\u20094 matrices in a finite field, where AlphaTensor\u2019s algorithm improves on Strassen\u2019s two-level algorithm for the first time, to our knowledge, since its discovery 50 years ago2. We further showcase the flexibility of AlphaTensor through different use-cases: algorithms with state-of-the-art complexity for structured matrix multiplication and improved practical efficiency by optimizing matrix multiplication for runtime on specific hardware. Our results highlight AlphaTensor\u2019s ability to accelerate the process of algorithmic discovery on a range of problems, and to optimize for different criteria.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "168.0",
      "Training time notes": "In practice, the procedure  takes a week to converge.",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "64.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "57646.09262932102",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache 2.0 for algorithmic outputs\nhttps://github.com/google-deepmind/alphatensor",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DiffDock",
      "Organization": "Massachusetts Institute of Technology (MIT)",
      "Publication date": "2022-10-04",
      "Domain": "Biology",
      "Task": "Proteins",
      "Parameters": "20240000.0",
      "Parameters notes": "\"For determining the hyperparameters of DIFFDOCK\u2019s score model, we trained\nsmaller models (3.97 million parameters) that fit into 48GB of GPU RAM before scaling it up to the final model (20.24 million parameters) that was trained on four 48GB GPUs\"\n\nThere's a separate 4.77M \"confidence model\" that helps make predictions along with the score model",
      "Training compute (FLOP)": "7.2e+19",
      "Training compute notes": "\"We trained our final score model on four 48GB RTX A6000 GPUs for 850 epochs (around 18 days).\"\n\n4 * 38.7 teraflops * 18 days * 24 * 3600 * 0.3 = 7.2e19\n\nhttps://www.techpowerup.com/gpu-specs/rtx-a6000.c3686",
      "Training dataset": "PDB (Protein Data Bank)",
      "Training dataset size (gradients)": "4352000",
      "Dataset size notes": "\"We employ the time-split of PDBBind proposed by Stark et al. [2022] with 17k complexes from 2018 or earlier for training/validation and 363 test structures from 2019 with no ligand overlap with the training complexes\"",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2210.01776, https://docs.nvidia.com/bionemo-framework/latest/models/diffdock.html",
      "Reference": "DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking",
      "Citations": "617.0",
      "Authors": "Gabriele Corso, Hannes St\u00e4rk, Bowen Jing, Regina Barzilay, Tommi Jaakkola",
      "Abstract": "Predicting the binding structure of a small molecule ligand to a protein -- a task known as molecular docking -- is critical to drug design. Recent deep learning methods that treat docking as a regression problem have decreased runtime compared to traditional search-based methods but have yet to offer substantial improvements in accuracy. We instead frame molecular docking as a generative modeling problem and develop DiffDock, a diffusion generative model over the non-Euclidean manifold of ligand poses. To do so, we map this manifold to the product space of the degrees of freedom (translational, rotational, and torsional) involved in docking and develop an efficient diffusion process on this space. Empirically, DiffDock obtains a 38% top-1 success rate (RMSD<2A) on PDBBind, significantly outperforming the previous state-of-the-art of traditional docking (23%) and deep learning (20%) methods. Moreover, while previous methods are not able to dock on computationally folded structures (maximum accuracy 10.4%), DiffDock maintains significantly higher precision (21.7%). Finally, DiffDock has fast inference times and provides confidence estimates with high selective accuracy.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"DiffDock obtains a 38% top-1 success rate (RMSD<2A) on PDBBind, significantly outperforming the previous state-of-the-art of traditional docking (23%) and deep learning (20%) methods\"",
      "Epochs": "850.0",
      "Training time (hours)": "432.0",
      "Training time notes": "18 days",
      "Training hardware": "NVIDIA RTX A6000",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT license\nhttps://github.com/gcorso/DiffDock",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Make-A-Video",
      "Organization": "Meta AI",
      "Publication date": "2022-09-29",
      "Domain": "Video",
      "Task": "Video generation,Text-to-video",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "LAION,WebVid-10M,HD-VILA-100M",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/2209.14792",
      "Reference": "Make-A-Video: Text-to-Video Generation without Text-Video Data",
      "Citations": "1763.0",
      "Authors": "Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, Yaniv Taigman",
      "Abstract": "We propose Make-A-Video -- an approach for directly translating the tremendous recent progress in Text-to-Image (T2I) generation to Text-to-Video (T2V). Our intuition is simple: learn what the world looks like and how it is described from paired text-image data, and learn how the world moves from unsupervised video footage. Make-A-Video has three advantages: (1) it accelerates training of the T2V model (it does not need to learn visual and multimodal representations from scratch), (2) it does not require paired text-video data, and (3) the generated videos inherit the vastness (diversity in aesthetic, fantastical depictions, etc.) of today's image generation models. We design a simple yet effective way to build on T2I models with novel and effective spatial-temporal modules. First, we decompose the full temporal U-Net and attention tensors and approximate them in space and time. Second, we design a spatial temporal pipeline to generate high resolution and frame rate videos with a video decoder, interpolation model and two super resolution models that can enable various applications besides T2V. In all aspects, spatial and temporal resolution, faithfulness to text, and quality, Make-A-Video sets the new state-of-the-art in text-to-video generation, as determined by both qualitative and quantitative measures.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Table 2",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Hosted access (no API)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Whisper",
      "Organization": "OpenAI",
      "Publication date": "2022-09-21",
      "Domain": "Speech",
      "Task": "Speech recognition (ASR)",
      "Parameters": "1550000000.0",
      "Parameters notes": "Table 1",
      "Training compute (FLOP)": "4.2072663e+21",
      "Training compute notes": "See figure 9",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "12403200000",
      "Dataset size notes": "\"When scaled to 680,000 hours of multilingual and multitask\nsupervision, the resulting models generalize well\nto standard benchmarks and are often competitive\nwith prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning.\"\n\n\n13,680 words/h * 680,000h = 9,302,400,000 words",
      "Confidence": "Likely",
      "Link": "https://cdn.openai.com/papers/whisper.pdf\n\nhttps://arxiv.org/abs/2212.04356",
      "Reference": "Robust Speech Recognition via Large-Scale Weak Supervision",
      "Citations": "5597.0",
      "Authors": "Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever",
      "Abstract": "We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We achieve a new state of the art of 29.1 BLEU zero-shot without using any of the CoVoST2 training data.\"",
      "Epochs": "3.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "256.0",
      "Batch size notes": "Table 17",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "MIT for weights:\nhttps://github.com/openai/whisper\n\nthe repo looks like just inference code to me. also, this paper says it's just inference code and they reproduced their version of Whisper through other means: https://arxiv.org/pdf/2309.13876 ",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "SauTech",
      "Organization": "Saudi Data and Artificial Intelligence Authority,Saudi Company for Artificial Intelligence",
      "Publication date": "2022-09-14",
      "Domain": "Speech,Audio",
      "Task": "Speech recognition (ASR),Speech-to-text",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "The model has been trained on approximately 16k hours of dialectal Arabic speech. The total number of datasets is 8. The training data was balanced, with approximately 2,000 hours representing each dialect.",
      "Confidence": "Unknown",
      "Link": "https://globalaisummit.org/en/News/Pages/NewsDetails.aspx?NewsId=73\nhttps://saudigazette.com.sa/article/624984/SAUDI-ARABIA/SDAIA-SCAI-unveil-SauTech-speech-to-text-software",
      "Reference": "SDAIA and SCAI Unveil \"SauTech\" Speech-to-text Software\n",
      "Citations": "",
      "Authors": "",
      "Abstract": "The model is based on the wav2vec 2.0 architecture which uses CTC loss/decoding. The model consists of a convolutional feature encoder for extracting latent speech representations, a transformer-based context network for modeling long-range dependencies, and a quantization module for discretizing features. This design enables the model to produce rich, context-aware speech representations suitable for downstream speech recognition tasks.",
      "Organization categorization": "Industry,Government,Industry,Government",
      "Country (of organization)": "Saudi Arabia,Saudi Arabia",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "The model outperforms the next best system by approximately 8% and 9% on Modern Standard Arabic and Saudi dialects respectively. \n\nThey don't report any results on standard benchmarks",
      "Epochs": "",
      "Training time (hours)": "720.0",
      "Training time notes": "",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "8.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "6408.117502334789",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Hosted access (no API)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "PaLI",
      "Organization": "Google",
      "Publication date": "2022-09-14",
      "Domain": "Language,Vision,Multimodal",
      "Task": "Visual question answering,Language modeling/generation,Image captioning",
      "Parameters": "16900000000.0",
      "Parameters notes": "3.9b Image Encoder, \n14b Multimodal Encoder-Decoder",
      "Training compute (FLOP)": "1.69e+23",
      "Training compute notes": "Pre-training the ViT component involved 1.1 million steps (they train over 1M steps but run the last 100k twice and then average the two resulting models). Batch size is 16384 and the inputs are 224x224. Table 8 indicates a forward pass with ViT-e/14 on a 224 image takes 1980 GFLOPs, so total training compute for the ViT-e/14 model is:\n1980e9 * 16384 * 1.1 million * 3 (account for backward passes) = 1.07e23\n\nIn the \"overal model\" section, they then say: \"The largest model, PaLI-17B, is pretrained using 1,024 GCP-TPUv4 chips for 7 days\". It is then trained for another 3 days on 512 chips at higher resolution. \n\nI assume the stated TPUv4 training does not include the ViT pretraining, since it amounts to fewer FLOPs than we estimate above for the ViT.\n\n275 teraFLOP/s * ((1024 * 7) + (512 * 3)) * 24 * 3600 * 0.3 (utilization assumption) = 6.2e22\n\nTotal: 1.07e23 + 6.2e22 = 1.69e23",
      "Training dataset": "WebLI",
      "Training dataset size (gradients)": "143507000000",
      "Dataset size notes": "\"During training, the model passes over 1.6B images, one epoch over the entire pretraining dataset\"",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2209.06794v4",
      "Reference": "PaLI: A Jointly-Scaled Multilingual Language-Image Model",
      "Citations": "897.0",
      "Authors": "Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, Radu Soricut",
      "Abstract": "Effective scaling and a flexible task interface enable large language models to excel at many tasks. We present PaLI (Pathways Language and Image model), a model that extends this approach to the joint modeling of language and vision. PaLI generates text based on visual and textual inputs, and with this interface performs many vision, language, and multimodal tasks, in many languages. To train PaLI, we make use of large pre-trained encoder-decoder language models and Vision Transformers (ViTs). This allows us to capitalize on their existing capabilities and leverage the substantial cost of training them. We find that joint scaling of the vision and language components is important. Since existing Transformers for language are much larger than their vision counterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the benefits from even larger-capacity vision models. To train PaLI, we create a large multilingual mix of pretraining tasks, based on a new image-text training set containing 10B images and texts in over 100 languages. PaLI achieves state-of-the-art in multiple vision and language tasks (such as captioning, visual question-answering, scene-text understanding), while retaining a simple, modular, and scalable design.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"PaLI achieves state-of-the-art in multiple vision and language tasks (such as captioning, visual question-answering, scene-text understanding)\"\n\n\"PaLI-17B achieves state-of-the-art (SOTA) results on multiple benchmarks, outperforming some strong models. Specifically, PaLI outperforms recent and concurrent models on the long-standing\nCOCO Captioning benchmark (Chen et al., 2015), with 149.1 CIDEr score on the Karpathy split (Karpathy & Fei-Fei, 2015). PaLI also achieves a new SOTA of 84.3% on VQAv2 (Goyal et al., 2017) while using an open-vocabulary text generative setting that is similar to Flamingo (Alayrac et al., 2022).\"",
      "Epochs": "1.0",
      "Training time (hours)": "240.0",
      "Training time notes": "10",
      "Training hardware": "Google TPU v4",
      "Hardware quantity": "1024.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "50878.10777366616",
      "Compute cost notes": "",
      "Training power draw (W)": "697203.184254025",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "BEIT-3",
      "Organization": "Microsoft",
      "Publication date": "2022-08-22",
      "Domain": "Multimodal,Vision,Language",
      "Task": "Object detection,Semantic segmentation,Image classification,Visual question answering,Image captioning,Language generation",
      "Parameters": "1900000000.0",
      "Parameters notes": "1.9B from Table 2",
      "Training compute (FLOP)": "7e+19",
      "Training compute notes": "from Table 11, 1M training steps with batch size 6144. \nFrom Table 2 we have that model have 1.9B parameters.\nModel is VIT",
      "Training dataset": "ImageNet21k,COCO,English Wikipedia,BookCorpus (BooksCorpus, Toronto Book Corpus)",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "from Table 3\n21M pairs image text,\n14M images,160GB documents",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2208.10442",
      "Reference": "Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks",
      "Citations": "704.0",
      "Authors": "Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks",
      "Abstract": "A big convergence of language, vision, and multimodal pretraining is emerging. In this work, we introduce a general-purpose multimodal foundation model BEiT-3, which achieves state-of-the-art transfer performance on both vision and vision-language tasks. Specifically, we advance the big convergence from three aspects: backbone architecture, pretraining task, and model scaling up. We introduce Multiway Transformers for general-purpose modeling, where the modular architecture enables both deep fusion and modality-specific encoding. Based on the shared backbone, we perform masked \"language\" modeling on images (Imglish), texts (English), and image-text pairs (\"parallel sentences\") in a unified manner. Experimental results show that BEiT-3 obtains state-of-the-art performance on object detection (COCO), semantic segmentation (ADE20K), image classification (ImageNet), visual reasoning (NLVR2), visual question answering (VQAv2), image captioning (COCO), and cross-modal retrieval (Flickr30K, COCO). ",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "from abstract: 'In this work, we introduce a general-purpose multimodal foundation model BEiT-3, which achieves state-of-the-art transfer performance on both vision and vision-language tasks.'\n\n\"Experimental results show that BEiT-3 obtains state-of-the-art performance on object detection (COCO), semantic segmentation (ADE20K), image classification (ImageNet), visual reasoning (NLVR2), visual question answering (VQAv2), image captioning (COCO), and cross-modal retrieval (Flickr30K, COCO).\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "apache 2.0\nhttps://github.com/microsoft/unilm/tree/master/beit3\n\nIt seems that there are no pre-training code, only fine-tuning code",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "BlenderBot 3",
      "Organization": "McGill University,Meta AI,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms)",
      "Publication date": "2022-08-10",
      "Domain": "Language",
      "Task": "Chat",
      "Parameters": "175000000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "4.3e+23",
      "Training compute notes": "(taken from OPT-175 base)",
      "Training dataset": "BlenderBot 3 Data",
      "Training dataset size (gradients)": "1300000000",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2208.03188, https://github.com/facebookresearch/ParlAI/blob/main/parlai/zoo/bb3/model_card.md\n\ntraining code: https://parl.ai/projects/bb3/ ",
      "Reference": "BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage",
      "Citations": "268.0",
      "Authors": "Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, Morteza Behrooz, William Ngan, Spencer Poff, Naman Goyal, Arthur Szlam, Y-Lan Boureau, Melanie Kambadur, Jason Weston",
      "Abstract": "We present BlenderBot 3, a 175B parameter dialogue model capable of open-domain conversation with access to the internet and a long-term memory, and having been trained on a large number of user defined tasks. We release both the model weights and code, and have also deployed the model on a public web page to interact with organic users. This technical report describes how the model was built (architecture, model and training scheme), and details of its deployment, including safety mechanisms. Human evaluations show its superiority to existing open-domain dialogue agents, including its predecessors (Roller et al., 2021; Komeili et al., 2022). Finally, we detail our plan for continual learning using the data collected from deployment, which will also be publicly released. The goal of this research program is thus to enable the community to study ever-improving responsible agents that learn through interaction.",
      "Organization categorization": "Academia,Industry,Academia",
      "Country (of organization)": "Canada,United States of America,Canada",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Human evaluations show its superiority to existing open-domain dialogue agents, including its predecessors\"\n\nThey don't claim absolute SOTA",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA A100 SXM4 40 GB",
      "Hardware quantity": "128.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "102609.82584809877",
      "Base model": "OPT-175B",
      "Finetune compute (FLOP)": "1.5e+21",
      "Finetune compute notes": "\"The 30B and 175B parameter BlenderBot 3 models were each trained for one epoch of the training data\non 64 (30B) or 128 (175B) x 40gb A100 GPUs; we found that the model (especially the 175B version)\noverfit significantly when seeing the training data more than once. The 175B model was trained with\na batch size of 2^18 and the 30B model was trained with a batch size of 2^19, resulting in roughly 5600\nupdates and 2800 updates respectively.\"\n\n175b params * 5600 * 2^18 * 6 = 1.5e21\n",
      "Batch size": "262144.0",
      "Batch size notes": "Note that this is batch size for fine-tuning. Blenderbot is based on OPT-175B which had batch size 2M.\n\n\"The 175B model was trained with a batch size of 2^18\"\n2^18 = 262144",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "weights have a non-commercial license, must go through request form: https://docs.google.com/forms/d/e/1FAIpQLSfRzw8xVzxaxgRyuodTZtkcYADAjzYjN5gcxx6DMa4XaGwwhQ/viewform\n\nmeanwhile training code is here. repo is MIT-licensed https://github.com/facebookresearch/ParlAI/blob/main/parlai/scripts/train_model.py ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "1828568.0841233819",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GLM-130B",
      "Organization": "Tsinghua University",
      "Publication date": "2022-08-04",
      "Domain": "Language",
      "Task": "Language modeling/generation,Translation",
      "Parameters": "130000000000.0",
      "Parameters notes": "Dense model",
      "Training compute (FLOP)": "3.5490054945e+23",
      "Training compute notes": "\"96 NVIDIA A100 (40G * 8) servers for 2 months\"\n\n312 TFLOPS/GPU * 96 servers * 8 GPU/server * 2 months * 32.5% utilization = 4.037e23\n\nutilization rate - citation from the paper: \"we report hardware FLOPs utilization (HFU) of 43.3% and model FLOPs utilization (MFU) of 32.5% due to re-materialization.\"\n\nAligns pretty well with 6ND:\n6 * 400B * 130B = 3.12E23\n\nGeometric mean: sqrt(4.037e23 * 3.12e23) = 3.549e23",
      "Training dataset": "The Pile,WuDao Corpora",
      "Training dataset size (gradients)": "152000000000",
      "Dataset size notes": "400B \"We completed the 400B-token training and evaluation of GLM-130B in July, and subsequently released the model and pre-training details in August 2022. \"  from https://arxiv.org/pdf/2406.12793\n\n\"As of July 3rd, 2022, GLM-130B has been trained on over 400 billion text tokens (200B each for Chinese and English)\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2210.02414",
      "Reference": "GLM-130B: An Open Bilingual Pre-trained Model",
      "Citations": "1207.0",
      "Authors": "Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao Dong, Jie Tang",
      "Abstract": "GLM-130B (ICLR 2023) is an open bilingual (English & Chinese) bidirectional dense model with 130 billion parameters, pre-trained using the General Language Model (GLM) algorithm1. It is designed to support inference tasks with the 130B parameters on a single A100 (40G * 8) or V100 (32G * 8) server. As of July 3rd, 2022, GLM-130B has been trained on over 400 billion text tokens (200B each for Chinese and English) ",
      "Organization categorization": "Academia",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"GLM-130B achieves an accuracy of 80.2% on zero-shot LAMBADA (En), while 76.2% for GPT-3 175B and 77.9% for the SOTA offered by PaLM 540B.\"\n\n\"We compare GLM-130B to the largest existing Chinese monolingual language model\u2014the 260B ERNIE Titan 3.0 (Wang et al., 2021). <..> GLM-130B consistently outperforms ERNIE Titan 3.0 across 12 tasks (Cf. Figure 8).\"",
      "Epochs": "1.0",
      "Training time (hours)": "1440.0",
      "Training time notes": "\"During the 60-day access to the cluster, we manage to train GLM-130B for 400 billion tokens\"\n60 days * 24 = 1,440 hours",
      "Training hardware": "NVIDIA A100 SXM4 40 GB",
      "Hardware quantity": "768.0",
      "Hardware utilization (MFU)": "0.325",
      "Training compute cost (2023 USD)": "820296.6313095269",
      "Compute cost notes": "",
      "Training power draw (W)": "615741.2226117075",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "8650752.0",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "non commercial license. looks like inference but not training code: https://github.com/THUDM/GLM-130B/blob/main/MODEL_LICENSE",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "11018828.587786926",
      "Hardware utilization (HFU)": "0.433",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "AlexaTM 20B",
      "Organization": "Amazon",
      "Publication date": "2022-08-02",
      "Domain": "Language",
      "Task": "Language modeling,Translation,Question answering",
      "Parameters": "19750000000.0",
      "Parameters notes": "See Table 1 on p.3 of the paper",
      "Training compute (FLOP)": "2.04374016e+23",
      "Training compute notes": "Training throughput is reported as 154 TFLOP/s - see p.5 of the paper.\n\"We relied on an internal and optimized version of DeepSpeed that we have since open-sourced (Chiu & Zheng, 2022) to obtain training throughput of up to 154 TFLOPS/GPU on 16 AWS p4d.24xlarge compute instances.\"\n\nAccelerator compute days are reported as 15,360 days - see Table 17 on p.18 of the paper.",
      "Training dataset": "mC4,Wikipedia",
      "Training dataset size (gradients)": "1319000000000",
      "Dataset size notes": "See Table 2 on p.3 of the paper.\n\n119B Wikipedia tokens + 1.2T mC4 tokens = 1319000000000 tokens",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2208.01448",
      "Reference": "AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model",
      "Citations": "89.0",
      "Authors": "Saleh Soltan, Shankar Ananthakrishnan, Jack FitzGerald, Rahul Gupta, Wael Hamza, Haidar Khan, Charith Peris, Stephen Rawls, Andy Rosenbaum, Anna Rumshisky, Chandana Satya Prakash, Mukund Sridhar, Fabian Triefenbach, Apurv Verma, Gokhan Tur, Prem Natarajan",
      "Abstract": "In this work, we demonstrate that multilingual large-scale sequence-to-sequence (seq2seq) models, pre-trained on a mixture of denoising and Causal Language Modeling (CLM) tasks, are more efficient few-shot learners than decoder-only models on various tasks. In particular, we train a 20 billion parameter multilingual seq2seq model called Alexa Teacher Model (AlexaTM 20B) and show that it achieves state-of-the-art (SOTA) performance on 1-shot summarization tasks, outperforming a much larger 540B PaLM decoder model. AlexaTM 20B also achieves SOTA in 1-shot machine translation, especially for low-resource languages, across almost all language pairs supported by the model (Arabic, English, French, German, Hindi, Italian, Japanese, Marathi, Portuguese, Spanish, Tamil, and Telugu) on Flores-101 dataset. We also show in zero-shot setting, AlexaTM 20B outperforms GPT3 (175B) on SuperGLUE and SQuADv2 datasets and provides SOTA performance on multilingual tasks such as XNLI, XCOPA, Paws-X, and XWinograd. Overall, our results present a compelling case for seq2seq models as a powerful alternative to decoder-only models for Large-scale Language Model (LLM) training. ",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "The Abstract reports SOTA improvement on multiple benchmarks.\n\n\"provides SOTA performance on multilingual tasks such as XNLI, XCOPA, Paws-X, and XWinograd\"",
      "Epochs": "",
      "Training time (hours)": "2880.0",
      "Training time notes": "See p.5 of the paper: \"We trained AlexaTM 20B for 120 days on 128 A100 GPUs...\"",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "128.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "267943.21130997164",
      "Compute cost notes": "",
      "Training power draw (W)": "102628.10792703854",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "2000000.0",
      "Batch size notes": "\"We trained AlexaTM 20B for 120 days on 128 A100 GPUs for the total of 500k updates with the accumulated batch size of 2 million tokens\"",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "https://aws.amazon.com/about-aws/whats-new/2022/11/alexatm-20b-model-available-sagemaker-jumpstart/?nc1=h_ls",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "0.4935",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "OmegaPLM",
      "Organization": "Massachusetts Institute of Technology (MIT),Westlake University",
      "Publication date": "2022-07-22",
      "Domain": "Biology",
      "Task": "Proteins,Protein folding prediction",
      "Parameters": "670000000.0",
      "Parameters notes": "\"Our model contains 66 layers with around 670 million parameters without sharing parameters, which doubles the layer count of ESM-1b but roughly retains the parameter count.\"",
      "Training compute (FLOP)": "1.03514112e+22",
      "Training compute notes": "\"OmegaPLM is implemented in PyTorch (44) and trained for 2,560 GPU Nvidia A100 80G days.\" \n\"Default precision format in Nvidia A100 GPUs is set to TensorFloat-32 for matrix operations.\"\n\nAssume 0.3 utilization for language model\n\nEstimate: (2560 * 24 * 3600) s * 156e12 FLOP/s * 0.3 * = 1.04e22",
      "Training dataset": "UniRef50",
      "Training dataset size (gradients)": "1258291200000",
      "Dataset size notes": "Number of sequences: 35 x 10^6\nSequence length: 512\nTotal data points: 35 x 10^6 x 512 = 1.792 x 10^10 tokens\n\nFirst stage crop size: 256\nFirst stage data points: 35 x 10^6 x 256 = 8.96 x 10^9 tokens\n\nAdditional structural data: ~110,000 sequences = 7.68 x 10^7 tokens\n\nFinal estimate: 8.96 x 10^9 tokens",
      "Confidence": "Confident",
      "Link": "https://www.biorxiv.org/content/10.1101/2022.07.21.500999v1",
      "Reference": "High-resolution de novo structure prediction from primary sequence",
      "Citations": "445.0",
      "Authors": "Ruidong Wu, Fan Ding, Rui Wang, Rui Shen, Xiwen Zhang, Shitong Luo, Chenpeng Su, Zuofan Wu, Qi Xie, Bonnie Berger, Jianzhu Ma, Jian Peng",
      "Abstract": "Recent breakthroughs have used deep learning to exploit evolutionary information in multiple sequence alignments (MSAs) to accurately predict protein structures. However, MSAs of homologous proteins are not always available, such as with orphan proteins or fast-evolving proteins like antibodies, and a protein typically folds in a natural setting from its primary amino acid sequence into its three-dimensional structure, suggesting that evolutionary information and MSAs should not be necessary to predict a protein\u2019s folded form. Here, we introduce OmegaFold, the first computational method to successfully predict high-resolution protein structure from a single primary sequence alone. Using a new combination of a protein language model that allows us to make predictions from single sequences and a geometry-inspired transformer model trained on protein structures, OmegaFold outperforms RoseTTAFold and achieves similar prediction accuracy to AlphaFold2 on recently released structures. OmegaFold enables accurate predictions on orphan proteins that do not belong to any functionally characterized protein family and antibodies that tend to have noisy MSAs due to fast evolution. Our study fills a much-encountered gap in structure prediction and brings us a step closer to understanding protein folding in nature.",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "United States of America,China",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "\"Here, we introduce OmegaFold, the first computational method to successfully predict high-resolution protein structure from a single primary sequence alone. Using a new combination of a protein language model that allows us to make predictions from single sequences and a geometry-inspired transformer model trained on protein structures, OmegaFold outperforms RoseTTAFold and achieves similar prediction accuracy to AlphaFold2 on recently released structures\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "2,560 GPU Nvidia A100 80G days",
      "Training hardware": "NVIDIA A100 SXM4 80 GB",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "52400.32118528479",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "2097152.0",
      "Batch size notes": "\"[...] each batch contains 4,096 sequences and each sequence is padded or cropped to 512 residues\" 4096 * 512 = 2097152",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "TF32",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ESM2-15B",
      "Organization": "Meta AI,New York University (NYU),Stanford University,Massachusetts Institute of Technology (MIT)",
      "Publication date": "2022-07-21",
      "Domain": "Biology",
      "Task": "Proteins,Protein or nucleotide language model (pLM/nLM),Protein folding prediction",
      "Parameters": "15000000000.0",
      "Parameters notes": "\"we train models up to 15B parameters\"",
      "Training compute (FLOP)": "7.35000000001e+22",
      "Training compute notes": "from xTrimoPGLM paper Table 9 (https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1): 5.1e22 FLOP\n\nfrom Arb Research (https://arbresearch.com/files/gen_bio.pdf): \"ESM-2-15B: 270000 updates x 3.2M batch size x 15 B \u201cconnections\u201d x 6. : 7.8e22 FLOP\n\nfrom the paper's Supplementary Materials: \n\"We trained each model over 512 NVIDIA V100 GPUs. ESM2 700M took 8 days to train. The 3B parameter LM took 30 days. The 15B model took 60 days.\"\n60 days x 512 V100s x an imputed 30% utilization\": 1e23 FLOP\n\nGeometric mean: 7.35e22",
      "Training dataset": "UniRef50",
      "Training dataset size (gradients)": "15360000000",
      "Dataset size notes": "Section A.1.1:\n\"This allowed ESM-2 models to train on over 60M protein sequences.\"\n\nAverage protein sequence is 200 tokens, per https://epoch.ai/blog/biological-sequence-models-in-the-context-of-the-ai-directives#fn:4 \n60M * 200 = 12B tokens\n\nEpochs: 15B model used 270k steps at 3.2M token batch size\n270k * 3.2M / 12B = 72",
      "Confidence": "Confident",
      "Link": "https://www.science.org/doi/abs/10.1126/science.ade2574\nhttps://www.biorxiv.org/content/10.1101/2022.07.20.500902v2",
      "Reference": "Evolutionary-scale prediction of atomic-level protein structure with a language model",
      "Citations": "636.0",
      "Authors": "Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives",
      "Abstract": "\"Recent advances in machine learning have leveraged evolutionary information in multiple sequence alignments to predict protein structure. We demonstrate direct inference of full atomic-level protein structure from primary sequence using a large language model. As language models of protein sequences are scaled up to 15 billion parameters, an atomic-resolution picture of protein structure emerges in the learned representations. This results in an order-of-magnitude acceleration of high-resolution structure prediction, which enables large-scale structural characterization of metagenomic proteins. We apply this capability to construct the ESM Metagenomic Atlas by predicting structures for >617 million metagenomic protein sequences, including >225 million that are predicted with high confidence, which gives a view into the vast breadth and diversity of natural proteins.\"",
      "Organization categorization": "Industry,Academia,Academia,Academia",
      "Country (of organization)": "United States of America,United States of America,United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Table S3\n\"The resulting ESM-2 model family significantly outperforms previously state-of-the-art ESM-1b (a \u223c650 million parameter model) at a comparable number of parameters, and on structure prediction benchmarks it also outperforms other recent protein language models\"",
      "Epochs": "72.0",
      "Training time (hours)": "1440.0",
      "Training time notes": "",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "512.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "163467.82019979745",
      "Compute cost notes": "",
      "Training power draw (W)": "307966.61145938886",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT weights, CC BY 4.0 data\nhttps://github.com/facebookresearch/esm?tab=readme-ov-file#available-esmssd\n\nmay just be inference code in the repo^",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "BLOOM-176B",
      "Organization": "Hugging Face,BigScience",
      "Publication date": "2022-07-11",
      "Domain": "Language",
      "Task": "Language modeling,Translation,Code generation",
      "Parameters": "176247271424.0",
      "Parameters notes": "See \"Technical Specifications\" on Hugging Face:\nhttps://huggingface.co/bigscience/bloom",
      "Training compute (FLOP)": "3.65664e+23",
      "Training compute notes": "https://bigscience.huggingface.co/blog/bloom Blog post says 117 days.\n\n384 A100 GPUs * 314 TFLOPS throughput per GPU * 117 days * 0.3 (utilization assumption) = 3.65664e23\nhttps://www.wolframalpha.com/input?i=384+*+314+TFLOPS+*+117+days+*+0.3",
      "Training dataset": "BigScience ROOTS Corpus",
      "Training dataset size (gradients)": "379000000000",
      "Dataset size notes": "Table 3.5 https://arxiv.org/pdf/2211.05100\n\n366B (pretrain) + 13B (finetune) = 379B  tokens total ",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2211.05100",
      "Reference": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model",
      "Citations": "2720.0",
      "Authors": "Margaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos Mu\u00f1oz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana Ili\u0107, G\u00e9rard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff",
      "Abstract": "Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",
      "Organization categorization": "Industry,Research collective",
      "Country (of organization)": "United States of America,France",
      "Notability criteria": "Historical significance,Highly cited",
      "Notability criteria notes": "Was the largest open-source model at the time. 1000+ researchers, many from important orgs such as Microsoft and NVIDIA.\n\nhttps://huggingface.co/bigscience/bloom",
      "Epochs": "1.0",
      "Training time (hours)": "2808.0",
      "Training time notes": "117 days * 24 hours/day",
      "Training hardware": "NVIDIA A100 SXM4 80 GB",
      "Hardware quantity": "384.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "995819.1171",
      "Compute cost notes": "",
      "Training power draw (W)": "308035.2013244814",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "4194304.0",
      "Batch size notes": "Table 3. 2048*2048",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "responsible use restrictions: https://bigscience.huggingface.co/blog/the-bigscience-rail-license",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "0.5",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "NLLB",
      "Organization": "Meta AI",
      "Publication date": "2022-07-06",
      "Domain": "Language",
      "Task": "Translation",
      "Parameters": "54500000000.0",
      "Parameters notes": "Section 8.2.4: \"The model has a total of 54.5B parameters\nand FLOPs similar to that of a 3.3B dense model\"",
      "Training compute (FLOP)": "1.751113728e+22",
      "Training compute notes": "Section 8.8:\n\" To train NLLB-200, a cumulative\nof 51968 GPU hours of computation was performed on hardware of type A100-SXM-80GB\"\nSee also Table 48\n\nSection 8.2.4 states they use FP16\n\nNVIDIA Datasheet states 312TFLOPS for FP16\nhttps://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-nvidia-us-2188504-web.pdf\n\nAssuming 0.3 utilization:\n\n312e12*3600*51968*0.3\n\nAlso:\n\"Our final model is a Transformer\nencoder-decoder model in which we replace the Feed Forward Network (FFN) layer in\nevery 4th Transformer block with a Sparsely Gated Mixture of Experts layer containing 128\nexperts. We use model dimension 2048, FFN dimension 8192, 16 attention heads, 24 encoder\nlayers and 24 decoder layers. We use Pre-LayerNorm (Xiong et al., 2020) as described in\nSection 6.1.1. We share the embedding weights of the encoder input embedding, decoder\ninput embedding and decoder output embedding layers. We use an overall dropout of 0.3,\nattention dropout 0.1 and EOM with peom=0.2. The model has a total of 54.5B parameters\nand FLOPs similar to that of a 3.3B dense model.\"",
      "Training dataset": "",
      "Training dataset size (gradients)": "300000000000",
      "Dataset size notes": "[WORDS]\n\nSection 8.2.2: \"As we prepare to train on the final 202 language dataset comprising of over 18B sentence\npairs and 2440 language directions\"\n\n18B sentences * 20 words/sentence",
      "Confidence": "Confident",
      "Link": "https://research.facebook.com/publications/no-language-left-behind/",
      "Reference": "No Language Left Behind: Scaling Human-Centered Machine Translation",
      "Citations": "1569.0",
      "Authors": "Marta R. Costa-juss\u00e0, James Cross, Onur \u00c7elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco (Paco) Guzm\u00e1n, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Jeff Wang",
      "Abstract": "Driven by the goal of eradicating language barriers on a global scale, machine translation has solidified itself as a key focus of artificial intelligence research today. However, such efforts have coalesced around a small subset of languages, leaving behind the vast majority of mostly low-resource languages. What does it take to break the 200 language barrier while ensuring safe, high quality results, all while keeping ethical considerations in mind? In No Language Left Behind, we took on this challenge by first contextualizing the need for low-resource language translation support through exploratory interviews with native speakers. Then, we created datasets and models aimed at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using a human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety. Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system. Finally, we open source all contributions described in this work, accessible at https://github.com/facebookresearch/fairseq/tree/nllb.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA A100 SXM4 80 GB",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "50667.25034038439",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "1000000.0",
      "Batch size notes": "\"We train the model for 300k steps using the 4 phase curriculum\ndescribed in Section 8.2.3. We use an effective batch size of 1M tokens per update.\"",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT\n\ntrain code: https://github.com/facebookresearch/fairseq/blob/nllb/examples/nllb/modeling/README.md ",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "CodeT5-large",
      "Organization": "Salesforce",
      "Publication date": "2022-07-05",
      "Domain": "Language",
      "Task": "Code generation",
      "Parameters": "770000000.0",
      "Parameters notes": "\"We pretrain a CodeT5-large model (770M) from scratch following T5-large\u2019s architecture\"",
      "Training compute (FLOP)": "2.72e+21",
      "Training compute notes": "\"We perform our experiments on a kubernetes with 16 A100-40G GPUs on Google Cloud Platform and the total pretraining duration is around 21 days\"\n\n16 * 312tFLOP/s * 21 * 24 * 3600 * 0.3 (utilization assumption) = 2.72e21",
      "Training dataset": "GitHub",
      "Training dataset size (gradients)": "10500000000",
      "Dataset size notes": "10.5b tokens",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2207.01780",
      "Reference": "CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning",
      "Citations": "373.0",
      "Authors": "Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, Steven C.H. Hoi ",
      "Abstract": "\"Program synthesis or code generation aims to generate a program that satisfies a problem specification. Recent approaches using large-scale pretrained language models (LMs) have shown promising results, yet they have some critical limitations. In particular, they often follow a standard supervised fine-tuning procedure to train a code generation model only from the pairs of natural-language problem descriptions and ground-truth programs. Such paradigm largely ignores some important but potentially useful signals in the problem specification such as unit tests, which thus often results in poor performance when solving complex unseen coding tasks. To address the limitations, we propose \"CodeRL\", a new framework for program synthesis tasks through pretrained LMs and deep reinforcement learning (RL). Specifically, during training, we treat the code-generating LM as an actor network, and introduce a critic network that is trained to predict the functional correctness of generated programs and provide dense feedback signals to the actor. During inference, we introduce a new generation procedure with a critical sampling strategy that allows a model to automatically regenerate programs based on feedback from example unit tests and critic scores. For the model backbones, we extended the encoder-decoder architecture of CodeT5 with enhanced learning objectives, larger model sizes, and better pretraining data. Our method not only achieves new SOTA results on the challenging APPS benchmark, but also shows strong zero-shot transfer capability with new SOTA results on the simpler MBPP benchmark.\"",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Our method not only achieves new SOTA results on the challenging APPS benchmark, but also shows strong zero-shot transfer capability with new SOTA results on the simpler MBPP benchmark.\"",
      "Epochs": "150.0",
      "Training time (hours)": "504.0",
      "Training time notes": "21 days",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "4478.145684414144",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "BSD-3-Clause license\nhttps://github.com/salesforce/CodeT5\n\nhttps://huggingface.co/Salesforce/codet5-large",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Minerva (540B)",
      "Organization": "Google",
      "Publication date": "2022-06-29",
      "Domain": "Language",
      "Task": "Quantitative reasoning,Mathematical reasoning,Language modeling/generation,Question answering",
      "Parameters": "540350000000.0",
      "Parameters notes": "\"To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model (PaLM).\"\n\nOur approach is to start with the PaLM pretrained decoder-only transformer language models Chowdhery et al. (2022), and further train (finetune) them on our mathematical dataset using an autoregressive objective.\nTable 2 contains the main model and training hyperparameters.\n\nSee Table 2",
      "Training compute (FLOP)": "2.7415e+24",
      "Training compute notes": "Minerva was fine-tuned from PaLM using the same hardware. Assume the same model FLOPs utilization rate for pre-training and fine-tuning.\n\"the 540B model was trained for 29 days on a v4-1024\"\n\nPaLM pretraining time: 6144 TPU for 1200 hours + 3072 TPU for 336 hours = @8404992 TPU-hours\nMinerva finetuning time: 1024 TPU for 696 hours = 712704 TPU-hours\nSo fine-tuning added 8.5% more compute.\n\nMinerva total compute = PaLM pretraining compute * (712704+8404992)/(8404992) = 2.7415*10^24 FLOP\nhttps://www.wolframalpha.com/input?i=%28712704%2B8404992%29%2F%288404992%29+*+2.5272*10%5E24\n\nPalm pretraining: 2.5272e+24",
      "Training dataset": "arXiv",
      "Training dataset size (gradients)": "26000000000",
      "Dataset size notes": "\"Our models were trained on a dataset of 38.5B tokens\" + PaLM\n\nupd 38.5B tokens - sie of the dataset, the model saw 26B tokens in 399k steps (see Table 2)",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2206.14858",
      "Reference": "Solving Quantitative Reasoning Problems with Language Models",
      "Citations": "1275.0",
      "Authors": "Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, Vedant Misra",
      "Abstract": "Language models have achieved remarkable performance on a wide range of tasks that require natural language understanding. Nevertheless, state-of-the-art models have generally struggled with tasks that require quantitative reasoning, such as solving mathematics, science, and engineering problems at the college level. To help close this gap, we introduce Minerva, a large language model pretrained on general natural language data and further trained on technical content. The model achieves state-of-the-art performance on technical benchmarks without the use of external tools. We also evaluate our model on over two hundred undergraduate-level problems in physics, biology, chemistry, economics, and other sciences that require quantitative reasoning, and find that the model can correctly answer nearly a third of them.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We achieve state-of-the-art performance on MATH Hendrycks et al. (2021), GSM8k Cobbe et al. (2021), and a STEM subset of the MMLU Hendrycks et al. (2020) dataset\"",
      "Epochs": "",
      "Training time (hours)": "696.0",
      "Training time notes": "",
      "Training hardware": "Google TPU v4",
      "Hardware quantity": "1024.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "698399.733142376",
      "Base model": "PaLM (540B)",
      "Finetune compute (FLOP)": "2.1429e+23",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ProGen2-xlarge",
      "Organization": "Salesforce Research,Columbia University,Johns Hopkins University",
      "Publication date": "2022-06-27",
      "Domain": "Biology",
      "Task": "Proteins,Protein generation,Protein or nucleotide language model (pLM/nLM)",
      "Parameters": "6400000000.0",
      "Parameters notes": "\"We introduce a suite of protein language models, named ProGen2, that are scaled up to 6.4B parameters\"",
      "Training compute (FLOP)": "1.35e+22",
      "Training compute notes": "Estimate 1:\n\"350,000 steps x 1m batch size x 6.4 B \u201cconnections\u201d x 6\" - Arb Research (https://arbresearch.com/files/gen_bio.pdf)\nSteps and batches from Table 1. \nFLOP estimate: 1.3e22\n\nTable 9 from here: https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1.full.pdf\nFLOP estimate: 1.4e22\n\nGeometric mean = 1.35e22 FLOP",
      "Training dataset": "UniRef90,BFD30",
      "Training dataset size (gradients)": "350000000000",
      "Dataset size notes": "350B from Table 9 https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2206.13517",
      "Reference": "ProGen2: Exploring the Boundaries of Protein Language Models",
      "Citations": "409.0",
      "Authors": "Erik Nijkamp, Jeffrey Ruffolo, Eli N. Weinstein, Nikhil Naik, Ali Madani\n",
      "Abstract": "Attention-based models trained on protein sequences have demonstrated incredible success at classification and generation tasks relevant for artificial intelligence- driven protein design. However, we lack a sufficient understanding of how very large-scale models and data play a role in effective protein model development. We introduce a suite of protein language models, named ProGen2, that are scaled up to 6.4B parameters and trained on different sequence datasets drawn from over a billion proteins from genomic, metagenomic, and immune repertoire databases. ProGen2 models show state-of-the-art performance in capturing the distribution of observed evolutionary sequences, generating novel viable sequences, and predicting protein fitness without additional finetuning. As large model sizes and raw numbers of protein sequences continue to become more widely accessible, our results suggest that a growing emphasis needs to be placed on the data distribution provided to a protein sequence model. We release the ProGen2 models and code at https://github.com/salesforce/progen.",
      "Organization categorization": "Industry,Academia,Academia",
      "Country (of organization)": "United States of America,United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"ProGen2 models show state-of-the-art performance in capturing the distribution of observed evolutionary sequences, generating novel viable sequences, and pre- dicting protein fitness without additional finetuning.\"\n\n\"In particular for the GB1 library, a challenging low-homology protein mutated\nat positions with non-linear epistasis, our largest models may exhibit emergent behavior (Wei et al., 2022) in zero-shot identification of the highest fitness variants.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "11850.178410269727",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "BSD license (permissive)\nhttps://github.com/salesforce/progen?tab=BSD-3-Clause-1-ov-file#readme",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Parti",
      "Organization": "Google Research",
      "Publication date": "2022-06-22",
      "Domain": "Image generation",
      "Task": "Text-to-image,Image generation",
      "Parameters": "20000000000.0",
      "Parameters notes": "Abstract: \"we achieve consistent quality improvements\nby scaling the encoder-decoder Transformer model up to 20B parameters\"",
      "Training compute (FLOP)": "5.09607936e+23",
      "Training compute notes": "Calculated from architecture. Does not take into account the encoding and decoding of text and images, only the transformer stack.\n\nTable 1 shows for the 20B model\n16 encoder layers\n64 decoder layers\nDmodel = 4096\nDhidden = 16384\nNum heads = 64\n\nJust below table 1:\n\"We use a maximum length of text tokens of 128, and the length of image tokens are fixed to 1024\"\n\nI take the length of the sequence to be 100 for the encoder stack and 1024 for the decoder stack.\n\nSection 3, Training: \"a total\nof 450,000 steps and final ratio of 0.025. We use a global batch size of 8192 during training.\"\n\n6* 20B parameters * (1024+128) sequence length*450000 steps*8192 batch size= 5.096079e+23",
      "Training dataset": "LAION-400M,FIT400M,JFT-4B",
      "Training dataset size (gradients)": "4718592000000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2206.10789v1",
      "Reference": "Scaling Autoregressive Models for Content-Rich Text-to-Image Generation",
      "Citations": "1349.0",
      "Authors": "Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, Yonghui Wu",
      "Abstract": "We present the Pathways Autoregressive Text-to-Image (Parti) model, which generates high-fidelity photorealistic images and supports content-rich synthesis involving complex compositions and world knowledge. Parti treats text-to-image generation as a sequence-to-sequence modeling problem, akin to machine translation, with sequences of image tokens as the target outputs rather than text tokens in another language. This strategy can naturally tap into the rich body of prior work on large language models, which have seen continued advances in capabilities and performance through scaling data and model sizes. Our approach is simple: First, Parti uses a Transformer-based image tokenizer, ViT-VQGAN, to encode images as sequences of discrete tokens. Second, we achieve consistent quality improvements by scaling the encoder-decoder Transformer model up to 20B parameters, with a new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on MS-COCO. Our detailed analysis on Localized Narratives as well as PartiPrompts (P2), a new holistic benchmark of over 1600 English prompts, demonstrate the effectiveness of Parti across a wide variety of categories and difficulty aspects. We also explore and highlight limitations of our models in order to define and exemplify key areas of focus for further improvements. See https://parti.research.google/ for high-resolution images.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Second, we achieve consistent quality improvements by scaling the encoder-decoder Transformer model up to 20B parameters, with a new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on MS-COCO\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "Google TPU v4",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "427178.712",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "\"For these reasons, we have decided not to release our Parti models, code, or data for public use without further safeguards in place\"\nhttps://sites.research.google/parti/",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "CoCa",
      "Organization": "Google Research",
      "Publication date": "2022-06-14",
      "Domain": "Vision",
      "Task": "Image classification,Visual question answering,Image captioning",
      "Parameters": "2100000000.0",
      "Parameters notes": "\"Our largest CoCa model (\"CoCa\" in short) follows the ViT-giant setup in [21] with 1B-parameters in the image encoder and 2.1B-parameters altogether with the text decoder\"",
      "Training compute (FLOP)": "7.3e+22",
      "Training compute notes": "\"Pretraining CoCa takes about 5 days on 2,048 CloudTPUv4 chips\"\n\n275 teraFLOP/s * 2048 * 5 * 24 * 3600 * 0.3 (assumed utilization) = 7.3e22",
      "Training dataset": "JFT-3B,ALIGN",
      "Training dataset size (gradients)": "1351680000000",
      "Dataset size notes": "JFT is 3 billion captioned images, ALIGN is 1.8 billion captioned images\n\n\"we use a batch size of 65,536 image-text pairs, where half of each batch comes from JFT and ALIGN, respectively. All models are trained on the\ncombined contrastive and captioning objectives in Eq.(4) for 500k steps, roughly corresponding to 5 epochs on JFT and 10 epochs on ALIGN.\"\n\n(5*3b+10*1.8b)/4.8b=6.875 epochs on average",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2205.01917v2",
      "Reference": "CoCa: Contrastive Captioners are Image-Text Foundation Models",
      "Citations": "1587.0",
      "Authors": "Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, Yonghui Wu",
      "Abstract": "Exploring large-scale pretrained foundation models is of significant interest in computer vision because these models can be quickly transferred to many downstream tasks. This paper presents Contrastive Captioner (CoCa), a minimalist design to pretrain an image-text encoder-decoder foundation model jointly with contrastive loss and captioning loss, thereby subsuming model capabilities from contrastive approaches like CLIP and generative methods like SimVLM. In contrast to standard encoder-decoder transformers where all decoder layers attend to encoder outputs, CoCa omits cross-attention in the first half of decoder layers to encode unimodal text representations, and cascades the remaining decoder layers which cross-attend to the image encoder for multimodal image-text representations. We apply a contrastive loss between unimodal image and text embeddings, in addition to a captioning loss on the multimodal decoder outputs which predicts text tokens autoregressively. By sharing the same computational graph, the two training objectives are computed efficiently with minimal overhead. CoCa is pretrained end-to-end and from scratch on both web-scale alt-text data and annotated images by treating all labels simply as text, seamlessly unifying natural language supervision for representation learning. Empirically, CoCa achieves state-of-the-art performance with zero-shot transfer or minimal task-specific adaptation on a broad range of downstream tasks, spanning visual recognition (ImageNet, Kinetics-400/600/700, Moments-in-Time), crossmodal retrieval (MSCOCO, Flickr30K, MSR-VTT), multimodal understanding (VQA, SNLI-VE, NLVR2), and image captioning (MSCOCO, NoCaps). Notably on ImageNet classification, CoCa obtains 86.3% zero-shot top-1 accuracy, 90.6% with a frozen encoder and learned classification head, and new state-of-the-art 91.0% top-1 accuracy on ImageNet with a finetuned encoder.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Notably on ImageNet classification, CoCa obtains 86.3% zero-shot top-1 accuracy, 90.6% with a frozen encoder and learned classification head, and new state-of-the-art 91.0% top-1 accuracy on ImageNet with a finetuned encoder.\"",
      "Epochs": "6.875",
      "Training time (hours)": "120.0",
      "Training time notes": "5 days",
      "Training hardware": "Google TPU v4",
      "Hardware quantity": "2048.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "78043.3756911775",
      "Compute cost notes": "",
      "Training power draw (W)": "1397266.1319117176",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "65,536 image-text pairs",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "MetaLM",
      "Organization": "Microsoft Research",
      "Publication date": "2022-06-13",
      "Domain": "Multimodal,Language,Vision",
      "Task": "Language modeling,Visual question answering,Language modeling/generation,Image captioning",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "The Pile",
      "Training dataset size (gradients)": "646707200000",
      "Dataset size notes": "",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/2206.06336v1",
      "Reference": "Language Models are General-Purpose Interfaces",
      "Citations": "108.0",
      "Authors": "Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, Furu Wei",
      "Abstract": "Foundation models have received much attention due to their effectiveness across a broad range of downstream applications. Though there is a big convergence in terms of architecture, most pretrained models are typically still developed for specific tasks or modalities. In this work, we propose to use language models as a general-purpose interface to various foundation models. A collection of pretrained encoders perceive diverse modalities (such as vision, and language), and they dock with a language model that plays the role of a universal task layer. We propose a semi-causal language modeling objective to jointly pretrain the interface and the modular encoders. We subsume the advantages and capabilities from both causal and non-causal modeling, thereby combining the best of two worlds. Specifically, the proposed method not only inherits the capabilities of in-context learning and open-ended generation from causal language modeling, but also is conducive to finetuning because of the bidirectional encoders. More importantly, our approach seamlessly unlocks the combinations of the above capabilities, e.g., enabling in-context learning or instruction following with finetuned encoders. Experimental results across various language-only and vision-language benchmarks show that our model outperforms or is competitive with specialized models on finetuning, zero-shot generalization, and few-shot learning.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Abstract: \"Experimental results across various language-only and vision-language benchmarks show that our model outperforms or is competitive with specialized models on finetuning, zero-shot generalization, and few-shot learning.\"\n\n\"Table 7 and Table 8 show the zero-shot captioning results on COCO Karpathy test split, NoCaps validation set, and Flickr30k test set. METALM outperforms recent strong methods on three image captioning datasets.\"\n\nTable 12",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "2097152.0",
      "Batch size notes": "1024 * 2048 - \"The maximum input lengths for non-causal and semi-causal models are 512 and 2048, respectively.\" (Sec. 3.2, p. 9) and \"We pretrain METALM for 300k steps with a batch size of 1024\"",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "I don't see neither code nor weights here\nhttps://github.com/microsoft/unilm/tree/master/metalm",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DITTO",
      "Organization": "Tsinghua University,Apple,Westlake University,Chinese University of Hong Kong (CUHK)",
      "Publication date": "2022-06-06",
      "Domain": "Language",
      "Task": "Language modeling/generation,Text summarization",
      "Parameters": "750000000.0",
      "Parameters notes": "\"We train a Transformer model (750M parameters, similar to GPT-2 Large)\"\n\n\"Specifically, we use a 16-layer Transformer with 8 attention heads, hidden size 1024 and fully-connected dimension 4096.\"",
      "Training compute (FLOP)": "3.31776e+18",
      "Training compute notes": "6 FLOP / token / parameter * 160000 steps * 3 samples per batch * 1536 tokens per sample *  750000000 parameters = 3.31776 \u00d7 10^18 FLOP",
      "Training dataset": "WikiText-103",
      "Training dataset size (gradients)": "103000000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2206.02369",
      "Reference": "Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation",
      "Citations": "93.0",
      "Authors": "Jin Xu, Xiaojiang Liu, Jianhao Yan, Deng Cai, Huayang Li, Jian Li",
      "Abstract": "While large-scale neural language models, such as GPT2 and BART, have achieved impressive results on various text generation tasks, they tend to get stuck in undesirable sentence-level loops with maximization-based decoding algorithms (\\textit{e.g.}, greedy search). This phenomenon is counter-intuitive since there are few consecutive sentence-level repetitions in human corpora (e.g., 0.02\\% in Wikitext-103). To investigate the underlying reasons for generating consecutive sentence-level repetitions, we study the relationship between the probabilities of the repetitive tokens and their previous repetitions in the context. Through our quantitative experiments, we find that 1) Language models have a preference to repeat the previous sentence; 2) The sentence-level repetitions have a \\textit{self-reinforcement effect}: the more times a sentence is repeated in the context, the higher the probability of continuing to generate that sentence; 3) The sentences with higher initial probabilities usually have a stronger self-reinforcement effect. Motivated by our findings, we propose a simple and effective training method \\textbf{DITTO} (Pseu\\underline{D}o-Repet\\underline{IT}ion Penaliza\\underline{T}i\\underline{O}n), where the model learns to penalize probabilities of sentence-level repetitions from pseudo repetitive data. Although our method is motivated by mitigating repetitions, experiments show that DITTO not only mitigates the repetition issue without sacrificing perplexity, but also achieves better generation quality. Extensive experiments on open-ended text generation (Wikitext-103) and text summarization (CNN/DailyMail) demonstrate the generality and effectiveness of our method.",
      "Organization categorization": "Academia,Industry,Academia,Academia",
      "Country (of organization)": "China,United States of America,China,Hong Kong",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Achieves SOTA on CNN/DailyMail by fine-tuning and improving on BART-large, which is SOTA",
      "Epochs": "7.158",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "8.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "4816.802908983849",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "open code\nhttps://github.com/Jxu-Thu/DITTO\n\ntraining: https://github.com/Jxu-Thu/DITTO/blob/main/train.py ",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Diffusion-GAN",
      "Organization": "UT Austin,Microsoft",
      "Publication date": "2022-06-05",
      "Domain": "Image generation",
      "Task": "Image generation,Text-to-image",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "Must be <1e23 FLOP, all experiments were done with 4 or 8 V100s.",
      "Training dataset": "CIFAR-10,LSUN Bedroom,AFHQ,LSUN Church,STL-10,FFHQ",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/2206.02262v4",
      "Reference": "Diffusion-GAN: Training GANs with Diffusion",
      "Citations": "293.0",
      "Authors": "Zhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu Chen, Mingyuan Zhou",
      "Abstract": "Generative adversarial networks (GANs) are challenging to train stably, and a promising remedy of injecting instance noise into the discriminator input has not been very effective in practice. In this paper, we propose Diffusion-GAN, a novel GAN framework that leverages a forward diffusion chain to generate Gaussianmixture distributed instance noise. Diffusion-GAN consists of three components, including an adaptive diffusion process, a diffusion timestep-dependent discriminator, and a generator. Both the observed and generated data are diffused by the same adaptive diffusion process. At each diffusion timestep, there is a different noise-to-data ratio and the timestep-dependent discriminator learns to distinguish the diffused real data from the diffused generated data. The generator learns from the discriminator\u2019s feedback by backpropagating through the forward diffusion chain, whose length is adaptively adjusted to balance the noise and data levels. We theoretically show that the discriminator\u2019s timestep-dependent strategy gives consistent and helpful guidance to the generator, enabling it to match the true data distribution. We demonstrate the advantages of Diffusion-GAN over strong GAN baselines on various datasets, showing that it can produce more realistic images with higher stability and data efficiency than state-of-the-art GANs.",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Table 1\n\"We demonstrate the advantages of Diffusion-GAN over strong GAN baselines on various datasets, showing that it can produce more realistic images with higher stability and data efficiency than state-of-the-art GANs.\"\n\n\"Quantitatively, Diffusion StyleGAN2 outperforms all the GAN baselines in generation diversity, as measured by Recall, on all 6 benchmark datasets and outperforms them in FID by a clear margin on 5 out of the 6 benchmark datasets.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT license\nhttps://github.com/Zhendong-Wang/Diffusion-GAN\n\nhttps://huggingface.co/zhendongw/diffusion-gan/tree/main",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "CogVideo",
      "Organization": "Tsinghua University,Beijing Academy of Artificial Intelligence / BAAI",
      "Publication date": "2022-05-29",
      "Domain": "Video",
      "Task": "Video generation,Text-to-video",
      "Parameters": "9400000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "145000000000",
      "Dataset size notes": "\"trained on 5.4 million text-video pairs\"",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/2205.15868",
      "Reference": "CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers",
      "Citations": "876.0",
      "Authors": "Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, Jie Tang",
      "Abstract": "Large-scale pretrained transformers have created milestones in text (GPT-3) and text-to-image (DALL-E and CogView) generation. Its application to video generation is still facing many challenges: The potential huge computation cost makes the training from scratch unaffordable; The scarcity and weak relevance of text-video datasets hinder the model understanding complex movement semantics. In this work, we present 9B-parameter transformer CogVideo, trained by inheriting a pretrained text-to-image model, CogView2. We also propose multi-frame-rate hierarchical training strategy to better align text and video clips. As (probably) the first open-source large-scale pretrained text-to-video model, CogVideo outperforms all publicly available models at a large margin in machine and human evaluations.",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "China,China",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "The world's largest and first opensource large-scale pre-trained text-to-video model.",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "CogView2",
      "Finetune compute (FLOP)": "3.0456e+19",
      "Finetune compute notes": "6ND = 6*9400000000*5400000=3.0456e+17\n\n(number of epochs is unknown)",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "https://github.com/THUDM/CogVideo\nApache 2\ntrain code: https://github.com/THUDM/CogVideo/blob/CogVideo/pretrain_cogvideo.py ",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Tranception",
      "Organization": "University of Oxford,Harvard Medical School,Cohere",
      "Publication date": "2022-05-27",
      "Domain": "Biology",
      "Task": "Proteins,Protein pathogenicity prediction",
      "Parameters": "700000000.0",
      "Parameters notes": "\"Our largest transformer model, Tranception L, has 700M parameters and is trained on UniRef100 (Suzek et al., 2014)\"",
      "Training compute (FLOP)": "7.24e+21",
      "Training compute notes": "Trained using 64 A100 GPUs for two weeks.\n64 * 312 teraFLOP/s * 14 days * 24 hours/day * 3600 seconds/hour * 0.3 utilization (assumption)\n= 7.24e21",
      "Training dataset": "UniRef100",
      "Training dataset size (gradients)": "48230400000",
      "Dataset size notes": "Total tokens = Number of Sequences \u00d7 Average Sequence Length\n249,000,000 \u00d7 300 = 74,700,000,000 \u2248 7.5 \u00d7 10\u00b9\u2070 tokens",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2205.13760",
      "Reference": "Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval",
      "Citations": "225.0",
      "Authors": "Pascal Notin, Mafalda Dias, Jonathan Frazer, Javier Marchena-Hurtado, Aidan Gomez, Debora S. Marks, Yarin Gal",
      "Abstract": "The ability to accurately model the fitness landscape of protein sequences is critical to a wide range of applications, from quantifying the effects of human variants on disease likelihood, to predicting immune-escape mutations in viruses and designing novel biotherapeutic proteins. Deep generative models of protein sequences trained on multiple sequence alignments have been the most successful approaches so far to address these tasks. The performance of these methods is however contingent on the availability of sufficiently deep and diverse alignments for reliable training. Their potential scope is thus limited by the fact many protein families are hard, if not impossible, to align. Large language models trained on massive quantities of non-aligned protein sequences from diverse families address these problems and show potential to eventually bridge the performance gap. We introduce Tranception, a novel transformer architecture leveraging autoregressive predictions and retrieval of homologous sequences at inference to achieve state-of-the-art fitness prediction performance. Given its markedly higher performance on multiple mutants, robustness to shallow alignments and ability to score indels, our approach offers significant gain of scope over existing approaches. To enable more rigorous model testing across a broader range of protein families, we develop ProteinGym -- an extensive set of multiplexed assays of variant effects, substantially increasing both the number and diversity of assays compared to existing benchmarks.",
      "Organization categorization": "Academia,Academia,Industry",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland,United States of America,Canada",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Table 2\n\"We introduce Tranception, a novel transformer architecture leveraging autoregressive predictions and retrieval of homologous sequences at inference to achieve state-of-the-art fitness prediction performance. Given its markedly higher performance on multiple mutants, robustness to shallow alignments and ability to score indels, our approach offers significant gain of scope over existing approaches.\"",
      "Epochs": "",
      "Training time (hours)": "336.0",
      "Training time notes": "2 weeks",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "64.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "15247.43608848737",
      "Compute cost notes": "",
      "Training power draw (W)": "51390.67413498394",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT license\n\nhttps://github.com/OATML-Markslab/Tranception\n\nhttps://huggingface.co/OATML-Markslab/Tranception_Large",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GPT-2 Medium (FlashAttention)",
      "Organization": "Stanford University,University at Buffalo",
      "Publication date": "2022-05-27",
      "Domain": "Language",
      "Task": "Language modeling/generation",
      "Parameters": "355000000.0",
      "Parameters notes": "GPT-2 Medium from here, aka GPT-2 355M: https://huggingface.co/openai-community/gpt2-medium",
      "Training compute (FLOP)": "8.9280922e+20",
      "Training compute notes": "2.272e+21 (GPT-2 355M). 3x speedup claimed over the original implementation, but not clear how this interacts with overall training compute.\n\nAlternative data for calculation: 8x A100 80GB * 165.6 hours at 60% utilization (higher than average because FlashAttention-2 paper reports 72% utilization with larger models and FlashAttention-1 runs ~95% as fast as FA-2 on the smallest model then tested: GPT3-1.3B with 2K context, whereas this paper uses GPT2-355M with 1K context. Paper uses mixed precision training for GPT-2 Medium (PyTorch AMP). \n\n\"We train the model on 8\u00d7A100-40GB GPUs, and we measure the wall-clock training time. Training\nGPT-2 small takes between 2.7-9.5 days, and training GPT-2 medium takes between 6.9-21.0 days (Table 2).\"\n\n8 GPUs * 312000000000000 FLOP / GPU / sec * 165.6 hours * 3600 sec / hour * 0.6 [assumed utilization] = 8.9280922e+20 FLOP",
      "Training dataset": "OPENWEBTEXT",
      "Training dataset size (gradients)": "10130000000",
      "Dataset size notes": "From OpenWebText",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2205.14135",
      "Reference": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
      "Citations": "3256.0",
      "Authors": "Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher R\u00e9",
      "Abstract": "Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3 speedup on GPT-2 (seq. length 1K), and 2.4 speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "Highly cited,Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "165.6",
      "Training time notes": "6.9 days",
      "Training hardware": "NVIDIA A100 SXM4 40 GB",
      "Hardware quantity": "8.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "6.9 days * 8 A100s. No cloud price given for A100 SXM4 80GB",
      "Training power draw (W)": "6423.834266872993",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "Just checking - shouldn't use GPT-2 as the base model here since the paper isn't a fine-tune but a reimplementation, correct?",
      "Batch size": "512.0",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "",
      "Accessibility notes": "BSD-3-Clause license\nhttps://github.com/Dao-AILab/flash-attention",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "120615.79126705672",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Imagen",
      "Organization": "Google Brain",
      "Publication date": "2022-05-23",
      "Domain": "Image generation",
      "Task": "Text-to-image,Image generation",
      "Parameters": "7762000000.0",
      "Parameters notes": "2B 64x64 generation model, 600M 64->256 super-resolution model, 400M 256->1024 super-resolution model\nUses encodings from a frozen T5-XXL, which should be included in total parameter count. Loading the model directly, there are 4,762,310,656 parameters in the encoder.\n2B + 4.762B + 600M + 400M = 7.762 billion\n\nhere they claim it is 3B parameters: https://arxiv.org/pdf/2407.15811",
      "Training compute (FLOP)": "1.46e+22",
      "Training compute notes": "256 TPU v4 chips for 64x64, for 4 days\n128 TPU v4 chips for 64->256, for 2 days\n128 TPU v4 chips for 256->1024, for 2 days\n\n256 TPUs * 275 teraFLOPS/TPU * 4 days + 2 * (128 TPUs * 275 teraFLOPS/TPU * 2 days) * 40% utilization = 1.46e+22 FLOP",
      "Training dataset": "LAION-400M,Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "[IMAGE-TEXT PAIRS]\n\"We train on a combination of internal datasets, with \u2248 460M\nimage-text pairs, and the publicly available Laion dataset [61], with \u2248 400M image-text pairs.\"",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2205.11487",
      "Reference": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding",
      "Citations": "7396.0",
      "Authors": "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, Mohammad Norouzi",
      "Abstract": "We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Significant use,SOTA improvement,Highly cited",
      "Notability criteria notes": "\"Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset,\nwithout ever training on COCO\"\n\nAlso SOTA on MS-COCO (Table 5: https://arxiv.org/pdf/2206.10789v1)",
      "Epochs": "",
      "Training time (hours)": "96.0",
      "Training time notes": "4 days",
      "Training hardware": "Google TPU v4",
      "Hardware quantity": "256.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "7915.823806150154",
      "Compute cost notes": "",
      "Training power draw (W)": "174743.8571090156",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "SimCSE",
      "Organization": "Princeton University,Tsinghua University",
      "Publication date": "2022-05-18",
      "Domain": "Language",
      "Task": "Semantic embedding",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "27363505",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2104.08821",
      "Reference": "SimCSE: Simple Contrastive Learning of Sentence Embeddings",
      "Citations": "3970.0",
      "Authors": "Tianyu Gao, Xingcheng Yao, Danqi Chen",
      "Abstract": "This paper presents SimCSE, a simple contrastive learning framework that greatly advances state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation, and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework by using \"entailment\" pairs as positives and \"contradiction\" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearman's correlation respectively, a 4.2% and 2.2% improvement compared to the previous best results. We also show -- both theoretically and empirically -- that the contrastive learning objective regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "United States of America,China",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\"We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearman's correlation respectively, a 4.2% and 2.2% improvement compared to the previous best results.\"\n\nTable 5",
      "Epochs": "3.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "RoBERTa Large",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Unclear lisense: https://huggingface.co/princeton-nlp/sup-simcse-roberta-large\n\nMIT license: \nhttps://github.com/princeton-nlp/SimCSE?tab=readme-ov-file",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Gato",
      "Organization": "DeepMind",
      "Publication date": "2022-05-12",
      "Domain": "Multimodal,Robotics,Games,Language",
      "Task": "Atari,Image captioning,Chat,Robotic manipulation",
      "Parameters": "1180000000.0",
      "Parameters notes": "\"This section focuses on in-simulation evaluation.\nFigure 10 compares the full 1.18B parameter Gato\" p.10",
      "Training compute (FLOP)": "4.02e+21",
      "Training compute notes": "256 (16x16x) TPUv3 chips x 123e12 FLOPS/chip x 4 days x 86400 seconds/day * 0.4 utilization = 4.35e21 FLOPs\n\nSimilar value by 6NC:\n6 * 524288000000 * 1.18B = 3.71e21\n\nUsing geometric mean:\nsqrt(4.35e21 * 3.71e21) = 4.02e21",
      "Training dataset": "",
      "Training dataset size (gradients)": "524288000000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2205.06175",
      "Reference": "A Generalist Agent",
      "Citations": "966.0",
      "Authors": "Scott Reed, Konrad \u017bo\u0142na, Emilio Parisotto, Sergio G\u00f3mez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gim\u00e9nez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, Nando de Freitas",
      "Abstract": "Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "SOTA at Meta-World MT50 tasks (96.6%) page 14, section 5.5",
      "Epochs": "",
      "Training time (hours)": "96.0",
      "Training time notes": "4 days",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "256.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "3523.0649800752253",
      "Compute cost notes": "",
      "Training power draw (W)": "231335.2960762711",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "UL2",
      "Organization": "Google Research,Google Brain",
      "Publication date": "2022-05-10",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering,Text summarization",
      "Parameters": "20000000000.0",
      "Parameters notes": "Taken from Directory of LLMs",
      "Training compute (FLOP)": "1.2e+23",
      "Training compute notes": "Trained on 1T tokens\n20B * 1T * 6 = 1.2e23 \n\nSecond source: Section 5.1 says model was trained on 512 TPUv4 chips, and took slightly over 1 month\n512 * 2.75e14 * 31 * 24 * 3600 * 0.3 = 1.13e23",
      "Training dataset": "C4",
      "Training dataset size (gradients)": "1000000000000",
      "Dataset size notes": "1T tokens",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2205.05131v1",
      "Reference": "Unifying Language Learning Paradigms",
      "Citations": "357.0",
      "Authors": "Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler",
      "Abstract": "Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized and unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5 and/or GPT-like models across multiple diverse setups. Finally, by scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised NLP tasks ranging from language generation (with automated and human evaluation), language understanding, text classification, question answering, commonsense reasoning, long text reasoning, structured knowledge grounding and information retrieval. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. We release Flax-based T5X model checkpoints for the 20B model at \\url{this https URL}.",
      "Organization categorization": "Industry,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"by scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised NLP tasks\"",
      "Epochs": "",
      "Training time (hours)": "744.0",
      "Training time notes": "around 31 days from 'Pre-training took approximately slight more than one month for about 1 trillion\ntokens.' from section 5.1\nso around 31*24 = 744\n",
      "Training hardware": "Google TPU v4",
      "Hardware quantity": "512.0",
      "Hardware utilization (MFU)": "0.2992518703",
      "Training compute cost (2023 USD)": "126785.76203549477",
      "Compute cost notes": "",
      "Training power draw (W)": "349588.9061965891",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "524288.0",
      "Batch size notes": "1024 * 512",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Apache 2.0\n\nhttps://huggingface.co/google/ul2",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DeBERTaV3large + KEAR",
      "Organization": "Microsoft",
      "Publication date": "2022-05-04",
      "Domain": "Language",
      "Task": "Question answering,Language modeling/generation",
      "Parameters": "418000000.0",
      "Parameters notes": "DeBERTaV3-large had 418M params, per Table 2",
      "Training compute (FLOP)": "",
      "Training compute notes": "this is a fine-tuned version of DeBERTaV3-large",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2112.03254v3",
      "Reference": "Human Parity on CommonsenseQA: Augmenting Self-Attention with External Attention",
      "Citations": "62.0",
      "Authors": "Yichong Xu, Chenguang Zhu, Shuohang Wang, Siqi Sun, Hao Cheng, Xiaodong Liu, Jianfeng Gao, Pengcheng He, Michael Zeng, Xuedong Huang",
      "Abstract": "Most of today's AI systems focus on using self-attention mechanisms and transformer architectures on large amounts of diverse data to achieve impressive performance gains. In this paper, we propose to augment the transformer architecture with an external attention mechanism to bring external knowledge and context to bear. By integrating external information into the prediction process, we hope to reduce the need for ever-larger models and increase the democratization of AI systems. We find that the proposed external attention mechanism can significantly improve the performance of existing AI systems, allowing practitioners to easily customize foundation AI models to many diverse downstream applications. In particular, we focus on the task of Commonsense Reasoning, demonstrating that the proposed external attention mechanism can augment existing transformer models and significantly improve the model's reasoning capabilities. The proposed system, Knowledgeable External Attention for commonsense Reasoning (KEAR), reaches human parity on the open CommonsenseQA research benchmark with an accuracy of 89.4\\% in comparison to the human accuracy of 88.9\\%.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"The proposed system, Knowledgeable External Attention for commonsense Reasoning (KEAR), reaches human parity on the open CommonsenseQA research benchmark with an accuracy of 89.4\\% in comparison to the human accuracy of 88.9\\%.\"\n\nSOTA per https://paperswithcode.com/sota/common-sense-reasoning-on-commonsenseqa",
      "Epochs": "10.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "DeBERTaV3large",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "",
      "Accessibility notes": "no attributed license \nhttps://github.com/microsoft/KEAR?tab=readme-ov-file",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "OPT-175B",
      "Organization": "Meta AI",
      "Publication date": "2022-05-02",
      "Domain": "Language",
      "Task": "Language modeling,Chat,Language modeling/generation,Question answering",
      "Parameters": "175000000000.0",
      "Parameters notes": "\"In line with Meta AI\u2019s commitment to open science, we are sharing Open Pretrained Transformer (OPT-175B), a language model with 175 billion parameters trained on publicly available data sets\"",
      "Training compute (FLOP)": "4.3e+23",
      "Training compute notes": "https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/final_update.md\n\n\"As of yesterday, at 12:46pm PST on January 6, our 175B model finally completed its training run on 300B tokens. This required ~4.30E+23 FLOPs of compute\"",
      "Training dataset": "The Pile,BookCorpus (BooksCorpus, Toronto Book Corpus),CC-Stories,Pushshift Reddit",
      "Training dataset size (gradients)": "180000000000",
      "Dataset size notes": "\"The training data contains 180B tokens corresponding to 800 GB of data\"\n\n1 token ~ 0.75 words",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2205.01068",
      "Reference": "OPT: Open Pre-trained Transformer Language Models",
      "Citations": "4334.0",
      "Authors": "Susan Zhang\u2217 , Stephen Roller\u2217 , Naman Goyal\u2217 , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott\u2020 , Sam Shleifer\u2020 , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",
      "Abstract": "Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3,1 while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Significant use,Highly cited",
      "Notability criteria notes": "https://ai.meta.com/blog/opt-175b-large-language-model-applications/",
      "Epochs": "1.6667",
      "Training time (hours)": "793.5",
      "Training time notes": "4.3*10^23 FLOP / (147 TFLOPS) = 813000 A100-hours\nhttps://www.wolframalpha.com/input?i=4.3*10%5E23+FLOP+%2F+%28147+TFLOPS%29\n\n\"As of yesterday, at 12:46pm PST on January 6, our 175B model finally completed its training run on 300B tokens. This required ~4.30E+23 FLOPs of compute, or roughly ~33 days of continuous training on 1024 80GB A100s (assuming no hardware issues, no numerical instabilities, etc.).\"",
      "Training hardware": "NVIDIA A100 SXM4 80 GB",
      "Hardware quantity": "1024.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "733634.637",
      "Compute cost notes": "",
      "Training power draw (W)": "822708.6888139298",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "2000000.0",
      "Batch size notes": "Table 1",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "non-commercial for weights:\nhttps://github.com/facebookresearch/metaseq/blob/main/projects/OPT/MODEL_LICENSE.md\n\ntraining code (MIT) https://github.com/facebookresearch/metaseq/blob/main/docs/training.md ",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "0.47115",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Flamingo",
      "Organization": "DeepMind",
      "Publication date": "2022-04-29",
      "Domain": "Multimodal,Vision,Language,Video",
      "Task": "Visual question answering,Image captioning",
      "Parameters": "80000000000.0",
      "Parameters notes": "\"We obtain three models, Flamingo-3B, Flamingo-9B and Flamingo-80B\"\n\n\" The Flamingo-80B model builds on top of the frozen Chinchilla 70B language model [42]. Starting from the very first layer and before every seventh transformer blocks, we add a GATED XATTN-DENSE layer attending to the visual inputs; this accounts for 10B additional learned parameters. For simplicity, we refer to this model as simply Flamingo throughout the paper\"",
      "Training compute (FLOP)": "2.18972000000001e+23",
      "Training compute notes": "1536 TPU v4 chips for 15 days. Assuming 40% utilization:\nC = 1536 TPU * 275*10^12 FLOP/s/TPU * 15 day * 86400 s/day * 0.40 = 2.2*10^23 FLOP\n\n\"All training and evaluation was performed on TPUv4 instances. The largest model containing 80 billion parameters is trained on QUSV chips for 15 days and sharded across 16 devices.\"\n\n\"All trained parameters and optimizer accumulators are stored and updated in float32; all activations and gradients are computed in bfloat16 after downcasting of parameters from float32 to bfloat16\"",
      "Training dataset": "MultiModal MassiveWeb,LTIP,VTP,ALIGN",
      "Training dataset size (gradients)": "458333333333",
      "Dataset size notes": "Flamingo was trained on a mixture of web-scraped datasets:\n43M pages of text with interleaved images (MultiModal MassiveWeb dataset)\n312M image-text pairs (LTIP dataset)\n27M video-text pairs (VTP dataset)\n1.8B image-alt text pairs (ALIGN dataset)\n\nTraining dataset size is at least 2.1 billion.",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2204.14198",
      "Reference": "Flamingo: a Visual Language Model for Few-Shot Learning",
      "Citations": "4744.0",
      "Authors": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, Karen Simonyan",
      "Abstract": "Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement,Discretionary",
      "Notability criteria notes": "Figure 2\n\"For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.\"",
      "Epochs": "",
      "Training time (hours)": "360.0",
      "Training time notes": "1536 TPU v4 chips for 15 days",
      "Training hardware": "Google TPU v4",
      "Hardware quantity": "1536.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "183423.16330597224",
      "Compute cost notes": "",
      "Training power draw (W)": "1049023.659188776",
      "Base model": "Chinchilla",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "XMC-GAN",
      "Organization": "Google Research",
      "Publication date": "2022-04-14",
      "Domain": "Image generation",
      "Task": "Image generation,Text-to-image",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "\"Models are trained with a batch size of 256. For reporting results in our paper, models are trained for 1000 epochs, and we report the scores corresponding to the checkpoint with the best FID score on the validation set.\"",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/2101.04702",
      "Reference": "Cross-Modal Contrastive Learning for Text-to-Image Generation",
      "Citations": "",
      "Authors": "Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, Yinfei Yang",
      "Abstract": "The output of text-to-image synthesis systems should be coherent, clear, photo-realistic scenes with high semantic fidelity to their conditioned text descriptions. Our Cross-Modal Contrastive Generative Adversarial Network (XMC-GAN) addresses this challenge by maximizing the mutual information between image and text. It does this via multiple contrastive losses which capture inter-modality and intra-modality correspondences. XMC-GAN uses an attentional self-modulation generator, which enforces strong text-image correspondence, and a contrastive discriminator, which acts as a critic as well as a feature encoder for contrastive learning. The quality of XMC-GAN's output is a major step up from previous models, as we show on three challenging datasets. On MS-COCO, not only does XMC-GAN improve state-of-the-art FID from 24.70 to 9.33, but--more importantly--people prefer XMC-GAN by 77.3 for image quality and 74.1 for image-text alignment, compared to three other recent models. XMC-GAN also generalizes to the challenging Localized Narratives dataset (which has longer, more detailed descriptions), improving state-of-the-art FID from 48.70 to 14.12. Lastly, we train and evaluate XMC-GAN on the challenging Open Images data, establishing a strong benchmark FID score of 26.91.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"XMC-GAN also generalizes to the challenging Localized Narratives dataset (which has longer, more detailed descriptions), improving state-of-the-art FID from 48.70 to 14.12\"",
      "Epochs": "1000.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Sparse all-MLP",
      "Organization": "Meta AI",
      "Publication date": "2022-04-14",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "9410000000.0",
      "Parameters notes": "Table 2: \"In Section 4.4, we run our large model (9.41B parameters)\"",
      "Training compute (FLOP)": "5.32224e+20",
      "Training compute notes": "112 hours on 32 V100 GPUs\nassumed 0.33 util rate\n\n112 hours *3600 seconds / hour *0.33 utilization *32 gpus *125000000000000 FLOPs=532224000000000000000\n",
      "Training dataset": "RoBERTa dataset,CC100",
      "Training dataset size (gradients)": "100000000000",
      "Dataset size notes": "100B tokens (Table 2) so 75B words.",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2203.06850",
      "Reference": "Efficient Language Modeling with Sparse all-MLP",
      "Citations": "15.0",
      "Authors": "Ping Yu, Mikel Artexte, Myle Ott, Sam Shleifer, Hongyu Gong, Ves Stoyanov, Xian Li",
      "Abstract": "All-MLP architectures have attracted increasing interest as an alternative to attention-based models. In NLP, recent work like gMLP shows that all-MLPs can match Transformers in language modeling, but still lag behind in downstream tasks. In this work, we analyze the limitations of MLPs in expressiveness, and propose sparsely activated MLPs with mixture-of-experts (MoEs) in both feature and input (token) dimensions. Such sparse all-MLPs significantly increase model capacity and expressiveness while keeping the compute constant. We address critical challenges in incorporating conditional computation with two routing strategies. The proposed sparse all-MLP improves language modeling perplexity and obtains up to 2\u00d7 improvement in training efficiency compared to both Transformer-based MoEs (GShard, Switch Transformer, Base Layers and HASH Layers) as well as dense Transformers and all-MLPs. Finally, we evaluate its zero-shot in-context learning performance on six downstream tasks, and find that it surpasses Transformer-based MoEs and dense Transformers.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Through extensive evaluations on language modeling, we show that sMLP outperforms stateof-the-art sparse Transformer-based MoE models in terms of generalization and 2\u00d7 improvement in training efficiency.\"\n\n\nnot an absolute SOTA\n\n\"Table 6. Zero-shot priming evaluation: we provide head-to-head comparison of our sMLP model with FLOPs-matched state-of-the-art sparse Transformers on six representative NLP tasks evaluated in GPT-3 in-context learning(Brown et al., 2020b).\"",
      "Epochs": "",
      "Training time (hours)": "112.0",
      "Training time notes": "",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Stable Diffusion (LDM-KL-8-G)",
      "Organization": "Runway,Ludwig Maximilian University of Munich,Heidelberg University",
      "Publication date": "2022-04-13",
      "Domain": "Image generation",
      "Task": "Image generation,Text-to-image",
      "Parameters": "1450000000.0",
      "Parameters notes": "See Table 2",
      "Training compute (FLOP)": "5.0000000000000004e+22",
      "Training compute notes": "\"I get 5e22 FLOP. 150k hours on A100 [1] gives 150*10^3 hours * 3600 seconds/hour * 3.12E+14 peak performance of A100 * 0.33 utilisation = 5e22  FLOP\"\n\n[1] https://twitter.com/EMostaque/status/1563870674111832066",
      "Training dataset": "LAION-400M",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2112.10752",
      "Reference": "High-Resolution Image Synthesis with Latent Diffusion Models",
      "Citations": "20685.0",
      "Authors": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer",
      "Abstract": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at this https URL .",
      "Organization categorization": "Industry,Academia,Academia",
      "Country (of organization)": "United States of America,Germany,Germany",
      "Notability criteria": "Significant use,Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "585.9375",
      "Training time notes": "total chip-hours divided by number of GPUs\n150k/256",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "256.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "111248.21698072631",
      "Compute cost notes": "",
      "Training power draw (W)": "205764.21634205984",
      "Base model": "Stable Diffusion 1.2",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT license for code and weights\nhttps://github.com/CompVis/latent-diffusion",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "BERT-RBP",
      "Organization": "Waseda University",
      "Publication date": "2022-04-07",
      "Domain": "Biology",
      "Task": "Proteins,Protein interaction prediction,RNA-Protein interaction prediction",
      "Parameters": "110000000.0",
      "Parameters notes": "Base model is BERT base (110M parameters), pre-trained on human reference genome (DNABert: https://academic.oup.com/bioinformatics/article/37/15/2112/6128680)",
      "Training compute (FLOP)": "1.4e+20",
      "Training compute notes": "See DNABert entry:\n\n\"Since the pre-training of DNABERT model is resource-intensive (about 25\u2009days on 8 NVIDIA 2080Ti GPUs)\"\n\nAssuming FP16 and 30% utilization\n\nCalculation = (25 * 24 *3600) s * 2.7e13 FLOP/s per GPU * 8 GPUs * 0.3 utilization = 1.4e20 FLOP",
      "Training dataset": "RBPSuite",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "RBPs: 154\nSequences per RBP: 15,000\nTotal sequences = 154 \u00d7 15,000 = 2,310,000\nTokens per sequence = 34\nTotal tokens = 2,310,000 \u00d7 34 = 78,540,000\nFinal estimate: 7.8 \u00d7 10\u2077 tokens",
      "Confidence": "Confident",
      "Link": "https://academic.oup.com/bioinformaticsadvances/article/2/1/vbac023/6564689",
      "Reference": "Prediction of RNA\u2013protein interactions using a nucleotide language model",
      "Citations": "68.0",
      "Authors": "Keisuke Yamada, Michiaki Hamada",
      "Abstract": "Motivation\nThe accumulation of sequencing data has enabled researchers to predict the interactions between RNA sequences and RNA-binding proteins (RBPs) using novel machine learning techniques. However, existing models are often difficult to interpret and require additional information to sequences. Bidirectional encoder representations from transformer (BERT) is a language-based deep learning model that is highly interpretable. Therefore, a model based on BERT architecture can potentially overcome such limitations.\n\nResults\nHere, we propose BERT-RBP as a model to predict RNA\u2013RBP interactions by adapting the BERT architecture pretrained on a human reference genome. Our model outperformed state-of-the-art prediction models using the eCLIP-seq data of 154 RBPs. The detailed analysis further revealed that BERT-RBP could recognize both the transcript region type and RNA secondary structure only based on sequence information. Overall, the results provide insights into the fine-tuning mechanism of BERT in biological contexts and provide evidence of the applicability of the model to other RNA-related problems.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Japan",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Our model outperformed state-of-the-art prediction models using the eCLIP-seq data of 154 RBPs\" [Abstract] - SOTA improvement on a very specific task\n\n\"Using the eCLIP-seq data of 154 different RBPs, our model outperformed state-of-the-art methods and the baseline BERT model.\"",
      "Epochs": "3.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "DNABERT",
      "Finetune compute (FLOP)": "2.2e+16",
      "Finetune compute notes": "\"The models were trained on four NVIDIA Tesla V100 GPUs (128\nGB memory). The training of one RBP model using 19 200 samples\ntook <10 min.\"\n\nCalculation assuming FP16 and 30% utlization and NVIDIA Tesla V100 SMX2 model: \n10 min * 60 sec/min * 3.1e13 FLOP/s * 4 GPU * 0.3 utilization = 2.2e16",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "Open access (non-commercial)",
      "Accessibility notes": "No clear license: https://github.com/kkyamada/bert-rbp\n\ntrain script: https://github.com/kkyamada/bert-rbp/blob/master/examples/run_finetune.py \n\ndata also doesn't have a clear license: http://www.csbio.sjtu.edu.cn/bioinf/RBPsuite/dataset_new.html",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DALL\u00b7E 2",
      "Organization": "OpenAI",
      "Publication date": "2022-04-06",
      "Domain": "Image generation",
      "Task": "Text-to-image,Image generation",
      "Parameters": "3500000000.0",
      "Parameters notes": "\"Our decoder architecture is the 3.5 billion parameter GLIDE model\"",
      "Training compute (FLOP)": "3.3695784e+23",
      "Training compute notes": "Decoder architecture is similar to Imagen (1.46E+22), but trained on 1.6e9 datapoints (Table 3) rather than Imagen's 5.1e9 datapoints.\n\nDALL-E 2 uses two models as priors. I estimate the prior model's FLOP as 6*N*D = 6 * 1e9 * 4096 * 1e6 = 2.5e19 FLOP. However, this seems low compared to CLIP.\n\nSo it may be possible to estimate DALL-E 2's compute by analogy to Imagen, but there is a lot of uncertainty and more research would be needed.\n\nhere (https://arxiv.org/pdf/2407.15811) they claim the DALL-E.2 model was trained on the equivalent of 5208.3 days on 8*A100 GPUs:\n\n312000000000000 FLOP / sec / GPU * 8 GPUs * 5208.3 days * 24 hours / day * 3600 sec / hour * 0.3 [assumed utilization] = 3.3695784e+23 FLOP",
      "Training dataset": "CLIP,DALL-E",
      "Training dataset size (gradients)": "167050000000",
      "Dataset size notes": "\"When training the encoder, we sample from the CLIP [39] and DALL-E [40] datasets (approximately 650M images in total) with equal probability\"",
      "Confidence": "Speculative",
      "Link": "https://cdn.openai.com/papers/dall-e-2.pdf",
      "Reference": "Hierarchical Text-Conditional Image Generation with CLIP Latents",
      "Citations": "8200.0",
      "Authors": "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen",
      "Abstract": "Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "Table 2",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "PaLM (540B)",
      "Organization": "Google Research",
      "Publication date": "2022-04-04",
      "Domain": "Language",
      "Task": "Language modeling,Code generation,Translation",
      "Parameters": "540350000000.0",
      "Parameters notes": "\"To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model (PaLM).\"",
      "Training compute (FLOP)": "2.5272e+24",
      "Training compute notes": "See Table 20.\n\n6144 TPUv4 for 1200 hours + 3072 TPUv4 for 336 hours.\nEquivalent to 6144 TPUv4 for 1368 hours.\n\n46.2% model FLOPs utilization\n\n\"The 540B-parameter PaLM model sustained a remarkable 57.8% of the peak hardware floating point performance over 50 days while training on TPU v4 supercomputers.\" https://cloud.google.com/blog/topics/systems/tpu-v4-enables-performance-energy-and-co2e-efficiency-gains",
      "Training dataset": "Wikipedia,GLaM dataset,LaMBDA dataset,GitHub",
      "Training dataset size (gradients)": "780000000000",
      "Dataset size notes": "\"The PaLM pretraining dataset consists of a high-quality corpus of 780 billion tokens that represent a wide range of natural language use cases.\"\n\n1 token ~ 0.75 words",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2204.02311",
      "Reference": "PaLM: Scaling Language Modeling with Pathways",
      "Citations": "7351.0",
      "Authors": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev,, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta ,Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, Noah Fiedel",
      "Abstract": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,SOTA improvement,Training cost",
      "Notability criteria notes": "Table 4\nDemonstrates continued benefits of scaling, as well as discontinuous improvements in performance",
      "Epochs": "1.0",
      "Training time (hours)": "1536.0",
      "Training time notes": "6144 TPUv4 for 1200 hours + 3072 TPUv4 for 336 hours.\n\nEquivalent to 6144 TPUv4 for 1368 hours.",
      "Training hardware": "Google TPU v4",
      "Hardware quantity": "6144.0",
      "Hardware utilization (MFU)": "0.462",
      "Training compute cost (2023 USD)": "3060364.5969860666",
      "Compute cost notes": "Training compute and utilization rate exclude rematerialization FLOP, but cost should account for rematerialization.",
      "Training power draw (W)": "4198431.396906301",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "4000000.0",
      "Batch size notes": "\"For the largest model, we use batch size 512 (1M tokens) until step 50k, then double it to 1024 (2M tokens) until step 115k, and finally double again it to 2048 (4M tokens) until training is complete at step 255k\"",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "0.578",
      "Training compute cost (cloud)": "14451957.906571811",
      "Training compute cost (upfront)": "67883583.58684365"
    },
    {
      "Model": "Chinchilla",
      "Organization": "DeepMind",
      "Publication date": "2022-03-29",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "70000000000.0",
      "Parameters notes": "\"We test this hypothesis by training a predicted compute-optimal model, \\chinchilla, that uses the same compute budget as \\gopher but with 70B parameters and 4\u00d7 more more data. \\chinchilla uniformly and significantly outperforms \\Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks.\"",
      "Training compute (FLOP)": "5.76e+23",
      "Training compute notes": "\"Both Chinchilla and Gopher have been trained for the same number of FLOPs but differ in the size of the model and the number of training tokens.\"\n\nWe see the number of flops in table 3",
      "Training dataset": "MassiveWeb,C4,GitHub,Wikipedia,books",
      "Training dataset size (gradients)": "1400000000000",
      "Dataset size notes": "Table 1 shows Chinchilla was training on 1.4 trillion tokens\n\n1 token ~ 0.75 words",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2203.15556",
      "Reference": "Training Compute-Optimal Large Language Models",
      "Citations": "2585.0",
      "Authors": "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, Laurent Sifre",
      "Abstract": "We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over \\nummodels language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, \\chinchilla, that uses the same compute budget as \\gopher but with 70B parameters and 4\u00d7 more more data. \\chinchilla uniformly and significantly outperforms \\Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that \\chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, \\chinchilla reaches a state-of-the-art average accuracy of 67.5\\% on the MMLU benchmark, greater than a 7\\% improvement over \\gopher.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement,Historical significance",
      "Notability criteria notes": "Proposes new scaling law, with good empirical results",
      "Epochs": "1.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "Google TPU v4,Google TPU v3",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "3000000.0",
      "Batch size notes": "Table 1. \"1.5M \u2192 3M\"",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Make-A-Scene",
      "Organization": "Meta AI",
      "Publication date": "2022-03-24",
      "Domain": "Image generation",
      "Task": "Image generation,Text-to-image",
      "Parameters": "4000000000.0",
      "Parameters notes": "\"Experiments were performed with a 4 billion parameter\ntransformer, generating a sequence of 256 text tokens, 256\nscene tokens, and 1024 image tokens, that are then decoded\ninto an image with a resolution of 256 \u00d7 256 or 512 \u00d7 512\npixels (depending on the model of choice)\"",
      "Training compute (FLOP)": "6.4172851e+21",
      "Training compute notes": "6 FLOP / parameter / token * 4 * 10^9 parameters * 267386880000 tokens [see dataset size notes] = 6.4172851e+21 FLOP",
      "Training dataset": "Conceptual Captions 12M (CC12M),YFCC-100M,Redcaps",
      "Training dataset size (gradients)": "267386880000",
      "Dataset size notes": "\"The scene-based transformer is trained on a union of CC12m [7], CC [51], and subsets of YFCC100m [55] and Redcaps [10], amounting to 35m text-image pairs.\"\n\n\"The models were trained for a total of 170k iterations, with a batch size of 1024\"\n\n\"Experiments were performed with a 4 billion parameter\ntransformer, generating a sequence of 256 text tokens, 256\nscene tokens, and 1024 image tokens, that are then decoded\ninto an image with a resolution of 256 \u00d7 256 or 512 \u00d7 512\npixels (depending on the model of choice).\"\n\nTotal tokens: \n(256 + 256 + 1024) * 1024 * 170000 = 267386880000",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2203.13131",
      "Reference": "Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors",
      "Citations": "",
      "Authors": "Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, Yaniv Taigman",
      "Abstract": "Recent text-to-image generation methods provide a simple yet exciting conversion capability between text and image domains. While these methods have incrementally improved the generated image fidelity and text relevancy, several pivotal gaps remain unanswered, limiting applicability and quality. We propose a novel text-to-image method that addresses these gaps by (i) enabling a simple control mechanism complementary to text in the form of a scene, (ii) introducing elements that substantially improve the tokenization process by employing domain-specific knowledge over key image regions (faces and salient objects), and (iii) adapting classifier-free guidance for the transformer use case. Our model achieves state-of-the-art FID and human evaluation results, unlocking the ability to generate high fidelity images in a resolution of 512x512 pixels, significantly improving visual quality. Through scene controllability, we introduce several new capabilities: (i) Scene editing, (ii) text editing with anchor scenes, (iii) overcoming out-of-distribution text prompts, and (iv) story illustration generation, as demonstrated in the story we wrote.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"FID is calculated over a subset of 30k images generated from the MS-COCO validation set text prompts with no reranking, and provided in Tab. 4.6. The evaluated models are divided into two groups: trained with and without (denoted as filtered) the MS-COCO training set. In both scenarios our model achieves the lowest FID.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Segatron-XL large, M=384 + HCP",
      "Organization": "Microsoft Research,University of Waterloo",
      "Publication date": "2022-03-21",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "256999999.99999997",
      "Parameters notes": "257M\n(Table 1)\n\n18 layers, 16 heads, hidden size\n1024 batch size 128.",
      "Training compute (FLOP)": "2.65e+19",
      "Training compute notes": "6 FLOP / token / parameter * 257000000 parameters * 384 tokens/batch \u00d7 128 [batch size] * 350000 steps = 2.6527334e+19 FLOP",
      "Training dataset": "WikiText-103",
      "Training dataset size (gradients)": "103000000",
      "Dataset size notes": "103000000 tokens - size of wikitext 103\n\nTraining steps: 350,000 steps \nSequence length: 384 tokens\nBatch size: 128\nTokens per batch = 384 tokens/batch \u00d7 128 = 49,152 tokens/batch\nTotal tokens = 49,152 tokens/batch \u00d7 350,000 steps = 17.2 billion tokens \n\n17200000000/103000000 = 167 epochs",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2203.10692",
      "Reference": "Better Language Model with Hypernym Class Prediction",
      "Citations": "16.0",
      "Authors": "He Bai, Tong Wang, Alessandro Sordoni, Peng Shi",
      "Abstract": "Class-based language models (LMs) have been long devised to address context sparsity in n-gram LMs. In this study, we revisit this approach in the context of neural LMs. We hypothesize that class-based prediction leads to an implicit context aggregation for similar words and thus can improve generalization for rare words. We map words that have a common WordNet hypernym to the same class and train large neural LMs by gradually annealing from predicting the class to token prediction during training. Empirically, this curriculum learning strategy consistently improves perplexity over various large, highly-performant state-of-the-art Transformer-based models on two datasets, WikiText-103 and Arxiv. Our analysis shows that the performance improvement is achieved without sacrificing performance on rare words. Finally, we document other attempts that failed to yield empirical gains, and discuss future directions for the adoption of class-based LMs on a larger scale.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,Canada",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Empirically, this curriculum learning strategy consistently improves perplexity over various large, highly-performant state-of-the-art Transformer-based models on two datasets, WikiText-103 and ARXIV\"",
      "Epochs": "167.02",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "Open access (non-commercial)",
      "Accessibility notes": "code, no license\nhttps://github.com/richardbaihe/robustlm",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ViT-G (model soup)",
      "Organization": "University of Washington,Columbia University,Google,Meta AI,Tel Aviv University",
      "Publication date": "2022-03-10",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "1843000000.0",
      "Parameters notes": "This is from the original ViT-G paper",
      "Training compute (FLOP)": "3.4e+21",
      "Training compute notes": "This is a fine-tuned version of ViT-G, which required 3.4e21 to train per PCD/Akronomicon.\n\nFine-tuning compute is likely minor in comparision:\n\"Models are fine-tuned at a batch size of 512 for either 10,000 or 20,000 steps (approximately 4 or 8 epochs)... all models are fine-tuned at 518 \u00d7 518 resolution\"\nAt 20k steps, we have (518^2) * 512 * 20k = 2.75e12 pixels seen in fine-tuning, compared to (224^2) * 32768 * 5M = 8.22e15 in pre-training.",
      "Training dataset": "",
      "Training dataset size (gradients)": "1752320000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2203.05482v3",
      "Reference": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
      "Citations": "1267.0",
      "Authors": "Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, Ludwig Schmidt",
      "Abstract": "The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models fine-tuned with different hyperparameter configurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs -- we call the results \"model soups.\" When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on ImageNet, achieved a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logit-ensembling to flatness of the loss and confidence of the predictions, and validate this relation empirically. Code is available at this https URL.",
      "Organization categorization": "Academia,Academia,Industry,Industry,Academia",
      "Country (of organization)": "United States of America,United States of America,United States of America,United States of America,Israel",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on ImageNet, achieved a new state of the art.\"",
      "Epochs": "8.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open access (non-commercial)",
      "Accessibility notes": "no license\ncode here, may just be inference code: https://github.com/mlfoundations/model-soups ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "MegaSyn",
      "Organization": "Collaborations Pharmaceuticals",
      "Publication date": "2022-03-07",
      "Domain": "Medicine",
      "Task": "Drug discovery",
      "Parameters": "",
      "Parameters notes": "model details here: https://chemrxiv.org/engage/chemrxiv/article-details/61551803d1fc335b7cf8fd45\n\n\"The variational autoencoder utilizes an encoder-decoder architecture to map chemical space into a latent vector 34. The encoder is composed of 3 LSTM layers of 512 units each followed by a linear layer of 64 units (the latent space).\nOur decoder is comprised of 3 LSTM layers of 512 units each with dropout of 0.2 between\nall layers\"",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "ChEMBL",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9544280/",
      "Reference": "Dual Use of Artificial Intelligence-powered Drug Discovery",
      "Citations": "148.0",
      "Authors": "Fabio Urbina, Filippa Lentzos, C\u00e9dric Invernizzi, Sean Ekins",
      "Abstract": "An international security conference explored how artificial intelligence (AI) technologies for drug discovery could be misused for de novo design of biochemical weapons. A thought experiment evolved into a computational proof.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "Notable example of an AI model having a potential dual use for bio/chemical weapons:\n\n\"To narrow the universe of molecules we chose to drive the generative model towards compounds like the nerve agent VX, one of the most toxic chemical warfare agents developed during the 20th century\u2014a few salt-sized grains of VX, (6\u201310 mg)5, is sufficient to kill a person. Nerve agents such as Novichoks have also been in the headlines recently6.\n\nIn less than 6 hours after starting on our in-house server, our model generated forty thousand molecules that scored within our desired threshold. In the process, the AI designed not only VX, but many other known chemical warfare agents that we identified through visual confirmation with structures in public chemistry databases. Many new molecules were also designed that looked equally plausible. These new molecules were predicted to be more toxic based on the predicted LD50 in comparison to publicly known chemical warfare agents (Figure 1). This was unexpected as the datasets we used for training the AI did not include these nerve agents. The virtual molecules even occupied a region of molecular property space that was entirely separate to the many thousands of molecules in the organism-specific LD50 model, which is mainly made up of pesticides, environmental toxins, and drugs (Figure 1). By inverting the use of our machine learning models, we had transformed our innocuous generative model from a helpful tool of medicine to a generator of likely deadly molecules.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Hosted access (no API)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "\"We can use MegaSyn in fee for service work for you.\nWe can provide an annual license for you to access this software on your own server.\nWe provide maintenance and customization options.\"\nhttps://www.collaborationspharma.com/megasyn",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Statement Curriculum Learning",
      "Organization": "OpenAI",
      "Publication date": "2022-03-02",
      "Domain": "Language,Mathematics",
      "Task": "Automated theorem proving,Language modeling/generation,Question answering,Mathematical reasoning",
      "Parameters": "774000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "1.7901648e+22",
      "Training compute notes": "pretraining: \n6 FLOP/parameter/token * 774000000 parameters * 372000000000 tokens = 1.727568e+21 FLOP \n\nunknown number of epochs -> \"Likely\" confidence\n\nexpert iteration: \n312000000000000 FLOP/GPU/sec * 48000 GPU-hours * 3600 sec / hour * 0.3 [assumed utilization] = 1.617408e+22 FLOP\n\n1.617408e+22 FLOP + 1.727568e+21 FLOP = 1.7901648e+22 FLOP",
      "Training dataset": "Common Crawl,WebMath",
      "Training dataset size (gradients)": "372000000000",
      "Dataset size notes": "Table on p12 gives WebMath dataset size in GB of code. Uncompressed code probably has a similar number of tokens per gigabyte as natural language text, on the order of 3e8 tokens per GB.",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2202.01344",
      "Reference": "Formal Mathematics Statement Curriculum Learning",
      "Citations": "148.0",
      "Authors": "Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, Ilya Sutskever ",
      "Abstract": "We explore the use of expert iteration in the context of language modeling applied to formal mathematics. We show that at same compute budget, expert iteration, by which we mean proof search interleaved with learning, dramatically outperforms proof search only. We also observe that when applied to a collection of formal statements of sufficiently varied difficulty, expert iteration is capable of finding and solving a curriculum of increasingly difficult problems, without the need for associated ground-truth proofs. Finally, by applying this expert iteration to a manually curated set of problem statements, we achieve state-of-the-art on the miniF2F benchmark, automatically solving multiple challenging problems drawn from high school olympiads.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"by applying this expert iteration to a manually curated set of problem statements, we achieve state-of-the-art on the miniF2F benchmark, automatically solving multiple challenging problems drawn from high school olympiads.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "\"with our 774m parameters model, running a full expert iteration to train \u03b8 full required about 2000 A100\ndays of compute\"\n\n2000 days = 48000 hours",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "It seems that only inference code is here, no model weights or pre-training code:\nhttps://github.com/openai/lean-gym\nApache 2.0\n\n\"We present lean-gym\u2019s API\"",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DeepNet",
      "Organization": "Microsoft Research",
      "Publication date": "2022-03-01",
      "Domain": "Language",
      "Task": "Language modeling,Translation,Language modeling/generation",
      "Parameters": "3200000000.0",
      "Parameters notes": "\"Remarkably, on a multilingual benchmark with 7,482 translation directions, our 200-layer model with 3.2B parameters significantly outperforms the 48-layer state-of-the-art model with 12B parameters by 5 BLEU points, which indicates a promising scaling direction\"\n\nEDIT 05/05/2022: The 12B model was presented in an earlier paper. This paper presents a 3.2B model",
      "Training compute (FLOP)": "",
      "Training compute notes": "They show results on par with the original Transformer, so probably less than 2.3e19 FLOP.",
      "Training dataset": "CCMatrix,OPUS",
      "Training dataset size (gradients)": "272629760000",
      "Dataset size notes": "\" The final data consists of 102 languages, 1932 directions, and\n12B sentence pairs.\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2203.00555",
      "Reference": "DeepNet: Scaling Transformers to 1,000 Layers",
      "Citations": "196.0",
      "Authors": "Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, Furu Wei",
      "Abstract": "In this paper, we propose a simple yet effective method to stabilize extremely deep Transformers. Specifically, we introduce a new normalization function (DeepNorm) to modify the residual connection in Transformer, accompanying with theoretically derived initialization. In-depth theoretical analysis shows that model updates can be bounded in a stable way. The proposed method combines the best of two worlds, i.e., good performance of Post-LN and stable training of Pre-LN, making DeepNorm a preferred alternative. We successfully scale Transformers up to 1,000 layers (i.e., 2,500 attention and feed-forward network sublayers) without difficulty, which is one order of magnitude deeper than previous deep Transformers. Remarkably, on a multilingual benchmark with 7,482 translation directions, our 200-layer model with 3.2B parameters significantly outperforms the 48-layer state-of-the-art model with 12B parameters by 5 BLEU points, which indicates a promising scaling direction.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Remarkably, on a multilingual benchmark with 7,482 translation directions, our 200-layer model with 3.2B parameters significantly outperforms the 48-layer state-of-the-art model with 12B parameters by 5 BLEU points\"",
      "Epochs": "4.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "",
      "Accessibility notes": "model code is said to be part of the torchscale library (I am not sure if full pre-training code is here):\nhttps://github.com/microsoft/torchscale\n\nMIT license",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "PolyCoder",
      "Organization": "Carnegie Mellon University (CMU)",
      "Publication date": "2022-02-26",
      "Domain": "Language",
      "Task": "Code generation",
      "Parameters": "2700000000.0",
      "Parameters notes": "2.7B for largest model",
      "Training compute (FLOP)": "1.1e+21",
      "Training compute notes": "\"We use GPT-NeoX toolkit 11 to\ntrain the model efficiently in parallel with 8 Nvidia RTX 8000 GPUs on a single machine. The wall\ntime used to train the largest 2.7B model is about 6 weeks\"\n\n8 * 130 TFLOP/s * 6 * 7 * 24 * 3600 * 0.3 (utilization) ~= 1.1e21",
      "Training dataset": "",
      "Training dataset size (gradients)": "39300000000",
      "Dataset size notes": "249GB\n\nThey trained on 39B tokens per Table 3, but I'm not sure how many epochs that is. May be <1. ",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2202.13169",
      "Reference": "A Systematic Evaluation of Large Language Models of Code",
      "Citations": "785.0",
      "Authors": "Frank F. Xu, Uri Alon, Graham Neubig, Vincent J. Hellendoorn",
      "Abstract": "Large language models (LMs) of code have recently shown tremendous promise in completing code and synthesizing code from natural language descriptions. However, the current state-of-the-art code LMs (e.g., Codex (Chen et al., 2021)) are not publicly available, leaving many questions about their model and data design decisions. We aim to fill in some of these blanks through a systematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo, GPT-NeoX-20B, and CodeParrot, across various programming languages. Although Codex itself is not open-source, we find that existing open-source models do achieve close results in some programming languages, although targeted mainly for natural language modeling. We further identify an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code. We release a new model, PolyCoder, with 2.7B parameters based on the GPT-2 architecture, which was trained on 249GB of code across 12 programming languages on a single machine. In the C programming language, PolyCoder outperforms all models including Codex. Our trained models are open-source and publicly available at this https URL, which enables future research and application in this area.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"In the C programming language, PolyCoder outperforms all models including Codex\"\n\nseems to be SOTA in open-source, not overall",
      "Epochs": "",
      "Training time (hours)": "1000.0",
      "Training time notes": "6 weeks",
      "Training hardware": "NVIDIA Quadro RTX 8000",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "MIT license for model weights\nhttps://huggingface.co/NinedayWang/PolyCoder-2.7B\n\nIt seems that there is no pretraining code here: https://github.com/VHellendoorn/Code-LMs",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "FourCastNet",
      "Organization": "NVIDIA,NERSC, Lawrence Berkeley National Laboratory,University of Michigan,Rice University,California Institute of Technology,Purdue University",
      "Publication date": "2022-02-22",
      "Domain": "Earth science",
      "Task": "Weather forecasting",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "3.4504704e+20",
      "Training compute notes": "312000000000000 FLOP / GPU / sec [A100] * 64 GPUs * 16 hours * 3600 sec / hour * 0.3 [assumed utilization] = 3.4504704e+20 FLOP",
      "Training dataset": "ERA5",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "\" The model is pre-trained using a cosine learning-rate schedule with a starting learning rate `1 for 80 epochs. Following the pre-training, the model is fine-tuned for a further 50 epochs\"\n\n\"The training dataset consists of\n54020 samples while the validation dataset contains 2920 samples\"\n\nGlobal batch size 64\nPatch size p \u00d7 p 8 \u00d7 8",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2202.11214",
      "Reference": "FourCastNet: A Global Data-driven High-resolution Weather Model using Adaptive Fourier Neural Operators",
      "Citations": "",
      "Authors": "Jaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh Chattopadhyay, Morteza Mardani, Thorsten Kurth, David Hall, Zongyi Li, Kamyar Azizzadenesheli, Pedram Hassanzadeh, Karthik Kashinath, Animashree Anandkumar",
      "Abstract": "FourCastNet, short for Fourier Forecasting Neural Network, is a global data-driven weather forecasting model that provides accurate short to medium-range global predictions at  resolution. FourCastNet accurately forecasts high-resolution, fast-timescale variables such as the surface wind speed, precipitation, and atmospheric water vapor. It has important implications for planning wind energy resources, predicting extreme weather events such as tropical cyclones, extra-tropical cyclones, and atmospheric rivers. FourCastNet matches the forecasting accuracy of the ECMWF Integrated Forecasting System (IFS), a state-of-the-art Numerical Weather Prediction (NWP) model, at short lead times for large-scale variables, while outperforming IFS for variables with complex fine-scale structure, including precipitation. FourCastNet generates a week-long forecast in less than 2 seconds, orders of magnitude faster than IFS. The speed of FourCastNet enables the creation of rapid and inexpensive large-ensemble forecasts with thousands of ensemble-members for improving probabilistic forecasting. We discuss how data-driven deep learning models such as FourCastNet are a valuable addition to the meteorology toolkit to aid and augment NWP models.",
      "Organization categorization": "Industry,Government,Academia,Academia,Academia,Academia",
      "Country (of organization)": "United States of America,United States of America,United States of America,United States of America,United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"FourCastNet matches the forecasting accuracy of the ECMWF Integrated\nForecasting System (IFS), a state-of-the-art Numerical Weather Prediction (NWP) model, at short lead times for large-scale variables, while outperforming IFS for small-scale variables, including precipitation. \"\n\n\"the FourCastNet model has many characteristics that make it superior to the prior SOTA DLWP model\"",
      "Epochs": "130.0",
      "Training time (hours)": "16.0",
      "Training time notes": "\"The end to end training takes about 16 hours\nwall-clock time on a cluster of 64 Nvidia A100 GPUs.\"",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "64.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "51498.36398258959",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ST-MoE",
      "Organization": "Google,Google Brain,Google Research",
      "Publication date": "2022-02-17",
      "Domain": "Language",
      "Task": "Language modeling/generation",
      "Parameters": "269000000000.0",
      "Parameters notes": "269B. it's called ST-MoE-32B because it's equivalent to a 32B dense model.",
      "Training compute (FLOP)": "2.9e+23",
      "Training compute notes": "The paper claims \"scaling a sparse model to 269B parameters, with a computational cost comparable to a 32B dense encoder-decoder\". If this is true for training cost, then 6*32e9*1.5e12 = 2.9e23",
      "Training dataset": "C4",
      "Training dataset size (gradients)": "1500000000000",
      "Dataset size notes": "\"We pre-train for 1.5T tokens on a mixture of English-only C4 dataset (Raffel et al., 2019) and the dataset from GLaM (Du et al., 2021) summarized in Appendix E\"\n",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2202.08906v2",
      "Reference": "ST-MoE: Designing Stable and Transferable Sparse Expert Models",
      "Citations": "295.0",
      "Authors": "Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, William Fedus",
      "Abstract": "Scale has opened new frontiers in natural language processing -- but at a high cost. In response, Mixture-of-Experts (MoE) and Switch Transformers have been proposed as an energy efficient path to even larger and more capable language models. But advancing the state-of-the-art across a broad set of natural language tasks has been hindered by training instabilities and uncertain quality during fine-tuning. Our work focuses on these issues and acts as a design guide. We conclude by scaling a sparse model to 269B parameters, with a computational cost comparable to a 32B dense encoder-decoder Transformer (Stable and Transferable Mixture-of-Experts or ST-MoE-32B). For the first time, a sparse model achieves state-of-the-art performance in transfer learning, across a diverse set of tasks including reasoning (SuperGLUE, ARC Easy, ARC Challenge), summarization (XSum, CNN-DM), closed book question answering (WebQA, Natural Questions), and adversarially constructed tasks (Winogrande, ANLI R3).",
      "Organization categorization": "Industry,Industry,Industry",
      "Country (of organization)": "United States of America,United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"ST-MoE-32B improves the current state-of-the-art on the test server submissions for both ARC Easy (92.7 \u2192 94.8) and ARC Challenge (81.4 \u2192 86.5).\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "1000000.0",
      "Batch size notes": "\" We use 1M tokens per batch\"",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache License 2.0\nCode for our models is available at https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/moe.py",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Midjourney V1",
      "Organization": "Midjourney",
      "Publication date": "2022-02-15",
      "Domain": "Image generation",
      "Task": "Image generation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "",
      "Reference": "",
      "Citations": "",
      "Authors": "",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Significant use,Historical significance",
      "Notability criteria notes": "Significant historical usage and controversy as one of the first generative AI art models. For example:\nhttps://www.nytimes.com/2022/09/02/technology/ai-artificial-intelligence-artists.html",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Hosted access (no API)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ProteinBERT",
      "Organization": "Hebrew University of Jerusalem,Ben-Gurion University of the Negev,Deep Trading",
      "Publication date": "2022-02-10",
      "Domain": "Biology",
      "Task": "Proteins,Protein generation,Protein representation learning",
      "Parameters": "16000000.0",
      "Parameters notes": "\"Altogether, it includes \u223c16M trainable parameters, making it substantially smaller than other protein language models\"",
      "Training compute (FLOP)": "6.5e+19",
      "Training compute notes": "\"Pretraining speed on a single GPU (Nvidia Quadro RTX 5000) was 280 protein records per second. We trained the model for 28\u2009days over \u223c670M records\"\n\n28 * 24 * 3600 * 89 TFLOP/s * 0.3 (assumed utilization) = 6.5e19\nhttps://www.wolframalpha.com/input?i=28+days+*+89+TFLOP%2Fs+*+0.3",
      "Training dataset": "UniRef90",
      "Training dataset size (gradients)": "37598200000",
      "Dataset size notes": "Number of proteins: 106,000,000\nAverage protein length: 300 amino acids\n\nTotal unique tokens = 106,000,000 \u00d7 300 = 31,800,000,000 \u2248 3.2e10 tokens",
      "Confidence": "Confident",
      "Link": "https://academic.oup.com/bioinformatics/article/38/8/2102/6502274",
      "Reference": "ProteinBERT: a universal deep-learning model of protein sequence and function",
      "Citations": "775.0",
      "Authors": "Nadav Brandes, Dan Ofer, Yam Peleg, Nadav Rappoport, Michal Linial",
      "Abstract": "Self-supervised deep language modeling has shown unprecedented success across natural language tasks, and has recently been repurposed to biological sequences. However, existing models and pretraining methods are designed and optimized for text analysis. We introduce ProteinBERT, a deep language model specifically designed for proteins. Our pretraining scheme combines language modeling with a novel task of Gene Ontology (GO) annotation prediction. We introduce novel architectural elements that make the model highly efficient and flexible to long sequences. The architecture of ProteinBERT consists of both local and global representations, allowing end-to-end processing of these types of inputs and outputs. ProteinBERT obtains near state-of-the-art performance, and sometimes exceeds it, on multiple benchmarks covering diverse protein properties (including protein structure, post-translational modifications and biophysical attributes), despite using a far smaller and faster model than competing deep-learning methods. Overall, ProteinBERT provides an efficient framework for rapidly training protein predictors, even with limited labeled data.",
      "Organization categorization": "Academia,Academia,Industry",
      "Country (of organization)": "Israel,Israel,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Table 2: evaluated on TAPE benchmark, not absolute SOTA on all of it, only on Stability (Spearman \u03c1 = 0.76)\n\n\"ProteinBERT obtains near state-of-the-art performance, and sometimes exceeds it, on multiple benchmarks covering diverse protein properties (including protein structure, post-translational modifications and biophysical attributes)\"",
      "Epochs": "6.4",
      "Training time (hours)": "672.0",
      "Training time notes": "28 days",
      "Training hardware": "NVIDIA Quadro RTX 5000",
      "Hardware quantity": "1.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "254.28806247515058",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "26008.0",
      "Batch size notes": "Supplementary materials: \"During pretraining we used batch sizes of 128, 64 or 32 for episodes of 128, 512 or 1,024 tokens, respectively\" Since they seem to be used in equal parts, taking geometric mean: ((128*128)*(64*512)*(32*1024))**(1/3) = 26,008",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "",
      "Accessibility notes": "MIT license\n\nhttps://github.com/nadavbra/protein_bert",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "1574.8894543870347",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "LaMDA",
      "Organization": "Google",
      "Publication date": "2022-02-10",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "137000000000.0",
      "Parameters notes": "\"LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters\"",
      "Training compute (FLOP)": "3.55e+23",
      "Training compute notes": "\"The total FLOPS is 56.5% * 123 TFLOPS/s * 1024 chips * 57.7 days\n= 3.55E+23\"\nFrom https://arxiv.org/pdf/2201.08239.pdf p.18\n",
      "Training dataset": "Infiniset",
      "Training dataset size (gradients)": "2080000000000",
      "Dataset size notes": "\"and are pre-trained on 1.56T words of public dialog data and web text\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2201.08239",
      "Reference": "LaMDA: Language Models for Dialog Applications",
      "Citations": "1769.0",
      "Authors": "Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, Quoc Le",
      "Abstract": "We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "1385.0",
      "Training time notes": "57.7 days * 24",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "1024.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "229949.98625999544",
      "Compute cost notes": "",
      "Training power draw (W)": "927218.301405043",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "256000.0",
      "Batch size notes": "\"All models were trained with 256K tokens per batch\"",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "0.565",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GPT-NeoX-20B",
      "Organization": "EleutherAI",
      "Publication date": "2022-02-09",
      "Domain": "Language",
      "Task": "Language modeling/generation",
      "Parameters": "20000000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "9.31627008e+22",
      "Training compute notes": "Trained for 3 months on 96 A100s (according to correspondence with author). Let's say 0.4 utilization rate.",
      "Training dataset": "The Pile",
      "Training dataset size (gradients)": "341173367965",
      "Dataset size notes": "\"In aggregate, the Pile consists of over 825GiB of raw text data\"\n\nFigure 4",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2204.06745",
      "Reference": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model",
      "Citations": "943.0",
      "Authors": "Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel Weinbach",
      "Abstract": "We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model that has publicly available weights at the time of submission. In this work, we describe \\model{}'s architecture and training and evaluate its performance on a range of language-understanding, mathematics, and knowledge-based tasks. We find that GPT-NeoX-20B is a particularly powerful few-shot reasoner and gains far more in performance when evaluated five-shot than similarly sized GPT-3 and FairSeq models. We open-source the training and evaluation code, as well as the model weights, at this https URL.",
      "Organization categorization": "Research collective",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "1.4",
      "Training time (hours)": "2160.0",
      "Training time notes": "see other notes\n",
      "Training hardware": "NVIDIA A100 SXM4 40 GB",
      "Hardware quantity": "96.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "184272.80736002637",
      "Compute cost notes": "",
      "Training power draw (W)": "77269.91251696396",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "3150000.0",
      "Batch size notes": "\"we opt to use the same batch size as OpenAI\u2019s 175B model\u2013approximately 3.15M tokens, or 1538 contexts of 2048 tokens each, and train for a total of 150,000 steps\"",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache 2.0. training code: https://github.com/EleutherAI/gpt-neox ",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "1563104.4089395767",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "MaskGIT (ImageNet)",
      "Organization": "Google Research",
      "Publication date": "2022-02-08",
      "Domain": "Image generation",
      "Task": "Image generation,Image-to-image",
      "Parameters": "227000000.0",
      "Parameters notes": "227M\nTable 1",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "ImageNet",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "\"For each dataset, we only train a single autoencoder, decoder, and codebook with 1024 tokens on cropped 256x256 images for all the experiments. The image is always compressed by a fixed factor of 16, i.e. from H \u02c6W to a grid of tokens in the size of h \u02c6 w, where h=H{16 and w=W{16. We find that this autoencoder, together with the codebook, can be reused to synthesize 512\u02c6512 images.\"\n\n\"All models are trained on 4x4 TPU devices with a batch size of 256. ImageNet models are trained for 300 epochs while the Places2 model is trained for 200 epochs.\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2202.04200",
      "Reference": "MaskGIT: Masked Generative Image Transformer",
      "Citations": "",
      "Authors": "Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, William T. Freeman",
      "Abstract": "Generative transformers have experienced rapid popularity growth in the computer vision community in synthesizing high-fidelity and high-resolution images. The best generative transformer models so far, however, still treat an image naively as a sequence of tokens, and decode an image sequentially following the raster scan ordering (i.e. line-by-line). We find this strategy neither optimal nor efficient. This paper proposes a novel image synthesis paradigm using a bidirectional transformer decoder, which we term MaskGIT. During training, MaskGIT learns to predict randomly masked tokens by attending to tokens in all directions. At inference time, the model begins with generating all tokens of an image simultaneously, and then refines the image iteratively conditioned on the previous generation. Our experiments demonstrate that MaskGIT significantly outperforms the state-of-the-art transformer model on the ImageNet dataset, and accelerates autoregressive decoding by up to 64x. Besides, we illustrate that MaskGIT can be easily extended to various image editing tasks, such as inpainting, extrapolation, and image manipulation.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Our experiments demonstrate that MaskGIT significantly outperforms the state-of-the-art transformer model on the ImageNet dataset, and accelerates autoregressive decoding by up to 64x\"",
      "Epochs": "300.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "16.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "RETRO-7B",
      "Organization": "DeepMind",
      "Publication date": "2022-02-07",
      "Domain": "Language",
      "Task": "Language modeling/generation,Language modeling",
      "Parameters": "7500000000.0",
      "Parameters notes": "\"Retro provides a constant gain for models ranging from 150M to 7B parameters, and Retro can be improved at evaluation time by increasing the database size and the number of retrieved neighbours. \"",
      "Training compute (FLOP)": "1.68e+22",
      "Training compute notes": "C=6ND = 6 * 7e9 * 400e9 = 1.7e22 ",
      "Training dataset": "WikiText-103",
      "Training dataset size (gradients)": "419430400000",
      "Dataset size notes": "\"we train for 419,430,400,000 training tokens\" ~= 315B words.",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2112.04426",
      "Reference": "Improving language models by retrieving from trillions of tokens",
      "Citations": "1401.0",
      "Authors": "Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero,Karen Simonyan, Jack W. Rae\u2021, Erich Elsen\u2021 and Laurent Sifre",
      "Abstract": "We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a 2 trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25\u00d7 fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Our largest model obtains state-of-the-art results on a range of downstream evaluation datasets including Wikitext103\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "2097152.0",
      "Batch size notes": "1024 * 2048",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "AlphaCode",
      "Organization": "DeepMind",
      "Publication date": "2022-02-02",
      "Domain": "Language",
      "Task": "Code generation",
      "Parameters": "41100000000.0",
      "Parameters notes": "41.1B. Table 3",
      "Training compute (FLOP)": "2.38010000000001e+23",
      "Training compute notes": "Using C=6ND, we have C = 6 FLOP/token/param * 41.1B params * 967B tokens = 2.38e23 FLOP.\n\nFigure 7 (a) shows a maximum training compute budget of approx 23000 TPU-days per model. This matches the operation-counting estimate at 44% utilization.",
      "Training dataset": "CodeContests,Unspecified unreleased",
      "Training dataset size (gradients)": "967000000000",
      "Dataset size notes": "Appendix part A has answers for pretraining.",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2203.07814",
      "Reference": "Competition-Level Code Generation with AlphaCode",
      "Citations": "1819.0",
      "Authors": "Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, Oriol Vinyals",
      "Abstract": "Programming is a powerful and ubiquitous problem-solving tool. Developing systems that can assist programmers or even generate programs independently could make programming more productive and accessible, yet so far incorporating innovations in AI has proven challenging. Recent large-scale language models have demonstrated an impressive ability to generate code, and are now able to complete simple programming tasks. However, these models still perform poorly when evaluated on more complex, unseen problems that require problem-solving skills beyond simply translating instructions into code. For example, competitive programming problems which require an understanding of algorithms and complex natural language remain extremely challenging. To address this gap, we introduce AlphaCode, a system for code generation that can create novel solutions to these problems that require deeper reasoning. In simulated evaluations on recent programming competitions on the Codeforces platform, AlphaCode achieved on average a ranking of top 54.3% in competitions with more than 5,000 participants. We found that three key components were critical to achieve good and reliable performance: (1) an extensive and clean competitive programming dataset for training and evaluation, (2) large and efficient-to-sample transformer-based architectures, and (3) large-scale model sampling to explore the search space, followed by filtering based on program behavior to a small set of submissions.\n",
      "Organization categorization": "Industry",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\" To the best of our knowledge, this is the first time that a computer system has been\ncompetitive with human participants in programming competitions\"",
      "Epochs": "",
      "Training time (hours)": "147.2",
      "Training time notes": "Figure 7 (a) shows that the models were trained for around 23000 TPU-days. We know they trained on TPUv4s, and in appendix D.1 they say they have 3750 TPUv4 and TPUv4i. Assuming they trained only on the 3750 TPUv4s, that suggests 23000 / 3750 = 6.13 days, or 147.2 hours.",
      "Training hardware": "Google TPU v4",
      "Hardware quantity": "3750.0",
      "Hardware utilization (MFU)": "0.4364",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "2566002.5357062262",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "4718592.0",
      "Batch size notes": "2304 token sequences, 2048 batch size. 2304 * 2048 = 4718592\n\ntrained on 967B tokens and 205k steps. 967B/205k = 4717073, so seems they didn't do warmup",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "InstructGPT 175B",
      "Organization": "OpenAI",
      "Publication date": "2022-01-27",
      "Domain": "Language",
      "Task": "Language modeling/generation",
      "Parameters": "175000000000.0",
      "Parameters notes": "\"We train three model sizes (1.3B, 6B, and 175B parameters)\"",
      "Training compute (FLOP)": "3.19181e+23",
      "Training compute notes": "\"training our 175B PPO-ptx model requires 60 petaflops/s-days, compared to 3,640 petaflops/s-days for GPT-3 (Brown et al., 2020)\"\n\n60/3640 = +1.65% to base model compute\n\nbase model was reported 3.14e+23 FLOP\n\n3.14e+23 * 1.0165 = 319181000000000000000000",
      "Training dataset": "",
      "Training dataset size (gradients)": "16969897",
      "Dataset size notes": "Table 6 - describes **number of prompts**\n\n26584 + 6623 = 33207\n\nThis is added to GPT-3 dataset size.",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/pdf/2203.02155",
      "Reference": "Training language models to follow instructions with human feedback",
      "Citations": "17150.0",
      "Authors": "Long Ouyang, Pamela Mishkin, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright,John Schulman, Amanda Askell, Fraser Kelton, Peter Welinder, Luke Miller, Maddie Simens, Paul Christiano, Ryan Lowe, Chong Zhang, Jacob Hilton, Sandhini Agarwal, Katarina Slama, Alex Ray, Jan Leike",
      "Abstract": "Making language models bigger does not inherently make them better at following a user\u2019s intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback (RLHF). We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance,Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "GPT-3 175B (davinci)",
      "Finetune compute (FLOP)": "5.181e+21",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "used to be accessible via API, now deprecated",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "InstructGPT 6B",
      "Organization": "OpenAI",
      "Publication date": "2022-01-27",
      "Domain": "Language",
      "Task": "Language modeling/generation",
      "Parameters": "6000000000.0",
      "Parameters notes": "\"We train three model sizes (1.3B, 6B, and 175B parameters)\"",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2203.02155",
      "Reference": "Training language models to follow instructions with human feedback",
      "Citations": "17150.0",
      "Authors": "Long Ouyang, Pamela Mishkin, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, John Schulman, Amanda Askell, Fraser Kelton, Peter Welinder, Luke Miller, Maddie Simens, Paul Christiano, Ryan Lowe, Chong Zhang, Jacob Hilton, Sandhini Agarwal, Katarina Slama, Alex Ray, Jan Leike",
      "Abstract": "Making language models bigger does not inherently make them better at following a user\u2019s intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback (RLHF). We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance,Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "GPT-3 6.7B",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Unreleased",
      "Accessibility notes": "used to be accessible via API, now deprecated",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "InstructGPT 1.3B",
      "Organization": "OpenAI",
      "Publication date": "2022-01-27",
      "Domain": "Language",
      "Task": "Language modeling/generation",
      "Parameters": "1300000000.0",
      "Parameters notes": "\"We train three model sizes (1.3B, 6B, and 175B parameters)\"",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2203.02155",
      "Reference": "Training language models to follow instructions with human feedback",
      "Citations": "17150.0",
      "Authors": "Long Ouyang, Pamela Mishkin, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, John Schulman, Amanda Askell, Fraser Kelton, Peter Welinder, Luke Miller, Maddie Simens, Paul Christiano, Ryan Lowe, Chong Zhang, Jacob Hilton, Sandhini Agarwal, Katarina Slama, Alex Ray, Jan Leike",
      "Abstract": "Making language models bigger does not inherently make them better at following a user\u2019s intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback (RLHF). We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance,Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "GPT-3 XL",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Unreleased",
      "Accessibility notes": "used to be accessible via API, now deprecated",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "OntoProtein",
      "Organization": "Zhejiang University (ZJU)",
      "Publication date": "2022-01-23",
      "Domain": "Biology,Language",
      "Task": "Proteins,Protein or nucleotide language model (pLM/nLM),Protein interaction prediction,Protein function prediction,Protein representation learning",
      "Parameters": "420000000.0",
      "Parameters notes": "\"For the protein encoder, we use the pre-trained ProtBert from Elnaggar et al. (2020).\"",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "ProteinKG25",
      "Training dataset size (gradients)": "2868520960",
      "Dataset size notes": "Data Summary:\n- Protein tokens: 488,000 \u00d7 300 = 1.464 \u00d7 10^8\n- GO term tokens: 612,483 \u00d7 20 = 1.224966 \u00d7 10^7\n- Knowledge graph triples: 4,990,097\nTotal: (1.464 \u00d7 10^8 + 1.224966 \u00d7 10^7 + 4.99 \u00d7 10^6) \u2248 1.626 \u00d7 10^8 data points",
      "Confidence": "Likely",
      "Link": "https://openreview.net/pdf?id=yfe1VMYAXa4",
      "Reference": "ONTOPROTEIN: PROTEIN PRETRAINING WITH GENE ONTOLOGY EMBEDDING",
      "Citations": "",
      "Authors": "Ningyu Zhang, Zhen Bi, Xiaozhuan Liang, Siyuan Cheng, Shumin Deng, Qiang Zhang, Jiazhang Lian, Huajun Chen, Haosen Hong",
      "Abstract": "Self-supervised protein language models have proved their effectiveness in learn- ing the proteins representations. With the increasing computational power, cur- rent protein language models pre-trained with millions of diverse sequences can advance the parameter scale from million-level to billion-level and achieve re- markable improvement. However, those prevailing approaches rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowl- edge facts for better protein representations. We argue that informative biology knowledge in KGs can enhance protein representation with external knowledge. In this work, we propose OntoProtein, the first general framework that makes use of structure in GO (Gene Ontology) into protein pre-training models. We construct a novel large-scale knowledge graph that consists of GO and its related proteins, and gene annotation texts or protein sequences describe all nodes in the graph. We propose novel contrastive learning with knowledge-aware negative sampling to jointly optimize the knowledge graph and protein embedding during pre-training. Experimental results show that OntoProtein can surpass state-of-the-art methods with pre-trained protein language models in TAPE benchmark and yield better performance compared with baselines in protein-protein interaction and protein function prediction1.",
      "Organization categorization": "Academia",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Not absolute SOTA, outperforms protein-language models but not MSA transformer.\n\nExperimental results show that OntoProtein can surpass state-of-the-art methods with pre-trained protein language models in TAPE benchmark and yield better performance compared with baselines in protein-protein interaction and protein function prediction1.",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "ProtBERT-BFD",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT license\nhttps://github.com/zjunlp/OntoProtein\n\nunclear license\nhttps://huggingface.co/zjunlp/OntoProtein",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "AbLang (heavy sequences)",
      "Organization": "University of Oxford",
      "Publication date": "2022-01-22",
      "Domain": "Biology",
      "Task": "Proteins,Antibody property prediction,Protein or nucleotide language model (pLM/nLM)",
      "Parameters": "355000000.0",
      "Parameters notes": "\"The hyperparameters were selected to be similar to those used\nin the RoBERTa paper (Liu et al., 2019).\"\n\nLiu et al., 2019 link: https://arxiv.org/pdf/1907.11692.pdf\n\"We begin by training RoBERTa following the BERTLARGE architecture (L = 24, H = 1024, A = 16, 355M parameters)\"",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Observed Antibody Space (OAS) database",
      "Training dataset size (gradients)": "2260275840,2290000001",
      "Dataset size notes": "Heavy Chain: 14,126,724 sequences \u00d7 160 residues = 2,260,275,840 datapoints\nLight Chain: 187,068 sequences \u00d7 160 residues = 29,930,880 datapoints\nTotal: 2,260,275,840 + 29,930,880 = 2,290,206,720 datapoints (2.29B)",
      "Confidence": "Confident",
      "Link": "https://academic.oup.com/bioinformaticsadvances/article/2/1/vbac046/6609807",
      "Reference": "AbLang: an antibody language model for completing antibody sequences",
      "Citations": "176.0",
      "Authors": "Tobias H Olsen, Iain H Moal, Charlotte M Deane",
      "Abstract": "Motivation\nGeneral protein language models have been shown to summarize the semantics of protein sequences into representations that are useful for state-of-the-art predictive methods. However, for antibody specific problems, such as restoring residues lost due to sequencing errors, a model trained solely on antibodies may be more powerful. Antibodies are one of the few protein types where the volume of sequence data needed for such language models is available, e.g. in the Observed Antibody Space (OAS) database.\n\nResults\nHere, we introduce AbLang, a language model trained on the antibody sequences in the OAS database. We demonstrate the power of AbLang by using it to restore missing residues in antibody sequence data, a key issue with B-cell receptor repertoire sequencing, e.g. over 40% of OAS sequences are missing the first 15 amino acids. AbLang restores the missing residues of antibody sequences better than using IMGT germlines or the general protein language model ESM-1b. Further, AbLang does not require knowledge of the germline of the antibody and is seven times faster than ESM-1b.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"AbLang restores residues more accurately and faster than a current state-of-the-art protein language model ESM-1b, emphasizing the benefits and potential of an antibody specific language model\" - SOTA improvement for a very specific task\n\nI haven't found information about SOTA claims on any standard benchmarks",
      "Epochs": "20.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open source",
      "Accessibility notes": " BSD-3-Clause license\nhttps://github.com/oxpig/AbLang\n\nAbLang is a python package",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "data2vec (vision)",
      "Organization": "Meta AI",
      "Publication date": "2022-01-20",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "705134592.0",
      "Parameters notes": "Section 4: \"We experiment with two model sizes: data2vec Base and\ndata2vec Large, containing either L = 12 or L = 24 Trans-\nformer blocks with H = 768 or H = 1024 hidden dimen-\nsion (with 4 \u00d7 H feed-forward inner-dimension)\"\n",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "ImageNet-1k",
      "Training dataset size (gradients)": "251108732",
      "Dataset size notes": "Section 5.1: \n\"we pretrain data2vec on the images of the ImageNet-1K training\nset\"",
      "Confidence": "",
      "Link": "https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language/",
      "Reference": "Data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",
      "Citations": "1021.0",
      "Authors": "Alexei Baevski,  Wei-Ning Hsu,  Qiantong Xu , Arun Babu,  Jiatao Gu,  Michael Auli",
      "Abstract": "While the general idea of self-supervised learning is identical across modalities, the actual algorithms and objectives differ widely because they were developed with a single modality in mind. To get us closer to general self-supervised learning, we present data2vec, a framework that uses the same learning method for either speech,NLP or computer vision. The core idea is to predict latent representations of the full input data based on a masked view of the input in a selfdistillation setup using a standard Transformer architecture. Instead of predicting modality-specific targets such as words, visual tokens or units of human speech which are local in nature, data2vec predicts contextualized latent representations that contain information from the entire input. Experiments on the major benchmarks of speech recognition, image classification, and natural language understanding demonstrate a new state of the art or competitive performance to predominant approaches.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Experiments on the major benchmarks of speech recognition, image classification, and natural lan guage understanding demonstrate a new state of the art or competitive performance to predominant approaches\"\n\n\"Experimental results show data2vec to be effective in all three modalities, setting a new state of the art for ViT-B and ViT-L on ImageNet-1K\"\n\"",
      "Epochs": "800.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "",
      "Accessibility notes": "MIT license\nhttps://github.com/facebookresearch/data2vec_vision/tree/main/beit",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "data2vec (speech)",
      "Organization": "Meta AI",
      "Publication date": "2022-01-20",
      "Domain": "Speech",
      "Task": "Speech recognition (ASR)",
      "Parameters": "705134592.0",
      "Parameters notes": "Section 4: \"We experiment with two model sizes: data2vec Base and\ndata2vec Large, containing either L = 12 or L = 24 Trans-\nformer blocks with H = 768 or H = 1024 hidden dimen-\nsion (with 4 \u00d7 H feed-forward inner-dimension)\"\n",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "LibriSpeech",
      "Training dataset size (gradients)": "17510400",
      "Dataset size notes": "Section 5.2:\n\"we pre-train data2vec on the 960\nhours of speech audio data from Librispeech (LS-960)\"\n\n13,680 words per hour",
      "Confidence": "",
      "Link": "https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language/",
      "Reference": "Data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",
      "Citations": "1021.0",
      "Authors": "Alexei Baevski,  Wei-Ning Hsu,  Qiantong Xu , Arun Babu,  Jiatao Gu,  Michael Auli",
      "Abstract": "While the general idea of self-supervised learning is identical across modalities, the actual algorithms and objectives differ widely because they were developed with a single modality in mind. To get us closer to general self-supervised learning, we present data2vec, a framework that uses the same learning method for either speech,NLP or computer vision. The core idea is to predict latent representations of the full input data based on a masked view of the input in a selfdistillation setup using a standard Transformer architecture. Instead of predicting modality-specific targets such as words, visual tokens or units of human speech which are local in nature, data2vec predicts contextualized latent representations that contain information from the entire input. Experiments on the major benchmarks of speech recognition, image classification, and natural language understanding demonstrate a new state of the art or competitive performance to predominant approaches.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Experiments on the major benchmarks of speech recognition, image classification, and natural lan guage understanding demonstrate a new state of the art or competitive performance to predominant approaches\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "",
      "Accessibility notes": "MIT license\nhttps://github.com/facebookresearch/fairseq/tree/main/examples/data2vec",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "data2vec (language)",
      "Organization": "Meta AI",
      "Publication date": "2022-01-20",
      "Domain": "Language",
      "Task": "Language modeling/generation",
      "Parameters": "705134592.0",
      "Parameters notes": "Section 4: \"We experiment with two model sizes: data2vec Base and\ndata2vec Large, containing either L = 12 or L = 24 Trans-\nformer blocks with H = 768 or H = 1024 hidden dimen-\nsion (with 4 \u00d7 H feed-forward inner-dimension)\"",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "BookCorpus (BooksCorpus, Toronto Book Corpus),English Wikipedia",
      "Training dataset size (gradients)": "131072000000",
      "Dataset size notes": "Section 5.3: \"we adopt the same training setup as BERT (Devlin et al., 2019)\nby pre-training on the Books Corpus (Zhu et al., 2015) and English Wikipedia data over 1M updates and a batch size of 256 sequences.\"",
      "Confidence": "Likely",
      "Link": "https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language/",
      "Reference": "Data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",
      "Citations": "1021.0",
      "Authors": "Alexei Baevski,  Wei-Ning Hsu,  Qiantong Xu , Arun Babu,  Jiatao Gu,  Michael Auli",
      "Abstract": "While the general idea of self-supervised learning is identical across modalities, the actual algorithms and objectives differ widely because\nthey were developed with a single modality in mind. To get us closer to general self-supervised learning, we present data2vec, a framework that\nuses the same learning method for either speech, NLP or computer vision. The core idea is to predict latent representations of the full input data based on a masked view of the input in a selfdistillation setup using a standard transformer architecture. Instead of predicting modality-specific\ntargets such as words, visual tokens or units of human speech which are local in nature, data2vecpredicts contextualized latent representations that\ncontain information from the entire input. Experiments on the major benchmarks of speech recognition, image classification, and natural language understanding demonstrate a new state of the art or competitive performance to predominant approaches. Models and code are available at www.github.com/pytorch/fairseq/tree/master/examples/data2vec.\n",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Experiments on the major benchmarks of speech recognition, image classification, and natural lan guage understanding demonstrate a new state of the art or competitive performance to predominant approaches\"\n\n\"To our knowledge this is the first successful pre-trained NLP model which does not use discrete units (words, subwords, characters or bytes) as the training target. Instead, the model predicts a contextualized latent representation emerging from self-attention over the entire unmasked text sequence.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT License\nModels and code are available at www.github.com/pytorch/fairseq/tree/master/examples/data2vec",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Detic",
      "Organization": "Meta AI,University of Texas at Austin",
      "Publication date": "2022-01-07",
      "Domain": "Vision",
      "Task": "Object detection,Image classification",
      "Parameters": "88000000.0",
      "Parameters notes": "from https://github.com/microsoft/Swin-Transformer Swin-B have 88M, \nfrom page 8 :  'Training our ResNet50 model takes \u223c 22 hours on 8 V100 GPUs. The large 21K Swin-B model trains in \u223c 24 hours on 32 GPUs.'",
      "Training compute (FLOP)": "2.34399744e+19",
      "Training compute notes": "28.26e12* 32 * 24*3600*0.3 =2.34e19 = peak flops * num gpus * num seconds * assumed utilization rate\nfor Swin-B model from page 8 :  'Training our ResNet50 model takes \u223c 22 hours on 8 V100 GPUs. The large 21K Swin-B model trains in \u223c 24 hours on 32 GPUs.'",
      "Training dataset": "ImageNet21k,Conceptual Captions (CC3M),LVIS",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "14M + 1.5M + 1.2M + 100K + 100K = 16900000.0\ntable above section 5.1",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/2201.02605",
      "Reference": "Detecting Twenty-thousand Classes using Image-level Supervision",
      "Citations": "745.0",
      "Authors": "Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp Kr\u00e4henb\u00fchl, Ishan Misra",
      "Abstract": " Current object detectors are limited in vocabulary size due to the small scale of detection datasets. Image classifiers, on the other hand, reason about much larger vocabularies, as their datasets are larger and easier to collect. We propose Detic, which simply trains the classifiers of a detector on image classification data and thus expands the vocabulary of detectors to tens of thousands of concepts. Unlike prior work, Detic does not need complex assignment schemes to assign image labels to boxes based on model predictions, making it much easier to implement and compatible with a range of detection architectures and backbones. Our results show that Detic yields excellent detectors even for classes without box annotations. It outperforms prior work on both open-vocabulary and long-tail detection benchmarks. Detic provides a gain of 2.4 mAP for all classes and 8.3 mAP for novel classes on the open-vocabulary LVIS benchmark. On the standard LVIS benchmark, Detic obtains 41.7 mAP when evaluated on all classes, or only rare classes, hence closing the gap in performance for object categories with few samples. For the first time, we train a detector with all the twenty-one-thousand classes of the ImageNet dataset and show that it generalizes to new datasets without finetuning. Code is available at \\url{this https URL}. ",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"On open-vocabulary COCO, our method outperforms the previous state-of-the-art OVR-CNN [ 72 ] by 5 point with the same detector and data\"",
      "Epochs": "",
      "Training time (hours)": "24.0",
      "Training time notes": "from page 8 :  'Training our ResNet50 model takes \u223c 22 hours on 8 V100\nGPUs. The large 21K Swin-B model trains in \u223c 24 hours on 32 GPUs.'",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "32.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "191.44581825616135",
      "Compute cost notes": "",
      "Training power draw (W)": "19331.67955389363",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache, models and code: https://github.com/facebookresearch/Detic",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ERNIE-ViLG",
      "Organization": "Baidu",
      "Publication date": "2021-12-31",
      "Domain": "Multimodal,Image generation,Vision,Language",
      "Task": "Vision-language generation,Image generation,Text-to-image,Image captioning,Language modeling/generation,Visual question answering",
      "Parameters": "10000000000.0",
      "Parameters notes": "\"To explore the landscape of large-scale pre-training for bidirectional text-image generation, we pre-train a 10-billion parameter model on a large-scale dataset of 145 million high-quality Chinese image-text pairs.\"",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "To explore the landscape of large-scale pre-training for bidirectional text-image generation, we pre-train a 10-billion parameter model on a large-scale dataset of 145 million high-quality Chinese image-text pairs.",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/2112.15283",
      "Reference": "ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation",
      "Citations": "54.0",
      "Authors": "Han Zhang, Weichong Yin, Yewei Fang, Lanxin Li, Boqiang Duan, Zhihua Wu, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang",
      "Abstract": "Conventional methods for the image-text generation tasks mainly tackle the naturally bidirectional generation tasks separately, focusing on designing task-specific frameworks to improve the quality and fidelity of the generated samples. Recently, Vision-Language Pre-training models have greatly improved the performance of the image-to-text generation tasks, but large-scale pre-training models for text-to-image synthesis task are still under-developed. In this paper, we propose ERNIE-ViLG, a unified generative pre-training framework for bidirectional image-text generation with transformer model. Based on the image quantization models, we formulate both image generation and text generation as autoregressive generative tasks conditioned on the text/image input. The bidirectional image-text generative modeling eases the semantic alignments across vision and language. For the text-to-image generation process, we further propose an end-to-end training method to jointly learn the visual sequence generator and the image reconstructor. To explore the landscape of large-scale pre-training for bidirectional text-image generation, we train a 10-billion parameter ERNIE-ViLG model on a large-scale dataset of 145 million (Chinese) image-text pairs which achieves state-of-the-art performance for both text-to-image and image-to-text tasks, obtaining an FID of 7.9 on MS-COCO for text-to-image synthesis and best results on COCO-CN and AIC-ICC for image captioning.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"we train a 10-billion parameter ERNIE-ViLG model on a large-scale dataset of 145 million (Chinese) image-text pairs which achieves state-of-the-art performance for both text-to-image and image-to-text tasks\"\n\n\"chieves state-of-the-art performance for both text-to-image and image-to-text tasks, obtaining an FID of 7.9 on MS-COCO for text-to-image synthesis and best results on COCO-CN and AIC-ICC for image captioning\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "it seems that the model was completely substituted by its successor ViLG 2.0 - I haven't found niether model weights nor code or API.",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ERNIE 3.0 Titan",
      "Organization": "Baidu,Peng Cheng Laboratory",
      "Publication date": "2021-12-23",
      "Domain": "Language",
      "Task": "Language modeling,Language modeling/generation,Relation extraction,Sentiment classification,Text classification",
      "Parameters": "260000000000.0",
      "Parameters notes": "\"[We] developed... distributed training technology, including fine-grained parallelism, heterogeneous hardware-aware training, and fault tolerance mechanism to train the 260B model on both Nvidia V100 GPU and Ascend 910 NPU clusters.\"\nSee also:\nhttps://twitter.com/BaiduResearch/status/1468633977242243078?t=6q4zuLNdTSc4GUBe9OM5Aw&s=19",
      "Training compute (FLOP)": "1.0421e+24",
      "Training compute notes": "The paper suggests that ERNIE 3.0 Titan uses more compute than GPT-3. This is consistent with the 6ND approximation.\n\nC = 6ND = 6 (FLOP/param/token) * (260B params) * (668B tokens) = 1.0421*10^24 FLOP",
      "Training dataset": "ERNIE 3.0 Corpus",
      "Training dataset size (gradients)": "668000000000",
      "Dataset size notes": "\"To ensure the success of the pre-training of ERNIE 3.0 Titan, we utilize the ERNIE 3.0 Corpus [ 2 ], a large-scale, wide-variety, and high-quality Chinese text corpora amounting to 4TB\"\n\nAssuming 167M words/tokens per GB",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2112.12731",
      "Reference": "ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation",
      "Citations": "84.0",
      "Authors": "Shuohuan Wang, Yu Sun, Yang Xiang, Zhihua Wu, Siyu Ding, Weibao Gong, Shikun Feng",
      "Abstract": "Pre-trained language models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. GPT-3 has shown that scaling up pre-trained language models can further exploit their enormous potential. A unified framework named ERNIE 3.0 was recently proposed for pre-training large-scale knowledge enhanced models and trained a model with 10 billion parameters. ERNIE 3.0 outperformed the state-of-the-art models on various NLP tasks. In order to explore the performance of scaling up ERNIE 3.0, we train a hundred-billion-parameter model called ERNIE 3.0 Titan with up to 260 billion parameters on the PaddlePaddle platform. Furthermore, we design a self-supervised adversarial loss and a controllable language modeling loss to make ERNIE 3.0 Titan generate credible and controllable texts. To reduce the computation overhead and carbon emission, we propose an online distillation framework for ERNIE 3.0 Titan, where the teacher model will teach students and train itself simultaneously. ERNIE 3.0 Titan is the largest Chinese dense pre-trained model so far. Empirical results show that the ERNIE 3.0 Titan outperforms the state-of-the-art models on 68 NLP datasets.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "China,China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Empirical results show that the ERNIE 3.0 Titan outperforms the state-of-the-art models on 68 NLP datasets.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA Tesla V100 DGXS 32 GB,Huawei Ascend 910",
      "Hardware quantity": "1920.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "1048576.0",
      "Batch size notes": "\"The maximum sequence length of context and\nthe memory length of language generation is 512 and 128, respectively\"\n\nIn table 1, they use a global batch size of 512 when data parallelism is \"1\" and 2048 when DP is \"4\". Not sure I fully understand this part but I guess they'd use parallelism as much as possible given how they talk about it.\n\n2048 * 512 = 1048576.",
      "Model accessibility": "Hosted access (no API)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "The Ernie 3.0 Titan model was used in Ernie bot. Today, ERNIE has been widely deployed across finance, healthcare, insurance, equity, Internet, logistics, and other fields.\n\nhttp://research.baidu.com/Blog/index-view?id=165",
      "Numerical format": "FP16",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "XGLM-7.5B",
      "Organization": "Meta AI,Facebook AI Research",
      "Publication date": "2021-12-20",
      "Domain": "Language",
      "Task": "Translation,Question answering,Language modeling/generation",
      "Parameters": "7500000000.0",
      "Parameters notes": "\"Our largest model with 7.5 billion parameters sets new state of the art\"",
      "Training compute (FLOP)": "2.25e+22",
      "Training compute notes": "\"The XGLM 7.5B model was trained on 256 A100 GPUs for about 3 weeks, at a speed of 311.6k words per second\"\n\n256 * 312 teraFLOP/s * 21 * 24 * 3600 * 0.3 utilization assumption ~= 4.3e22\n\nalso, it was trained for 500B tokens. Using Compute = 6ND, we have\n6 * 500B * 7.5B = 2.25e22\n\n311k tokens per second * 7.5B params * 6 is 1.35e16 FLOP/s. divide that by 312 teraFLOP/s, which is A100 peak compute, gets 43, suggesting low utilization (17%) of the 256-GPU cluster, or somewhat higher if there's more than one token per word. So I'll use the 6ND number.",
      "Training dataset": "Subset of CC100-XL,CC100-XL,Common Crawl",
      "Training dataset size (gradients)": "500000000000",
      "Dataset size notes": "Training Data. Our models are trained on a static multilingual corpus extracted from CommonCrawl, with English text comprising 32.6% of the total number of tokens corresponding to 163B tokens.\n163B / 0.326 = 500B total\n\nNote that this dataset is sampled from the much larger CC100-XL, outlined in Appendix F and here: https://huggingface.co/facebook/xglm-7.5B#training-data-statistics\n\nThe huggingface link sums to 1.64T tokens, while the Data Card in the appendix claims 1.9T tokens.",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2112.10668",
      "Reference": "Few-shot Learning with Multilingual Language Models",
      "Citations": "351.0",
      "Authors": "Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li",
      "Abstract": "Large-scale generative language models such as GPT-3 are competitive few-shot learners. While these models are known to be able to jointly represent many different languages, their training data is dominated by English, potentially limiting their cross-lingual generalization. In this work, we train multilingual generative language models on a corpus covering a diverse set of languages, and study their few- and zero-shot learning capabilities in a wide range of tasks. Our largest model with 7.5 billion parameters sets new state of the art in few-shot learning in more than 20 representative languages, outperforming GPT-3 of comparable size in multilingual commonsense reasoning (with +7.4% absolute accuracy improvement in 0-shot settings and +9.4% in 4-shot settings) and natural language inference (+5.4% in each of 0-shot and 4-shot settings). On the FLORES-101 machine translation benchmark, our model outperforms GPT-3 on 171 out of 182 directions with 32 training examples, while surpassing the official supervised baseline in 45 directions. We conduct an in-depth analysis of different multilingual prompting approaches, showing in particular that strong few-shot learning performance across languages can be achieved via cross-lingual transfer through both templates and demonstration examples. Finally, we evaluate our models in social value tasks such as hate speech detection in five languages and find it has limitations similar to comparable sized GPT-3 models. ",
      "Organization categorization": "Industry,Industry",
      "Country (of organization)": "United States of America,United States of America,France",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Our largest model (XGLM7.5B) sets a new state of the art performance for few-shot learning in more than 20 representative languages (including medium- and low-resource languages) for the tasks of commonsense reasoning, natural language inference and machine translation.\"",
      "Epochs": "1.0",
      "Training time (hours)": "504.0",
      "Training time notes": "appendix A : \"The XGLM 7.5B model was trained on 256 A100 GPUs for about 3 weeks, at a speed of 311.6k words per second\"",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "256.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "104152.22590136188",
      "Compute cost notes": "",
      "Training power draw (W)": "206287.25531193183",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "4000000.0",
      "Batch size notes": "\"All models are trained with data parallel and an effective batch size of 4M tokens\"",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "MIT license\nhttps://github.com/facebookresearch/fairseq/tree/main/examples/xglm\n\nhttps://github.com/facebookresearch/fairseq/blob/main/examples/xglm/model_card.md#primary-intended-use",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "LDM-1.45B",
      "Organization": "Heidelberg University,Runway",
      "Publication date": "2021-12-20",
      "Domain": "Image generation",
      "Task": "Image generation,Text-to-image",
      "Parameters": "1450000000.0",
      "Parameters notes": "1.45B",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "LAION-400M",
      "Training dataset size (gradients)": "291985200000",
      "Dataset size notes": "400M image-text pairs",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2112.10752",
      "Reference": "High-Resolution Image Synthesis with Latent Diffusion Models",
      "Citations": "20685.0",
      "Authors": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer",
      "Abstract": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at this https URL.",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "Germany,United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "0.66",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT: https://github.com/CompVis/latent-diffusion/blob/main/LICENSE",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Contriever",
      "Organization": "Meta AI,University College London (UCL),PSL University,Universit\u00e9 Grenoble Alpes",
      "Publication date": "2021-12-16",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "110000000.0",
      "Parameters notes": "Based on BERT base, which had 110m params.\n\n\"We initialize the network with the publicly available BERT base uncased model.\"",
      "Training compute (FLOP)": "1.57e+20",
      "Training compute notes": "Pre-training:\n\"We use the random cropping data augmentation, with documents of 256 tokens... batch size of 2,048 and 500,000 steps\"\n256 * 2048 * 500k * 100M * 6 = 1.57e20\n\nFine-tuning looks unlikely to move final sum much beyond this.",
      "Training dataset": "Wikipedia,CCNet",
      "Training dataset size (gradients)": "262144000000",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2112.09118",
      "Reference": "Unsupervised Dense Information Retrieval with Contrastive Learning",
      "Citations": "1212.0",
      "Authors": "Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, Edouard Grave",
      "Abstract": "Recently, information retrieval has seen the emergence of dense retrievers, using neural networks, as an alternative to classical sparse methods based on term-frequency. These models have obtained state-of-the-art results on datasets and tasks where large training sets are available. However, they do not transfer well to new applications with no training data, and are outperformed by unsupervised term-frequency methods such as BM25. In this work, we explore the limits of contrastive learning as a way to train unsupervised dense retrievers and show that it leads to strong performance in various retrieval settings. On the BEIR benchmark our unsupervised model outperforms BM25 on 11 out of 15 datasets for the Recall@100. When used as pre-training before fine-tuning, either on a few thousands in-domain examples or on the large MS~MARCO dataset, our contrastive model leads to improvements on the BEIR benchmark. Finally, we evaluate our approach for multi-lingual retrieval, where training data is even scarcer than for English, and show that our approach leads to strong unsupervised performance. Our model also exhibits strong cross-lingual transfer when fine-tuned on supervised English data only and evaluated on low resources language such as Swahili. We show that our unsupervised models can perform cross-lingual retrieval between different scripts, such as retrieving English documents from Arabic queries, which would not be possible with term matching methods.",
      "Organization categorization": "Industry,Academia,Academia,Academia",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland,France,France",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We observe that when used as pre-training, contrastive learning leads to strong performance: contriever obtains the best results among dense bi-encoder methods for the nDCG@10, and is state-of-the-art for the recall@100 (improving the average recall@100 from 65.0 to 67.1). This strong recall@100 performance can be further exploited by using a cross-encoder2 to re-rank the retrieved documents: this leads to the state-of-the-art on 8 datasets of the\nBEIR benchmark for the nDCG@10, as well as on average\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "BERT-Large",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "actually BERT base",
      "Batch size": "262144.0",
      "Batch size notes": "\"500k steps with a batch size of 2048 with on average 128 tokens per example\" according to https://openreview.net/forum?id=jKN1pXi7b0",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "Open access (non-commercial)",
      "Accessibility notes": "non-commercial for weights/code: https://github.com/facebookresearch/contriever/blob/main/LICENSE",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "LongT5",
      "Organization": "Google Research",
      "Publication date": "2021-12-15",
      "Domain": "Language",
      "Task": "Text summarization",
      "Parameters": "3000000000.0",
      "Parameters notes": "3B from section 4.1",
      "Training compute (FLOP)": "",
      "Training compute notes": "architecture is sparse so we cannot use 6ND method,\nfrom 3.1.1 \"we simply replace the encoder\nself-attention operation in T5 with a sparse sliding-\nwindow local attention operation following the im-\nplementation in ETC \"\nat the end of section 3.1.2 there is information about \ncomplexity O(l(r + l/k)) of local attention\nfrom 4.1.1 \"We pre-train LongT5 models for 1M steps on\n4096 input sequence length and 910 output se-\nquence length.\nbatch size is 128 (from 4.1 configurations section)\nso with l = 4096, k = 16, r = 127, \nso l(r+l/k) = 1568768, but we are not sure about constant.\n\nif normal attention have complexity O(l^2), and l^2 = 16777216\n16777216/1568768 = 10.7\nWe can try to estimate that LongT5 would have 10 times less compute that normal architecture.",
      "Training dataset": "C4",
      "Training dataset size (gradients)": "524288000000",
      "Dataset size notes": "size of C4, from https://huggingface.co/datasets/c4 , C4 dataset is a collection of about 750GB of English-language text\n200M word/GB * 4/3 token/word * 750GB = 200000000000 tokens\n\nActual tokens seen:\n1M steps * (4096 input len + 910 output len) * 128 batch size = 641B tokens, so around 3.2 epochs.",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2112.07916",
      "Reference": "LongT5: Efficient Text-To-Text Transformer for Long Sequences",
      "Citations": "362.0",
      "Authors": "Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang",
      "Abstract": "Recent work has shown that either (1) increasing the input length or (2) increasing model size can improve the performance of Transformer-based neural models. In this paper, we present a new model, called LongT5, with which we explore the effects of scaling both the input length and model size at the same time. Specifically, we integrated attention ideas from long-input transformers (ETC), and adopted pre-training strategies from summarization pre-training (PEGASUS) into the scalable T5 architecture. The result is a new attention mechanism we call {\\em Transient Global} (TGlobal), which mimics ETC's local/global attention mechanism, but without requiring additional side-inputs. We are able to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on question answering tasks.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "from abstract: \"We are able to achieve state-of-the-art results on several summarization tasks and outperform the original T5 models on question answering tasks.\"\n\n\"State-of-the-art results on the arXiv, PubMed, BigPatent, MediaSum, and TriviaQA datasets.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "128.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "116049.50214083787",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "524288.0",
      "Batch size notes": "4096 * 128",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache 2.0: https://github.com/google-research/longt5\ntrain code: https://github.com/google-research/longt5/blob/master/longt5/tasks.py ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GLaM",
      "Organization": "Google",
      "Publication date": "2021-12-13",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering",
      "Parameters": "1200000000000.0",
      "Parameters notes": "1.2 trillion parameters",
      "Training compute (FLOP)": "3.6363112434e+23",
      "Training compute notes": "The network activates 96.6 billion parameters per token and trained for 600B tokens.\n\n6 * 600B * 96.6B = 3.478e23\n\nDigitizing figure 4 (d) indicates 139.67 TPU-years of training. \n2.75e14 * 139.67 * 365.25 * 24 * 3600 * 0.3 = 3.636e23\n\nSince these are close, we will use the 6NC estimate and derive hardware utilization from the training time information.\n\nLater they say they measured 326W power usage per chip, which could maybe be used to estimate utilization.",
      "Training dataset": "Wikipedia,GLaM dataset",
      "Training dataset size (gradients)": "600000000000",
      "Dataset size notes": "The dataset is made of 1.6 trillion tokens, but later in the paper they say they only train the largest model for 600b tokens. 600b / 0.75 words/token = 800b words.\n\n\"The complete GLaM training using 600B tokens consumes only 456 MWh and emits 40.2 net tCO2e.\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2112.06905",
      "Reference": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts",
      "Citations": "1034.0",
      "Authors": "Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma, Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V Le, Yonghui Wu, Zhifeng Chen, Claire Cui",
      "Abstract": "Scaling language models with more data, compute and parameters has driven significant progress in natural language processing. For example, thanks to scaling, GPT-3 was able to achieve strong results on in-context learning tasks. However, training these large dense models requires significant amounts of computing resources. In this paper, we propose and develop a family of language models named GLaM (Generalist Language Model), which uses a sparsely activated mixture-of-experts architecture to scale the model capacity while also incurring substantially less training cost compared to dense variants. The largest GLaM has 1.2 trillion parameters, which is approximately 7x larger than GPT-3. It consumes only 1/3 of the energy used to train GPT-3 and requires half of the computation flops for inference, while still achieving better overall zero-shot and one-shot performance across 29 NLP tasks.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"As shown in Table 5, GLaM (64B/64E) is better than the dense model and outperforms the previous finetuned state-of-the-art (SOTA) on this dataset in the open-domain setting\"",
      "Epochs": "",
      "Training time (hours)": "1366.0",
      "Training time notes": "Note that they give several energy estimates. Use the complete training figures for 600B tokens, not the GPT-3 comparison values with 280B tokens.\n\n\"326W measured system power per TPU-v4 chip\"\n\"The complete GLaM training using 600B tokens consumes only\n456 MWh\"\n1024 TPU v4 chips\n(456 MWh) / (326W/chip * 1024 chips) = 1366 hours",
      "Training hardware": "Google TPU v4",
      "Hardware quantity": "1024.0",
      "Hardware utilization (MFU)": "0.2869636964",
      "Training compute cost (2023 USD)": "541437.4162400038",
      "Compute cost notes": "",
      "Training power draw (W)": "701486.0111048243",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "1000000.0",
      "Batch size notes": "\"We use a maximum sequence\nlength of 1024 tokens, and pack each input example to have\nup to 1 million tokens per batch.\"",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Gopher (280B)",
      "Organization": "DeepMind",
      "Publication date": "2021-12-08",
      "Domain": "Language",
      "Task": "Language modeling,Question answering",
      "Parameters": "280000000000.0",
      "Parameters notes": "Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identification of toxic language, but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and model's behaviour, covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms.",
      "Training compute (FLOP)": "6.31e+23",
      "Training compute notes": "Table A26\n6.31E+08 Train PFLOPs",
      "Training dataset": "MassiveTex",
      "Training dataset size (gradients)": "300000000000",
      "Dataset size notes": "\"We train all models for 300 billion tokens with a 2048 token context window, using the Adam (Kingma and Ba, 2014) optimiser.\"\n\n1 token ~ 0.75 words",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2112.11446",
      "Reference": "\"Scaling Language Models: Methods, Analysis & Insights from Training Gopher\"",
      "Citations": "1501.0",
      "Authors": "Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, Geoffrey Irving",
      "Abstract": "We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a 2 trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25\u00d7 fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority\"",
      "Epochs": "1.0",
      "Training time (hours)": "920.0",
      "Training time notes": "\"We trained Gopher for 920 hours in November and December 2020 in Google\u2019s Georgia datacentre. The PUE of the datacenter at this time was 1.08; the net tCO2e per MWh in October 2020 was 0.33. Using an estimate of 283W drawn per chip, this leads to a total of 380 net tCO2e\"",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "4096.0",
      "Hardware utilization (MFU)": "0.378",
      "Training compute cost (2023 USD)": "640616.8522346151",
      "Compute cost notes": "",
      "Training power draw (W)": "3714163.0079064844",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "6000000.0",
      "Batch size notes": "Table 1. \"Furthermore, we increase Gopher\u2019s batch size from three to six million tokens per batch during training\"",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "BF16",
      "Frontier model": "True",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "3636030.5562741663",
      "Training compute cost (upfront)": "46909499.95890752"
    },
    {
      "Model": "Student of Games",
      "Organization": "DeepMind",
      "Publication date": "2021-12-06",
      "Domain": "Games",
      "Task": "Chess,Go,Poker",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "3.6679273004682866e+22",
      "Training compute notes": "\"We trained a version of AlphaZero using its original settings in chess and Go, e.g. , using 800 MCTS simulations during training, with 3500 concurrent actors each on a single TPUv4, for a total of 800k training steps. SOG was trained using a similar amount of TPU resources.\"",
      "Training dataset": "",
      "Training dataset size (gradients)": "245760000000",
      "Dataset size notes": "",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/2112.03178",
      "Reference": "Player of Games",
      "Citations": "28.0",
      "Authors": "Martin Schmid, Matej Moravcik, Neil Burch, Rudolf Kadlec, Josh Davidson, Kevin Waugh, Nolan Bard, Finbarr Timbers, Marc Lanctot, Zach Holland, Elnaz Davoodi, Alden Christianson, Michael Bowling",
      "Abstract": "Games have a long history as benchmarks for progress in artificial intelligence. Approaches using search and learning produced strong performance across many perfect information games, and approaches using game-theoretic reasoning and learning demonstrated strong performance for specific imperfect information poker variants. We introduce Student of Games, a general-purpose algorithm that unifies previous approaches, combining guided search, self-play learning, and game-theoretic reasoning. Student of Games achieves strong empirical performance in large perfect and imperfect information games -- an important step towards truly general algorithms for arbitrary environments. We prove that Student of Games is sound, converging to perfect play as available computation and approximation capacity increases. Student of Games reaches strong performance in chess and Go, beats the strongest openly available agent in heads-up no-limit Texas hold'em poker, and defeats the state-of-the-art agent in Scotland Yard, an imperfect information game that illustrates the value of guided search, learning, and game-theoretic reasoning.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Player of Games reaches strong performance in chess and Go, beats the strongest openly available agent in heads-up no-limit Texas hold'em poker (Slumbot), and defeats the state-of-the-art agent in Scotland Yard\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "T-NLRv5 XXL",
      "Organization": "Microsoft",
      "Publication date": "2021-12-03",
      "Domain": "Language",
      "Task": "",
      "Parameters": "5400000000.0",
      "Parameters notes": "Table 1 of the blogpost",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://www.marketscreener.com/quote/stock/MICROSOFT-CORPORATION-4835/news/Microsoft-Turing-NLRv5-achieves-new-performance-milestones-37207301/\nhttps://www.microsoft.com/en-us/research/blog/efficiently-and-effectively-scaling-up-language-model-pretraining-for-best-language-representation-model-on-glue-and-superglue/",
      "Reference": "Microsoft : Turing-NLRv5 achieves new performance milestones",
      "Citations": "",
      "Authors": "",
      "Abstract": "As part of Microsoft AI at Scale(opens in new tab), the Turing family of NLP models are being used at scale across Microsoft to enable the next generation of AI experiences. Today, we are happy to announce that the latest Microsoft Turing model (T-NLRv5) is the state of the art at the top of SuperGLUE and GLUE leaderboards, further surpassing human performance and other models. Notably, T-NLRv5 first achieved human parity on MNLI and RTE on the GLUE benchmark, the last two GLUE tasks which human parity had not yet met. In addition, T-NLRv5 is more efficient than recent pretraining models, achieving comparable effectiveness with 50% fewer parameters and pretraining computing costs.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Highest score at SuperGLUE leaderboard version 2.0 in terms of CB (CommitmentBank-Av. F1/Accuracy; together with Ernie 3.0) and ReCoRD (Reading Comprehsention with Commonsense Reasoning-F1/Accuracy)\nhttps://super.gluebenchmark.com/leaderboard/ ",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "I cannot find repo with model weights or training code though they said:\n\"We will make T-NLRv5 and its capabilities available in the same way as with other Microsoft Turing models.\nWe will leverage its increased capabilities to further improve the execution of popular language tasks in A(opens in new tab)zure Cognitive Services(opens in new tab). Customers will automatically benefit from these.\n\nCustomers interested in using Turing models for their own specific task can submit a request to join the Turing Private Preview. Finally, we will make T-NLRv5 available to researchers for collaborative projects via the Microsoft Turing Academic Program.\"",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "N\u00dcWA",
      "Organization": "Microsoft Research,Peking University",
      "Publication date": "2021-11-24",
      "Domain": "Multimodal,Vision,Image generation,Video,Language",
      "Task": "Image generation,Video generation,Text-to-image,Text-to-video",
      "Parameters": "870000000.0",
      "Parameters notes": "Section 4.1",
      "Training compute (FLOP)": "7.24598784e+21",
      "Training compute notes": "From AI Tracker:\n\"Compute cost: End of Sec 4.1: \"We pre-train on 64 A100 GPUs for two weeks\". \n\nHalf precision FLOPs of A100: 312000000000000\n\n64 gpus *312000000000000 FLOPs *0.3 utilization * 14 day* (24*60*60) seconds / day=7.245988e+21\n\n",
      "Training dataset": "Conceptual Captions (CC3M),Moments in Time,VATEX",
      "Training dataset size (gradients)": "5547780000",
      "Dataset size notes": "we first pre-train N  \u0308UWA on three\ndatasets: Conceptual Captions [22] for text-to-image (T2I)\ngeneration, which includes 2.9M text-image pairs, Mo-\nments in Time [26] for video prediction (V2V), which in-\ncludes 727K videos, and VATEX dataset [43] for text-to-\nvideo (T2V) generation, which includes 241K text-video\npairs.",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2111.12417",
      "Reference": "N\u00dcWA: Visual Synthesis Pre-training for Neural visUal World creAtion",
      "Citations": "340.0",
      "Authors": "Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, Nan Duan",
      "Abstract": "This paper presents a unified multimodal pre-trained model called N\u00dcWA that can generate new or manipulate existing visual data (i.e., images and videos) for various visual synthesis tasks. To cover language, image, and video at the same time for different scenarios, a 3D transformer encoder-decoder framework is designed, which can not only deal with videos as 3D data but also adapt to texts and images as 1D and 2D data, respectively. A 3D Nearby Attention (3DNA) mechanism is also proposed to consider the nature of the visual data and reduce the computational complexity. We evaluate N\u00dcWA on 8 downstream tasks. Compared to several strong baselines, N\u00dcWA achieves state-of-the-art results on text-to-image generation, text-to-video generation, video prediction, etc. Furthermore, it also shows surprisingly good zero-shot capabilities on text-guided image and video manipulation tasks. Project repo is this https URL.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"N\u00dcWA achieves state-of-the-art results on text-to-image generation, text-to-video generation, video prediction, etc\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "https://github.com/microsoft/NUWA",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Florence",
      "Organization": "Microsoft",
      "Publication date": "2021-11-22",
      "Domain": "Vision",
      "Task": "Image captioning,Visual question answering,Image classification,Object detection",
      "Parameters": "893000000.0",
      "Parameters notes": "\"Our Florence pretrained model has in total 893M parameters, including the language transformer with 256M parameters and the CoSwin-H transformer with 637M parameters.\"",
      "Training compute (FLOP)": "4.831e+22",
      "Training compute notes": "\"The model takes 10 days to train on 512 NVIDIA A100 GPUs with 40GB memory per GPU.\"\n512 * 312 teraFLOPS * 10 days * 35% utilization = 4.831e22 FLOP",
      "Training dataset": "FLD-900M",
      "Training dataset size (gradients)": "7500000000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2111.11432v1",
      "Reference": "Florence: A New Foundation Model for Computer Vision",
      "Citations": "1041.0",
      "Authors": "Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng Liu, Yumao Lu, Yu Shi, Lijuan Wang, JianFeng Wang, Bin Xiao, Zhen Xiao, Jianwei Yang, Michael Zeng, Luowei Zhou, Pengchuan Zhang",
      "Abstract": "Automated visual understanding of our diverse and open world demands computer vision models to generalize well with minimal customization for specific tasks, similar to human vision. Computer vision foundation models, which are trained on diverse, large-scale dataset and can be adapted to a wide range of downstream tasks, are critical for this mission to solve real-world computer vision applications. While existing vision foundation models such as CLIP, ALIGN, and Wu Dao 2.0 focus mainly on mapping images and textual representations to a cross-modal shared representation, we introduce a new computer vision foundation model, Florence, to expand the representations from coarse (scene) to fine (object), from static (images) to dynamic (videos), and from RGB to multiple modalities (caption, depth). By incorporating universal visual-language representations from Web-scale image-text data, our Florence model can be easily adapted for various computer vision tasks, such as classification, retrieval, object detection, VQA, image caption, video retrieval and action recognition. Moreover, Florence demonstrates outstanding performance in many types of transfer learning: fully sampled fine-tuning, linear probing, few-shot transfer and zero-shot transfer for novel images and objects. All of these properties are critical for our vision foundation model to serve general purpose vision tasks. Florence achieves new state-of-the-art results in majority of 44 representative benchmarks, e.g., ImageNet-1K zero-shot classification with top-1 accuracy of 83.74 and the top-5 accuracy of 97.18, 62.4 mAP on COCO fine tuning, 80.36 on VQA, and 87.8 on Kinetics-600.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance,SOTA improvement",
      "Notability criteria notes": "\"Florence achieves new state-of-the-art results in majority of 44 representative benchmarks, e.g., ImageNet-1K zero-shot classification with top-1 accuracy of 83.74 and the top-5 accuracy of 97.18, 62.4 mAP on COCO fine tuning, 80.36 on VQA, and 87.8 on Kinetics-600.\"\n\n\"image retrieval zero-shot (90.9/76.7 R@1 on Flickr30K image-to-text / text-to-image, 64.7/47.2 R@1 on MSCOCO image-to-text / text-to-image) and fine-tuning (97.2/87.9 R@1 on Flickr30K image-to-text / text-to-image, 81.8/63.2 R@1 on MSCOCO image-to-text/ text-to-image), object detection (62.4 mAP on COCO, 39.3 mAP on Object365, 16.2 AP50 on Visual Genome), VQA (80.36), text-to-video retrieval zero-shot (37.6 R@1 on MSR-VTT), and video action recognition (top-1 accuracy 86.5/87.8 on Kinetics-400 / Kinetics-600).\"",
      "Epochs": "",
      "Training time (hours)": "240.0",
      "Training time notes": "10 days on 512 A100 40GB",
      "Training hardware": "NVIDIA A100 SXM4 40 GB",
      "Hardware quantity": "512.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "106950.61569328007",
      "Compute cost notes": "",
      "Training power draw (W)": "412831.84854484117",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "8823653.146774352",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "BASIC-L",
      "Organization": "Google",
      "Publication date": "2021-11-19",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "3070000000.0",
      "Parameters notes": "2.4B image model + 670M text model",
      "Training compute (FLOP)": "4.12e+22",
      "Training compute notes": "6.9k + 1k + 0.8k = 8.7k TPUv4 core-days for BASIC-L, per Table 8\n\nTwo cores per chip, and 275 teraflop/s per chip \n(https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu_v4)\n\n275 teraflops * 8700/2 * 24 * 3600 * 0.4 (assumed utilization) = 8.3e22",
      "Training dataset": "JFT,ALIGN",
      "Training dataset size (gradients)": "8905031712000",
      "Dataset size notes": "6.7B image-text pairs",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2111.10050",
      "Reference": "Combined Scaling for Zero-shot Transfer Learning",
      "Citations": "227.0",
      "Authors": "Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams Wei Yu, Jiahui Yu, Yi-Ting Chen, Minh-Thang Luong, Yonghui Wu, Mingxing Tan, Quoc V. Le",
      "Abstract": "We present a combined scaling method - named BASIC - that achieves 85.7% top-1 accuracy on the ImageNet ILSVRC-2012 validation set without learning from any labeled ImageNet example. This accuracy surpasses best published similar models - CLIP and ALIGN - by 9.3%. Our BASIC model also shows significant improvements in robustness benchmarks. For instance, on 5 test sets with natural distribution shifts such as ImageNet-{A,R,V2,Sketch} and ObjectNet, our model achieves 84.3% top-1 average accuracy, only a small drop from its original ImageNet accuracy. To achieve these results, we scale up the contrastive learning framework of CLIP and ALIGN in three dimensions: data size, model size, and batch size. Our dataset has 6.6B noisy image-text pairs, which is 4x larger than ALIGN, and 16x larger than CLIP. Our largest model has 3B weights, which is 3.75x larger in parameters and 8x larger in FLOPs than ALIGN and CLIP. Finally, our batch size is 65536 which is 2x more than CLIP and 4x more than ALIGN. We encountered two main challenges with the scaling rules of BASIC. First, the main challenge with implementing the combined scaling rules of BASIC is the limited memory of accelerators, such as GPUs and TPUs. To overcome the memory limit, we propose two simple methods which make use of gradient checkpointing and model parallelism. Second, while increasing the dataset size and the model size has been the defacto method to improve the performance of deep learning models like BASIC, the effect of a large contrastive batch size on such contrastive-trained image-text models is not well-understood. To shed light on the benefits of large contrastive batch sizes, we develop a theoretical framework which shows that larger contrastive batch sizes lead to smaller generalization gaps for image-text models such as BASIC.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "SOTA on ImageNet for a model that was not trained on ImageNet images:\n\"We present a combined scaling method \u2013 named BASIC \u2013 that achieves 85.7% top-1 accuracy on the ImageNet ILSVRC-2012 validation set without learning from any labeled ImageNet example. This accuracy surpasses best-published similar models \u2013 CLIP and ALIGN \u2013 by 9.3%\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "Google TPU v4",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "1684.770712126102",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "65536, but these are image-text pairs not tokens\n\"For the batch size, we use 65536 contrastive\nlearning examples per minibatch\"",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Swin Transformer V2 (SwinV2-G)",
      "Organization": "Microsoft Research Asia",
      "Publication date": "2021-11-18",
      "Domain": "Vision,Video",
      "Task": "Action recognition,Image classification",
      "Parameters": "3000000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "1.1e+21",
      "Training compute notes": "trained on \"<0.5k\" TPUv3 core-days per Table 2 (not trained on TPUs, this is a comparison with other papers)\n\nA core is 123/2 teraflops\n\n500 core-days\n= 500 * 123/2 trillion * 24 * 3600 * 0.4 utilization\n~= 1.1e21",
      "Training dataset": "ImageNet21k",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2111.09883v2",
      "Reference": "Swin Transformer V2: Scaling Up Capacity and Resolution",
      "Citations": "2358.0",
      "Authors": "Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo",
      "Abstract": "Large-scale NLP models have been shown to significantly improve the performance on language tasks with no signs of saturation. They also demonstrate amazing few-shot capabilities like that of human beings. This paper aims to explore large-scale models in computer vision. We tackle three major issues in training and application of large vision models, including training instability, resolution gaps between pre-training and fine-tuning, and hunger on labelled data. Three main techniques are proposed: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) A log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) A self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images. Through these techniques, this paper successfully trained a 3 billion-parameter Swin Transformer V2 model, which is the largest dense vision model to date, and makes it capable of training with images of up to 1,536\u00d71,536 resolution. It set new performance records on 4 representative vision tasks, including ImageNet-V2 image classification, COCO object detection, ADE20K semantic segmentation, and Kinetics-400 video action classification. Also note our training is much more efficient than that in Google's billion-level visual models, which consumes 40 times less labelled data and 40 times less training time. Code is available at \\url{this https URL}.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement,Highly cited",
      "Notability criteria notes": "\"It set new performance records on 4 representative vision tasks, including ImageNet-V2 image classification, COCO object detection, ADE20K semantic segmentation, and Kinetics-400 video action classification.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA A100 SXM4 40 GB",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "2326.6636503781665",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT license\n\nhttps://github.com/microsoft/Swin-Transformer\n\ntraining code: https://github.com/microsoft/Swin-Transformer/blob/main/get_started.md ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ViT-G/14 (LiT)",
      "Organization": "Google Research",
      "Publication date": "2021-11-15",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "3005000000.0",
      "Parameters notes": "Table 7",
      "Training compute (FLOP)": "",
      "Training compute notes": "They start with the ViT-G/14 image model and train their own text model. ViT-G/14 is 3.4e21. \n\nThey also say \"We use 128 TPU cores by default for the above experiments, and 256 TPU cores for our best run with 18 billion seen image-text pairs\" which may be relevant.",
      "Training dataset": "Conceptual Captions 12M (CC12M),YFCC-100M,Unspecified unreleased",
      "Training dataset size (gradients)": "1040000000000",
      "Dataset size notes": "Largest dataset is \"4 billion image and alt-text pairs\". This is rounded down slightly; the other datasets are much smaller.",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2111.07991v3",
      "Reference": "Zero-Shot Transfer with Locked-image Text Tuning",
      "Citations": "659.0",
      "Authors": "Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, Lucas Beyer",
      "Abstract": "This paper presents contrastive-tuning, a simple method employing contrastive training to align image and text models while still taking advantage of their pre-training. In our empirical study we find that locked pre-trained image models with unlocked text models work best. We call this instance of contrastive-tuning \"Locked-image Tuning\" (LiT), which just teaches a text model to read out good representations from a pre-trained image model for new tasks. A LiT model gains the capability of zero-shot transfer to new vision tasks, such as image classification or retrieval. The proposed LiT is widely applicable; it works reliably with multiple pre-training methods (supervised and unsupervised) and across diverse architectures (ResNet, Vision Transformers and MLP-Mixer) using three different image-text datasets. With the transformer-based pre-trained ViT-g/14 model, the LiT model achieves 85.2% zero-shot transfer accuracy on the ImageNet test set, and 82.5% on the challenging out-of-distribution ObjectNet test set.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"For example, it achieves 82.5% accuracy on the challenging ObjectNet test set [1], outperforming the previous state-of-the-art method [46] by 10.2%.\"",
      "Epochs": "4.5",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "ViT-G/14",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "",
      "Accessibility notes": "Apache 2.0 license\n\nhttps://colab.research.google.com/github/google-research/big_vision/blob/main/big_vision/configs/proj/image_text/lit.ipynb\n\nhttps://github.com/google-research/big_vision",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Masked Autoencoders ViT-H",
      "Organization": "Facebook AI Research",
      "Publication date": "2021-11-11",
      "Domain": "Vision",
      "Task": "Semantic segmentation,Image classification,Image generation",
      "Parameters": "632000000.0",
      "Parameters notes": "Three models:\nViT-B (86M), ViT-L (304M), ViT-H (632M)",
      "Training compute (FLOP)": "4.6e+20",
      "Training compute notes": "128 TPU-v3 cores trained for 1600 epochs. Times are given for 800 epochs in Table 2; largest model (ViT-H) took 34.5 hrs for 800.\n128 TPU-v3 cores * 0.5 chips/core * 34.5 hours * 2 * 1.23E+14 FLOP/sec / chip * 3600 sec/hour  * 40% utilization = 7.84e20 FLOP\n\nNote that the operations counting method disagrees:\n2 \u00d7 632000000 connections \u00d7 3 \u00d7 1281167 training examples \u00d7 1600 of epochs  = 7.8e18 FLOP\n\nManual calculation with `calflops` package roughly agrees with hardware-time calculation: \n286.21 GFLOPS/observation * 1281167 observations * 1600 epochs = 5.86e20 FLOP\n\nSee reproduction here: https://colab.research.google.com/drive/1KCsmrfPzT9BgGO_YQthnz4oP3QRqbw5o?usp=sharing\n\nWeighting three estimates equally:\n(7.84e20 + 7.8e18 + 5.86e20)/3 = 4.6e20",
      "Training dataset": "ImageNet-1k",
      "Training dataset size (gradients)": "327978752,192851506176",
      "Dataset size notes": "",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/2111.06377",
      "Reference": "Masked Autoencoders Are Scalable Vision Learners",
      "Citations": "9910.0",
      "Authors": "Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, Ross Girshick",
      "Abstract": "This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,France",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\"By fine-tuning with a 448 size, we achieve 87.8% accuracy, using only IN1K data. The previous best accuracy, among all methods using only IN1K data, is 87.1% (512 size)... We improve over the state-of-the-art by a nontrivial margin in the highly competitive benchmark of IN1K (no external data). Our result is based on vanilla ViT, and we expect advanced networks will perform better.\"\n\nSee Table 3",
      "Epochs": "1600.0",
      "Training time (hours)": "69.0",
      "Training time notes": "Table 2 gives wall times for training ViT-L and ViT-H to 800 epochs; later it is stated that the systems are each trained for 1600 epochs.\n(34.5 hours / 800 epochs) * 1600 epochs = 69 hours",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "$0.62 / 32 cores of TPU-v3 * (128 cores / 32 cores) = $2.65/hour\nCPI conversion to 2020: $2.25\n$2.25/hour * 69 hours = $155.25",
      "Training power draw (W)": "",
      "Base model": "ViT-Huge/14",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "UNCERTAIN\n128 TPU-v3 cores trained for 1600 epochs. Times are given for 800 epochs in Table 2; largest model (ViT-H) took 34.5 hrs for 800.\n128 TPU-v3 cores * 0.5 chips/core * 34.5 hours * 2 * 1.23E+14 FLOP/sec / chip * 3600 sec/hour  * 40% utilization = 7.84e20 FLOP\n\nNote that the operations counting method disagrees:\n2 \u00d7 632000000 connections \u00d7 3 \u00d7 1281167 training examples \u00d7 1600 of epochs  = 7.8e18 FLOP\n",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "Open access (non-commercial)",
      "Accessibility notes": "Code at https://github.com/facebookresearch/mae\n\nThis project is under the CC-BY-NC 4.0 license\n\ntraining code: https://github.com/facebookresearch/mae/blob/main/PRETRAIN.md ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Projected GAN",
      "Organization": "Heidelberg University",
      "Publication date": "2021-11-01",
      "Domain": "Image generation",
      "Task": "Image generation",
      "Parameters": "",
      "Parameters notes": "Possibly calculable from Appendix Table 8",
      "Training compute (FLOP)": "1.05e+19",
      "Training compute notes": "\"With this setting, each experiment takes roughly 100-200 GPU hours on a NVIDIA V100,\nfor more details we refer to the appendix.\"\n\n\"We conduct our experiments on an internal cluster with several nodes, each with up to 8 Quadro RTX\n6000 or NVIDIA V100 using PyTorch 1.7.1 and CUDA 11.0.\"\n\nIn appendix table 7, takes 10.1 seconds per 1k images on 8 Quadro RTX 6000s. Longest training run for Projected GAN appears to be in Figure 4 (left), at 14M images, though this is overtrained and the largest checkpoint used for evaluations was 10M.\n10M images * 10.1 s/1000 images * 8 * 3.26e13 FLOP/s * 0.4 = 1.05e19",
      "Training dataset": "",
      "Training dataset size (gradients)": "3000000",
      "Dataset size notes": "They experiment with 22 image datasets. Largest appears to be LSUN-Bedroom at 3M images.",
      "Confidence": "Confident",
      "Link": "https://proceedings.neurips.cc/paper/2021/hash/9219adc5c42107c4911e249155320648-Abstract.html",
      "Reference": "Projected GANs Converge Faster",
      "Citations": "280.0",
      "Authors": "Axel Sauer, Kashyap Chitta, Jens M\u00fcller, Andreas Geiger",
      "Abstract": "Generative Adversarial Networks (GANs) produce high-quality images but are challenging to train. They need careful regularization, vast amounts of compute, and expensive hyper-parameter sweeps. We make significant headway on these issues by projecting generated and real samples into a fixed, pretrained feature space. Motivated by the finding that the discriminator cannot fully exploit features from deeper layers of the pretrained model, we propose a more effective strategy that mixes features across channels and resolutions. Our Projected GAN improves image quality, sample efficiency, and convergence speed. It is further compatible with resolutions of up to one Megapixel and advances the state-of-the-art Fr\u00e9chet Inception Distance (FID) on twenty-two benchmark datasets. Importantly, Projected GANs match the previously lowest FIDs up to 40 times faster, cutting the wall-clock time from 5 days to less than 3 hours given the same computational resources.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Germany",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Table 3\n\"It is further compatible with resolutions of up to one Megapixel and advances the state-of-the-art Fr\u00e9chet Inception Distance (FID) on twenty-two benchmark datasets\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA V100,NVIDIA Quadro RTX 6000",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "",
      "Accessibility notes": "MIT license\n\nhttps://github.com/autonomousvision/projected-gan",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "CodeT5-base",
      "Organization": "Salesforce,Nanyang Technological University",
      "Publication date": "2021-11-01",
      "Domain": "Language",
      "Task": "Code generation",
      "Parameters": "220000000.0",
      "Parameters notes": "\"We build CodeT5 based on Huggingface\u2019s T5 (Raffel et al., 2020) PyTorch implementation and employ two sizes of CodeT5-small (60M) and CodeT5-base (220M)\"",
      "Training compute (FLOP)": "1.56e+21",
      "Training compute notes": "\"We pre-train the model with the denoising objective for 100 epochs and bimodal dual training for further 50 epochs on a cluster of 16 NVIDIA A100 GPUs with 40G memory. The total training time for CodeT5-small and CodeT5- base is 5 and 12 days, respectively\"\n\n16 * 312 teraFLOP/s * 12 * 24 * 3600 * 0.3 (utilization assumption) = 1.56e21",
      "Training dataset": "CodeSearchNet,BigQuery",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "\"In total, we employ around 8.35 million instances for pretraining\" \nInstances meaning code snippets/examples, not tokens.",
      "Confidence": "Likely",
      "Link": "https://aclanthology.org/2021.emnlp-main.685/",
      "Reference": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation",
      "Citations": "1964.0",
      "Authors": "Yue Wang, Weishi Wang, Shafiq Joty, Steven C.H. Hoi",
      "Abstract": "\"Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code. Our code and pre-trained models are released at https://github.com/salesforce/CodeT5.\"",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,Singapore",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Extensive experiments show that CodeT5 yields state-of-the-art results on the fourteen sub-tasks in CodeXGLUE.\"",
      "Epochs": "150.0",
      "Training time (hours)": "288.0",
      "Training time notes": "\"The total training time for CodeT5-small and CodeT5- base is 5 and 12 days, respectively\"",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "3114.8690946174474",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "BSD-3-Clause license\n\nhttps://github.com/salesforce/CodeT5",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "S4",
      "Organization": "Stanford University",
      "Publication date": "2021-10-31",
      "Domain": "Language",
      "Task": "Language modeling/generation",
      "Parameters": "249000000.00000003",
      "Parameters notes": "249M (Table 8)",
      "Training compute (FLOP)": "7.8328627e+19",
      "Training compute notes": "6 FLOP / token / parameter * 249000000 parameters * 8 GPUs * 8192 tokens/step/GPU * 800000 steps = 7.8328627e+19 FLOP\n \n\"our S4 model was trained with the simpler AdamW optimizer with a single cosine learning rate cycle with a maximum of 800000 steps.\nThe initial learning rate was set to 0.0005. We used 8 A100 GPUs with a batch size of 1 per gpu and context size 8192.\"\n\n",
      "Training dataset": "WikiText-103",
      "Training dataset size (gradients)": "103000000",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2111.00396",
      "Reference": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "Citations": "2778.0",
      "Authors": "Albert Gu, Karan Goel, Christopher R\u00e9",
      "Abstract": "A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of 10000 or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) \\( x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) \\), and showed that for appropriate choices of the state matrix \\( A \\), this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning \\( A \\) with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91\\% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation 60\u00d7 faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"S4 achieves strong empirical results across a diverse range of established benchmarks, including... SoTA on every task from the Long Range Arena benchmark\"\n\n\"We perform speech classification using the SC10 subset <...> S4 achieves 98.3% accuracy, higher than all baselines that use the 100\u00d7 shorter MFCC features\"",
      "Epochs": "509.02",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "8.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "6453.658675376135",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache 2 0\nrepo with training, inference, checkpoints:\nhttps://github.com/state-spaces/s4",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "EfficientZero",
      "Organization": "Tsinghua University,University of California (UC) Berkeley,Shanghai Qi Zhi institute",
      "Publication date": "2021-10-30",
      "Domain": "Games",
      "Task": "Atari",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "\"Our implementation is computationally friendly. To train an Atari agent for 100k steps, it only needs 4 GPUs to train 7 hours.\"",
      "Training dataset": "",
      "Training dataset size (gradients)": "100000",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/2111.00210",
      "Reference": "Mastering Atari Games with Limited Data",
      "Citations": "295.0",
      "Authors": "Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, Yang Gao",
      "Abstract": "Reinforcement learning has achieved great success in many applications. However, sample efficiency remains a key challenge, with prominent methods requiring millions (or even billions) of environment steps to train. Recently, there has been significant progress in sample efficient image-based RL algorithms; however, consistent human-level performance on the Atari game benchmark remains an elusive goal. We propose a sample efficient model-based visual RL algorithm built on MuZero, which we name EfficientZero. Our method achieves 194.3% mean human performance and 109.0% median performance on the Atari 100k benchmark with only two hours of real-time game experience and outperforms the state SAC in some tasks on the DMControl 100k benchmark. This is the first time an algorithm achieves super-human performance on Atari games with such little data. EfficientZero's performance is also close to DQN's performance at 200 million frames while we consume 500 times less data. EfficientZero's low sample complexity and high performance can bring RL closer to real-world applicability. We implement our algorithm in an easy-to-understand manner and it is available at this https URL. We hope it will accelerate the research of MCTS-based RL algorithms in the wider community.",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "China,United States of America,China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Our method is 176% and 163% better than the previous SoTA performance, in mean and median human normalized score respectively\"\n\nTable 1\n\"EfficientZero achieves superhuman performance with only 2 hours of real-time game play. Our method is 176% and 163% better\nthan the previous SoTA performance, in mean and median human normalized score respectively.\"\n\nTable 2: Scores achieved by EfficientZero (mean & standard deviation for 10 seeds) and some\nbaselines on some low-dimensional environments on the DMControl 100k benchmark. EfficientZero\nachieves state-of-art performance and comparable results to the state-based SAC.\n",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "GPL-3.0 (copyleft + prohibits incorporating the software into proprietary software)\nhttps://github.com/YeWR/EfficientZero?tab=readme-ov-file",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Eve",
      "Organization": "Harvard Medical School,University of Oxford",
      "Publication date": "2021-10-27",
      "Domain": "Biology",
      "Task": "Protein pathogenicity prediction,Proteins",
      "Parameters": "15010300.0",
      "Parameters notes": "\"The Bayesian VAE architecture in EVE is comprised of a symmetric 3-layer encoder & decoder architecture (with 2,000-1,000-300 and 300-1,000-2,000 units respectively) and a latent space of dimension 50 [...] We use a single set of parameters for the encoder (\u03d5p) and learn a fully-factorized gaussian distribution over the weights of the decoder (\u03b8p)\"\nThey train a new VAE for each protein, and it doesn't seem like they trim the input sequence length, so the largest model will be the one trained for the largest input protein. Supplementary materials 1 gives statistics for each protein; the longest is 5202, which would indicate a network of size 15,010,300",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "UniRef100",
      "Training dataset size (gradients)": "24167054120",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://www.nature.com/articles/s41586-021-04043-8#change-history",
      "Reference": "Disease variant prediction with deep generative models of evolutionary data",
      "Citations": "631.0",
      "Authors": "Jonathan Frazer, Pascal Notin, Mafalda Dias, Aidan Gomez, Joseph K. Min, Kelly Brock, Yarin Gal and Debora S. Marks",
      "Abstract": "Quantifying the pathogenicity of protein variants in human disease-related genes would have a marked effect on clinical decisions, yet the overwhelming majority (over 98%) of these variants still have unknown consequences1\u20133. In principle, computational methods could support the large-scale interpretation of genetic variants. However, state-of-the-art methods4\u201310 have relied on training machine learning models on known disease labels. As these labels are sparse, biased and of variable quality, the resulting models have been considered insufficiently reliable11. Here we propose an approach that leverages deep generative models to predict variant pathogenicity without relying on labels. By modelling the distribution of sequence variation across organisms, we implicitly capture constraints on the protein sequences that maintain fitness. Our model EVE (evolutionary model of variant effect) not only outperforms computational approaches that rely on labelled data but also performs on par with, if not better than, predictions from high-throughput experiments, which are increasingly used as evidence for variant classifcation12\u201316. We predict the pathogenicity of more than 36 million variants across 3,219 disease genes and provide evidence for the classification of more than 256,000 variants of unknown significance. Our work suggests that models of evolutionary information can provide valuable independent evidence for variant interpretation that will be widely useful in research and clinical settings.",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Our model EVE (evolutionary model of variant effect) not only outperforms computational approaches that rely on labelled data but also performs on par with, if not better than, predictions from high-throughput experiments, which are increasingly used as evidence for variant classification\" [Abstract] - SOTA improvement for very specific task",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "The model code is available at https://github.com/OATML-Markslab/EVE\nMIT license",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "base LM+GNN+kNN",
      "Organization": "Shannon.AI,Nanjing University,Nanyang Technological University,Zhejiang University (ZJU)",
      "Publication date": "2021-10-17",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "274000000.0",
      "Parameters notes": "274M (table 1)\n\n\"We use the base version of deep Transformer language model with adaptive embeddings (Baevski & Auli, 2018) as our base LM. This model has 16 decoder layers. The dimensionality of word representations is 1,024, the number of multi-attention heads is 16, and the inner dimensionality of feedforward layers is 4,096.\"",
      "Training compute (FLOP)": "5.2587456e+19",
      "Training compute notes": "base model compute: 4.47*10^19 FLOP \nfine-tune lower bound estimation: 1.69332e+17 FLOP -> 4.47*10^19 FLOP + 1.69332e+17 FLOP = 4.4869332e+19 FLOP total \nfine-tune upper bound estimation: 1.69332e+19 FLOP -> 4.47*10^19 FLOP + 1.69332e+19 FLOP = 6.16332e+19 FLOP total\n\ngeometric mean: sqrt(4.4869332e+19*6.16332e+19) = 5.2587456e+19 \n",
      "Training dataset": "WikiText-103",
      "Training dataset size (gradients)": "103000000",
      "Dataset size notes": "\"During training, data is partitioned into blocks of 3,072 contiguous\ntokens.\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2110.08743",
      "Reference": "GNN-LM: Language Modeling based on Global Contexts via GNN",
      "Citations": "45.0",
      "Authors": "Yuxian Meng, Shi Zong, Xiaoya Li, Xiaofei Sun, Tianwei Zhang, Fei Wu, Jiwei Li",
      "Abstract": "Inspired by the notion that ``{\\it to copy is easier than to memorize}``, in this work, we introduce GNN-LM, which extends the vanilla neural language model (LM) by allowing to reference similar contexts in the entire training corpus. We build a directed heterogeneous graph between an input context and its semantically related neighbors selected from the training corpus, where nodes are tokens in the input context and retrieved neighbor contexts, and edges represent connections between nodes. Graph neural networks (GNNs) are constructed upon the graph to aggregate information from similar contexts to decode the token. This learning paradigm provides direct access to the reference contexts and helps improve a model's generalization ability. We conduct comprehensive experiments to validate the effectiveness of the GNN-LM: GNN-LM achieves a new state-of-the-art perplexity of 14.8 on WikiText-103 (a 3.9 point improvement over its counterpart of the vanilla LM model), and shows substantial improvement on One Billion Word and Enwiki8 datasets against strong baselines. In-depth ablation studies are performed to understand the mechanics of GNN-LM. \\footnote{The code can be found at this https URL",
      "Organization categorization": "Industry,Academia,Academia,Academia",
      "Country (of organization)": "China,China,Singapore,China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"achieves a new state-of-the-art perplexity of 14.8 on WikiText-103 (a 3.9 point improvement over its counterpart of the vanilla LM model)\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "Transformer (Adaptive Input Embeddings) WT103",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "6 FLOP / token / parameter * 274000000 parameters * 103000000 tokens * 1 epoch [assumed for lower bound] = 1.69332e+17 FLOP\n\nupper bound [assuming 100 epochs]: 1.69332e+19 FLOP",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT License, training/eval code and weights\nhttps://github.com/ShannonAI/GNN-LM",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Yuan 1.0",
      "Organization": "Inspur",
      "Publication date": "2021-10-12",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "245730000000.0",
      "Parameters notes": "Table 2: Parameters of Yuan models.\n\"Parameters (billion)\"",
      "Training compute (FLOP)": "3.5380000000001e+23",
      "Training compute notes": "Table 9: 4095 petaFLOPS-days which equals 3.538*10^23 FLOP\n\nhttps://www.wolframalpha.com/input?i=4095+petaFLOPS+*+1+day\n",
      "Training dataset": "Common Crawl,Wikipedia,Sogue News",
      "Training dataset size (gradients)": "180000000000",
      "Dataset size notes": "\"Yuan 1.0 was trained on a new Chinese dataset of 5TB high-quality text that was built on 850TB raw data from Internet.\"\n\n1 GB ~ 167M words in English or 333M words in Chinese. For a mixed dataset of mostly Chinese, 5TB may be equivalent to around 1T words.\n\nTable 2: 180B training tokens",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2110.04725",
      "Reference": "Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning",
      "Citations": "67.0",
      "Authors": "Shaohua Wu, Xudong Zhao, Tong Yu, Rongguo Zhang, Chong Shen, Hongli Liu, Feng Li, Hong Zhu, Jiangang Luo, Liang Xu, Xuanwei Zhang, Jun Liu",
      "Abstract": "Recent work like GPT-3 has demonstrated excellent performance of Zero-Shot and Few-Shot learning on many natural language processing (NLP) tasks by scaling up model size, dataset size and the amount of computation. However, training a model like GPT-3 requires huge amount of computational resources which makes it challengeable to researchers. In this work, we propose a method that incorporates large-scale distributed training performance into model architecture design. With this method, Yuan 1.0, the current largest singleton language model with 245B parameters, achieves excellent performance on thousands GPUs during training, and the state-of-the-art results on NLP tasks. A data processing method is designed to efficiently filter massive amount of raw data. The current largest high-quality Chinese corpus with 5TB high quality texts is built based on this method. In addition, a calibration and label expansion method is proposed to improve the Zero-Shot and Few-Shot performance, and steady improvement is observed on the accuracy of various tasks. Yuan 1.0 presents strong capacity of natural language generation, and the generated articles are difficult to distinguish from the human-written ones.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"The zero-shot average scores of both LM and PLM are superior to the SOTA one. On Csldcp, Tnews and Iflytek tasks, we surpass the zero-shot SOTA by a large margin\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "2128.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "591664.5798708292",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "6881280.0",
      "Batch size notes": "Table 2. Batch size 3360, sequence length 2048. 3360*2048 = 6881280",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "https://github.com/Shawn-IEITSystems/Yuan-1.0",
      "Numerical format": "FP16",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "0.45",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": "48571462.85398424"
    },
    {
      "Model": "Megatron-Turing NLG 530B",
      "Organization": "Microsoft,NVIDIA",
      "Publication date": "2021-10-11",
      "Domain": "Language",
      "Task": "Language modeling,Language modeling/generation,Question answering",
      "Parameters": "530000000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "8.586e+23",
      "Training compute notes": "https://www.lesswrong.com/posts/bGuMrzhJdENCo8BxX/nvidia-and-microsoft-releases-530b-parameter-transformer?commentId=HSJSNspKp94tFcSCx\n\nsource: https://lair.lighton.ai/akronomicon/\n9938 PF-days * 3600 * 24 * 10^15  = 8.586432e+23\n\n6ND estimate: 6 * 530B * 270B = 8.586000e+23",
      "Training dataset": "Common Crawl,The Pile,CC-Stories,Realnews",
      "Training dataset size (gradients)": "270000000000",
      "Dataset size notes": "\"Our training dataset consists of 339 billion tokens and we\ntrained MT-NLG on 270 billions tokens by blending the 15 training datasets as described above. We also set aside 2% of our data for validation.\"\n\n1 token ~ 0.75 words",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2201.11990",
      "Reference": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",
      "Citations": "805.0",
      "Authors": "Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, Bryan Catanzaro",
      "Abstract": "Pretrained general-purpose language models can achieve state-of-the-art accuracies in various natural language processing domains by adapting to downstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of their success, the size of these models has increased rapidly, requiring high-performance hardware, software, and algorithmic techniques to enable training such large models. As the result of a joint effort between Microsoft and NVIDIA, we present details on the training of the largest monolithic transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530 billion parameters. In this paper, we first focus on the infrastructure as well as the 3D parallelism methodology used to train this model using DeepSpeed and Megatron. Next, we detail the training process, the design of our training corpus, and our data curation techniques, which we believe is a key ingredient to the success of the model. Finally, we discuss various evaluation results, as well as other interesting observations and new properties exhibited by MT-NLG. We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning accuracies on several NLP benchmarks and establishes new state-of-the-art results. We believe that our contributions will help further the development of large-scale training infrastructures, large-scale language models, and natural language generations.",
      "Organization categorization": "Industry,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement,Training cost",
      "Notability criteria notes": "The 105-layer, transformer-based MT-NLG improved upon the prior state-of-the-art models in zero-, one-, and few-shot settings",
      "Epochs": "",
      "Training time (hours)": "770.0",
      "Training time notes": "Total compute was 1.17*10^24 FLOP.\nThey don't directly report the utilization and training speed when using the full Selene supercomputer with 560 DGX * 8 A100/DGX = 4480 GPUs. See section 2.3 Hardware Setup.\n\nAt 280 DGX, the utilization is 126/312 = 40% and a batch takes 60 seconds; at 350, it is 39% for 50 seconds; at 420, it is 36% for 44 seconds.\n\nThe overall utilization was 30.2% and the full cluster has 560 DGX. Dividing the total compute by the total performance of 4480 A100 at 30.2% utilization gives 770 hours.",
      "Training hardware": "NVIDIA A100 SXM4 80 GB",
      "Hardware quantity": "4480.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "3848505.6256471733",
      "Compute cost notes": "",
      "Training power draw (W)": "3615658.868639838",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "3932160.0",
      "Batch size notes": "\"The sequence length is 2048 and the global batch size is 1920. We used 8-way tensor and 35-way pipeline parallelism. The learning rate is 5.0e \u22125 . We used one billion tokens for linear learning rate warmup. We used cosine decay for the learning rate targeting to reach 10% of its value over 340 billion tokens. Over the first 12 billion tokens, we started at a batch size of 32 and gradually increased the batch size in increments of 32, until we reach the final batch size of 1920\" \n\nFinal batch size is 1920 * 2048 = 3932160",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "BF16",
      "Frontier model": "True",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "0.302",
      "Training compute cost (cloud)": "6663667.97106352",
      "Training compute cost (upfront)": "147455699.58722904"
    },
    {
      "Model": "AlphaFold-Multimer",
      "Organization": "Google DeepMind,DeepMind",
      "Publication date": "2021-10-04",
      "Domain": "Biology",
      "Task": "Protein folding prediction,Proteins",
      "Parameters": "",
      "Parameters notes": "\"Multiple changes to the AlphaFold system were made to adapt it to training on protein complexes, which are detailed below. Summarizing briefly, we [...] make various small adjustments to the structure losses and the model architecture.\" [2. Methods]\n\nHence, this will have approximately the same amount of parameters as AlphaFold2",
      "Training compute (FLOP)": "4.35e+21",
      "Training compute notes": "Section: 2.5. Training Regimen\n\"We train the model to convergence (approximately 10M samples, for 2 weeks) across 128 TPUv3 cores [...]. Then we [...] run two separate fine-tuning stages (one further day of training each)\"\n\nAssuming: FP16 and utilization 0.4\n\nCalculation: (14+2) days * 24 hours/day * 60 min/hour * 60 sec/min * (128 TPU cores/2 cores per chip) * 1.23e14 FLOP/s per chip * 0.4 utilization = 4.35e21 FLOPs",
      "Training dataset": "PDB (Protein Data Bank)",
      "Training dataset size (gradients)": "56573952",
      "Dataset size notes": "See: https://www.rcsb.org/stats/growth/growth-released-structures for 2018\n\n\"We train the model to convergence (approximately 10M samples, for 2 weeks) across 128 TPUv3 cores with a batch size of 1 per TPU core. Then we halve the learning rate and double the number of sequences fed into the MSA stack before running two separate fine-tuning stages (one further day of training each)\"\n\n10000000/147328 ~ 68 epochs",
      "Confidence": "Confident",
      "Link": "https://www.biorxiv.org/content/10.1101/2021.10.04.463034v1",
      "Reference": "Protein complex prediction with AlphaFold-Multimer",
      "Citations": "2694.0",
      "Authors": "Richard Evans, Michael O\u2019Neill, Alexander Pritzel, Natasha Antropova, Andrew Senior, Tim Green, Augustin \u017d\u00eddek, Russ Bates, Sam Blackwell, Jason Yim, Olaf Ronneberger, Sebastian Bodenstein, Michal Zielinski, Alex Bridgland, Anna Potapenko, Andrew Cowie, Kathryn Tunyasuvunakool, Rishub Jain, Ellen Clancy, Pushmeet Kohli, John Jumper and Demis Hassabis",
      "Abstract": "While the vast majority of well-structured single protein chains can now be predicted to high accuracy due to the recent AlphaFold [1] model, the prediction of multi-chain protein complexes remains a challenge in many cases. In this work, we demonstrate that an AlphaFold model trained specifically for multimeric inputs of known stoichiometry, which we call AlphaFold-Multimer, significantly increases accuracy of predicted multimeric interfaces over input-adapted single-chain AlphaFold while maintaining high intra-chain accuracy. On a benchmark dataset of 17 heterodimer proteins without templates (introduced in [2]) we achieve at least medium accuracy (DockQ [3] \u2265 0.49) on 14 targets and high accuracy (DockQ \u2265 0.8) on 6 targets, compared to 9 targets of at least medium accuracy and 4 of high accuracy for the previous state of the art system (an AlphaFold-based system from [2]). We also predict structures for a large dataset of 4,433 recent protein complexes, from which we score all non-redundant interfaces with low template identity. For heteromeric interfaces we successfully predict the interface (DockQ \u2265 0.23) in 67% of cases, and produce high accuracy predictions (DockQ \u2265 0.8) in 23% of cases, an improvement of +25 and +11 percentage points over the flexible linker modification of AlphaFold [4] respectively. For homomeric interfaces we successfully predict the interface in 69% of cases, and produce high accuracy predictions in 34% of cases, an improvement of +5 percentage points in both instances.",
      "Organization categorization": "Industry,Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\"On a benchmark dataset of 17 heterodimer proteins without templates (introduced in [2]) we achieve at least medium accuracy (DockQ [3] \u2265 0.49) on 14 targets and high accuracy (DockQ \u2265 0.8) on 6 targets, compared to 9 targets of at least medium accuracy and 4 of high accuracy for the previous state of the art system (an AlphaFold-based system from [2])\"\n\n\"For heteromeric interfaces we successfully predict the interface (DockQ \u2265 0.23) in 67% of cases, and produce high accuracy predictions (DockQ \u2265 0.8) in 23% of cases, an improvement of +25 and +11 percentage points over the flexible linker modification of AlphaFold [4] respectively\"\n\n\"For homomeric interfaces we successfully predict the interface in 69% of cases, and produce high accuracy predictions in 34% of cases, an improvement of +5 percentage points in both instances\"",
      "Epochs": "68.0",
      "Training time (hours)": "384.0",
      "Training time notes": "Section: 2.5. Training Regimen\n\"We train the model to convergence (approximately 10M samples, for 2 weeks) across 128 TPUv3 cores [...]. Then we [...] run two separate fine-tuning stages (one further day of training each)\"",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "64.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "7966.2330131223",
      "Compute cost notes": "",
      "Training power draw (W)": "58117.86227766231",
      "Base model": "AlphaFold 2",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "While the AlphaFold code is licensed under the Apache 2.0 License, the AlphaFold parameters and CASP15 prediction data are made available under the terms of the CC BY 4.0 license\n\nhttps://github.com/google-deepmind/alphafold",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Turing ULRv5",
      "Organization": "Microsoft",
      "Publication date": "2021-09-28",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering,Translation",
      "Parameters": "2200000000.0",
      "Parameters notes": "2.2B",
      "Training compute (FLOP)": "2.8983951e+22",
      "Training compute notes": "312000000000000 FLOP / GPU / sec [A100] * 256 GPUs * 336 hours * 3600 sec / hour * 0.3 [assumed utilization] = 2.8983951e+22 FLOP",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://www.microsoft.com/en-us/research/blog/microsoft-turing-universal-language-representation-model-t-ulrv5-tops-xtreme-leaderboard-and-trains-100x-faster/",
      "Reference": "Microsoft Turing Universal Language Representation model, T-ULRv5, tops XTREME leaderboard and trains 100x faster",
      "Citations": "",
      "Authors": "",
      "Abstract": "Today, we are excited to announce that with our latest Turing universal language representation model (T-ULRv5), a Microsoft-created model is once again the state of the art and at the top of the Google XTREME public leaderboard. Resulting from a collaboration between the Microsoft Turing team and Microsoft Research, the 2.2 billion-parameter T-ULRv5 XL outperforms the current 2nd best model by an average score of 1.7 points. It is also the state of the art across each of the four subcategories of tasks on the leaderboard. These results demonstrate the strong capabilities of T-ULRv5, which, in addition to being more capable, trains 100 times faster than its predecessors.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "336.0",
      "Training time notes": "\"two weeks on 256 NVIDIA A100 GPUs\"",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "256.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "206668.900572444",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Hosted access (no API)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "\"Microsoft Turing models are also available for custom application building through our private preview program\"\n\n\"If you are a researcher who would like to work with us in assessing and improving Turing models, Microsoft Turing Academic Program (MS-TAP) allows you to submit a proposal and get access to these models in greater detail.\"",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "TrOCR",
      "Organization": "Beihang University,Microsoft Research Asia",
      "Publication date": "2021-09-21",
      "Domain": "Vision",
      "Task": "Character recognition (OCR)",
      "Parameters": "558000000.0",
      "Parameters notes": "558M table 5",
      "Training compute (FLOP)": "",
      "Training compute notes": "May be computed from github and datasets details. Uses pretrained BEiT and DeiT models.",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "The input data to the model are images.\n684M + 17.9M + 3.3M + 16M",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2109.10282",
      "Reference": "TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models",
      "Citations": "487.0",
      "Authors": "Minghao Li, Tengchao Lv, Jingye Chen, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei",
      "Abstract": "Text recognition is a long-standing research problem for document digitalization. Existing approaches are usually built based on CNN for image understanding and RNN for char-level text generation. In addition, another language model is usually needed to improve the overall accuracy as a post-processing step. In this paper, we propose an end-to-end text recognition approach with pre-trained image Transformer and text Transformer models, namely TrOCR, which leverages the Transformer architecture for both image understanding and wordpiece-level text generation. The TrOCR model is simple but effective, and can be pre-trained with large-scale synthetic data and fine-tuned with human-labeled datasets. Experiments show that the TrOCR model outperforms the current state-of-the-art models on the printed, handwritten and scene text recognition tasks. The TrOCR models and code are publicly available at https://aka.ms/trocr",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "China,China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "from conclusion \"Experiment results show that TrOCR achieves state-of-the-art results on printed, handwritten and scene text recognition with just a simple encoder-decoder model, without any post-processing steps\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "32.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "19378.229980216885",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT: https://github.com/microsoft/unilm/tree/master/trocr",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "PLATO-XL",
      "Organization": "Baidu",
      "Publication date": "2021-09-20",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "11000000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "9.9e+21",
      "Training compute notes": "\"In PLATO-XL, each model was trained for a total of 150B tokens, with\na batch size of 2M tokens.\"\n\n150B * 11B * 6 = 9.9e21",
      "Training dataset": "",
      "Training dataset size (gradients)": "150000000000",
      "Dataset size notes": "\"In PLATO-XL, each model was trained for a total of 150B tokens, with\na batch size of 2M tokens.\"\n811M English and 1.2B Chinese (context, response) samples. So if the average response is at least 75 tokens, the 150B tokens seen in training don't include repeat tokens. This seems plausible.",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2109.09519",
      "Reference": "PLATO-XL: Exploring the Large-scale Pre-training of Dialogue Generation",
      "Citations": "69.0",
      "Authors": "Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang, Wenquan Wu, Zhihua Wu, Zhen Guo, Hua Lu, Xinxian Huang, Xin Tian, Xinchao Xu, Yingzhan Lin, Zheng-Yu Niu",
      "Abstract": "To explore the limit of dialogue generation pre-training, we present the models of PLATO-XL with up to 11 billion parameters, trained on both Chinese and English social media conversations. To train such large models, we adopt the architecture of unified transformer with high computation and parameter efficiency. In addition, we carry out multi-party aware pre-training to better distinguish the characteristic information in social media conversations. With such designs, PLATO-XL successfully achieves superior performances as compared to other approaches in both Chinese and English chitchat. We further explore the capacity of PLATO-XL on other conversational tasks, such as knowledge grounded dialogue and task-oriented conversation. The experimental results indicate that PLATO-XL obtains state-of-the-art results across multiple conversational tasks, verifying its potential as a foundation model of conversational AI.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Table 4",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "256.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "155029.29220816068",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "apache 2.0\nhttps://github.com/PaddlePaddle/Knover/tree/develop/projects/PLATO-XL",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "HyperCLOVA 204B",
      "Organization": "NAVER",
      "Publication date": "2021-09-10",
      "Domain": "Language",
      "Task": "Language modeling/generation",
      "Parameters": "204000000000.0",
      "Parameters notes": "https://www.navercorp.com/navercorp_/ir/announce/2023/NAVER_CEO%20letter%20to%20shareholders_Aug%202023_Eng.pdf",
      "Training compute (FLOP)": "2.0000000001e+23",
      "Training compute notes": "Estimations for 82B model (marked as lower bound estimations)\n\n\"For experiments in Section 4, the model trained with 150B is used for fair comparison, because not all models are finished training at the same iteration. However, experiments in Section 5.2 use the model trained with 300B tokens, as HyperCLOVA Studio provided the 39B and 82B models trained with 300B tokens.\"\n\n82e9 connections * 2 FLOP/connection * 300e9 tokens * 3 backward pass = 1.476e23 FLOP\n\nCalculation using GPU time corroborates this:\n- \"Our model is based on megatron-LM (Shoeybi et al., 2019) and trained on the NVIDIA Superpod, which includes 128 strongly clustered DGX servers with 1,024 A100 GPUs.\"\n- \"It takes 13.4 days to train a model with 82B parameters with 150B tokens.\" Assume 300B tokens takes twice as long, 26.8 days.\n- Assume the default of 30% utilization rate for large language models.\n\n1024 A100 GPUs * 312e12 FLOP/second * 0.3 utilization * 26.8 days * 24 * 60 * 60 seconds/day = 2.219e+23 FLOP",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "560000000000",
      "Dataset size notes": "https://twitter.com/arankomatsuzaki/status/1397583304610783238\nhttps://venturebeat.com/ai/naver-trained-a-gpt-3-like-korean-language-model/",
      "Confidence": "Speculative",
      "Link": "https://aibusiness.com/nlp/south-korea-s-naver-unveils-hyperscale-ai-platform-language-model-with-more-parameters-than-gpt-3",
      "Reference": "South Korea's Naver unveils 'hyperscale AI' platform, language model with more parameters than GPT-3",
      "Citations": "92.0",
      "Authors": "",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "Korea (Republic of)",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"HyperCLOVA with our training configuration shows state-of-the-art in-context zero-shot and few-shot learning performances on various downstream tasks in Korean\"\n\nno evaluations on any standard benchmarks are reported",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "441770.0038182093",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Hosted access (no API)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Unreleased",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "PermuteFormer",
      "Organization": "Peking University",
      "Publication date": "2021-09-06",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "149697024.0",
      "Parameters notes": "Parameterization appears to be similar to a vanilla transformer. 6 layers, hidden dimension of 512, feed forward dimension of 1024, 8 attention heads. This would imply 20,447,232 parameters without embedding weights, and 512*vocab_size embedding weights (assuming tied embedding and unembedding projections)\n\nThey appear to use word-level tokenization: \"We evaluate unidirectional PermuteFormer on WikiText-103 (Merity et al., 2017). It is a language modeling dataset with about 103 million tokens,\" and I confirmed that word-level tokenization results in about 102M words across train-test-validation.\n\nIf this is the case, there are 267,735 unique words, so the embedding layer alone would be 137,080,320 parameters, for a total of 149,697,024.",
      "Training compute (FLOP)": "2.775e+18",
      "Training compute notes": "6 * (30 * 103M) * 149,697,024 = 2.775e18\n\nThis seems a bit small relative to their statement: \"It takes about 10 days on 8 V100 GPUs to get all the figures in this paper\" which suggests about 2.7e20 FLOPs at 30% MFU.\n\nTable 2 indicates that Performer and PermuteFormer take 0.23x to 0.58x as long to train as a Transformer model. Figure 2 appears to be the most compute intensive figure, and would take about 4 * (2.775e18) + 1 * (2.775e18 / 0.365) = 1.9e19 FLOPs. ",
      "Training dataset": "WikiText-103",
      "Training dataset size (gradients)": "103000000",
      "Dataset size notes": "WikiText-103 is about 103M tokens",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/2109.02377",
      "Reference": "PermuteFormer: Efficient Relative Position Encoding for Long Sequences",
      "Citations": "22.0",
      "Authors": "Peng Chen",
      "Abstract": "A recent variation of Transformer, Performer, scales Transformer to longer sequences with a linear attention mechanism. However, it is not compatible with relative position encoding, which has advantages over absolute position encoding. In this paper, we discuss possible ways to add relative position encoding to Performer. Based on the analysis, we propose PermuteFormer, a Performer-based model with relative position encoding that scales linearly on long sequences. PermuteFormer applies position-dependent transformation on queries and keys to encode positional information into the attention module. This transformation is carefully crafted so that the final output of self-attention is not affected by absolute positions of tokens. PermuteFormer introduces negligible computational overhead by design that it runs as fast as Performer. We evaluate PermuteFormer on Long-Range Arena, a dataset for long sequences, as well as WikiText-103, a language modeling dataset. The experiments show that PermuteFormer uniformly improves the performance of Performer with almost no computational overhead and outperforms vanilla Transformer on most of the tasks.",
      "Organization categorization": "Academia",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Results show that PermuteFormer uniformly improves the performance of Performer, accelerates convergence, and achieves state-of-the-art on some tasks.\"\n\n\"We conduct extensive experiments to evaluate PermuteFormer. It achieves strong empirical performance and obtains state-of-the-art on Long-Range Arena, a benchmark for efficient Transformers. It also improves the performance of Performer on language modeling\ntasks like WikiText-103\"",
      "Epochs": "30.0",
      "Training time (hours)": "",
      "Training time notes": "Running all code needed to produce plots took about 10 days on 8 V100s",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "8.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "4846.17604411253",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "no specific license. training code: https://github.com/cpcp1998/PermuteFormer/tree/master/language_model ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "MEB",
      "Organization": "Microsoft",
      "Publication date": "2021-09-04",
      "Domain": "Search,Language",
      "Task": "Search",
      "Parameters": "135000000000.0",
      "Parameters notes": "See paper title",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "500000000000",
      "Dataset size notes": "\"MEB uses three years of search logs from Bing as training data.\" TODO convert",
      "Confidence": "Confident",
      "Link": "https://www.microsoft.com/en-us/research/blog/make-every-feature-binary-a-135b-parameter-sparse-neural-network-for-massively-improved-search-relevance/",
      "Reference": "Make Every feature Binary: A 135B parameter sparse neural network for massively improved search relevance",
      "Citations": "26.0",
      "Authors": "W Liu, Z Wang, X Liu, N Zeng, Y Liu, FE Alsaadi",
      "Abstract": "Recently, Transformer-based deep learning models like GPT-3 have been getting a lot of attention in the machine learning world. These models excel at understanding semantic relationships, and they have contributed to large improvements in Microsoft Bing\u2019s search experience and surpassing human performance on the SuperGLUE academic benchmark. However, these models can fail to capture more nuanced relationships between query and document terms beyond pure semantics.\n\nIn this blog post, we are introducing \u201cMake Every feature Binary\u201d (MEB), a large-scale sparse model that complements our production Transformer models to improve search relevance for Microsoft customers using AI at Scale. To make search more accurate and dynamic, MEB better harnesses the power of large data and allows for an input feature space with over 200 billion binary features that reflect the subtle relationships between search queries and documents.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Significant use",
      "Notability criteria notes": "\"MEB is running in production for 100 percent of Bing searches, in all regions and languages.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Hosted access (no API)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "\"MEB is running in production for 100 percent of Bing searches, in all regions and languages. It is the largest universal model we\u2019re serving at Microsoft\"",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "FLAN 137B",
      "Organization": "Google Research",
      "Publication date": "2021-09-03",
      "Domain": "Language",
      "Task": "Language modeling,Question answering,Language modeling/generation",
      "Parameters": "137000000000.0",
      "Parameters notes": "Abstract:\n\"We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types.\"\n\nMany models seem to be using the same 137B base transformer model?",
      "Training compute (FLOP)": "2.047e+24",
      "Training compute notes": "From section 2.4: Pretraining was done over 2.49T tokens.\n6 * 2.49T * 137B = 2.047e24 \nAlso, \"instruction tuning takes around 60 hours on a TPUv3 with 128 cores.\" 128 TPUv3 cores = 64 TPUv3 chips. Environmental considerations section claims this took less than 2% of total time\n1.23e14 * 64 * 60 * 3600 * 0.3 = 5.10e20",
      "Training dataset": "Wikipedia,Unspecified unreleased",
      "Training dataset size (gradients)": "2490000000000",
      "Dataset size notes": "\"Model architecture and pretraining. In our experiments, we use LaMDA-PT, a dense left-to-right, decoder-only transformer language model of 137B parameters (Thoppilan et al., 2022). This model is pretrained on a collection of web documents (including those with computer code), dialog data, and Wikipedia, tokenized into 2.49T BPE tokens with a 32k vocabulary using the  SentencePiece library (Kudo & Richardson, 2018). Around 10% of the pretraining data was non-English. Note that LaMDA-PT only has language model pretraining (c.f. LaMDA, which was finetuned for dialog).\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2109.01652",
      "Reference": "Finetuned Language Models Are Zero-Shot Learners",
      "Citations": "4557.0",
      "Authors": "Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le",
      "Abstract": "This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning\u2014finetuning language models on a collection of datasets described via instructions\u2014substantially improves zeroshot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "Abstract: \n\"FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate.\"\n\n\"FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze\"\n\nSOTA is reported among unsupervised models",
      "Epochs": "1.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "LaMDA",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "\"In our experiments, we use LaMDA-PT, a dense left-to-right,\ndecoder-only transformer language model of 137B parameters (Thoppilan et al., 2022) [...] Note that LaMDA-PT only has language model pretraining (c.f. LaMDA, which was finetuned for dialog).\" In our entry for LaMDA we only measured pre-training compute, so we just specify LaMDA as the base model of FLAN 137B.",
      "Batch size": "",
      "Batch size notes": "just a finetune",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "XLMR-XXL",
      "Organization": "Facebook AI Research",
      "Publication date": "2021-08-17",
      "Domain": "Language",
      "Task": "Translation,Language modeling/generation",
      "Parameters": "10700000000.0",
      "Parameters notes": "Section 2.1:\n\" ...XLM-RXXL (L= 48, H = 4096, A = 32, 10.7B params)\"",
      "Training compute (FLOP)": "3.366e+22",
      "Training compute notes": "Trained for 500k steps at a batch size of 2048 with sequence length of 512 = 524,288,000,000 tokens seen.\n6 * 10700000000 * 524,288,000,000 = 3.366e22",
      "Training dataset": "CC100",
      "Training dataset size (gradients)": "167000000000",
      "Dataset size notes": "\"We pretrain the models on the CC100 dataset, which corresponds to 167B tokens in 100 languages.\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2105.00572",
      "Reference": "Larger-Scale Transformers for Multilingual Masked Language Modeling",
      "Citations": "145.0",
      "Authors": "Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau",
      "Abstract": "Recent work has demonstrated the effectiveness of cross-lingual language model pretraining for cross-lingual understanding. In this study, we present the results of two larger multilingual masked language models, with 3.5B and 10.7B parameters. Our two new models dubbed XLM-R XL and XLM-R XXL outperform XLM-R by 1.8% and 2.4% average accuracy on XNLI. Our model also outperforms the RoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on average while handling 99 more languages. This suggests pretrained models with larger capacity may obtain both strong performance on high-resource languages while greatly improving low-resource languages. We make our code and models publicly available.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,France",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Abstract:\n\"Our model also outperforms\nthe RoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on average while handling 99 more languages.\"",
      "Epochs": "3.1394491018",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "1048576.0",
      "Batch size notes": "Batches of 2048 with sequence length of 512",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "https://github.com/facebookresearch/fairseq/tree/main/examples/xlmr",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DNABERT",
      "Organization": "Northeastern University",
      "Publication date": "2021-08-15",
      "Domain": "Biology",
      "Task": "Protein or nucleotide language model (pLM/nLM)",
      "Parameters": "110000000.0",
      "Parameters notes": "\"We used the same model architecture as the BERT base, which consists of 12 Transformer layers with 768 hidden units and 12 attention heads in each layer, and the same parameter setting across all the four DNABERT models during pre-training\"\n\nKnown to have 110 million parameters as reported in: https://arxiv.org/pdf/1810.04805v2.pdf\n\"We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) [...]\"",
      "Training compute (FLOP)": "1.07e+20",
      "Training compute notes": "\"Since the pre-training of DNABERT model is resource-intensive (about 25\u2009days on 8 NVIDIA 2080Ti GPUs)\"\n\nAssuming FP16 and 30% utilization\n\nCalculation = (25 * 24 *3600) s * 2.7e13 FLOP/s per GPU * 8 GPUs * 0.3 utilization = 1.4e20 FLOP\n\nAlternatively:\n\"DNABERT takes a sequence with a max length of 512 as input... We pre-trained DNABERT for 120k steps with a batch size of 2000\"\n6 * 512 * 2000 * 120k * 110M = 8.11e19\n\nGeometric mean: 1.07e20",
      "Training dataset": "Human Reference Genome (GRCh38/hg38)",
      "Training dataset size (gradients)": "1444128539.3656242",
      "Dataset size notes": "The human genome is around 3 billion base pairs (https://useast.ensembl.org/Homo_sapiens/Info/Annotation).\nThe authors use both non-overlapping sampling and random sampling from a human genome, though the source is unspecified.",
      "Confidence": "Confident",
      "Link": "https://academic.oup.com/bioinformatics/article/37/15/2112/6128680",
      "Reference": "DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome",
      "Citations": "958.0",
      "Authors": "Yanrong Ji, Zhihan Zhou, Han Liu, Ramana V Davuluri",
      "Abstract": "Motivation\nDeciphering the language of non-coding DNA is one of the fundamental problems in genome research. Gene regulatory code is highly complex due to the existence of polysemy and distant semantic relationship, which previous informatics methods often fail to capture especially in data-scarce scenarios.\n\nResults\nTo address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, to capture global and transferrable understanding of genomic DNA sequences based on up and downstream nucleotide contexts. We compared DNABERT to the most widely used programs for genome-wide regulatory elements prediction and demonstrate its ease of use, accuracy and efficiency. We show that the single pre-trained transformers model can simultaneously achieve state-of-the-art performance on prediction of promoters, splice sites and transcription factor binding sites, after easy fine-tuning using small task-specific labeled data. Further, DNABERT enables direct visualization of nucleotide-level importance and semantic relationship within input sequences for better interpretability and accurate identification of conserved sequence motifs and functional genetic variant candidates. Finally, we demonstrate that pre-trained DNABERT with human genome can even be readily applied to other organisms with exceptional performance. We anticipate that the pre-trained DNABERT model can be fined tuned to many other sequence analyses tasks.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We show that the single pre-trained transformers model can simultaneously achieve state-of-the-art performance on prediction of promoters, splice sites and transcription factor binding sites, after easy fine-tuning using small task-specific labeled data.\" [Abstract] - SOTA improvement on very specific task",
      "Epochs": "4.04",
      "Training time (hours)": "600.0",
      "Training time notes": "\"Since the pre-training of DNABERT model is resource-intensive (about 25\u2009days on 8 NVIDIA 2080Ti GPUs)\"",
      "Training hardware": "NVIDIA GeForce RTX 2080 Ti 11GB",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache 2.0, code and weights: https://github.com/jerryji1993/DNABERT",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Zidong Taichu",
      "Organization": "Chinese Academy of Sciences,Wuhan AI Computing Center",
      "Publication date": "2021-08-11",
      "Domain": "Multimodal,Speech,Vision,Language",
      "Task": "Language modeling/generation,Speech recognition (ASR),Image captioning",
      "Parameters": "3200000000.0",
      "Parameters notes": "\u517132\u4ebf\u53c2\u6570 translated as A total of 3.2 billion parameters ",
      "Training compute (FLOP)": "8.016e+20",
      "Training compute notes": "4.175e10 * 3.2e9 * 6 = 8.016e20 FLOP",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "\u4e3b\u8981\u91c7\u7528CLUE\u4e0eWMT\u4e2d\u6536\u96c6\u7684\u4e2d\u6587\u6570\u636e\uff0c\u540c\u65f6\u6211\u4eec\u52a0\u5165\u4e86\u989d\u5916\u6536\u96c6\u7684\u5bf9\u8bdd\u6570\u636e\u4ee5\u53ca\u7ffb\u8bd1\u5e73\u884c\u8bed\u6599\u4e2d\u7684\u4e2d\u6587\u90e8\u5206\uff0c\u603b\u5171\u7ea6250G\u7684\u4e2d\u6587\u8bed\u6599\uff0c\u9886\u57df\u8986\u76d6\u5e7f\u6cdb\u3002\n\nFrom context, seems to mean 250GB\n\n250GB * 167M tokens/GB = 4.175e+10 tokens\n\nhttps://gitee.com/zidongtaichu/multi-modal-models/tree/master/text#%E6%95%B0%E6%8D%AE%E9%9B%86",
      "Confidence": "Confident",
      "Link": "https://gitee.com/zidongtaichu/multi-modal-models",
      "Reference": "Zidong Ancestral multi-modal large model",
      "Citations": "",
      "Authors": "",
      "Abstract": "",
      "Organization categorization": "Academia,Government",
      "Country (of organization)": "China,China",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "The world\u2019s first image, language, and audio trimodal pre-trained model.",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Apache 2.0\nhttps://gitee.com/zidongtaichu/multi-modal-models\n\nI don't see training code there",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Jurassic-1-Jumbo",
      "Organization": "AI21 Labs",
      "Publication date": "2021-08-11",
      "Domain": "Language",
      "Task": "Language modeling/generation,Chat",
      "Parameters": "178000000000.0",
      "Parameters notes": "\"Jurassic-1 models come in two sizes, where the Jumbo version, at 178B parameters, is the largest and most sophisticated language model ever released for general use by developers.\"",
      "Training compute (FLOP)": "3.7e+23",
      "Training compute notes": "see here https://docs.google.com/document/d/1B8x6XYcmB1u6Tmq3VcbAtj5bzhDaj2TcIPyK6Wpupx4/edit\n\n6 * 178B * 300B = 3.204000e+23",
      "Training dataset": "",
      "Training dataset size (gradients)": "300000000000",
      "Dataset size notes": "\"Our model was trained with the conventional self-supervised auto-regressive training objective on 300B tokens drawn from publicly available resources\"\n\n1 token ~ 0.75 words",
      "Confidence": "Confident",
      "Link": "https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf",
      "Reference": "Jurassic-1: Technical Details and Evaluation",
      "Citations": "55.0",
      "Authors": "Opher Lieber, Or Sharir, Barak Lenz, Yoav Shoham",
      "Abstract": "Jurassic-1 is a pair of auto-regressive language models recently released by AI21 Labs, consisting of J1-Jumbo, a 178B-parameter model, and J1-Large, a 7B-parameter model. We describe their architecture and training, and evaluate their performance relative to GPT-3. The evaluation is in terms of perplexity, as well as zero-shot and few-shot learning. To that end, we developed a zeroshot and few-shot test suite, which we made publicly available (https://github.com/ai21labs/ lm-evaluation) as a shared resource for the evaluation of mega language models.",
      "Organization categorization": "Industry",
      "Country (of organization)": "Israel",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "\"Training such a large model, on over 800 GPUs over many months\"\n\nLower-bound cost estimate:\n800 GPUs * $1/GPU-hour * 4 months = $2.3M\nTrue cost was probably substantially higher. \"many months\" implies more than 4, and the GPUs were probably more expensive than $1/hour.",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "836700.050829598",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "3200000.0",
      "Batch size notes": "\"Namely, we used a base learning rate of 1.2 \u00d7 10\u22124 and 0.6 \u00d7 10\u22124 , and a batch size of 2M and 3.2M tokens, for J1-Large and J1-Jumbo, respectively. We also used a linear warm-up over roughly the first 375 million tokens, and gradually increased the batch size from 32K tokens up to its target value for the first few billion tokens.\"",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "W2v-BERT",
      "Organization": "Google Brain,Massachusetts Institute of Technology (MIT)",
      "Publication date": "2021-08-07",
      "Domain": "Speech",
      "Task": "Speech recognition (ASR)",
      "Parameters": "1000000000.0",
      "Parameters notes": "1B for XXL model",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "LibriLight",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2108.06209v2",
      "Reference": "W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training",
      "Citations": "492.0",
      "Authors": "Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, Yonghui Wu",
      "Abstract": "Motivated by the success of masked language modeling~(MLM) in pre-training natural language processing models, we propose w2v-BERT that explores MLM for self-supervised speech representation learning. w2v-BERT is a framework that combines contrastive learning and MLM, where the former trains the model to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the model to learn contextualized speech representations via solving a masked prediction task consuming the discretized tokens. In contrast to existing MLM-based speech pre-training frameworks such as HuBERT, which relies on an iterative re-clustering and re-training process, or vq-wav2vec, which concatenates two separately trained modules, w2v-BERT can be optimized in an end-to-end fashion by solving the two self-supervised tasks~(the contrastive task and MLM) simultaneously. Our experiments show that w2v-BERT achieves competitive results compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks when using the Libri-Light~60k corpus as the unsupervised data. In particular, when compared to published models such as conformer-based wav2vec~2.0 and HuBERT, our model shows~5\\% to~10\\% relative WER reduction on the test-clean and test-other subsets. When applied to the Google's Voice Search traffic dataset, w2v-BERT outperforms our internal conformer-based wav2vec~2.0 by more than~30\\% relatively.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Our experiments show that w2v-BERT achieves competitive results compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks when using the Libri-Light 60k corpus as the unsupervised data. In particular, when compared to published models such as conformer-based wav2vec 2.0 and HuBERT, our model shows 5% to 10% relative WER reduction on the test-clean and\ntest-other subsets\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "YOLOX-X",
      "Organization": "Megvii Inc",
      "Publication date": "2021-08-06",
      "Domain": "Vision",
      "Task": "Object detection",
      "Parameters": "99100000.0",
      "Parameters notes": "99.1M, table 3",
      "Training compute (FLOP)": "6.34275e+20",
      "Training compute notes": "\"We train the models for a total of 300 epochs with 5 epochs warmup on COCO train2017 [17]. We use stochastic gradient descent (SGD) for training ... The batch size is 128 by default to typical 8-GPU devices ... input size is evenly drawn from 448 to 832 with 32 strides\"\n\nTraining is done on 300 epochs of the 2.5 million image-label pairs in COCO train2017.\n\nTable 3 indicates 281.9 GFLOP per forward pass on a 640x640 image. The mean image width/height is 640, though using this to estimate training FLOPs is probably a slight underestimate as FLOPs will scale roughly linearly in the number of pixels (which scale at width^2).\n\nIgnoring this slight issue: 281.9e9 * 2.5M * 300 * 3 = 6.34e20",
      "Training dataset": "COCO 2017",
      "Training dataset size (gradients)": "2500000",
      "Dataset size notes": "2.5 million image-label pairs, per Coco paper https://arxiv.org/abs/1405.0312",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2107.08430",
      "Reference": "YOLOX: Exceeding YOLO Series in 2021",
      "Citations": "5206.0",
      "Authors": "Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, Jian Sun",
      "Abstract": "In this report, we present some experienced improvements to YOLO series, forming a new high-performance detector -- YOLOX. We switch the YOLO detector to an anchor-free manner and conduct other advanced detection techniques, i.e., a decoupled head and the leading label assignment strategy SimOTA to achieve state-of-the-art results across a large scale range of models: For YOLO-Nano with only 0.91M parameters and 1.08G FLOPs, we get 25.3% AP on COCO, surpassing NanoDet by 1.8% AP; for YOLOv3, one of the most widely used detectors in industry, we boost it to 47.3% AP on COCO, outperforming the current best practice by 3.0% AP; for YOLOX-L with roughly the same amount of parameters as YOLOv4-CSP, YOLOv5-L, we achieve 50.0% AP on COCO at a speed of 68.9 FPS on Tesla V100, exceeding YOLOv5-L by 1.8% AP. Further, we won the 1st Place on Streaming Perception Challenge (Workshop on Autonomous Driving at CVPR 2021) using a single YOLOX-L model. We hope this report can provide useful experience for developers and researchers in practical scenes, and we also provide deploy versions with ONNX, TensorRT, NCNN, and Openvino supported. Source code is at this https URL.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "Table 6",
      "Epochs": "300.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "8.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "4849.522759284234",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "128.0",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache 2.0 for code/weights\nhttps://github.com/Megvii-BaseDetection/YOLOX/blob/main/LICENSE",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "6-Act Tether",
      "Organization": "Facebook AI Research,Georgia Institute of Technology",
      "Publication date": "2021-08-03",
      "Domain": "Robotics",
      "Task": "Object detection",
      "Parameters": "5000000.0",
      "Parameters notes": "\"Agent parameter counts were all 5 \u2212 6 million parameters, excluding parameters in auxiliary modules\"",
      "Training compute (FLOP)": "",
      "Training compute notes": "\"In our experiments, we train each of our agents for 8 GPU-weeks (192 GPU-hours)\". No GPU specified.",
      "Training dataset": "Matterport",
      "Training dataset size (gradients)": "125000000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://openaccess.thecvf.com/content/ICCV2021/html/Ye_Auxiliary_Tasks_and_Exploration_Enable_ObjectGoal_Navigation_ICCV_2021_paper.html\nhttps://arxiv.org/abs/2104.04112",
      "Reference": "Auxiliary Tasks and Exploration Enable ObjectGoal Navigation",
      "Citations": "119.0",
      "Authors": "Joel Ye, Dhruv Batra, Abhishek Das, Erik Wijmans",
      "Abstract": "ObjectGoal Navigation (ObjectNav) is an embodied task wherein agents are to navigate to an object instance in an unseen  environment. Prior works have shown that end-to-end ObjectNav agents that use vanilla visual and recurrent modules, e.g. a CNN+RNN, perform poorly due to overfitting and sample inefficiency. This has motivated current state-of-the-art methods to mix analytic and learned components and operate on explicit spatial maps of the environment. We instead re-enable a generic learned agent by adding auxiliary learning tasks and an exploration reward. Our agents achieve 24.5% success and 8.1% SPL, a 37% and 8% relative improvement over prior state-of-the-art, respectively, on the Habitat ObjectNav Challenge. From our analysis, we propose that agents will act to simplify their visual inputs so as to smooth their RNN dynamics, and that auxiliary tasks reduce overfitting by minimizing effective RNN dimensionality; i.e. a performant ObjectNav agent that must maintain coherent plans over long horizons does so by learning smooth, low-dimensional recurrent dynamics.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,France,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Our agents achieve 24.5% success and 8.1% SPL, a 37% and 8% relative improvement over prior state-of-the-art, respectively, on the Habitat ObjectNav Challenge\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "",
      "Accessibility notes": "MIT license\n\nhttps://github.com/joel99/objectnav",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "SEER",
      "Organization": "Facebook AI Research,INRIA",
      "Publication date": "2021-07-29",
      "Domain": "Vision",
      "Task": "Image embedding,Image classification",
      "Parameters": "1300000000.0",
      "Parameters notes": "From abstract:\n\" Our final SElf-supERvised (SEER) model, a RegNetY with 1.3B parameters...\"",
      "Training compute (FLOP)": "1.8e+22",
      "Training compute notes": "Numbers from section 3.2, they specifically mention using mixed precision training.\n6125 ms / batch * 114890 batches = 8.14 days (they round to 8 in the text)\n\n512 GPUs * 8.14 days * 24h/day * 3600s/h * 125 TFLOP/s * 0.4 (assumed utilization) = 1.800e22\n\n\"on 512 V100 32GB NVIDIA GPUs. Training this model on 1 billion images requires 114, 890 training iterations for a batch size of 8, 704 images, summing to 8 days of training over 512 GPUs.\"",
      "Training dataset": "Instagram",
      "Training dataset size (gradients)": "1000000000",
      "Dataset size notes": "\"Overall, we train\non 1B images for a total of 122K iterations.\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2103.01988",
      "Reference": "Self-supervised Pretraining of Visual Features in the Wild",
      "Citations": "296.0",
      "Authors": "Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek Pai, Mannat Singh, Vitaliy Liptchinsky, Ishan Misra, Armand Joulin, Piotr Bojanowski",
      "Abstract": "Recently, self-supervised learning methods like MoCo, SimCLR, BYOL and SwAV have reduced the gap with supervised methods. These results have been achieved in a control environment, that is the highly curated ImageNet dataset. However, the premise of self-supervised learning is that it can learn from any random image and from any unbounded dataset. In this work, we explore if self-supervision lives to its expectation by training large models on random, uncurated images with no supervision. Our final SElf-supERvised (SEER) model, a RegNetY with 1.3B parameters trained on 1B random images with 512 GPUs achieves 84.2% top-1 accuracy, surpassing the best self-supervised pretrained model by 1% and confirming that self-supervised learning works in a real world setting. Interestingly, we also observe that self-supervised models are good few-shot learners achieving 77.9% top-1 with access to only 10% of ImageNet. Code: this https URL",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,France,France",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "SOTA for self-supervised models on ImageNet, which seems fair to consider a different benchmark than ImageNet for supervised models.\n\n\"Our final SElf-supERvised (SEER) model, a RegNetY with 1.3B parameters trained on 1B random images with 512 GPUs achieves 84.2% top-1 accuracy, surpassing the best self-supervised pretrained model by 1%\"",
      "Epochs": "",
      "Training time (hours)": "195.5",
      "Training time notes": "6125 ms / batch * 114890 batches = 195.5 hours",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "512.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "34114.252463690456",
      "Compute cost notes": "",
      "Training power draw (W)": "310424.75538121304",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "Open access (non-commercial)",
      "Accessibility notes": "https://github.com/facebookresearch/vissl/tree/main/projects/SEER\n\nWe share instructions on how to train SEER model on GPUs using PyTorch. First, Install VISSL and follow the data setup instructions to easily setup your data input with VISSL.\n\nhttps://github.com/facebookresearch/vissl/blob/main/projects/SEER/MODEL_LICENSE.md",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "HuBERT",
      "Organization": "Facebook AI Research",
      "Publication date": "2021-07-27",
      "Domain": "Speech",
      "Task": "Speech recognition (ASR)",
      "Parameters": "1000000000.0",
      "Parameters notes": "From abstract:\n\"Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets\"",
      "Training compute (FLOP)": "5.54e+21",
      "Training compute notes": "GPU NOT SPECIFIED - for the sake of argument I assume something on the order of 1 TFLOP/s\n\nNumbers from Section IV part C\n0.1 * (960h * 32GPUs + 60000h * 256 GPUs) * 3600s/h * 1 TFLOP/s/GPU",
      "Training dataset": "LibriSpeech,LibriLight",
      "Training dataset size (gradients)": "864000000",
      "Dataset size notes": "\"When the HuBERT model is pre-trained on either the standard Librispeech 960h [24] or the Libri-Light 60k hours [25], it either matches or improves upon the state-of-theart wav2vec 2.0 [6] performance on all fine-tuning subsets of 10mins, 1h, 10h, 100h, and 960h.\"\n\n1h ~ 13,680 words\n13,680 * 60,000 = 820800000",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/2106.07447",
      "Reference": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
      "Citations": "3947.0",
      "Authors": "Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed",
      "Abstract": "Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,France",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "Abstract: \n\" the\nHuBERT model either matches or improves upon the state-ofthe-art wav2vec 2.0 performance on the Librispeech (960h) and\nLibri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and\n960h fine-tuning subsets.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "https://github.com/facebookresearch/fairseq/tree/main/examples/hubert\n\nfairseq(-py) is MIT-licensed. The license applies to the pre-trained models as well.\nincludes training code",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GOAT",
      "Organization": "DeepMind",
      "Publication date": "2021-07-27",
      "Domain": "Games",
      "Task": "Open ended play",
      "Parameters": "3472816.0",
      "Parameters notes": "estimate described here: https://docs.google.com/document/d/1S9xZyCeITDOs-P1W_-liNW0WgVN-OLsSudVrPXMaLqw/edit?usp=sharing",
      "Training compute (FLOP)": "2.412e+22",
      "Training compute notes": "[Final calculation]\n(8 TPUs) * (1.23e14 FLOP/TPU-s) * (0.1 utilization) / (50k steps/s) = 1.968e9 FLOP/step\n\n(32 agents) * (383B steps/agent) * (1.968e9 FLOP/step) = 2.412e22 FLOPs\n\n==========================\nNOTES BELOW\n\n6.1: Each agent is trained using 8 TPUv3s and consumes approximately 50,000 agent steps (observations) per second.\nMultiple agents interacting probably mean a fairly low utilization rate, so let\u2019s assume 0.10\n8 * 1.23e14 * 0.1 / 50k = 1.968e9 FLOPs per step\n\nThe paper doesn\u2019t say exactly how many agents they train in each population. The original PBT paper uses 32 agents for one task (in general it uses between 10 and 80), so as a guesstimate let\u2019s go with that.\n\nFigure 16: They train over 5 generations. Summing the number of steps, it looks like there were roughly 383B steps\n32 * 383B * 1.968e9 = 2.412e22\n\nFinal estimate:\n2.412e22\n\nI do a confidence interval analysis here and find a 90% CI of 6.9e21 to 1.3e23, so we can call this estimate \"likely\" (within 1 OOM): https://colab.research.google.com/drive/1wGSTQxBExY6Fa0-d7msVumf5-KnsWLe6?usp=sharing",
      "Training dataset": "XLand",
      "Training dataset size (gradients)": "798720000000000",
      "Dataset size notes": "Figure 16 shows steps per generation and agent. In total there are 1.5e10 + 4.0e10 + 2.5e10 + 1.1e11 + 2e11 = 3.9e11 steps per agent.",
      "Confidence": "Speculative",
      "Link": "https://deepmind.com/blog/article/generally-capable-agents-emerge-from-open-ended-play\n\nhttps://arxiv.org/abs/2107.12808",
      "Reference": "Open-Ended Learning Leads to Generally Capable Agents",
      "Citations": "218.0",
      "Authors": "Open-Ended Learning Team*, Adam Stooke, Anuj Mahajan, Catarina Barros, Charlie Deck, Jakob Bauer, Jakub Sygnowski, Maja Trebacz, Max Jaderberg, Michael Mathieu, Nat McAleese, Nathalie Bradley-Schmieg, Nathaniel Wong, Nicolas Porcel, Roberta Raileanu, Steph Hughes-Fitt, Valentin Dalibard and Wojciech Marian Czarnecki",
      "Abstract": "In this work we create agents that can perform well beyond a single, individual task, that exhibit much wider generalisation of behaviour to a massive, rich space of challenges. We define a universe of tasks within an environment domain and demonstrate the ability to train agents that are generally capable across this vast space and beyond. The environment is natively multi-agent, spanning the continuum of competitive, cooperative, and independent games, which are situated within procedurally generated physical 3D worlds. The resulting space is exceptionally diverse in terms of the challenges posed to agents, and as such, even measuring the learning progress of an agent is an open research problem. We propose an iterative notion of improvement between successive generations of agents, rather than seeking to maximise a singular objective, allowing us to quantify progress despite tasks being incomparable in terms of achievable rewards. We show that through constructing an open-ended learning process, which dynamically changes the training task distributions and training objectives such that the agent never stops learning, we achieve consistent learning of new behaviours. The resulting agent is able to score reward in every one of our humanly solvable evaluation levels, with behaviour generalising to many held-out points in the universe of tasks. Examples of this zero-shot generalisation include good performance on Hide and Seek, Capture the Flag, and Tag. Through analysis and hand-authored probe tasks we characterise the behaviour of our agent, and find interesting emergent heuristic behaviours such as trial-and-error experimentation, simple tool use, option switching, and cooperation. Finally, we demonstrate that the general capabilities of this agent could unlock larger scale transfer of behaviour through cheap finetuning.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "likely qualitatively SOTA\n\nI do not see any standard benchmarks that they are claiming SOTA on",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "see other notes\n",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "84799.78517435073",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "64.0",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Codex",
      "Organization": "OpenAI",
      "Publication date": "2021-07-07",
      "Domain": "Language",
      "Task": "Code autocompletion",
      "Parameters": "12000000000.0",
      "Parameters notes": "\"With just a single sample, a 12B parameter Codex solves 28.8% of these problems, and a 300M parameter Codex solves 13.2% of these problems\"",
      "Training compute (FLOP)": "7.344e+22",
      "Training compute notes": "\"The original training of GPT-3-12B consumed hundreds of petaflop/sdays of compute, while fine-tuning it to create Codex-12B\nconsumed a similar amount of compute.\"\n1 PFLOP/s-day = 8.64e19 FLOPs.\n\"Hundreds\" is likely between 200 and 900, geometric mean = 425.\n2 * 425 * 8.64e19 = 7.344e22\n",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "52788000000",
      "Dataset size notes": "\"Our training dataset was collected in May 2020 from 54 million public software repositories hosted on GitHub, containing 179 GB of unique Python files under 1 MB. We filtered out files which were likely auto-generated, had average line\nlength greater than 100, had maximum line length greater\nthan 1000, or contained a small percentage of alphanumeric\ncharacters. After filtering, our final dataset totaled 159 GB.\"\n\n1 GB ~ 200M words",
      "Confidence": "Likely",
      "Link": "https://openai.com/blog/openai-codex/\nhttps://arxiv.org/abs/2107.03374",
      "Reference": "Evaluating Large Language Models Trained on Code",
      "Citations": "7595.0",
      "Authors": "Mark Chen , Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger,  Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji,  Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, Wojciech Zaremba ",
      "Abstract": "We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Significant use,Discretionary",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "GPT-3 13B",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Codex was available via the OpenAI API from announcement: https://openai.com/index/openai-codex/\n\nIt is still available via the Research Access Program. https://x.com/OfficialLoganK/status/1638336152800206858",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ERNIE 3.0",
      "Organization": "Baidu",
      "Publication date": "2021-07-05",
      "Domain": "Language",
      "Task": "Language modeling,Language modeling/generation,Text classification,Question answering",
      "Parameters": "10000000000.0",
      "Parameters notes": "\"We trained the model with 10 billion parameters on a 4TB corpus consisting of plain texts and a large-scale knowledge graph.\"",
      "Training compute (FLOP)": "2.25e+22",
      "Training compute notes": "Section 3.3.3: \n\"\"The model is trained for a total of 375 billion tokens\"\n\nTotal compute approximated as 6*N*D",
      "Training dataset": "",
      "Training dataset size (gradients)": "375000000000",
      "Dataset size notes": "\"To ensure the success of the pre-training of ERNIE 3.0, we construct a large-scale, wide-variety and high-quality Chinese text corpora amounting to 4TB storage size in 11 different categories.\"\n\n1 GB ~ 167M chinese words",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2107.02137",
      "Reference": "ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation",
      "Citations": "552.0",
      "Authors": "Y Sun, S Wang, S Feng, S Ding, C Pang",
      "Abstract": "Pre-trained models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. Recent works such as T5 and GPT-3 have shown that scaling up pre-trained language models can improve their generalization abilities. Particularly, the GPT-3 model with 175 billion parameters shows its strong task-agnostic zero-shot/few-shot learning capabilities. Despite their success, these large-scale models are trained on plain texts without introducing knowledge such as linguistic knowledge and world knowledge. In addition, most large-scale models are trained in an auto-regressive way. As a result, this kind of traditional fine-tuning approach demonstrates relatively weak performance when solving downstream language understanding tasks. In order to solve the above problems, we propose a unified framework named ERNIE 3.0 for pre-training large-scale knowledge enhanced models. It fuses auto-regressive network and auto-encoding network, so that the trained model can be easily tailored for both natural language understanding and generation tasks with zero-shot learning, few-shot learning or fine-tuning. We trained the model with 10 billion parameters on a 4TB corpus consisting of plain texts and a large-scale knowledge graph. Empirical results show that the model outperforms the state-of-the-art models on 54 Chinese NLP tasks, and its English version achieves the first place on the SuperGLUE benchmark (July 3, 2021), surpassing the human performance by +0.8% (90.6% vs. 89.8%).",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"ERNIE 3.0 achieved new state-of-the-art results across 54 Chinese NLP tasks\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "384.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "39104.26710360624",
      "Compute cost notes": "",
      "Training power draw (W)": "232943.03314881725",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "apache 2.0 \ntrain code: https://github.com/PaddlePaddle/PaddleNLP/tree/develop/legacy/model_zoo/ernie-3.0#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83 ",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Adaptive Input Transformer + RD",
      "Organization": "Microsoft Research Asia,Soochow University",
      "Publication date": "2021-06-28",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "247000000.00000003",
      "Parameters notes": "",
      "Training compute (FLOP)": "8.58e+19",
      "Training compute notes": "\"We train Transformer model for 50k steps and Adaptive Input Transformer for 286k steps\"\n\nassuming 1 step took 1 second:\n\n125000000000000 FLOP / sec [assumed precision: bf16] * 286000 seconds * 8 GPUs * 0.3 [assumed utilization] = 8.58e+19 FLOP",
      "Training dataset": "",
      "Training dataset size (gradients)": "103000000",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2106.14448",
      "Reference": "R-Drop: Regularized Dropout for Neural Networks",
      "Citations": "507.0",
      "Authors": "Xiaobo Liang, Lijun Wu, Juntao Li, Yue Wang, Qi Meng, Tao Qin, Wei Chen, Min Zhang, Tie-Yan Liu",
      "Abstract": "Dropout is a powerful and widely used technique to regularize the training of deep neural networks. In this paper, we introduce a simple regularization strategy upon dropout in model training, namely R-Drop, which forces the output distributions of different sub models generated by dropout to be consistent with each other. Specifically, for each training sample, R-Drop minimizes the bidirectional KL-divergence between the output distributions of two sub models sampled by dropout. Theoretical analysis reveals that R-Drop reduces the freedom of the model parameters and complements dropout. Experiments on 5 widely used deep learning tasks (18 datasets in total), including neural machine translation, abstractive summarization, language understanding, language modeling, and image classification, show that R-Drop is universally effective. In particular, it yields substantial improvements when applied to fine-tune large-scale pre-trained models, e.g., ViT, RoBERTa-large, and BART, and achieves state-of-the-art (SOTA) performances with the vanilla Transformer model on WMT14 English\u2192German translation (30.91 BLEU) and WMT14 English\u2192French translation (43.95 BLEU), even surpassing models trained with extra large-scale data and expert-designed advanced variants of Transformer models. Our code is available at GitHub{\\url{this https URL}}.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "China,Taiwan",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"In particular, it yields substantial improvements when applied to fine-tune large-scale pre-trained models, e.g., ViT, RoBERTa-large, and BART, and achieves state-of-the-art (SOTA) performances with the vanilla Transformer model \"\n\nI don't see absolute SOTA claimes for this model",
      "Epochs": "",
      "Training time (hours)": "79.4",
      "Training time notes": "\" The training is on 8 Tesla V100 GPU cards\"\n286k steps ~ [assumption] 286000 seconds = 79.4 hours",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "8.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "4853.736425906205",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "train and inference code for translation models, MIT license. no weights\n\nhttps://github.com/dropreg/R-Drop/blob/main/fairseq_src/examples/translation_rdrop/README.md ",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Fold2Seq",
      "Organization": "IBM,Texas A&M",
      "Publication date": "2021-06-24",
      "Domain": "Biology",
      "Task": "Proteins,Protein generation,Protein inverse folding,Protein fold classification",
      "Parameters": "12427904.0",
      "Parameters notes": "Three components, a sequence encoder, fold encoder, and sequence decoder. (1) and (3) have similar architectures, a sequence embedder + transformer block (the encoder uses a transformer encoder, the decoder uses a decoder block). \n\nThe fold encoder adds 6 residual conv blocks and has a 3d positional encoder rather than sequence embedder. Each residual block has two 3D-convolutional layers (3\u00d73\u00d73) and batch normalization layers.\n\nEach transformer block has 4 layers and d = 256 latent dimensions.\n\nCalculations here: https://docs.google.com/document/d/1luTCTQLmfaBfmnjbsBpWuG51x_IwuiN7mqNoOXtoi8Y/edit?usp=sharing",
      "Training compute (FLOP)": "1.4e+17",
      "Training compute notes": "\"We train our model on 2 Tesla K80 GPUs, with batch size 128. In every training stage we train up to 200 epochs with an early stopping strategy based on the validation loss\"\nSee calculations here: https://docs.google.com/document/d/1luTCTQLmfaBfmnjbsBpWuG51x_IwuiN7mqNoOXtoi8Y/edit?usp=sharing\n\nBlock 1:\nCNN 1: 2*20*20*20*3*3*3*4*8=13824000\nCNN 2: 2*20*20*20*3*3*3*8*8=27648000\nBlock 2:\nCNN 1: 2*20*20*20*3*3*3*8*16=55296000\nCNN 2: 2*20*20*20*3*3*3*16*16=110592000\nBlock 3:\nCNN 1: 2*20*20*20*3*3*3*16*32=221184000\nCNN 2: 2*20*20*20*3*3*3*32*32=442368000\nBlock 4:\nCNN 1: 2*10*10*10*3*3*3*32*64=110592000\nCNN 2: 2*10*10*10*3*3*3*64*64=221184000\nBlock 5:\nCNN 1: 2*10*10*10*3*3*3*64*128=442368000\nCNN 2: 2*10*10*10*3*3*3*128*128=884736000\nBlock 6:\nCNN 1: 2*10*10*10*3*3*3*128*256=1769472000\nCNN 2: 2*10*10*10*3*3*3*256*256=3538944000\nCNN forward FLOP: 13824000+27648000+55296000+110592000+221184000+442368000+110592000+221184000+442368000+884736000+1769472000+3538944000=7838208000\nTraining FLOP: 45995*200*(3299020800+4074393600+7838208000)=1.4e17\n\n\nNote I make several assumptions about the CNN architecture which could change this value.",
      "Training dataset": "CATH",
      "Training dataset size (gradients)": "45995",
      "Dataset size notes": "\"Training set includes 45995 proteins belonging to a total of 971 folds\"",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/2106.13058",
      "Reference": "Fold2Seq: A Joint Sequence(1D)-Fold(3D) Embedding-based Generative Model for Protein Design",
      "Citations": "53.0",
      "Authors": "Yue Cao, Payel Das, Vijil Chenthamarakshan, Pin-Yu Chen, Igor Melnyk, Yang Shen",
      "Abstract": "Designing novel protein sequences for a desired 3D topological fold is a fundamental yet nontrivial task in protein engineering. Challenges exist due to the complex sequence\u2013fold relationship, as well as the difficulties to capture the diversity of the sequences (therefore structures and functions) within a fold. To overcome these challenges, we propose Fold2Seq, a novel transformer-based\ngenerative framework for designing protein sequences conditioned on a specific target fold. To model the complex sequence\u2013structure relationship, Fold2Seq jointly learns a sequence embedding using a transformer and a fold embedding from the density of secondary structural elements in 3D voxels. On test sets with single, high-resolution and complete structure inputs for individual folds, our experiments demonstrate improved or comparable performance of Fold2Seq in terms of speed, coverage, and reliability for sequence design, when compared to existing state-of-the-art methods that include datadriven deep generative models and physics-based RosettaDesign. The unique advantages of foldbased Fold2Seq, in comparison to a structurebased deep model and RosettaDesign, become more evident on three additional real-world challenges originating from low-quality, incomplete, or ambiguous input structures. Source code and data are available at https://github.com/IBM/fold2seq.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"On test sets with single, high-resolution and complete structure inputs for individual folds, our experiments demonstrate improved or comparable performance of Fold2Seq in terms of speed, coverage, and reliability for sequence design, when compared to existing state-of-the-art methods that include data-driven deep generative models and physics-based RosettaDesign.\" [Abstract]",
      "Epochs": "200.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA Tesla K80",
      "Hardware quantity": "2.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "1213.542200949413",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "128.0",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "",
      "Accessibility notes": "Apache 2.0 \n\nhttps://github.com/IBM/fold2seq",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "3030.5218027501137",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "EfficientNetV2-XL",
      "Organization": "Google,Google Brain",
      "Publication date": "2021-06-23",
      "Domain": "Vision",
      "Task": "Image classification,Neural Architecture Search - NAS",
      "Parameters": "208000000.0",
      "Parameters notes": "208M for XL version (Table 7, page 7)",
      "Training compute (FLOP)": "9.56e+19",
      "Training compute notes": "Table 7, page 7: 45 hours on 32 TPUv3 cores.\n\n\"Each v3 TPU chip contains two TensorCores.\"\nTPU performance per chip = 123e12 FLOP/s\n32 cores = 16 chips\n\n123e12 FLOP/s per chip * (32 cores / 2 cores per chip) * 45 hours * 3600 seconds/hour * 0.30 utilization = 9.56e19 FLOP\n\nhttps://www.wolframalpha.com/input?i=123+terahertz+*+16+*+45+hours+*+0.3",
      "Training dataset": "ImageNet21k,ILSVRC 2012 subset of ImageNet",
      "Training dataset size (gradients)": "14180000,14180000",
      "Dataset size notes": "\"ImageNet21k (Russakovsky et al., 2015) contains about 13M training images with 21,841 classes. The original ImageNet21k doesn\u2019t have train/eval split, so we reserve randomly picked 100,000 images as validation set and use the remaining as training set...\nAfter pretrained on ImageNet21k, each model is finetuned on ILSVRC2012 for 15 epochs using cosine learning rate decay.\"\n\n12.9M + 1.28M ~= 14,180,000",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2104.00298",
      "Reference": "EfficientNetV2: Smaller Models and Faster Training",
      "Citations": "3637.0",
      "Authors": "Mingxing Tan, Quoc V. Le",
      "Abstract": "This paper introduces EfficientNetV2, a new family of convolutional networks that have faster training speed and better parameter efficiency than previous models. To develop this family of models, we use a combination of training-aware neural architecture search and scaling, to jointly optimize training speed and parameter efficiency. The models were searched from the search space enriched with new ops such as Fused-MBConv. Our experiments show that EfficientNetV2 models train much faster than state-of-the-art models while being up to 6.8x smaller.\nOur training can be further sped up by progressively increasing the image size during training, but it often causes a drop in accuracy. To compensate for this accuracy drop, we propose to adaptively adjust regularization (e.g., dropout and data augmentation) as well, such that we can achieve both fast training and good accuracy.\nWith progressive learning, our EfficientNetV2 significantly outperforms previous models on ImageNet and CIFAR/Cars/Flowers datasets. By pretraining on the same ImageNet21k, our EfficientNetV2 achieves 87.3% top-1 accuracy on ImageNet ILSVRC2012, outperforming the recent ViT by 2.0% accuracy while training 5x-11x faster using the same computing resources. Code will be available at this https URL.",
      "Organization categorization": "Industry,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\"EfficientNetV2 achieves 87.3% top-1 accuracy on ImageNet ILSVRC2012, outperforming the recent ViT by 2.0% accuracy while \n training 5x-11x faster using the same computing resources.\"",
      "Epochs": "30.0",
      "Training time (hours)": "45.0",
      "Training time notes": "Table 7",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "16.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "104.34013401561589",
      "Compute cost notes": "",
      "Training power draw (W)": "14562.830712865913",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "4096.0",
      "Batch size notes": "\"Each model is trained for 350 epochs with total batch size 4096\"",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "code and weights: https://github.com/google/automl/tree/master/efficientnetv2\n\nApache-2.0 license",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "StyleGAN3-T",
      "Organization": "NVIDIA,Aalto University",
      "Publication date": "2021-06-21",
      "Domain": "Image generation",
      "Task": "Image generation",
      "Parameters": "2230000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "1.70208e+21",
      "Training compute notes": "125000000000000 FLOP / GPU / sec [V100] * 8 GPUs * 1576 hours [see training time notes] * 3600 sec / hour * 0.3 [assumed utilization] = 1.70208e+21 FLOP",
      "Training dataset": "FFHQ,METFACES,AFHQ",
      "Training dataset size (gradients)": "50000000",
      "Dataset size notes": "\"We used 8 GPUs for all our training runs and continued the training until the discriminator had seen a total of 25M real images when training from scratch, or 5M images when using transfer learning\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2106.12423",
      "Reference": "Alias-Free Generative Adversarial Networks",
      "Citations": "2031.0",
      "Authors": "Tero Karras, Miika Aittala, Samuli Laine, Erik H\u00e4rk\u00f6nen, Janne Hellsten, Jaakko Lehtinen, Timo Aila",
      "Abstract": "We observe that despite their hierarchical convolutional nature, the synthesis process of typical generative adversarial networks depends on absolute pixel coordinates in an unhealthy manner. This manifests itself as, e.g., detail appearing to be glued to image coordinates instead of the surfaces of depicted objects. We trace the root cause to careless signal processing that causes aliasing in the generator network. Interpreting all signals in the network as continuous, we derive generally applicable, small architectural changes that guarantee that unwanted information cannot leak into the hierarchical synthesis process. The resulting networks match the FID of StyleGAN2 but differ dramatically in their internal representations, and they are fully equivariant to translation and rotation even at subpixel scales. Our results pave the way for generative models better suited for video and animation.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,Finland",
      "Notability criteria": "Historical significance,Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "1576.0",
      "Training time notes": "\" This entire project consumed 92 GPU years and 225 MWh of electricity on an in-house cluster of NVIDIA V100s\"\n\n92 GPU years = 805920 GPU-hours\n\n\"In FFHQ (1024\u00d71024) the three generators had\n30.0M, 22.3M and 15.8M parameters, while the training times were 1106, 1576 (+42%) and 2248 (+103%) GPU hours.\"",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "8.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "4854.493112492733",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "32.0",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "",
      "Accessibility notes": "NVIDIA Source Code License\nhttps://github.com/NVlabs/stylegan3",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "StyleGAN3-R",
      "Organization": "NVIDIA,Aalto University",
      "Publication date": "2021-06-21",
      "Domain": "Image generation",
      "Task": "Image generation",
      "Parameters": "1580000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "2.42784e+21",
      "Training compute notes": "125000000000000 FLOP / GPU / sec [V100] * 8 GPUs * 2248 hours [see training time notes] * 3600 sec / hour * 0.3 [assumed utilization] = 2.42784e+21 FLOP",
      "Training dataset": "FFHQ,METFACES,AFHQ",
      "Training dataset size (gradients)": "50000000",
      "Dataset size notes": "\"We used 8 GPUs for all our training runs and continued the training until the discriminator had seen a total of 25M real images when training from scratch, or 5M images when using transfer learning\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/pdf/2106.12423",
      "Reference": "Alias-Free Generative Adversarial Networks",
      "Citations": "2031.0",
      "Authors": "Tero Karras, Miika Aittala, Samuli Laine, Erik H\u00e4rk\u00f6nen, Janne Hellsten, Jaakko Lehtinen, Timo Aila",
      "Abstract": "We observe that despite their hierarchical convolutional nature, the synthesis process of typical generative adversarial networks depends on absolute pixel coordinates in an unhealthy manner. This manifests itself as, e.g., detail appearing to be glued to image coordinates instead of the surfaces of depicted objects. We trace the root cause to careless signal processing that causes aliasing in the generator network. Interpreting all signals in the network as continuous, we derive generally applicable, small architectural changes that guarantee that unwanted information cannot leak into the hierarchical synthesis process. The resulting networks match the FID of StyleGAN2 but differ dramatically in their internal representations, and they are fully equivariant to translation and rotation even at subpixel scales. Our results pave the way for generative models better suited for video and animation.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,Finland",
      "Notability criteria": "Historical significance,Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "2248.0",
      "Training time notes": "\" This entire project consumed 92 GPU years and 225 MWh of electricity on an in-house cluster of NVIDIA V100s\"\n\n92 GPU years = 805920 GPU-hours\n\n\"In FFHQ (1024\u00d71024) the three generators had\n30.0M, 22.3M and 15.8M parameters, while the training times were 1106, 1576 (+42%) and 2248 (+103%) GPU hours.\"",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "8.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "4854.493112492733",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "32.0",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "",
      "Accessibility notes": "NVIDIA Source Code License\nhttps://github.com/NVlabs/stylegan3",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Denoising Diffusion Probabilistic Models (LSUN Bedroom)",
      "Organization": "University of California (UC) Berkeley",
      "Publication date": "2021-06-11",
      "Domain": "Vision",
      "Task": "Image generation",
      "Parameters": "256000000.0",
      "Parameters notes": "Appendix B: \n\" Our CIFAR10 model has 35.7 million parameters, and our LSUN and\nCelebA-HQ models have 114 million parameters. We also trained a larger variant of the LSUN Bedroom model with approximately 256 million parameters by increasing filter count.\"",
      "Training compute (FLOP)": "7.840125000000001e+19",
      "Training compute notes": "Numbers in Appendix B\n\"Our CelebA-HQ/LSUN (2562) models train at 2.2 steps per second at batch size 64, [...] The larger LSUN Bedroom model was trained for 1.15M steps.\"\n10.6h for the CIFAR model (batch size 128, 21 step/s)\n2.2 step/s for the LSUN model, 1.15M steps so 702.8 hours\n\n1 step takes 1/2.2 =0.4545 seconds\n1.15M steps * 0.4545 seconds = 522675 seconds = 145 hours\n\nThis is for TPUv3-8's, which seems to mean 8 cores (standard chip is 125 teraflop/s for 2 cores) -> 4 chips\nhttps://cloud.google.com/tpu/docs/regions-zones\n\n1.25E14 FLOP/s * (4 chips) * 522675 seconds * 0.3 = 78401250000000010000",
      "Training dataset": "LSUN Bedroom",
      "Training dataset size (gradients)": "596320321536",
      "Dataset size notes": "\"We trained on CelebA-HQ for 0.5M steps, LSUN Bedroom for 2.4M steps, LSUN Cat for 1.8M steps, and LSUN Church for 1.2M steps.\"\n\n\"The CelebA-HQ dataset is a high-quality version of CelebA that consists of 30,000 images at 1024\u00d71024 resolution.\"\nhttps://paperswithcode.com/dataset/celeba-hq\n\nLSUN bedroom has 3,033,042 examples. LSUN cat has 1,657,266 examples. LSUN church has 126,227 examples.\nhttps://www.tensorflow.org/datasets/catalog/lsun\n",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2006.11239",
      "Reference": "Denoising Diffusion Probabilistic Models",
      "Citations": "25266.0",
      "Authors": "Jonathan Ho, Ajay Jain, Pieter Abbeel",
      "Abstract": "We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at this https URL",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "Novel approach to image synthesis that yields SOTA results on datasets like CIFAR-10\n\nAbstract: \n\"On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. \"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "436.308484475363",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "https://github.com/hojonathanho/diffusion\n\neverything is openly avaiable but no license or terms of use information\n\ntrain code: https://github.com/hojonathanho/diffusion/blob/master/scripts/run_lsun.py ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ALIGN",
      "Organization": "Google Research",
      "Publication date": "2021-06-11",
      "Domain": "Multimodal,Vision,Language",
      "Task": "Representation learning,Image classification,Image representation",
      "Parameters": "820000000.0",
      "Parameters notes": "From author communication\n\n480M (image tower) + 340 M (text tower)",
      "Training compute (FLOP)": "2.598670000001e+22",
      "Training compute notes": "From author communication\n14.82K TPUv3 core-days\nPrecision: bfloat16\n\nEstimation\nTPUv3 at float16: 123 TFLOPS/chip\n\n123*10^12 TFLOPS/chip * (1 chip / 2 cores) * 14820 TPU core-days * 86400 s/day * 33% utilization = 2.599*10^22 FLOP\nhttps://www.wolframalpha.com/input?i=14820+days+*+123+teraFLOPS+%2F+2+*+0.33",
      "Training dataset": "Conceptual Captions (CC3M),FIT400M",
      "Training dataset size (gradients)": "1800000000",
      "Dataset size notes": "Dataset contains 1.8B image-text pairs, then some duplicates are removed.",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2102.05918",
      "Reference": "Scaling up visual and vision-language representation learning with noisy text supervision",
      "Citations": "4822.0",
      "Authors": "Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig",
      "Abstract": "Pre-trained representations are becoming crucial for many NLP and perception tasks. While representation learning in NLP has transitioned to training on raw text without human annotations, visual and vision-language representations still rely heavily on curated training datasets that are expensive or require expert knowledge. For vision applications, representations are mostly learned using datasets with explicit class labels such as ImageNet or OpenImages. For vision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all involve a non-trivial data collection (and cleaning) process. This costly curation process limits the size of datasets and hence hinders the scaling of trained models. In this paper, we leverage a noisy dataset of over one billion image alt-text pairs, obtained without expensive filtering or post-processing steps in the Conceptual Captions dataset. A simple dual-encoder architecture learns to align visual and language representations of the image and text pairs using a contrastive loss. We show that the scale of our corpus can make up for its noise and leads to state-of-the-art representations even with such a simple learning scheme. Our visual representation achieves strong performance when transferred to classification tasks such as ImageNet and VTAB. The aligned visual and language representations enables zero-shot image classification and also set new state-of-the-art results on Flickr30K and MSCOCO image-text retrieval benchmarks, even when compared with more sophisticated cross-attention models. The representations also enable cross-modality search with complex text and text + image queries.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\"The aligned visual and language representations... set new state-of-the-art results on Flickr30K and MSCOCO image-text retrieval benchmarks\"",
      "Epochs": "",
      "Training time (hours)": "347.3",
      "Training time notes": "14820 TPU core-hours / 1024 TPU cores = 347.3 hours",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "512.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "32852.91660437908",
      "Compute cost notes": "",
      "Training power draw (W)": "466135.13260508416",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "16384.0",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DeBERTa",
      "Organization": "Microsoft",
      "Publication date": "2021-06-10",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering",
      "Parameters": "1500000000.0",
      "Parameters notes": "\"...we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters\"\n\nOther versions are smaller and use a smaller pre-training dataset. These are distinguished in the paper (e.g. DeBERTa1.5B is the version of DeBERTa with 1.5 billion parameters).",
      "Training compute (FLOP)": "2.588e+22",
      "Training compute notes": "Table 8: 16 DGX-2 nodes (x16 V100s each) for 30 days\n16 * 16 * 1.3e14 * 30 * 24 * 3600 * 0.3 = 2.588e22",
      "Training dataset": "Wikipedia,CC-Stories,OPENWEBTEXT,BookCorpus (BooksCorpus, Toronto Book Corpus)",
      "Training dataset size (gradients)": "20800000000",
      "Dataset size notes": "\" DeBERTa is pretrained on 78G training data\"\n\n1GB ~ 200M words",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2006.03654",
      "Reference": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention",
      "Citations": "3349.0",
      "Authors": "Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen",
      "Abstract": "Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a new virtual adversarial training method is used for fine-tuning to improve models' generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understanding (NLU) and natural langauge generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). Notably, we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters. The significant performance boost makes the single DeBERTa model surpass the human performance on the SuperGLUE benchmark (Wang et al., 2019a) for the first time in terms of macro-average score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the SuperGLUE leaderboard as of January 6, 2021, out performing the human baseline by a decent margin (90.3 versus 89.8).",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\"DeBERTa significantly outperforms all existing PLMs of similar size on MNLI and creates a new state of the art\"",
      "Epochs": "49.2",
      "Training time (hours)": "720.0",
      "Training time notes": "30 days (Table 8)",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "256.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "6682.2289986716",
      "Compute cost notes": "",
      "Training power draw (W)": "155381.83775233384",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT\n\nhttps://github.com/microsoft/DeBERTa\n\npretrain code: https://github.com/microsoft/DeBERTa/tree/master/experiments/language_model ",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "EMDR",
      "Organization": "Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),McGill University,DeepMind",
      "Publication date": "2021-06-09",
      "Domain": "Language",
      "Task": "Question answering",
      "Parameters": "440000000.0",
      "Parameters notes": "Table 2",
      "Training compute (FLOP)": "1.91e+21",
      "Training compute notes": "\"We run all of our experiments on a machine with 96 CPUs, 1.3TB physical memory, and 16 A100 GPUs. We use PyTorch (Paszke et al., 2019) to implement our proposed model. With this hardware setup, our experiments on NQ and TriviaQA took approximately 25 hours to complete, while experiments on WebQ took roughly 8 hours to complete. Before supervised training, we also perform a one-time unsupervised MSS pre-training for 82,000 steps that took roughly 1 week.\"\n\n1 week + 25 hours * 16 A100s\n= ~193 * 16 A100-hours\n= 193 * 16 * 3600 * 312 trillion * 0.3 = 1.04e21\n\nAdditionally, the model uses BERT, ICT, and T5 models. These required:\n- BERT: 6 * 110M parameters * (1M * 256 * 256) inputs = 4.33e19 FLOP\n- ICT: 6 * 220M parameters * (100k * 4096 * 256) inputs = 1.38e20 FLOP\n- T5: 6 * 220M parameters * (1M * 2048 * 256) inputs = 6.92e20 FLOP\n\nTotal: 1.04e21 + 4.33e19 + 1.38e20 + 6.92e20 = 1.91e21",
      "Training dataset": "Wikipedia,NQ (Natural Questions),TriviaQA",
      "Training dataset size (gradients)": "171600000000",
      "Dataset size notes": "At the time of publication there were about 4B words (5.3B tokens) on English Wikipedia: https://en.wikipedia.org/wiki/Wikipedia:Size_of_Wikipedia#Yearly_statistics\nBookCorpus has about 1B words (1.3B tokens), C4 has about 156B tokens, and OpenWebText has about 9B tokens.\n\nFrom Table 6, it looks like all datasets were trained on for over one epoch.\n\nBERT: 1M steps, batches of 256, sequence length 256 = 65.5B tokens vs 6.6B in Wikipedia + BookCorpus\nICT: 100k steps, batches of 4096, sequence length 256 = 104.9B tokens vs 5.3B in Wikipedia\nT5: 1M steps, batches of 2048, sequence length 256 = 524.3B tokens vs 170.3B tokens in C4 + Wikipedia + OpenWebText\n\nTotal tokens: 171.6 billion\n\nSome tokens were probably seen more times than others, but overall this corresponds to 4.05 epochs on the pre-training data. ",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2106.05346v2",
      "Reference": "End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering",
      "Citations": "182.0",
      "Authors": "Devendra Singh Sachan, Siva Reddy, William Hamilton, Chris Dyer, Dani Yogatama",
      "Abstract": "We present an end-to-end differentiable training method for retrieval-augmented open-domain question answering systems that combine information from multiple retrieved documents when generating answers. We model retrieval decisions as latent variables over sets of relevant documents. Since marginalizing over sets of retrieved documents is computationally hard, we approximate this using an expectation-maximization algorithm. We iteratively estimate the value of our latent variable (the set of relevant documents for a given question) and then use this estimate to update the retriever and reader parameters. We hypothesize that such end-to-end training allows training signals to flow to the reader and then to the retriever better than staged-wise training. This results in a retriever that is able to select more relevant documents for a question and a reader that is trained on more accurate documents to generate an answer. Experiments on three benchmark datasets demonstrate that our proposed method outperforms all existing approaches of comparable size by 2-3% absolute exact match points, achieving new state-of-the-art results. Our results also demonstrate the feasibility of learning to retrieve to improve answer generation without explicit supervision of retrieval decisions.",
      "Organization categorization": "Academia,Academia,Industry",
      "Country (of organization)": "Canada,Canada,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Experiments on three benchmark datasets demonstrate that our proposed method outperforms all existing approaches of comparable size by 2-3% absolute exact match points, achieving new state-of-the-art results.\"",
      "Epochs": "4.05",
      "Training time (hours)": "355.0",
      "Training time notes": "\"We run all of our experiments on a machine with 96 CPUs, 1.3TB physical memory, and 16 A100 GPUs [...] our experiments on NQ and TriviaQA took approximately 25 hours to complete, while experiments on WebQ took roughly 8 hours to complete. Before supervised training, we also perform a one-time unsupervised MSS pre-training for 82,000 steps that took roughly 1 week\"\n\nAdditionally, they pre-trained BERT, ICT, and T5 models, which took a combined 8.733e20 FLOPs. On 16 A100s at 0.3 utilization, that would have taken approximately 162 hours.\n\nSo total time for the largest experiment (NQ or TriviaQA) is around:\n25 + 168 + 162 = 355",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "2773.3124307816734",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "https://github.com/DevSinghSachan/emdr2\n\ntraining scripts: https://github.com/DevSinghSachan/emdr2/tree/main/examples ",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "CoAtNet",
      "Organization": "Google,Google Research,Google Brain",
      "Publication date": "2021-06-09",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "2440000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "4.27e+22",
      "Training compute notes": "20.1K TPU-v3 core-days\n\nTPUs have two cores per chip, and a chip is 123 teraflop/s \nhttps://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu_v3\n\n123 teraflop/s * 20100/2 * 24 * 3600 * 0.4 (utilization assumption for non-language models) = 4.27e22",
      "Training dataset": "JFT-3B",
      "Training dataset size (gradients)": "88779000000000",
      "Dataset size notes": "Used JFT-3B (3 billion images), but not stated for how many epochs. \n\nBased on GPU time, training took 4.27e+22 FLOPs. Table 5 indicates 2.586e12 FLOPs per image. Since training is roughly 3x the FLOP cost of inference, implies inference on full training set took 1.42e22 FLOP\nThen # images trained over is around 1.42e22 / 2.586e12 = 5,491,105,955\n\nSo probably ~1.83 epochs on 3B images",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2106.04803v2",
      "Reference": "CoAtNet: Marrying Convolution and Attention\nfor All Data Sizes",
      "Citations": "1445.0",
      "Authors": "Zihang Dai, Hanxiao Liu, Quoc V. Le, Mingxing Tan",
      "Abstract": "Transformers have attracted increasing interests in computer vision, but they still fall behind state-of-the-art convolutional networks. In this work, we show that while Transformers tend to have larger model capacity, their generalization can be worse than convolutional networks due to the lack of the right inductive bias. To effectively combine the strengths from both architectures, we present CoAtNets (pronounced \u201ccoat\u201d nets), a family of hybrid models built from two key insights: (1) depthwise Convolution and self-Attention can be naturally unified via simple relative attention; (2) vertically stacking convolution layers and attention layers in a principled way is surprisingly effective in improving generalization, capacity and efficiency. Experiments show that our CoAtNets achieve state-of-the-art performance under different resource constraints across various datasets: Without extra data, CoAtNet achieves 86.0% ImageNet top-1 accuracy; When pre-trained with 13M images from ImageNet-21K, our CoAtNet achieves 88.56% top-1 accuracy, matching ViT-huge pre-trained with 300M images from JFT-300M while using 23x less data; Notably, when we further scale up CoAtNet with JFT-3B, it achieves 90.88% top-1 accuracy on ImageNet, establishing a new state-of-the-art result.",
      "Organization categorization": "Industry,Industry,Industry",
      "Country (of organization)": "United States of America,United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Notably, when we further scale up CoAtNet with JFT-3B, it achieves 90.88% top-1 accuracy on ImageNet, establishing a new state-of-the-art result.\"",
      "Epochs": "1.83",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "1887.1635925610951",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ViT-G/14",
      "Organization": "Google Brain,Google Research",
      "Publication date": "2021-06-08",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "1843000000.0",
      "Parameters notes": "Table 2 of paper",
      "Training compute (FLOP)": "5.85e+22",
      "Training compute notes": "Digitizing Figure 9 indicates training used 27,200 TPUv3 core-days. TPUv3 is 123 teraflop/s per chip, 2 cores per chip.\n\n1.23e14 * (1/2) * 27,200 * 24 * 3600 * 0.4 = 5.78e22\n\nAlternatively, Table 2 indicates 965.3e9 FLOPs per forward pass on a 224^2 image. Table 4 indicates 5 million steps at a (normalized) batch size of 4096, and total flops including backward pass would be 3x the FLOPs from forward passes alone, so we get:\n4096 * 5e6 * 965.3e9 * 3 = 5.93e22\n\n(Note that actual batch size appears to have been 32,768)\n\nGeometric mean: sqrt(5.78e22*5.93e22) = 5.85e22\n\nHowever note that this leaderboard claims ViT-G/14 took 34 PF-days, or 2.94e21 FLOPs: https://web.archive.org/web/20211218185755/https://lair.lighton.ai/akronomicon/",
      "Training dataset": "JFT-3B,ImageNet",
      "Training dataset size (gradients)": "3000000000",
      "Dataset size notes": "\"For this study, we use the proprietary JFT-3B dataset, a larger version of the JFT-300M dataset used in many previous works on large-scale computer vision models [31, 18, 11]. This dataset consists of nearly 3 billion images, annotated with a class-hierarchy of around 30k labels via a semi-automatic\npipeline\"\n\nEpochs: 5M steps (Table 11) * 32768 (batch size) / 3B = 54.6 epochs",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2106.04560",
      "Reference": "Scaling Vision Transformers",
      "Citations": "1298.0",
      "Authors": "X Zhai, A Kolesnikov, N Houlsby, L Beyer",
      "Abstract": "Attention-based neural networks such as the Vision Transformer (ViT) have recently attained state-of-the-art results on many computer vision benchmarks. Scale is a primary ingredient in attaining excellent results, therefore, understanding a model's scaling properties is a key to designing future generations effectively. While the laws for scaling Transformer language models have been studied, it is unknown how Vision Transformers scale. To address this, we scale ViT models and data, both up and down, and characterize the relationships between error rate, data, and compute. Along the way, we refine the architecture and training of ViT, reducing memory consumption and increasing accuracy of the resulting models. As a result, we successfully train a ViT model with two billion parameters, which attains a new state-of-the-art on ImageNet of 90.45% top-1 accuracy. The model also performs well for few-shot transfer, for example, reaching 84.86% top-1 accuracy on ImageNet with only 10 examples per class.",
      "Organization categorization": "Industry,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"we successfully train a ViT model with two billion parameters, which attains a new state-of-the-art on ImageNet of 90.45% top-1 accuracy\"",
      "Epochs": "54.6",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "2048.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "3847.8613922131217",
      "Compute cost notes": "",
      "Training power draw (W)": "1864665.1010174435",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "32768.0",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "About the weights: https://twitter.com/giffmana/status/1402507421029916672\n\nAbout the code: Apache 2.0\nhttps://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/scaling_laws/train_vit_g.py",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ByT5-XXL",
      "Organization": "Google,Google Research",
      "Publication date": "2021-05-28",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "12900000000.0",
      "Parameters notes": "12.9B, from Table 1",
      "Training compute (FLOP)": "8.1e+22",
      "Training compute notes": "\"Like mT5, we set our sequence length to 1024 (bytes rather than tokens), and train for 1 million steps over batches of 2^20 tokens.\"\n\n12.9 billion * 1 million * 2^20 * 6 = ~8.1e22",
      "Training dataset": "mC4",
      "Training dataset size (gradients)": "1048576000000",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2105.13626",
      "Reference": "ByT5: Towards a token-free future with pre-trained byte-to-byte models",
      "Citations": "601.0",
      "Authors": "Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel",
      "Abstract": "Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units. By comparison, token-free models that operate directly on raw text (bytes or characters) have many benefits: they can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.",
      "Organization categorization": "Industry,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"On the most realistic in-language setting, where some gold training data is available in all languages, ByT5 surpasses the previous state-of-art mT5 on all tasks and model sizes\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "Table 9 indicates pretraining completes 25 sequences per second on a TPUv3-64 device. \n\n\"we set our sequence length to 1024 (bytes rather than tokens), and train for 1 million steps over batches of 2^20 tokens.\" \n\nSo 1024 sequences per step * 1M steps = 1.024 billion sequences\n1.024 B / 25 = 40.96M seconds = 11378 hours or 474 days. This seems implausible, so probably they just used a bigger TPU slice for the full training, but this is not indicated.",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "92453.37635669747",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "1048576.0",
      "Batch size notes": "\"Like mT5, we set our sequence length to 1024 (bytes rather than tokens), and train for 1 million steps over batches of 2^20 tokens\"",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache:\nhttps://github.com/google-research/byt5/blob/master/LICENSE\n\nsee mC4 notes for data accessibility\n\ntraining script: https://github.com/google-research/byt5/blob/master/README.md ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Transformer local-attention (NesT-B)",
      "Organization": "Google Cloud,Google Research",
      "Publication date": "2021-05-26",
      "Domain": "Vision",
      "Task": "Image classification,Image generation",
      "Parameters": "90100000.0",
      "Parameters notes": "Table A2, NesT-B is the largest size.",
      "Training compute (FLOP)": "2.40576e+19",
      "Training compute notes": "17.9 GFLOPS per forward pass\n300 epochs\n1.28M training examples\n3.5 f_to_b pass ratio\n(From Imagenet paper-data, Besiroglu et al., forthcoming) \n\n17.9e9 FLOP *300 epoch *1.28M images *3.5 forward-backward-ratio = 24057600000000000000",
      "Training dataset": "ImageNet-1k",
      "Training dataset size (gradients)": "1280000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2105.12723v4",
      "Reference": "Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding",
      "Citations": "5734.0",
      "Authors": "Zizhao Zhang, Han Zhang, Long Zhao, Ting Chen, Sercan Ar\u0131k, Tomas Pfister",
      "Abstract": "Hierarchical structures are popular in recent vision transformers, however, they require sophisticated designs and massive datasets to work well. In this paper, we explore the idea of nesting basic local transformers on non-overlapping image blocks and aggregating them in a hierarchical way. We find that the block aggregation function plays a critical role in enabling cross-block non-local information communication. This observation leads us to design a simplified architecture that requires minor code changes upon the original vision transformer. The benefits of the proposed judiciously-selected design are threefold: (1) NesT converges faster and requires much less training data to achieve good generalization on both ImageNet and small datasets like CIFAR; (2) when extending our key ideas to image generation, NesT leads to a strong decoder that is 8\u00d7 faster than previous transformer-based generators; and (3) we show that decoupling the feature learning and abstraction processes via this nested hierarchy in our design enables constructing a novel method (named GradCAT) for visually interpreting the learned model. Source code is available this https URL.",
      "Organization categorization": "Industry,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache-2.0 license, includes train code and evaluation\nhttps://github.com/google-research/nested-transformer",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "CogView",
      "Organization": "Tsinghua University,Alibaba DAMO Academy",
      "Publication date": "2021-05-26",
      "Domain": "Image generation",
      "Task": "Text-to-image,Image generation",
      "Parameters": "4000000000.0",
      "Parameters notes": "\"We propose CogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to advance this problem.\"",
      "Training compute (FLOP)": "2.68e+22",
      "Training compute notes": "source: https://lair.lighton.ai/akronomicon/\narchived: https://github.com/lightonai/akronomicon/tree/main/akrodb",
      "Training dataset": "WuDao Corpora",
      "Training dataset size (gradients)": "964800000000",
      "Dataset size notes": "\"We collected about 30 million text-image pairs from multiple channels, and built a 2.5TB new dataset (after tokenization, the size becomes about 250GB).\"\n\n250GB * (1 word / 5 bytes) = 50 billion words or 67 billion tokens\n\nSo 30M text-image pairs and 50 billion words",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2105.13290",
      "Reference": "CogView: Mastering Text-to-Image Generation via Transformers",
      "Citations": "913.0",
      "Authors": "Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, Jie Tang",
      "Abstract": "Text-to-Image generation in the general domain has long been an open problem, which requires both a powerful generative model and cross-modal understanding. We propose CogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to advance this problem. We also demonstrate the finetuning strategies for various downstream tasks, e.g. style learning, super-resolution, text-image ranking and fashion design, and methods to stabilize pretraining, e.g. eliminating NaN losses. CogView achieves the state-of-the-art FID on the blurred MS COCO dataset, outperforming previous GAN-based models and a recent similar work DALL-E.",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "China,China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"CogView achieves the state-of-the-art FID on the blurred MS COCO dataset, outperforming previous GAN-based models and a recent similar work DALL-E\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA Tesla V100 DGXS 16 GB",
      "Hardware quantity": "512.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "60071.706664791694",
      "Compute cost notes": "",
      "Training power draw (W)": "259056.25043301377",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache 2 license\n\nhttps://github.com/THUDM/CogView\n\ntrain script: https://github.com/THUDM/CogView/blob/main/scripts/pretrain_single_node.sh ",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "MedBERT",
      "Organization": "Peng Cheng Laboratory,University of Texas at Houston",
      "Publication date": "2021-05-20",
      "Domain": "Medicine",
      "Task": "Medical diagnosis,Text classification,Prediction of hospital stay duration,Prediction of diabetic heart failure (DHF),Prediction of onset of pancreatic cancer (PaCa)",
      "Parameters": "17000000.0",
      "Parameters notes": "17M from \"This is possibly due to the fact that the untrained Med-BERT is an over-parameterized model (around 17 million parameters) with a huge\nnumber of configurations, so it might overfit to the training data\"",
      "Training compute (FLOP)": "9.47e+18",
      "Training compute notes": "flops = (1) * (3.13e13) * (24*7 * 3600) * (0.5) = 9.47e18\n(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)\nI assume higher utilization rate, because only 1 GPU is used.\nCitation from the text:\n\"We used a single Nvidia Tesla V100GPU of 32 GB graphics memory capacity, and we trained the model for a week for more than 45 million steps, for which each step consists of 32 patients (batch size).\" - page 11\n\nNote that public code appears not to make use of the tensor core speed up, thus I use 3.13e13 FLOP/sec",
      "Training dataset": "Cerner Health Facts",
      "Training dataset size (gradients)": "14587212800",
      "Dataset size notes": "data about 28M patients\n\"Our pretraining cohort for Med-BERT is consisting of 28 million\npatients extracted from Cerner\"",
      "Confidence": "Likely",
      "Link": "https://www.nature.com/articles/s41746-021-00455-y",
      "Reference": "Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction",
      "Citations": "835.0",
      "Authors": "Laila Rasmy, Yang Xiang, Ziqian Xie, Cui Tao, Degui Zhi",
      "Abstract": "Deep learning (DL)-based predictive models from electronic health records (EHRs) deliver impressive performance in many clinical tasks. Large training cohorts, however, are often required by these models to achieve high accuracy, hindering the adoption of DL-based models in scenarios with limited training data. Recently, bidirectional encoder representations from transformers (BERT) and related models have achieved tremendous successes in the natural language processing domain. The pretraining of BERT on a very large training corpus generates contextualized embeddings that can boost the performance of models trained on smaller datasets. Inspired by BERT, we propose Med-BERT, which adapts the BERT framework originally developed for the text domain to the structured EHR domain. Med-BERT is a contextualized embedding model pretrained on a structured EHR dataset of 28,490,650 patients. Fine-tuning experiments showed that Med-BERT substantially improves the prediction accuracy, boosting the area under the receiver operating characteristics curve (AUC) by 1.21\u20136.14% in two disease prediction tasks from two clinical databases. In particular, pretrained Med-BERT obtains promising performances on tasks with small fine-tuning training sets and can boost the AUC by more than 20% or obtain an AUC as high as a model trained on a training set ten times larger, compared with deep learning models without Med-BERT. We believe that Med-BERT will benefit disease prediction studies with small local training datasets, reduce data collection expenses, and accelerate the pace of artificial intelligence aided healthcare.",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "China,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"This work is the first demonstration of significantly boosted performance over state-of-the-art methods on multiple clinical tasks with phenotyped cohorts.\"\n\nTable 4",
      "Epochs": "50.5",
      "Training time (hours)": "168.0",
      "Training time notes": "\"We used a single Nvidia Tesla V100GPU of 32 GB graphics memory capacity, and we trained the model for a week for more than 45 million steps, for which each step consists of 32 patients (batch size).\" - page 11",
      "Training hardware": "NVIDIA Tesla V100 DGXS 32 GB",
      "Hardware quantity": "1.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "62.48645493610712",
      "Compute cost notes": "",
      "Training power draw (W)": "278.0422249147422",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache 2\nhttps://github.com/ZhiGroup/Med-BERT\ntraining guide: https://github.com/ZhiGroup/Med-BERT/tree/master/Pretraining%20Code \n\n\"Initially we really hoped to share our models but unfortunately, the pre-trained models are no longer sharable. According to SBMI Data Service Office: \"Under the terms of our contracts with data vendors, we are not permitted to share any of the data utilized in our publications, as well as large models derived from those data.\"",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ADM",
      "Organization": "OpenAI",
      "Publication date": "2021-05-11",
      "Domain": "Image generation",
      "Task": "Image generation,Text-to-image",
      "Parameters": "559000000.0",
      "Parameters notes": "Largest model is denoted ImageNet 512, has 559M parameters",
      "Training compute (FLOP)": "6.2e+21",
      "Training compute notes": "Largest run with their architecture improvements is the ImageNet 512 variant. Table 7 suggests utilization is around 30% for largest models (though we only see 256 x 256 and 128 -> 512)\n\nTable 10: ImageNet 512 variant took 1914 V100-days of training\n125e12 FLOP/sec * 1914 days * 24 h/day * 3600 sec/h * 0.3 = 6.2e21",
      "Training dataset": "LSUN,ILSVRC 2012 subset of ImageNet",
      "Training dataset size (gradients)": "130191200000000",
      "Dataset size notes": "Biggest models are trained on ImageNet 512x512. ImageNet ILSVRC has 1,281,167 images in the training set, but it is possible some were filtered due to size.\n\nNote that a smaller model was trained on LSUN {bedroom, horse, cat}, which forms a larger dataset:\n3,033,042 + 2,000,340 + 1,657,266 = 6,690,648 images\n\nEpochs \u2248 (1,940,000 * 256) / 1,300,000 \u2248 381 epochs",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2105.05233",
      "Reference": "Diffusion Models Beat GANs on Image Synthesis",
      "Citations": "10151.0",
      "Authors": "Prafulla Dhariwal, Alex Nichol",
      "Abstract": "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128\u00d7128, 4.59 on ImageNet 256\u00d7256, and 7.72 on ImageNet 512\u00d7512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256\u00d7256 and 3.85 on ImageNet 512\u00d7512.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\"We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models\"",
      "Epochs": "381.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "11274.484326547095",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "These models are intended to be used for research purposes only. In particular, they can be used as a baseline for generative modeling research, or as a starting point to build off of for such research.\nThese models are not intended to be commercially deployed. Additionally, they are not intended to be used to create propaganda or offensive imagery.\n\nrepo is here with training code, MIT License\nhttps://github.com/openai/guided-diffusion",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ProtT5-XL-U50",
      "Organization": "Technical University of Munich,Med AI Technology,NVIDIA,Oak Ridge National Laboratory,Google,Seoul National University",
      "Publication date": "2021-05-04",
      "Domain": "Biology",
      "Task": "Proteins,Protein or nucleotide language model (pLM/nLM),Protein contact and distance prediction",
      "Parameters": "3000000000.0",
      "Parameters notes": "Table 2",
      "Training compute (FLOP)": "1.8704498688e+22",
      "Training compute notes": "991K steps, 2048 batch, 512 sequence length (Table 2)\n\nTotal tokens: 991K*2048*512=1039138816000\n\nFLOP: 6*1039138816000*3000000000=18704498688000000000000",
      "Training dataset": "UniRef50",
      "Training dataset size (gradients)": "19582371375",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3.full.pdf",
      "Reference": "ProtTrans: Towards Cracking the Language of Life\u2019s Code Through Self-Supervised Learning",
      "Citations": "",
      "Authors": "Ahmed Elnaggar, Michael Heinzinger,  Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, Debsindhu Bhowmik, Burkhard Rost",
      "Abstract": "Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models taken from NLP. These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive models (Transformer-XL, XLNet) and four auto-encoder models (BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393 billion amino acids. The LMs were trained on the Summit supercomputer using 5616 GPUs and TPU Pod up-to 1024 cores. Dimensionality reduction revealed that the raw protein LM-embeddings from unlabeled data captured some biophysical features of protein sequences. We validated the advantage of using the embeddings as exclusive input for several subsequent tasks. The first was a per-residue prediction of protein secondary structure (3-state accuracy Q3=81%-87%); the second were per-protein predictions of protein sub-cellular localization (ten-state accuracy: Q10=81%) and membrane vs. water-soluble (2-state accuracy Q2=91%). For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art without using evolutionary information thereby bypassing expensive database searches. Taken together, the results implied that protein LMs learned some of the grammar of the language of life. To facilitate future work, we released our models at https://github.com/agemagician/ProtTrans.",
      "Organization categorization": "Academia,Industry,Government,Industry,Academia",
      "Country (of organization)": "Germany,China,United States of America,United States of America,United States of America,Korea (Republic of)",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "Table 3\n\n\"For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art without using evolutionary information\"\n\n SOTA performance on New364 (new test set introduced in this paper)",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "256.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Academic Free License v3.0 License\nhttps://huggingface.co/Rostlab/prot_t5_xl_uniref50\n\nIt seems that there is no training code here, only inference and fine-tuning\nhttps://github.com/agemagician/ProtTrans",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ProtBERT-BFD",
      "Organization": "Technical University of Munich,NVIDIA,Seoul National University,Google,Oak Ridge National Laboratory,Med AI Technology",
      "Publication date": "2021-05-04",
      "Domain": "Biology",
      "Task": "Proteins,Protein or nucleotide language model (pLM/nLM)",
      "Parameters": "420000000.0",
      "Parameters notes": "Table 2",
      "Training compute (FLOP)": "3.9e+22",
      "Training compute notes": "FLOP = 420M * 6 * (800k*512*32k + 200k*2048*6k) \n1M steps total split into two phases, (1) 800k steps, seq length 512 (batch size 32k) and (2) 200k steps, seq length 2048 (batch size 6k)\nsingle TPU Pod V3-1024 (64 nodes and 1024 TPUs) info from paper and https://huggingface.co/Rostlab/prot_bert_bfd",
      "Training dataset": "BFD (Big Fantastic Dataset)",
      "Training dataset size (gradients)": "58950000000",
      "Dataset size notes": "\"ProtBERT-BFD (420M parameters) saw around 27B proteins during pre-training\" \n\nTable 1: BFD has 2122M proteins, 393B amino acids, 572 GB\nSuggests average amino acid length of 185\n\nImplies 27B * 185 = 5T amino acids seen in training\n\nHowever, Table 2 suggests number of tokens (amino acids) seen in training was:\n(512*32768*800k) + (2048*6144*200k) = 15.9T amino acids in training\n\nGeometric mean = 8.9T",
      "Confidence": "Confident",
      "Link": "https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3 or \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9477085",
      "Reference": "ProtTrans:Towards Cracking the Language of Life's Code Through Self-Supervised Learning",
      "Citations": "",
      "Authors": "Ahmed Elnaggar, Michael Heinzinger,  Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger,  Debsindhu Bhowmik, Burkhard Rost",
      "Abstract": "Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models taken from NLP. These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive models (Transformer-XL, XLNet) and four auto-encoder models (BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393 billion amino acids. The LMs were trained on the Summit supercomputer using 5616 GPUs and TPU Pod up-to 1024 cores.\n\nDimensionality reduction revealed that the raw protein LM-embeddings from unlabeled data captured some biophysical features of protein sequences. We validated the advantage of using the embeddings as exclusive input for several subsequent tasks. The first was a per-residue prediction of protein secondary structure (3-state accuracy Q3=81%-87%); the second were per-protein predictions of protein sub-cellular localization (ten-state accuracy: Q10=81%) and membrane vs. water-soluble (2-state accuracy Q2=91%). For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art without using evolutionary information thereby bypassing expensive database searches. Taken together, the results implied that protein LMs learned some of the grammar of the language of life. To facilitate future work, we released our models at https://github.com/agemagician/ProtTrans.",
      "Organization categorization": "Academia,Industry,Academia,Industry,Government",
      "Country (of organization)": "Germany,United States of America,Korea (Republic of),United States of America,United States of America,China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art without using evolutionary information thereby bypassing expensive database searches.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "figure 3 shows 19 hours per epoch, though this was on a different GPU setup than the one used for training.",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "1024.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "45350.735956744036",
      "Compute cost notes": "",
      "Training power draw (W)": "933059.519872234",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Licensed under the Academic Free License version 3.0\n\nThe ProtTrans project is a open source project supported by various partner companies and research institutions. We are committed to share all our pre-trained models and knowledge. \nhttps://github.com/agemagician/ProtTrans",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "PLUG",
      "Organization": "Alibaba",
      "Publication date": "2021-04-19",
      "Domain": "Language",
      "Task": "Language modeling/generation",
      "Parameters": "27000000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "3.5997696e+22",
      "Training compute notes": "128 Nvidia A100 for 35 days",
      "Training dataset": "",
      "Training dataset size (gradients)": "60000000000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://mp.weixin.qq.com/s/DAQomIkDa52Sef-ruyH5qg",
      "Reference": "",
      "Citations": "",
      "Authors": "",
      "Abstract": "ExperienceAfter the Great Refining Model, the field of artificial intelligence is entering the era of \"Renging Model\".Since OpenAI released the ultra-large-scale pre-training language model GPT-3 in the English field last year, the training process of similar models in the Chinese field has received much attention.Today, the Aridal Moral Court released 27 billion parameters, 1TB + training data, the world's largest Chinese pre-training language model PLUG, and refreshed the historical record of the CLUE classification list with a score of 80.614.\n\n(autotranslate from Chinese)",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Was a SOTA in CLUE 1.0 https://www.cluebenchmarks.com/classification10.html",
      "Epochs": "",
      "Training time (hours)": "840.0",
      "Training time notes": "35 days",
      "Training hardware": "NVIDIA A100",
      "Hardware quantity": "128.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "108672.69136370042",
      "Compute cost notes": "",
      "Training power draw (W)": "103707.91685199105",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "not listed",
      "Model accessibility": "Hosted access (no API)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "https://nlp.aliyun.com/portal#/BigText_chinese",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Unicorn",
      "Organization": "Allen Institute for AI",
      "Publication date": "2021-03-24",
      "Domain": "Language",
      "Task": "Question answering,Language modeling/generation",
      "Parameters": "11000000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "RAINBOW",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "Train set contains 324742 datapoints.\nhttps://colab.research.google.com/drive/1MKmS96_kYWT4JsNW1GmEtgU63RqPCJ3P#scrollTo=vrAW7-DErrkf\n\nEach dataset is a multiple choice QA, so one gradient calculated per question.\n\nThey first train on all six datasets simultaneously; then on each dataset separately for leaderboard submissions.\n\nEpochs: it seems like they test both batch size 16 and 32 for pretraining over 25k steps, the larger of which would correspond to 2.46 epochs over the full dataset.\nThey then do 25k steps for each dataset, with a batch size of either 16 or 32 depending on the dataset. This means the fine-tuning involved the following number of epochs for each dataset:\n- Anli: 2.36\n- CosmosQA: 31.67\n- HellaSWAG: 20.05\n- PIQA: 49.65\n- SocialIQA: 23.94\n- WinoGrande: 9.90",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2103.13009",
      "Reference": "UNICORN on RAINBOW: A Universal Commonsense Reasoning Model on a New Multitask Benchmark",
      "Citations": "",
      "Authors": "Nicholas Lourie, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi",
      "Abstract": "Commonsense AI has long been seen as a near impossible goal -- until recently. Now, research interest has sharply increased with an influx of new benchmarks and models.\nWe propose two new ways to evaluate commonsense models, emphasizing their generality on new tasks and building on diverse, recently introduced benchmarks. First, we propose a new multitask benchmark, RAINBOW, to promote research on commonsense models that generalize well over multiple tasks and datasets. Second, we propose a novel evaluation, the cost equivalent curve, that sheds new insight on how the choice of source datasets, pretrained language models, and transfer learning methods impacts performance and data efficiency.\nWe perform extensive experiments -- over 200 experiments encompassing 4800 models -- and report multiple valuable and sometimes surprising findings, e.g., that transfer almost always leads to better or equivalent performance if following a particular recipe, that QA-based commonsense datasets transfer well with each other, while commonsense knowledge graphs do not, and that perhaps counter-intuitively, larger models benefit more from transfer than smaller ones.\nLast but not least, we introduce a new universal commonsense reasoning model, UNICORN, that establishes new state-of-the-art performance across 8 popular commonsense benchmarks, aNLI (87.3%), CosmosQA (91.8%), HellaSWAG (93.9%), PIQA (90.1%), SocialIQa (83.2%), WinoGrande (86.6%), CycIC (94.0%) and CommonsenseQA (79.3%).",
      "Organization categorization": "Research collective",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"establishes new state-of-the-art performance across 8 popular commonsense benchmarks, aNLI (87.3%), CosmosQA (91.8%), HellaSWAG (93.9%), PIQA (90.1%), SocialIQa (83.2%), WinoGrande (86.6%), CycIC (94.0%) and CommonsenseQA (79.3%)\"",
      "Epochs": "2.46",
      "Training time (hours)": "3.0",
      "Training time notes": "\"Training times usually took several hours per run\"\nGuessing ",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "T5-11B",
      "Finetune compute (FLOP)": "3.3e+22",
      "Finetune compute notes": "All experiments were run on Google Cloud using two Google Compute Engine virtual machine (VM) instances communicating with various TPUs. [...] Each VM had 20 vCPUs with 75GB of memory [...] For hardware acceleration, we ran all the experiments using v3-8 TPUs when building off of T5-LARGE or smaller. [...] The T5-11B models were trained using TPU v2-256 and v3-256s with a model parallelism of 16. Training times usually took several hours per run, so we ran many experiments in parallel on the VMs using GNU Parallel (Tange 2011).\n\nSpeculatively, \"several hours\" ~= 3 hours on v3-256 (128 chips) would suggest 3 * 3600 * 1.23e14 * 128 * 0.3 = 5.1e19 FLOPs\n\nSo unlikely to significantly move the 3.3e22 FLOPs from T5-11B pretraining.",
      "Batch size": "32.0",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "https://github.com/allenai/rainbow#downloading-the-weights\nApache 2.0\nfinetune code: https://github.com/allenai/rainbow/blob/master/bin/fine-tune.py ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "M6-T",
      "Organization": "Alibaba",
      "Publication date": "2021-03-05",
      "Domain": "Multimodal,Language,Vision",
      "Task": "Chat,Image captioning",
      "Parameters": "1002700000000.0",
      "Parameters notes": "Table 5. Note model is sparse MoE with 960 experts; not all parameters are activated on the forward pass.",
      "Training compute (FLOP)": "5.5e+21",
      "Training compute notes": "Estimate taken from https://www.governance.ai/research-paper/recent-trends-chinas-llm-landscape",
      "Training dataset": "M6-Corpus",
      "Training dataset size (gradients)": "111800000000",
      "Dataset size notes": "60.5B images and 111.8B tokens of text",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2105.15082",
      "Reference": "M6-T: Exploring Sparse Expert Models and Beyond",
      "Citations": "76.0",
      "Authors": "An Yang, Junyang Lin, Rui Men, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Jiamang Wang, Yong Li, Di Zhang, Wei Lin, Lin Qu, Jingren Zhou, Hongxia Yang",
      "Abstract": "Mixture-of-Experts (MoE) models can achieve promising results with outrageous large amount of parameters but constant computation cost, and thus it has become a trend in model scaling. Still it is a mystery how MoE layers bring quality gains by leveraging the parameters with sparse activation. In this work, we investigate several key factors in sparse expert models. We observe that load imbalance may not be a significant problem affecting model quality, contrary to the perspectives of recent studies, while the number of sparsely activated experts k and expert capacity C in top-k routing can significantly make a difference in this context. Furthermore, we take a step forward to propose a simple method called expert prototyping that splits experts into different prototypes and applies k top-1 routing. This strategy improves the model quality but maintains constant computational costs, and our further exploration on extremely large-scale models reflects that it is more effective in training larger models. We push the model scale to over 1 trillion parameters and implement it on solely 480 NVIDIA V100-32GB GPUs, in comparison with the recent SOTAs on 2048 TPU cores. The proposed giant model achieves substantial speedup in convergence over the same-size baseline.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Improves on hardware SOTA for similar problems\n\nAbstract: \n\"We push the model scale to over 1 trillion parameters and implement it on solely 480 NVIDIA V100-32GB GPUs, in comparison with the recent SOTAs [11; 6] on 2048 TPU cores.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA Tesla V100 DGXS 32 GB",
      "Hardware quantity": "480.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "13156.86123531961",
      "Compute cost notes": "",
      "Training power draw (W)": "243309.1333554511",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Generative BST",
      "Organization": "Facebook AI Research",
      "Publication date": "2021-03-05",
      "Domain": "Language",
      "Task": "Language modeling/generation,Chat,Question answering",
      "Parameters": "9431810048.0",
      "Parameters notes": "The largest model is a transformer with 9.4B parameters (Table 2)",
      "Training compute (FLOP)": "1.449e+22",
      "Training compute notes": "\"Both our 2.7B and 9.4B parameter models were trained with batches of approximately 500k label BPE tokens per batch [...] The 9.4B parameter model was trained [...] for a total of 200k SGD steps.\"\n\nAlso note that the full dataset contains 56.8B label BPE tokens and 88.8B context tokens, so for each batch of 500k label tokens, there are likely 500k * 88.8B / 56.8B = 780k context tokens.\n\n6 * 9.4318B * 200k * (500k + 780k) = 1.449e22",
      "Training dataset": "",
      "Training dataset size (gradients)": "56800000000",
      "Dataset size notes": "Section 6. Pre-training is done on Pushshift.io Reddit: \"Our final dataset contains 1.50B comments totaling 56.8B label BPE tokens and 88.8B context tokens.\"\nNone of the fine-tuning datasets put a significant dent in the total dataset size.\n\nEpochs: they do 200k steps, where each batch has 500k label tokens = 100B label tokens seen. 56.8B label tokens in pre-training dataset, so 1.76 epochs",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2004.13637",
      "Reference": "Recipes for building an open-domain chatbot",
      "Citations": "1078.0",
      "Authors": "Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston",
      "Abstract": "Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, and displaying knowledge, empathy and personality appropriately, while maintaining a consistent persona. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,France",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Abstract:\n\"Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.\"",
      "Epochs": "1.7605633803",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT license\n\nhttps://github.com/facebookresearch/ParlAI\nhttps://parl.ai/projects/recipes/",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Meta Pseudo Labels",
      "Organization": "Google Brain,Google AI",
      "Publication date": "2021-03-01",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "480000000.0",
      "Parameters notes": "Table 4\n 480M",
      "Training compute (FLOP)": "4.79e+22",
      "Training compute notes": "From communication with author:\n\n22671 TPU days on specific hardware.\n\nWhich hardware did you use and in which configuration?\n2048 cores of TPU v3.\n\nPrecision: Mixed. bfloat16 for activations, float32 for weights and optimizer slots.\n\n2048 TPUv3 cores means 1024 TPUv3 chips, and the spec is 123e12 FLOP/second per chip with bfloat16 precision (Source: https://cloud.google.com/tpu/docs/system-architecture-tpu-vm)\n\nSo the compute estimate is:\n1024 chips * 123e12 FLOP/second * 0.4 utilization * 11 days * 24 * 60 * 60 = 4.788191232e+22 FLOP",
      "Training dataset": "ImageNet,JFT-300M",
      "Training dataset size (gradients)": "131280000",
      "Dataset size notes": "Section 4\nDatasets. For this experiment, we use the entire ImageNet\ntraining set as labeled data, and use the JFT dataset as unlabeled data. The JFT dataset has 300 million images, and\nthen is filtered down to 130 million images by Noisy Student\nusing confidence thresholds and up-sampling [77]. We use\nthe same 130 million images as Noisy Student",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2003.10580",
      "Reference": "Meta pseudo labels",
      "Citations": "729.0",
      "Authors": "Hieu Pham, Zihang Dai, Qizhe Xie, Minh-Thang Luong, and Quoc V. Le",
      "Abstract": "We present Meta Pseudo Labels, a semi-supervised learning method that achieves a new state-of-the-art top-1 accuracy of 90.2% on ImageNet, which is 1.6% better than the existing state-of-the-art. Like Pseudo Labels, Meta Pseudo Labels has a teacher network to generate pseudo labels on unlabeled data to teach a student network. However, unlike Pseudo Labels where the teacher is fixed, the teacher in Meta Pseudo Labels is constantly adapted by the feedback of the student's performance on the labeled dataset. As a result, the teacher generates better pseudo labels to teach the student. Our code will be available at this https URL.",
      "Organization categorization": "Industry,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We present Meta Pseudo Labels, a semi-supervised learning method that achieves a new state-of-the-art top-1 accuracy of 90.2% on ImageNet, which is 1.6% better than the existing state-of-the-art.\"",
      "Epochs": "",
      "Training time (hours)": "264.0",
      "Training time notes": "11 days from section 4:\n\"We train the model for 1 million steps in total,\nwhich takes about 11 days for EfficientNet-L2 and 10 days\nfor EfficientNet-B6-Wide. \"\n\n\"Specifically, our training process runs on a cluster of 2,048\nTPUv3 cores. \"\n",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "1024.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "53844.28059190435",
      "Compute cost notes": "",
      "Training power draw (W)": "934390.3015161053",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache-2.0 license\nhttps://github.com/google-research/google-research/blob/master/meta_pseudo_labels/README.md",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "SRU++ Large",
      "Organization": "ASAPP",
      "Publication date": "2021-02-24",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "234000000.0",
      "Parameters notes": "Table 5",
      "Training compute (FLOP)": "2.1173704e+19",
      "Training compute notes": "6 FLOP / token / parameter * 234000000 parameters * 1024 tokens per sample * 8*8 samples per batch * 400000 steps = 3.6805018e+19 FLOP\n\n31330000000000 FLOP / sec * 360 GPU-hours * 3600 sec / hour * 0.3 [assumed utilization] = 1.2181104e+19 FLOP\n\nsqrt(3.6805018e+19*1.2181104e+19) = 2.1173704e+19 \n\n",
      "Training dataset": "WikiText-103",
      "Training dataset size (gradients)": "103000000",
      "Dataset size notes": "Table 12:\n400K training steps\nbatch size: 8*8 = 64\nsequence length = 1024\n\n1024*8*8*400000/103000000 = 254.5 epochs",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2102.12459",
      "Reference": "When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute",
      "Citations": "54.0",
      "Authors": "Tao Lei",
      "Abstract": "Large language models have become increasingly difficult to train because of the growing computation time and cost. In this work, we present SRU++, a highly-efficient architecture that combines fast recurrence and attention for sequence modeling. SRU++ exhibits strong modeling capacity and training efficiency. On standard language modeling tasks such as Enwik8, Wiki-103 and Billion Word datasets, our model obtains better bits-per-character and perplexity while using 3x-10x less training cost compared to top-performing Transformer models. For instance, our model achieves a state-of-the-art result on the Enwik8 dataset using 1.6 days of training on an 8-GPU machine. We further demonstrate that SRU++ requires minimal attention for near state-of-the-art performance. Our results suggest jointly leveraging fast recurrence with little attention as a promising direction for accelerating model training and inference.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"our model achieves a state-of-the-art result on the ENWIK8 dataset using 1.6 days of training on an 8-GPU machine. \"",
      "Epochs": "254.5",
      "Training time (hours)": "",
      "Training time notes": " 15\u2020 GPU-days = 360 GPU-hours",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "65536.0",
      "Batch size notes": "1024*8*8",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT license repo. says models available as package: https://github.com/asappresearch/sru\n\ntraining: https://github.com/asappresearch/sru/blob/master/language_model/train_lm.py ",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Rational DQN Average",
      "Organization": "TU Darmstadt",
      "Publication date": "2021-02-18",
      "Domain": "Games",
      "Task": "Atari",
      "Parameters": "1683456.0",
      "Parameters notes": "See figure 7",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/2102.09407v3",
      "Reference": "Recurrent Rational Networks",
      "Citations": "8.0",
      "Authors": "Q Delfosse, P Schramowski, A Molina",
      "Abstract": "Latest insights from biology show that intelligence not only emerges from the connections between neurons but that individual neurons shoulder more computational responsibility than previously anticipated. This perspective should be critical in the context of constantly changing distinct reinforcement learning environments, yet current approaches still primarily employ static activation functions. In this work, we motivate why rationals are suitable for adaptable activation functions and why their inclusion into neural networks is crucial. Inspired by recurrence in residual networks, we derive a condition under which rational units are closed under residual connections and formulate a naturally regularised version: the recurrent-rational. We demonstrate that equipping popular algorithms with (recurrent-)rational activations leads to consistent improvements on Atari games, especially turning simple DQN into a solid approach, competitive to DDQN and Rainbow.\n",
      "Organization categorization": "Academia",
      "Country (of organization)": "Germany",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "They don't claim absolute SOTA\n\n\"We demonstrate that equipping popular algorithms with (joint) rational activations leads to consistent improvements on Atari games, notably making DQN competitive to DDQN and Rainbow\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "",
      "Accessibility notes": "\"To get the trained agents, please contact Quentin Delfosse\"\n\nno clear license\nhttps://github.com/ml-research/rational_rl/",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "MSA Transformer",
      "Organization": "Facebook AI Research,University of California (UC) Berkeley,New York University (NYU)",
      "Publication date": "2021-02-13",
      "Domain": "Biology",
      "Task": "Proteins,Protein or nucleotide language model (pLM/nLM),Protein contact and distance prediction,Protein folding prediction",
      "Parameters": "100000000.0",
      "Parameters notes": "\"We train an MSA Transformer model with 100M parameters...\" ",
      "Training compute (FLOP)": "5.49e+21",
      "Training compute notes": "Based on: https://docs.google.com/spreadsheets/d/1enan21dFx03TkwufHgOwTVNBtuYlqNY9uurjIK6YS-8/edit#gid=0\n\nNumber of steps 4.5e5, batch size (tokens) 6.1e7, parameters 1e8\n\nCalculation = 4e8 FLOP/bp * 4.5e5 bp + 2e8 FLOP/fp * 2.75e13 fp\n\nBatch size: 512\nSeq length: 100 * 1192 tokens\nAll models are trained on 32 V100 GPUs for 100k updates. The four models with best contact precision are then further trained to 150k updates. Finally, the best model at 150k updates is trained to 450k updates.\n\n450k * 512 * 100 * 1192 * 100M * 6 = 1.65e22",
      "Training dataset": "UniRef50,UniRef30 (FKA UniClust30)",
      "Training dataset size (gradients)": "1395000000000",
      "Dataset size notes": "\"We train an MSA Transformer model with 100M parameters on a large dataset (4.3 TB) of 26 million MSAs, with an average of 1192 sequences per MSA.\"\nAverage sequence is ~300 amino acids/tokens long.\n26 million * 1192 * 300 = 9.3T tokens",
      "Confidence": "Likely",
      "Link": "https://proceedings.mlr.press/v139/rao21a/rao21a.pdf",
      "Reference": "MSA Transformer",
      "Citations": "644.0",
      "Authors": "Roshan Rao, Jason Liu, Robert Verkuil, Joshua Meier, John F. Canny, Pieter Abbeel, Tom Sercu, Alexander Rives",
      "Abstract": "Unsupervised protein language models trained across millions of diverse sequences learn structure and function of proteins. Protein language models studied to date have been trained to perform inference from individual sequences. The longstanding approach in computational biology has been to make inferences from a family of evolutionarily related sequences by fitting a model to each family independently. In this work we combine the two paradigms. We introduce a protein language model which takes as input a set of sequences in the form of a multiple sequence alignment. The model interleaves row and column attention across the input sequences and is trained with a variant of the masked language modeling objective across many protein families. The performance of the model surpasses current state-of-the-art unsupervised structure learning methods by a wide margin, with far greater parameter efficiency than prior state-of-the-art protein language models. ",
      "Organization categorization": "Industry,Academia,Academia",
      "Country (of organization)": "United States of America,France,United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"The performance of the model surpasses current state-of-the-art unsupervised structure learning methods by a wide margin, with far greater parameter efficiency than prior state-of-the-art protein language models\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA Tesla V100 DGXS 32 GB",
      "Hardware quantity": "32.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "13256.937301517895",
      "Compute cost notes": "",
      "Training power draw (W)": "16227.834954676886",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT: https://github.com/facebookresearch/esm\n\nlooks like no training code",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "top-down frozen classifier",
      "Organization": "University of Edinburgh,Toshiba Cambridge Research Laboratory",
      "Publication date": "2021-02-09",
      "Domain": "Speech",
      "Task": "Speech recognition (ASR)",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "WSJ",
      "Training dataset size (gradients)": "3408000",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/2102.04697",
      "Reference": "Train your classifier first: Cascade Neural Networks Training from upper layers to lower layers",
      "Citations": "2.0",
      "Authors": "Shucong Zhang, Cong-Thanh Do, Rama Doddipatla, Erfan Loweimi, Peter Bell, Steve Renals",
      "Abstract": "Although the lower layers of a deep neural network learn features which are transferable across datasets, these layers are not transferable within the same dataset. That is, in general, freezing the trained feature extractor (the lower layers) and retraining the classifier (the upper layers) on the same dataset leads to worse performance. In this paper, for the first time, we show that the frozen classifier is transferable within the same dataset. We develop a novel top-down training method which can be viewed as an algorithm for searching for high-quality classifiers. We tested this method on automatic speech recognition (ASR) tasks and language modelling tasks. The proposed method consistently improves recurrent neural network ASR models on Wall Street Journal, self-attention ASR models on Switchboard, and AWD-LSTM language models on WikiText-2.",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Table 2 demonstrates that, to the best of our knowledge, top-down training results in state-of-the art character error rates for LSTM-based endto-end models on WSJ\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DLWP",
      "Organization": "University of Washington,Microsoft Research",
      "Publication date": "2021-02-09",
      "Domain": "Earth science",
      "Task": "Weather forecasting",
      "Parameters": "2676376.0",
      "Parameters notes": "Table 1",
      "Training compute (FLOP)": "5.6845152e+18",
      "Training compute notes": "31330000000000 FLOP / GPU/ sec [Tesla V100] * 1 GPU * 168 hours * 3600 sec / hour * 0.3 [assumed utilization] = 5.6845152e+18 FLOP",
      "Training dataset": "ERA5",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2102.05107",
      "Reference": "Sub-seasonal forecasting with a large ensemble of deep-learning weather prediction models",
      "Citations": "",
      "Authors": "Jonathan A. Weyn, Dale R. Durran, Rich Caruana, Nathaniel Cresswell-Clay",
      "Abstract": "We present an ensemble prediction system using a Deep Learning Weather Prediction (DLWP) model that recursively predicts key atmospheric variables with six-hour time resolution. This model uses convolutional neural networks (CNNs) on a cubed sphere grid to produce global forecasts. The approach is computationally efficient, requiring just three minutes on a single GPU to produce a 320-member set of six-week forecasts at 1.4\u00b0 resolution. Ensemble spread is primarily produced by randomizing the CNN training process to create a set of 32 DLWP models with slightly different learned weights. Although our DLWP model does not forecast precipitation, it does forecast total column water vapor, and it gives a reasonable 4.5-day deterministic forecast of Hurricane Irma. In addition to simulating mid-latitude weather systems, it spontaneously generates tropical cyclones in a one-year free-running simulation. Averaged globally and over a two-year test set, the ensemble mean RMSE retains skill relative to climatology beyond two-weeks, with anomaly correlation coefficients remaining above 0.6 through six days. Our primary application is to subseasonal-to-seasonal (S2S) forecasting at lead times from two to six weeks. Current forecast systems have low skill in predicting one- or 2-week-average weather patterns at S2S time scales. The continuous ranked probability score (CRPS) and the ranked probability skill score (RPSS) show that the DLWP ensemble is only modestly inferior in performance to the European Centre for Medium Range Weather Forecasts (ECMWF) S2S ensemble over land at lead times of 4 and 5-6 weeks. At shorter lead times, the ECMWF ensemble performs better than DLWP.",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "168.0",
      "Training time notes": "\"Our DLWP model requires 6\u20138 days of computation to train on a single Tesla V100 GPU\n\n7 days = 168 hours",
      "Training hardware": "NVIDIA Tesla V100 DGXS 32 GB",
      "Hardware quantity": "1.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "278.6620974208019",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DeiT-B",
      "Organization": "Meta AI,Sorbonne University",
      "Publication date": "2021-01-15",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "86000000.0",
      "Parameters notes": "(DeiT-B)",
      "Training compute (FLOP)": "7.884e+19",
      "Training compute notes": "2*86000000 parameters*3*1280000 training examples*300 epochs=1.98144e+17 FLOPs\n\ncompute [FLOP] = training time [s] \u00d7 # of GPUs/TPUs \u00d7 peak FLOP/s \u00d7 utilization rate\n\n(53h+20h)*3600*8*125000000000000 peak FLOP/s*0.3=7.884e+19\n\n",
      "Training dataset": "ImageNet",
      "Training dataset size (gradients)": "3840000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2012.12877",
      "Reference": "Training data-efficient image transformers & distillation through attention",
      "Citations": "8168.0",
      "Authors": "Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Herv\u00e9 J\u00e9gou",
      "Abstract": "Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption.\nIn this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data.\nMore importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,France",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "300.0",
      "Training time (hours)": "53.0",
      "Training time notes": "A typical training of 300 epochs takes 37 hours with 2 nodes or 53 hours on a single node for the DeiT-B.\nIn this paper, we train a vision transformer on a single 8-GPU node in two\nto three days (53 hours of pre-training, and optionally 20 hours of fine-tuning) that is competitive with convnets having a similar number of parameters and efficiency. It uses Imagenet as the sole training set.",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "models, train, inference: https://github.com/facebookresearch/deit/blob/main/README_deit.md \n\nApache-2.0 license",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Switch",
      "Organization": "Google",
      "Publication date": "2021-01-11",
      "Domain": "Language",
      "Task": "Text autocompletion",
      "Parameters": "1571000000000.0",
      "Parameters notes": "\"Combining expert, model and data parallelism, we design two large Switch Transformer models, one with 395 billion and 1.6 trillion parameters\"\nTable 9 gives more precise count of 1571B parameters",
      "Training compute (FLOP)": "8.22e+22",
      "Training compute notes": "Table 4\nhttps://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf",
      "Training dataset": "C4",
      "Training dataset size (gradients)": "86400000000",
      "Dataset size notes": "\"In our protocol we pre-train with 2^20 (1,048,576) tokens\nper batch for 550k steps amounting to 576B total tokens.\"\n\n1 token ~ 0.75 words",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2101.03961",
      "Reference": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity",
      "Citations": "3008.0",
      "Authors": "William Fedus, Barret Zoph, Noam Shazeer",
      "Abstract": "In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the \"Colossal Clean Crawled Corpus\" and achieve a 4x speedup over the T5-XXL model.\n",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\" On ANLI (Nie et al., 2019), Switch XXL improves over the prior state-of-the-art to get a 65.7 accuracy versus the prior best of 49.4 (Yang et al., 2020)... Finally, we also conduct an early examination of the model\u2019s knowledge with three closed-book knowledge-based tasks: Natural Questions, WebQuestions and TriviaQA, without additional pre-training using Salient Span Masking (Guu et al., 2020). In all three cases, we observe improvements over the prior stateof-the-art T5-XXL model (without SSM)\"",
      "Epochs": "",
      "Training time (hours)": "648.0",
      "Training time notes": "see table 4 in https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf\n",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "1024.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "145100.89767585773",
      "Compute cost notes": "",
      "Training power draw (W)": "935410.4639575825",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache 2 for weights: https://huggingface.co/google/switch-c-2048 \n\npaper links to this repo but not clear that the training hyperparams for Switch are here:\nhttps://github.com/google-research/t5x\n\n",
      "Numerical format": "BF16",
      "Frontier model": "True",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "0.2796747967",
      "Training compute cost (cloud)": "643863.538608544",
      "Training compute cost (upfront)": "11793424.547210434"
    },
    {
      "Model": "BigSSL",
      "Organization": "Google,Apple",
      "Publication date": "2021-01-10",
      "Domain": "Speech",
      "Task": "Speech recognition (ASR),Audio classification",
      "Parameters": "8000000000.0",
      "Parameters notes": "\"... we study the utility of large models, with the parameter count ranging from 600M to 8B...\"",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "100530000000",
      "Dataset size notes": "Sum all values in Table VII, and add 34k for English VAD, and 926k for English Youtube = 3116k hours\n\nNote this involves significant self-training: \"Noisy student training (NST) [23], [41] is a self-training\nmethod where a teacher model generates pseudo-labels for a\nlarge unlabeled dataset, which is in turn used to train a student\nmodel with augmentation.\"\n\n1 hour ~ 13,680 words\n13680 * 3116000 = 42626880000",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/2109.13226",
      "Reference": "BigSSL: Exploring the Frontier of Large-Scale Semi-Supervised Learning for Automatic Speech Recognition",
      "Citations": "151.0",
      "Authors": "Yu Zhang,  Daniel S. Park, Wei Han,James Qin, Anmol Gulati, Joel Shor, Aren Jansen, Yuanzhong Xu, Yanping Huang, Shibo Wang, Zongwei Zhou, Bo Li, Min Ma, William Chan, Jiahui Yu, Yongqiang Wang, Liangliang Cao, Khe Chai Sim, Bhuvana Ramabhadran, Tara N. Sainath, Fran\u00e7oise Beaufays, Zhifeng Chen, Quoc V. Le, Chung-Cheng Chiu, Ruoming Pang and Yonghui Wu",
      "Abstract": "We summarize the results of a host of efforts using giant automatic speech recognition (ASR) models pre-trained using large, diverse unlabeled datasets containing approximately a million hours of audio. We find that the combination of pre-training, self-training and scaling up model size greatly increases data efficiency, even for extremely large tasks with tens of thousands of hours of labeled data. In particular, on an ASR task with 34k hours of labeled data, by fine-tuning an 8 billion parameter pre-trained Conformer model we can match state-of-the-art (SoTA) performance with only 3% of the training data and significantly improve SoTA with the full training set. We also report on the universal benefits gained from using big pre-trained and self-trained models for a large set of downstream tasks that cover a wide range of speech domains and span multiple orders of magnitudes of dataset sizes, including obtaining SoTA performance on many public benchmarks. In addition, we utilize the learned representation of pre-trained networks to achieve SoTA results on non-ASR tasks.",
      "Organization categorization": "Industry,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Figure 1\n\"In particular, on an ASR task with 34k hours of labeled data, by fine-tuning an 8 billion parameter pre-trained Conformer model we can match state-of-the-art (SoTA) performance with only 3% of the training data and significantly improve SoTA with the full training set\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DALL-E",
      "Organization": "OpenAI",
      "Publication date": "2021-01-05",
      "Domain": "Image generation",
      "Task": "Text-to-image,Image generation",
      "Parameters": "12000000000.0",
      "Parameters notes": "DALL\u00b7E is a 12-billion parameter version of GPT-3 trained to generate images from text descriptions",
      "Training compute (FLOP)": "4.7e+22",
      "Training compute notes": "source: https://lair.lighton.ai/akronomicon/\n\narchived: https://github.com/lightonai/akronomicon/tree/main/akrodb\n\nVAE training \"on 64 16 GB NVIDIA V100 GPUs, with a per-GPU batch size of 8, resulting in a total batch size of 512. It is trained for a total of 3,000,000 updates.\"\n\nTransformer training: \"We trained the model using 1024, 16 GB NVIDIA V100 GPUs and a total batch size of 1024, for a total of 430,000 updates.\"; \"We concatenate up to 256 BPE-encoded text tokens with the 32 \u00d7 32 = 1024 image tokens\"\nTotal tokens: 430000 steps * 1024 batch size * 1280 sequence length = 563609600000\n\nTransformer FLOP: 6 * 12B parameter * 563609600000 tokens = 4.057989e+22\n\nEstimating the VAE at ~15% seems reasonable",
      "Training dataset": "DALL-E",
      "Training dataset size (gradients)": "320000000000",
      "Dataset size notes": "\"To scale up to 12-billion parameters, we created a dataset of a similar scale to JFT-300M (Sun et al., 2017) by collecting\n250 million text-images pairs from the internet. \"\n\nnumber of epochs: \n1024 batch size * 430,000 updates / 250,000,000 = 1.76",
      "Confidence": "Likely",
      "Link": "https://openai.com/blog/dall-e/\n\nhttps://arxiv.org/abs/2102.12092",
      "Reference": "Zero-Shot Text-to-Image Generation",
      "Citations": "5915.0",
      "Authors": "Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever",
      "Abstract": "Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Significant use,Highly cited,Historical significance",
      "Notability criteria notes": "",
      "Epochs": "1.76",
      "Training time (hours)": "",
      "Training time notes": "\"We trained the model using 1024, 16 GB NVIDIA V100 GPUs and a total batch size of 1024, for a total of 430,000 updates.\nAt the start of training, we use a linear schedule to ramp up the step size to 4.5 \u00b7 10\u22124 over 5000 updates, and halved the\nstep size each time the training loss appeared to plateau. We did this a total of five times, ending training with a final step\nsize that was 32 times smaller than the initial one. \"",
      "Training hardware": "NVIDIA Tesla V100 DGXS 16 GB",
      "Hardware quantity": "1024.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "125818.6901",
      "Compute cost notes": "",
      "Training power draw (W)": "519741.92129196384",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "CLIP (ViT L/14@336px)",
      "Organization": "OpenAI",
      "Publication date": "2021-01-05",
      "Domain": "Multimodal,Vision,Language,Video",
      "Task": "Zero-shot image classification,Character recognition (OCR),Video description",
      "Parameters": "370000000.0",
      "Parameters notes": "Image encoder\nVision Transformer\nTable 1 in https://arxiv.org/pdf/2010.11929.pdf\nAuthors fine-tuned ViT L/14 at additional 336px resolution, hence the @336 (See ViT)\n307M params\n\nText encoder\n~Transformer (from paper)\n63M params",
      "Training compute (FLOP)": "1.05e+22",
      "Training compute notes": "https://docs.google.com/document/d/156miAJkFN9DDX06C3s03UDsretCtymCKiGDddLBCgQE/edit?usp=sharing\n\n",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "400000000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2103.00020\nhttps://huggingface.co/openai/clip-vit-large-patch14-336",
      "Reference": "Learning Transferable Visual Models From Natural Language Supervision",
      "Citations": "40482.0",
      "Authors": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever",
      "Abstract": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\"The best-performing CLIP model, using ViT-L/14 archiecture and 336-by-336 pixel images, achieved the state of the art in 21 of the 27 datasets, i.e. included in the Clopper-Pearson 99.5% confidence interval around each dataset\u2019s top score. \"\n\n\"On STL10,\nCLIP achieves 99.3% overall which appears to be a new\nstate of the art despite not using any training examples. \"",
      "Epochs": "",
      "Training time (hours)": "288.0",
      "Training time notes": "\u201cThe largest ResNet model, RN50x64, took 18 days to train on 592 V100 GPUs while the largest Vision Transformer took 12 days on 256 V100 GPUs\u201d",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "256.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "24638.518413409514",
      "Compute cost notes": "https://www.kdnuggets.com/2021/03/beginners-guide-clip-model.html\n",
      "Training power draw (W)": "155922.57638758916",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT License\n\nhttps://github.com/OpenAI/CLIP",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "CLIP (ResNet-50)",
      "Organization": "OpenAI",
      "Publication date": "2021-01-05",
      "Domain": "Multimodal,Vision,Language,Video",
      "Task": "Zero-shot image classification,Character recognition (OCR),Video description",
      "Parameters": "88600000.0",
      "Parameters notes": "Image encoder\n~ResNet-50 (from paper)\n25.6M params\n\nText encoder\n~Transformer (from paper)\n63M params",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "400000000",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2103.00020",
      "Reference": "Learning Transferable Visual Models From Natural Language Supervision",
      "Citations": "40482.0",
      "Authors": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever",
      "Abstract": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\"On STL10, CLIP achieves 99.3% overall which appears to be a new state of the art despite not using any training examples. \"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "MIT license\nhttps://github.com/OpenAI/CLIP",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ERNIE-Doc (247M)",
      "Organization": "Baidu",
      "Publication date": "2020-12-31",
      "Domain": "Language",
      "Task": "Language modeling,Language modeling/generation",
      "Parameters": "247000000.00000003",
      "Parameters notes": "",
      "Training compute (FLOP)": "3.0302798e+19",
      "Training compute notes": "6 FLOP / parameter / token * 247000000 parameters * 416000 steps * 128 sequences per batch * 384 tokens per sequence = 3.0302798e+19 FLOP",
      "Training dataset": "WikiText-103",
      "Training dataset size (gradients)": "103000000",
      "Dataset size notes": "Table 11:\nsequence length 384\nbatch size 128\ntraining steps 16000 + 400000 = 416000 \n\n416000*128*384/103000000 = 198.5  epochs",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2012.15688",
      "Reference": "ERNIE-Doc: A Retrospective Long-Document Modeling Transformer",
      "Citations": "62.0",
      "Authors": "Siyu Ding, Junyuan Shang, Shuohuan Wang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang",
      "Abstract": "Transformers are not suited for processing long documents, due to their quadratically increasing memory and time consumption. Simply truncating a long document or applying the sparse attention mechanism will incur the context fragmentation problem or lead to an inferior modeling capability against comparable model sizes. In this paper, we propose ERNIE-Doc, a document-level language pretraining model based on Recurrence Transformers. Two well-designed techniques, namely the retrospective feed mechanism and the enhanced recurrence mechanism, enable ERNIE-Doc, which has a much longer effective context length, to capture the contextual information of a complete document. We pretrain ERNIE-Doc to explicitly learn the relationships among segments with an additional document-aware segment-reordering objective. Various experiments were conducted on both English and Chinese document-level tasks. ERNIE-Doc improved the state-of-the-art language modeling result of perplexity to 16.8 on WikiText-103. Moreover, it outperformed competitive pretraining models by a large margin on most language understanding tasks, such as text classification and question answering.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"ERNIE-DOC improved the state-of-the-art language modeling result of perplexity to 16.8 on WikiText103\"",
      "Epochs": "190.88",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "weights available, not sure there's training code for WT-103: https://github.com/PaddlePaddle/ERNIE/tree/repro/ernie-doc",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "CT-MoS (WT2)",
      "Organization": "Google,National Tsing Hua University",
      "Publication date": "2020-12-25",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "45000000.0",
      "Parameters notes": "45M\nTable 2",
      "Training compute (FLOP)": "5.4e+17",
      "Training compute notes": "6 FLOP / parameter / token * 45000000 parameters * 2000000 tokens * 1000 epochs = 5.4e+17 FLOP",
      "Training dataset": "WikiText-2",
      "Training dataset size (gradients)": "2000000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2012.13575",
      "Reference": "Contextual Temperature for Language Modeling",
      "Citations": "33.0",
      "Authors": "Pei-Hsin Wang, Sheng-Iou Hsieh, Shih-Chieh Chang, Yu-Ting Chen, Jia-Yu Pan, Wei Wei, Da-Chang Juan",
      "Abstract": "Temperature scaling has been widely used as an effective approach to control the smoothness of a distribution, which helps the model performance in various tasks. Current practices to apply temperature scaling assume either a fixed, or a manually-crafted dynamically changing schedule. However, our studies indicate that the individual optimal trajectory for each class can change with the context. To this end, we propose contextual temperature, a generalized approach that learns an optimal temperature trajectory for each vocabulary over the context. Experimental results confirm that the proposed method significantly improves state-of-the-art language models, achieving a perplexity of 55.31 and 62.89 on the test set of Penn Treebank and WikiText-2, respectively. In-depth analyses show that the behaviour of the learned temperature schedules varies dramatically by vocabulary, and that the optimal schedules help in controlling the uncertainties. These evidences further justify the need for the proposed method and its advantages over fixed temperature schedules.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,Taiwan",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Experimental results confirm that the\nproposed method significantly improves state-of-the-art language models, achieving a perplexity of 55.31 and 62.89 on the test set of Penn Treebank and WikiText-2\"",
      "Epochs": "1000.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA GeForce GTX 1080 Ti",
      "Hardware quantity": "4.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "2030.739275278243",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "1770.2413749594477",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DensePhrases",
      "Organization": "Korea University,Princeton University",
      "Publication date": "2020-12-23",
      "Domain": "Language",
      "Task": "Question answering",
      "Parameters": "",
      "Parameters notes": "may be possible to estimate from batch size (8) and maximum memory of GPUs (96GB)",
      "Training compute (FLOP)": "2.09952e+18",
      "Training compute notes": " flops = (8) * (1215 * 10**10) * (20 * 3600) * 3 // 10 = 2099520000000000000\n(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)\n\nmodel of GPU from appendix B (Titan Xp)\nnumber of GPUs from table in appendix A\nflops from https://www.techpowerup.com/gpu-specs/titan-xp.c2948",
      "Training dataset": "SQuAD,NQ (Natural Questions)",
      "Training dataset size (gradients)": "58000000",
      "Dataset size notes": "from appendix D \"The number of generated questions is 327,302 and 1,126,354 for SQuAD and Natural Questions, respectively.\"\nassuming 40 words per question we get around ~ 58M",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/2012.12624v3",
      "Reference": "Learning Dense Representations of Phrases at Scale",
      "Citations": "126.0",
      "Authors": "Jinhyuk Lee, Mujeen Sung, Jaewoo Kang, Danqi Chen",
      "Abstract": "Open-domain question answering can be reformulated as a phrase retrieval problem, without the need for processing documents on-demand during inference (Seo et al., 2019). However, current phrase retrieval models heavily depend on sparse representations and still underperform retriever-reader approaches. In this work, we show for the first time that we can learn dense representations of phrases alone that achieve much stronger performance in open-domain QA. We present an effective method to learn phrase representations from the supervision of reading comprehension tasks, coupled with novel negative sampling methods. We also propose a query-side fine-tuning strategy, which can support transfer learning and reduce the discrepancy between training and inference. On five popular open-domain QA datasets, our model DensePhrases improves over previous phrase retrieval models by 15%-25% absolute accuracy and matches the performance of state-of-the-art retriever-reader models. Our model is easy to parallelize due to pure dense representations and processes more than 10 questions per second on CPUs. Finally, we directly use our pre-indexed dense phrase representations for two slot filling tasks, showing the promise of utilizing DensePhrases as a dense knowledge base for downstream tasks. ",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "Korea (Republic of),United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "from abstract \"our model DensePhrases improves over previous phrase retrieval models by 15%-25% absolute accuracy and matches the performance of state-of-the-art retriever-reader models. \"\n\n\"we achieve state-of-the-art performance on two slot filling tasks (Petroni et al., 2021), using less than 5% of the training data.\"",
      "Epochs": "4.0",
      "Training time (hours)": "20.0",
      "Training time notes": "appendix A row 3",
      "Training hardware": "NVIDIA TITAN Xp",
      "Hardware quantity": "8.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "4061.6594477324606",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache 2.0: https://github.com/princeton-nlp/DensePhrases",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "6200.93992019281",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "VQGAN + CLIP",
      "Organization": "Heidelberg University",
      "Publication date": "2020-12-17",
      "Domain": "Image generation",
      "Task": "Text-to-image",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "251988480000",
      "Dataset size notes": "I'm confused - I guess they pretrained on several different datasets? I think the model is also able to do zero-shot learning",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/2012.09841",
      "Reference": "Taming Transformers for High-Resolution Image Synthesis",
      "Citations": "3728.0",
      "Authors": "Patrick Esser, Robin Rombach, Bj\u00f6rn Ommer",
      "Abstract": "Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers and obtain the state of the art among autoregressive models on class-conditional ImageNet. Code and pretrained models can be found at this https URL .",
      "Organization categorization": "Academia",
      "Country (of organization)": "Germany",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\"The results shows that the transformer consistently outperforms PixelSNAIL across all tasks when trained for the same amount of time and the gap increases even further when trained for the same number of steps.\"\n\n\"obtain the state of the art among autoregressive models on class-conditional ImageNet\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "",
      "Accessibility notes": "MIT license\nhttps://github.com/CompVis/taming-transformers",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ESM1b",
      "Organization": "Facebook AI Research,New York University (NYU)",
      "Publication date": "2020-12-15",
      "Domain": "Biology",
      "Task": "Proteins,Protein or nucleotide language model (pLM/nLM)",
      "Parameters": "652400000.0",
      "Parameters notes": "See Table 9",
      "Training compute (FLOP)": "5.1e+21",
      "Training compute notes": "Information: \n128 NVIDIA V100 GPUs [Pre-training details]\n8.5 hours on 64 GPUs per epoch, 56 epochs of UR50/S [Appendix B, ESM-1b Hyperparameter optimization, Experimental set-up]\n128 NVIDIA V100 GPU, assuming  V100 PCIe half precision 130 TFLOPS and 0.3 utilization rate\n\nEstimate: (8.5*56*3600) s * 1.3e14 FLOP/s * 0.3 *64 = 4.277e21 FLOP\n\n6NC method:\nUR50/S has 27.1M sequences, which are capped at 1024 amino acids. \n27.1M * 1024 * 56 * 652.4M * 6 = 6.08e21 FLOP\n\nGeometric mean: 5.1e21",
      "Training dataset": "UniRef50",
      "Training dataset size (gradients)": "27750400000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://www.pnas.org/doi/abs/10.1073/pnas.2016239118",
      "Reference": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences",
      "Citations": "2637.0",
      "Authors": "Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus",
      "Abstract": "In the field of artificial intelligence, a combination of scale in data and model capacity enabled by unsupervised learning has led to major advances in representation learning and statistical generation. In the life sciences, the anticipated growth of sequencing promises unprecedented data on natural sequence diversity. Protein language modeling at the scale of evolution is a logical step toward predictive and generative artificial intelligence for biology. To this end, we use unsupervised learning to train a deep contextual language model on 86 billion amino acids across 250 million protein sequences spanning evolutionary diversity. The resulting model contains information about biological properties in its representations. The representations are learned from sequence data alone. The learned representation space has a multiscale organization\nreflecting structure from the level of biochemical properties of amino acids to remote homology of proteins. Information about secondary and tertiary structure is encoded in the representations and can be identified by linear projections. Representation learning\nproduces features that generalize across a range of applications, enabling state-of-the-art supervised prediction of mutational effect and secondary structure and improving state-of-the-art features for long-range contact prediction.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,France,United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\"We apply the representations to a range of prediction tasks and find that they improve state-of-art features across the applications.\"\n\nTable 4, Table 6",
      "Epochs": "56.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "128.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "924.3248891034536",
      "Compute cost notes": "",
      "Training power draw (W)": "77997.75584661258",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT: https://github.com/facebookresearch/esm",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "CPM-Large",
      "Organization": "Tsinghua University,Beijing Academy of Artificial Intelligence / BAAI",
      "Publication date": "2020-12-01",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "2600000000.0",
      "Parameters notes": "\"To the best of our knowledge, CPM, with 2.6 billion parameters and 100GB Chinese training data, is the largest Chinese pre-trained language mode\"",
      "Training compute (FLOP)": "2.6052e+20",
      "Training compute notes": "source: https://lair.lighton.ai/akronomicon/\n\narchived: https://github.com/lightonai/akronomicon/tree/main/akrodb\n\n6*2600000000 parameter *16700000000 tokens=2.605200e+20",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "16700000000",
      "Dataset size notes": "\"language model, with 2.6 billion parameters and 100GB Chinese training data.\"\n\nWe use the conversion factor 1GB ~ 167M words\n\n100GB ~ 16700000000 tokens / words\n\n",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2012.00413",
      "Reference": "CPM: A Large-scale Generative Chinese Pre-trained Language Model",
      "Citations": "127.0",
      "Authors": "Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun",
      "Abstract": "Pre-trained Language Models (PLMs) have proven to be beneficial for various downstream NLP tasks. Recently, GPT-3, with 175 billion parameters and 570GB training data, drew a lot of attention due to the capacity of few-shot (even zero-shot) learning. However, applying GPT-3 to address Chinese NLP tasks is still challenging, as the training corpus of GPT-3 is primarily English, and the parameters are not publicly available. In this technical report, we release the Chinese Pre-trained Language Model (CPM) with generative pre-training on large-scale Chinese training data. To the best of our knowledge, CPM, with 2.6 billion parameters and 100GB Chinese training data, is the largest Chinese pre-trained language model, which could facilitate several downstream Chinese NLP tasks, such as conversation, essay generation, cloze test, and language understanding. Extensive experiments demonstrate that CPM achieves strong performance on many NLP tasks in the settings of few-shot (even zero-shot) learning. The code and parameters are available at this https URL.",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "China,China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"CPM outperforms CDial-GPT with a large margin in the few-shot experiment, showing the generalization ability of our model.\"\n\nTable 5",
      "Epochs": "",
      "Training time (hours)": "336.0",
      "Training time notes": "\"It takes two weeks to train our largest model using 64 NVIDIA V100.\"",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "64.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "7340.099044719796",
      "Compute cost notes": "https://towardsdatascience.com/the-future-of-ai-is-decentralized-848d4931a29a#:~:text=Training%20GPT%2D3%20reportedly%20cost,a%20single%20training%20run%C2%B9.",
      "Training power draw (W)": "39011.038545758856",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "MIT license\n\nhttps://github.com/TsinghuaAI/CPM-1-Generate",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "AlphaFold 2",
      "Organization": "DeepMind",
      "Publication date": "2020-11-30",
      "Domain": "Biology",
      "Task": "Protein folding prediction,Proteins",
      "Parameters": "93000000.0",
      "Parameters notes": "https://arxiv.org/abs/2207.05477 reimplements AlphaFold 2 in a more efficient way, and states there are 93M parameters in the original version (Table 1)",
      "Training compute (FLOP)": "2.99e+21",
      "Training compute notes": "123 teraFLOPS / TPU v3 chip * 128 cores * (1 chip / 2 cores) * 11 days * 40% utilization = 2.99e21 FLOP\nhttps://www.wolframalpha.com/input?i=123+teraFLOPS+*+128+*+11+days+*+0.4\n\n\"Training regimen\" section: \n\"We train the model on Tensor Processing Unit (TPU) v3 with a batch size of 1 per TPU core, hence the model uses 128 TPU v3 cores. [...] The initial training stage takes approximately 1 week, and the fine-tuning stage takes approximately 4 additional days.\"",
      "Training dataset": "PDB (Protein Data Bank),UniRef30 (FKA UniClust30),UniRef90,MGnify,BFD (Big Fantastic Dataset),UniProtKB",
      "Training dataset size (gradients)": "5724000000",
      "Dataset size notes": "3 different types of input data to the network:\n(1) Amino acid sequence\n(2) Multiple sequence alignments (MSA) to sequences from evolutionarily related proteins\n(3) Template structures (3D atom coordinates of homologous structures), where available\n\nTraining data is processed into the following two datasets that are sampled with different probabilities. \nSupplementary Material, Section 1.2.4. Training data:\n\"With 75% probability a training example comes from the self-distillation set (see subsection 1.3) and with 25% probability the training example is a known structure from the Protein Data Bank\"\n\nSupplementary Material, Section 1.3 Self-distillation dataset:\n\"This gives a final dataset of 355,993 sequences\". An initial model was used to predict structures for these sequences.\n\nPDB dataset size in 2020: https://www.rcsb.org/stats/growth/growth-released-structures\n172788\n\nTherefore, estimate for number of protein structures available for training (for which amino acid sequence, MSA and homologue template info is also available as input to network): 528781 [~530k]",
      "Confidence": "Likely",
      "Link": "https://www.nature.com/articles/s41586-021-03819-2",
      "Reference": "Highly accurate protein structure prediction with AlphaFold",
      "Citations": "31909.0",
      "Authors": "John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Kathryn Tunyasuvunakool, Olaf Ronneberger, Russ Bates, Augustin \u017d\u00eddek, Alex Bridgland, Clemens Meyer, Simon A A Kohl, Anna Potapenko, Andrew J Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Martin Steinegger, Michalina Pacholska, David Silver, Oriol Vinyals, Andrew W Senior, Koray Kavukcuoglu, Pushmeet Kohli, Demis Hassabis.",
      "Abstract": "Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort, the structures of around 100,000 unique proteins have been determined, but this represents a small fraction of the billions of known protein sequences. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence\u2014the structure prediction component of the \u2018protein folding problem\u2019\u2014has been an important open research problem for more than 50 years. Despite recent progress, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14), demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Historical significance,Highly cited,SOTA improvement",
      "Notability criteria notes": "\"Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known\" [Abstract]\n\n>17790 citations\n\n\"In the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods.\"",
      "Epochs": "",
      "Training time (hours)": "264.0",
      "Training time notes": "7 days pretrain and 4 days finetune",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "3841.772612266474",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "While the AlphaFold code is licensed under the Apache 2.0 License, the AlphaFold parameters and CASP15 prediction data are made available under the terms of the CC BY 4.0 license\n\ncode in this repo is inference code:\nhttps://github.com/google-deepmind/alphafold",
      "Numerical format": "BF16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "KEPLER",
      "Organization": "Tsinghua University,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),HEC,CIFAR AI Research,Princeton University,University of Montreal / Universit\u00e9 de Montr\u00e9al",
      "Publication date": "2020-11-23",
      "Domain": "Language",
      "Task": "Relation extraction",
      "Parameters": "125000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "1.66e+21",
      "Training compute notes": "From author communication\n\n\"About 128 GPU-days using Nvidia V100 (16GB). \"\n\nprecision: float16\n\nV100 GPU for float16: 28000000000000 (2.8E+13)\n\n0.4 * 28TFLOP/s * 128 GPU-days * 24h/day * 3600s/h\n= 1.24E+20\n\n\"and use the released roberta.base parameters for\ninitialization, which is a common practice to save\npre-training time\"\n\nRoberta base FLOP: 1.536e+21\nTotal:1.660000e+21\n",
      "Training dataset": "Wikipedia,BookCorpus (BooksCorpus, Toronto Book Corpus),Wikidata5M",
      "Training dataset size (gradients)": "3533640000",
      "Dataset size notes": "For BookCorpus + English Wikipedia: 800M + 2500M\n\nFor Wikidata5M: 20614279\nSee table 1. Contains \"entities\", \"relations\", and \"triplets\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1911.06136",
      "Reference": "KEPLER: A Unified Model for Knowledge Embedding and Pre- trained Language Representation.",
      "Citations": "764.0",
      "Authors": "Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhiyuan Liu, Juanzi Li, and Jian Tang.",
      "Abstract": "Pre-trained language representation models (PLMs) cannot well capture factual knowledge from text. In contrast, knowledge embedding (KE) methods can effectively represent the relational facts in knowledge graphs (KGs) with informative entity embeddings, but conventional KE models cannot take full advantage of the abundant textual information. In this paper, we propose a unified model for Knowledge Embedding and Pre-trained LanguagE Representation (KEPLER), which can not only better integrate factual knowledge into PLMs but also produce effective text-enhanced KE with the strong PLMs. In KEPLER, we encode textual entity descriptions with a PLM as their embeddings, and then jointly optimize the KE and language modeling objectives. Experimental results show that KEPLER achieves state-of-the-art performances on various NLP tasks, and also works remarkably well as an inductive KE model on KG link prediction. Furthermore, for pre-training and evaluating KEPLER, we construct Wikidata5M, a large-scale KG dataset with aligned entity descriptions, and benchmark state-of-the-art KE methods on it. It shall serve as a new KE benchmark and facilitate the research on large KG, inductive KE, and KG with text. The source code can be obtained from this https URL.",
      "Organization categorization": "Academia,Academia,Academia,Research collective,Academia,Academia",
      "Country (of organization)": "China,Canada,France,Canada,United States of America,Canada",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Experimental results show that KEPLER achieves state-of-the-art performances\non various NLP tasks\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "RoBERTa Base",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT License, includes train code https://github.com/THU-KEG/KEPLER",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "wave2vec 2.0 LARGE",
      "Organization": "Facebook",
      "Publication date": "2020-10-22",
      "Domain": "Speech",
      "Task": "Speech completion",
      "Parameters": "317000000.0",
      "Parameters notes": "Section 5.1:\n\"We consider two model sizes: BASE (95m parameters) and LARGE (317m parameters)\n",
      "Training compute (FLOP)": "3.87072e+21",
      "Training compute notes": "From surveying the authors:\n\nWe trained the base model on 64 V100 GPUs for 400k updates. This takes about 3 days to complete. The large model is trained on 128 V100 GPUs for 1 million updates, and this takes about 7 days to complete.\n\nV100 GPU peak: 125TFLOP/s (https://www.nvidia.com/en-gb/data-center/tesla-v100/)\nAssume 40% utilization based on default for non-Language domain (https://epoch.ai/blog/estimating-training-compute)\n\n128 GPUs * 40% * 125TFLOP/s * 7 days * 24h/day * 3600s/h\n~= 3.870720e+21",
      "Training dataset": "LibriSpeech,LibriLight",
      "Training dataset size (gradients)": "4598395200",
      "Dataset size notes": "pg 4, section 4.1\n\n\"As unlabeled data we consider the Librispeech corpus [40] without transcriptions containing 960 hours of audio (LS-960) or the audio data from LibriVox (LV-60k). For the latter we follow the preprocessing of [27] resulting in 53.2k hours of audio.\"\n\n53.2k h * 13,680 words/h = 727776000 words",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2006.11477",
      "Reference": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
      "Citations": "7294.0",
      "Authors": "Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli",
      "Abstract": "We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "Arguably an \"important\" paper? \n\nAbstract: \n\"We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA Tesla V100 DGXS 32 GB",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "5021.241075362514",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "https://github.com/facebookresearch/fairseq/blob/1bba712622b8ae4efb3eb793a8a40da386fe11d0/examples/wav2vec/README.md\n\nfairseq(-py) is MIT-licensed. The license applies to the pre-trained models as well. Repo contains weights and pretrain and finetune code",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ViT-Huge/14",
      "Organization": "Google Brain,Google Research",
      "Publication date": "2020-10-22",
      "Domain": "Vision",
      "Task": "Image representation",
      "Parameters": "632000000.0",
      "Parameters notes": "Table 1 https://arxiv.org/pdf/2010.11929.pdf",
      "Training compute (FLOP)": "4.262e+21",
      "Training compute notes": "Table 6: 4.262e21 FLOPs\n\nAgrees with Table 2 (2.5k TPUv3-core days), if MFU is around 0.32. \n\n2500 * 24 * 3600 * (0.5 * 1.23e14) * 0.32 = 4.25e21",
      "Training dataset": "ImageNet-1k,ImageNet21k,JFT-300M",
      "Training dataset size (gradients)": "303000000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2010.11929",
      "Reference": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "Citations": "54158.0",
      "Authors": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby",
      "Abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",
      "Organization categorization": "Industry,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "14.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "0.3208",
      "Training compute cost (2023 USD)": "6724.023241261178",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache-2.0 license\nhttps://github.com/google-research/vision_transformer",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ViT-Base/32",
      "Organization": "Google Brain",
      "Publication date": "2020-10-22",
      "Domain": "Vision",
      "Task": "Image representation",
      "Parameters": "86000000.0",
      "Parameters notes": "Table 1 https://arxiv.org/pdf/2010.11929.pdf",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "JFT-300M",
      "Training dataset size (gradients)": "303000000",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/2010.11929",
      "Reference": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "Citations": "54158.0",
      "Authors": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby",
      "Abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "7.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "",
      "Accessibility notes": "Apache 2.0 \n\nhttps://huggingface.co/google/vit-base-patch16-224\n\nhttps://github.com/google-research/vision_transformer\nhttps://github.com/google-research/big_vision",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "German ELECTRA Large",
      "Organization": "deepset,Bayerische Staatsbibliothek Muenchen",
      "Publication date": "2020-10-21",
      "Domain": "Language",
      "Task": "Document classification,Named entity recognition (NER),Text classification",
      "Parameters": "335000000.0",
      "Parameters notes": "335M from Table 5",
      "Training compute (FLOP)": "1.42829568e+21",
      "Training compute notes": "flops = (64) * (123* 10**12) * (7 * 24 * 3600) * (0.3) = 1.4e21\n(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)\n\n'large models were trained on pods of 16 TPUs v3 (128 cores).' - from section 4.1 it was trained for 7 days from Table 2\n\nAgrees with 6CN:\nTokens seen: 512 (seq len) * 1024 (batch size) * 1 million (steps) = 5.24e11\nFLOPs: 6 * 335M * 5.24e11 = 1.05e21",
      "Training dataset": "Wikipedia,OPUS,OSCAR,OpenLegalData",
      "Training dataset size (gradients)": "36383733333",
      "Dataset size notes": "163.4GB from Table 1 in the paper\nassuming 167M words per GB (German Language) we have 163.4 * 167M * 4/3 tokens per word = 36,383,733,333",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2010.10906",
      "Reference": "German's Next Language Model",
      "Citations": "314.0",
      "Authors": "Branden Chan, Stefan Schweter, Timo M\u00f6ller",
      "Abstract": "In this work we present the experiments which lead to the creation of our BERT and ELECTRA based German language models, GBERT and GELECTRA. By varying the input training data, model size, and the presence of Whole Word Masking (WWM) we were able to attain SoTA performance across a set of document classification and named entity recognition (NER) tasks for both models of base and large size. We adopt an evaluation driven approach in training these models and our results indicate that both adding more data and utilizing WWM improve model performance. By benchmarking against existing German models, we show that these models are the best German models to date. Our trained models will be made publicly available to the research community. ",
      "Organization categorization": "Industry,Government",
      "Country (of organization)": "Germany,Germany",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "'we were able to attain SoTA performance across a set of document classification and named entity recognition (NER) tasks for both models of base and large size.'",
      "Epochs": "",
      "Training time (hours)": "168.0",
      "Training time notes": "7 days from Table 2",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "64.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "2392.2883349806307",
      "Compute cost notes": "",
      "Training power draw (W)": "58570.010422244966",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "MIT: https://huggingface.co/deepset/gelectra-large",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "mT5-XXL",
      "Organization": "Google,Google Research",
      "Publication date": "2020-10-20",
      "Domain": "Language",
      "Task": "Language modeling,Translation,Language modeling/generation,Question answering",
      "Parameters": "13000000000.0",
      "Parameters notes": "13 billion",
      "Training compute (FLOP)": "8.2e+22",
      "Training compute notes": "\"We pre-train our mT5 model variants for 1 million steps on batches of 1024 length-1024 input sequences, corresponding to roughly 1 trillion input tokens total.\"\n\n1 million steps * 1024 batchsize * 1024 length * 13 billion params * 6 = 8.2e22\n\nIgnores fine-tuning compute; this is likely a small fraction of pre-training compute.",
      "Training dataset": "mC4",
      "Training dataset size (gradients)": "1000000000000",
      "Dataset size notes": "The model was trained on a subset of 1 trillion tokens.\nFull mC4 corpus has data \"totaling 6.6B pages and 6.3T tokens\"\nDistribution by language is in Appendix A.",
      "Confidence": "Confident",
      "Link": "https://aclanthology.org/2021.naacl-main.41/",
      "Reference": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer",
      "Citations": "2892.0",
      "Authors": "Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel",
      "Abstract": "The recent \u201cText-to-Text Transfer Transformer\u201d (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent \u201caccidental translation\u201d in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.",
      "Organization categorization": "Industry,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\"Table 2 presents our main results, with perlanguage breakdowns for each task given in Appendix B. Our largest model mT5-XXL exceeds state-of-the-art on all classification and QA tasks and is near SOTA on NER (69.2 vs. 70.1).\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "179442.0291399545",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "1048576.0",
      "Batch size notes": "\"We pre-train our mT5 model variants for 1 million steps on batches of 1024 length-1024 input sequences, corresponding to roughly 1 trillion input tokens total.\"",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": " Apache 2.0 license\ntraining code: https://github.com/google-research/multilingual-t5",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Conformer + Wav2vec 2.0 + Noisy Student",
      "Organization": "Google,Google Research,Google Brain",
      "Publication date": "2020-10-20",
      "Domain": "Speech",
      "Task": "Speech recognition (ASR)",
      "Parameters": "1000000000.0",
      "Parameters notes": "1B for XXL model",
      "Training compute (FLOP)": "7.6e+21",
      "Training compute notes": "\"We train with global batch size 2048 on 256/512 Google TPU V3 cores for 3-4 days for the XL/XXL models respectively...\nWe fine-tune the pre-trained checkpoints (400k steps) with global batch\nsize 1024/512 on 256/512 Google TPU v3 cores for 1-3 days for the XL/XXL models\"\n\nTPU v3 chips are 123 teraflop/s. 2 chips per core\n\n512 cores * 7 days * 24 * 3600 * 123 tflops * (1 chip/2 cores) * 0.4 (assumed utilization) = 7.6e21",
      "Training dataset": "LibriLight",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2010.10504v2",
      "Reference": "Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition",
      "Citations": "325.0",
      "Authors": "Yu Zhang, James Qin, Daniel S. Park, Wei Han, Chung-Cheng Chiu, Ruoming Pang, Quoc V. Le, Yonghui Wu",
      "Abstract": "We employ a combination of recent developments in semi-supervised learning for automatic speech recognition to obtain state-of-the-art results on LibriSpeech utilizing the unlabeled audio of the Libri-Light dataset. More precisely, we carry out noisy student training with SpecAugment using giant Conformer models pre-trained using wav2vec 2.0 pre-training. By doing so, we are able to achieve word-error-rates (WERs) 1.4%/2.6% on the LibriSpeech test/test-other sets against the current state-of-the-art WERs 1.7%/3.3%.",
      "Organization categorization": "Industry,Industry,Industry",
      "Country (of organization)": "United States of America,United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"By doing so, we are able to achieve\nword-error-rates (WERs) 1.4%/2.6% on the LibriSpeech test/test-other sets against\nthe current state-of-the-art WERs 1.7%/3.3%.\"",
      "Epochs": "",
      "Training time (hours)": "168.0",
      "Training time notes": "7 days",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "256.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "9449.541661137231",
      "Compute cost notes": "",
      "Training power draw (W)": "234285.25901630375",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "LUKE",
      "Organization": "University of Washington,National Institute of Informatics",
      "Publication date": "2020-10-02",
      "Domain": "Language",
      "Task": "Question answering,Relation extraction,Named entity recognition (NER)",
      "Parameters": "483000000.0",
      "Parameters notes": "\"The total number of parameters is approximately 483 M, consisting of 355 M in RoBERTa and 128 M in our entity embeddings\"",
      "Training compute (FLOP)": "1.8144e+22",
      "Training compute notes": "Uses RoBERTa Large as a base model, which used 1.66e22 FLOPs in training.\n\nLUKE's additional training was:\n(16) * (1.25e14) * (30 * 24 * 3600) * (0.3) = 1.5552e21\n(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)\n\nfrom appendix A: \"Werun the pretraining on NVIDIA\u2019s PyTorch Docker\ncontainer 19.02 hosted on a server with two Intel Xeon Platinum 8168 CPUs and 16 NVIDIA Tesla V100 GPUs. The training takes approximately 30 days.\"\n\nAssuming 16 bit tensor core computations, 1.25e14 FLOP/s per V100\n\nTotal: 1.65888e22 + 1.5552e21 = 1.8144e22",
      "Training dataset": "Wikipedia",
      "Training dataset size (gradients)": "4666666667",
      "Dataset size notes": "\"As input corpus for pretraining, we use the December 2018 version of Wikipedia, comprising approximately 3.5 billion words and 11 million entity annotations. \"",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2010.01057v1",
      "Reference": "LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention",
      "Citations": "718.0",
      "Authors": "Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto",
      "Abstract": "Entity representations are useful in natural language tasks involving entities. In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer. The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them. Our model is trained using a new pretraining task based on the masked language model of BERT. The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia. We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores. The proposed model achieves impressive empirical performance on a wide range of entity-related tasks. In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering). Our source code and pretrained representations are available at this https://github.com/studio-ousia/luke",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America,Japan",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "from abstract \"In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering).\"",
      "Epochs": "90.0",
      "Training time (hours)": "720.0",
      "Training time notes": "see compute notes",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "16.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "4186.383565732907",
      "Compute cost notes": "",
      "Training power draw (W)": "9765.799615782105",
      "Base model": "RoBERTa Large",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "LUKE's additional training was:\n(16) * (1.25e14) * (30 * 24 * 3600) * (0.3) = 1.5552e21\n(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)\n\nfrom appendix A: \"Werun the pretraining on NVIDIA\u2019s PyTorch Docker\ncontainer 19.02 hosted on a server with two Intel Xeon Platinum 8168 CPUs and 16 NVIDIA Tesla V100 GPUs. The training takes approximately 30 days.\"\n\nAssuming 16 bit tensor core computations, 1.25e14 FLOP/s per V100LUKE's additional training was:LUKE's additional training was:\n(16) * (1.25e14) * (30 * 24 * 3600) * (0.3) = 1.5552e21(16) * (1.25e14) * (30 * 24 * 3600) * (0.3) = 1.5552e21\n(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)\n\nfrom appendix A: \"Werun the pretraining on NVIDIA\u2019s PyTorch Dockerfrom appendix A: \"Werun the pretraining on NVIDIA\u2019s PyTorch Docker\ncontainer 19.02 hosted on a server with two Intel Xeon Platinum 8168 CPUs and 16 NVIDIA Tesla V100 GPUs. The training takes approximately 30 days.\"container 19.02 hosted on a server with two Intel Xeon Platinum 8168 CPUs and 16 NVIDIA Tesla V100 GPUs. The training takes approximately 30 days.\"\n\nAssuming 16 bit tensor core computations, 1.25e14 FLOP/s per V100Assuming 16 bit tensor core computations, 1.25e14 FLOP/s per V100",
      "Batch size": "2048.0",
      "Batch size notes": "table in appendix A",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "apache 2.0: https://github.com/studio-ousia/luke?tab=readme-ov-file\n\ndata is wikimedia, which has a commercial license: https://dumps.wikimedia.org/legal.html\n\npretraining: https://github.com/studio-ousia/luke/blob/master/pretraining.md ",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ProBERTa",
      "Organization": "University of Illinois Urbana-Champaign (UIUC),Reed College",
      "Publication date": "2020-09-01",
      "Domain": "Biology",
      "Task": "Proteins,Protein representation learning,Protein classification,Protein interaction prediction",
      "Parameters": "44000000.0",
      "Parameters notes": "\"In total, our model has approximately 44M trainable parameters.\"",
      "Training compute (FLOP)": "9.72e+18",
      "Training compute notes": "\"we pre-train PRoBERTa on 4 NVIDIA V100 GPUs in 18 hours\"\n4 * 125 tFLOP/s * 18 * 3600 * 0.3 (assumed utilization) = 9.72e18",
      "Training dataset": "UniProtKB/Swiss-Prot",
      "Training dataset size (gradients)": "58320000",
      "Dataset size notes": "450k sequences * 129.6 tokens per sequence = 58,320,000 tokens",
      "Confidence": "Confident",
      "Link": "https://dl.acm.org/doi/10.1145/3388440.3412467",
      "Reference": "Transforming the Language of Life: Transformer Neural Networks for Protein Prediction Tasks",
      "Citations": "97.0",
      "Authors": "Ananthan Nambiar, Maeve Heflin, Simon Liu, Sergei Maslov, Mark Hopkins, Anna Ritz",
      "Abstract": "The scientific community is rapidly generating protein sequence information, but only a fraction of these proteins can be experimentally characterized. While promising deep learning approaches for protein prediction tasks have emerged, they have computational limitations or are designed to solve a specific task. We present a Transformer neural network that pre-trains task-agnostic sequence representations. This model is fine-tuned to solve two different protein prediction tasks: protein family classification and protein interaction prediction. Our method is comparable to existing state-of-the-art approaches for protein family classification while being much more general than other architectures. Further, our method outperforms all other approaches for protein interaction prediction. These results offer a promising framework for fine-tuning the pre-trained sequence representations for other protein prediction tasks.",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Furthermore, we used embeddings from PRoBERTa for a fundamentally different problem, PPI prediction, using two different\ndatasets generated from the HIPPIE database and found that with\nsufficient data, it substantially outperforms the current state-of-theart method in the conservative scenario.\"",
      "Epochs": "",
      "Training time (hours)": "18.0",
      "Training time notes": "",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "4.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "26.206992435694062",
      "Compute cost notes": "",
      "Training power draw (W)": "2443.1359420423914",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "",
      "Accessibility notes": "no clear license\nhttps://github.com/annambiar/PRoBERTa",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ERNIE-GEN (large)",
      "Organization": "Baidu",
      "Publication date": "2020-08-06",
      "Domain": "Language",
      "Task": "Language modeling,Language modeling/generation,Text summarization,Chat",
      "Parameters": "340000000.0",
      "Parameters notes": "\"We train a base model ERNIEGENBASE (L=12, H=768, A=12, Total Parameters=110M)1\nand a large model ERNIE-GENLARGE (L=24, H=1024,\nA=16, Total Parameters=340M) with parameters initialized\nby BERTBASE and BERTLARGE respectively\"",
      "Training compute (FLOP)": "2e+20",
      "Training compute notes": "430GB text for 1 epoch\n\napprox 430 * 200 million words = 86B words, or 100B tokens per https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.ieihc08p8dn0\n\n6 * 340 million params * 100 billion tokens ~= 2e20",
      "Training dataset": "CC-News,BookCorpus (BooksCorpus, Toronto Book Corpus),WebText2,Wikipedia,C4",
      "Training dataset size (gradients)": "114666666666.66667",
      "Dataset size notes": "approx 430 * 200 million words = ~86B words, per https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.ieihc08p8dn0",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/2001.11314",
      "Reference": "ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation",
      "Citations": "132.0",
      "Authors": "Dongling Xiao, Han Zhang, Yukun Li, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang",
      "Abstract": "Current pre-training works in natural language generation pay little attention to the problem of exposure bias on downstream tasks. To address this issue, we propose an enhanced multi-flow sequence to sequence pre-training and fine-tuning framework named ERNIE-GEN, which bridges the discrepancy between training and inference with an infilling generation mechanism and a noise-aware generation method. To make generation closer to human writing patterns, this framework introduces a span-by-span generation flow that trains the model to predict semantically-complete spans consecutively rather than predicting word by word. Unlike existing pre-training methods, ERNIE-GEN incorporates multi-granularity target sampling to construct pre-training data, which enhances the correlation between encoder and decoder. Experimental results demonstrate that ERNIE-GEN achieves state-of-the-art results with a much smaller amount of pre-training data and parameters on a range of language generation tasks, including abstractive summarization (Gigaword and CNN/DailyMail), question generation (SQuAD), dialogue generation (Persona-Chat) and generative question answering (CoQA).",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Empirically, ERNIE-GEN is particularly effective and achieves state-of-the-art results on a range of NLG tasks including abstractive summarization (Gigaword and CNN/DailyMail), question generation (SQuAD), dialogue response generation (Persona-Chat) and generative question answering (CoQA)\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "Open access (non-commercial)",
      "Accessibility notes": "https://github.com/PaddlePaddle/ERNIE/tree/repro/ernie-gen\n\ncode/weights with unclear license",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DeLighT",
      "Organization": "University of Washington,Allen Institute for AI,Facebook AI Research",
      "Publication date": "2020-08-03",
      "Domain": "Language",
      "Task": "Language modeling,Translation",
      "Parameters": "99000000.0",
      "Parameters notes": "99M (Table 4b)",
      "Training compute (FLOP)": "3.8016e+18",
      "Training compute notes": "6 FLOP / parameter / token * 99 * 10^6 parameters * 100000 steps * 64000 tokens per batch = 3.8016e+18 FLOP\n\n31330000000000 FLOP / second / GPU * 8 GPUs * 30 hours [assumed based on smaller models reported training time] * 3600 sec / hour * 0.3 [assumed utilization] = 8.120736e+18 FLOP\n\nOperation counting method uses less assumptions",
      "Training dataset": "WikiText-103",
      "Training dataset size (gradients)": "103000000",
      "Dataset size notes": "\"100K iterations with a context length of 512 and an effective\nbatch size of 64K tokens.\"\n\n100000*64000/103000000 = 62.14 epochs",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2008.00623",
      "Reference": "DeLighT: Deep and Light-weight Transformer",
      "Citations": "98.0",
      "Authors": "Sachin Mehta, Marjan Ghazvininejad, Srinivasan Iyer, Luke Zettlemoyer, Hannaneh Hajishirzi",
      "Abstract": "We introduce a deep and light-weight transformer, DeLighT, that delivers similar or better performance than standard transformer-based models with significantly fewer parameters. DeLighT more efficiently allocates parameters both (1) within each Transformer block using the DeLighT transformation, a deep and light-weight transformation, and (2) across blocks using block-wise scaling, which allows for shallower and narrower DeLighT blocks near the input and wider and deeper DeLighT blocks near the output. Overall, DeLighT networks are 2.5 to 4 times deeper than standard transformer models and yet have fewer parameters and operations. Experiments on benchmark machine translation and language modeling tasks show that DeLighT matches or improves the performance of baseline Transformers with 2 to 3 times fewer parameters on average. Our source code is available at: \\url{this https URL}",
      "Organization categorization": "Academia,Research collective,Industry",
      "Country (of organization)": "United States of America,United States of America,United States of America,France",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Comparison with state-of-the-art methods on machine translation corpora. DeLighT delivers similar or better performance than state-of-the-art models with fewer parameters.\"",
      "Epochs": "62.14",
      "Training time (hours)": "30.0",
      "Training time notes": "Table 5 reports training time for 54M translation model (23h)\nit should be more for the 99M language modeling model.",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "8.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "4889.428515149248",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "64000.0",
      "Batch size notes": "\"effective batch size of 64K tokens.\"",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT, training and evaluation for WT103: https://github.com/sacmehta/delight/blob/master/readme_files/lm/wikitext103.md ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "EfficientDet",
      "Organization": "Google Brain",
      "Publication date": "2020-07-27",
      "Domain": "Vision",
      "Task": "Object detection",
      "Parameters": "77000000.0",
      "Parameters notes": "\"EfficientDet-D7 achieves stateof-the-art 55.1 AP on COCO test-dev with 77M parameters and 410B FLOPs\"",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "COCO 2017",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/1911.09070",
      "Reference": "EfficientDet: Scalable and Efficient Object Detection",
      "Citations": "4701.0",
      "Authors": "Mingxing Tan, Ruoming Pang, Quoc V. Le",
      "Abstract": "Model efficiency has become increasingly important in computer vision. In this paper, we systematically study neural network architecture design choices for object detection and propose several key optimizations to improve efficiency. First, we propose a weighted bi-directional feature pyramid network (BiFPN), which allows easy and fast multiscale feature fusion; Second, we propose a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations and better backbones, we have developed a new family of object detectors, called EfficientDet, which consistently achieve much better efficiency than prior art across a wide spectrum of resource constraints. In particular, with single model and single-scale, our EfficientDet-D7 achieves state-of-the-art 55.1 AP on COCO test-dev with 77M parameters and 410B FLOPs, being 4x - 9x smaller and using 13x - 42x fewer FLOPs than previous detectors. Code is available at this https URL.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\"EfficientDet-D7 achieves stateof-the-art 55.1 AP on COCO test-dev with 77M parameters and 410B FLOPs\"",
      "Epochs": "600.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Repo is Apache 2.0:\nhttps://github.com/google/automl/tree/master/efficientdet",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Hopfield Networks (2020)",
      "Organization": "Johannes Kepler University Linz,Institute of Advanced Research in Artificial Intelligence,University of Oslo",
      "Publication date": "2020-07-16",
      "Domain": "Biology,Vision,Language,Medicine",
      "Task": "Drug discovery,Language modeling,Object recognition,Cancer diagnosis",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "BACE,SIDER",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/2008.02217",
      "Reference": "Hopfield Networks is All You Need",
      "Citations": "535.0",
      "Authors": "Hubert Ramsauer, Bernhard Sch\u00e4fl, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner, Milena Pavlovi\u0107, Geir Kjetil Sandve, Victor Greiff, David Kreil, Michael Kopp, G\u00fcnter Klambauer, Johannes Brandstetter, Sepp Hochreiter",
      "Abstract": "We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: this https URL",
      "Organization categorization": "Academia,Academia,Academia",
      "Country (of organization)": "Austria,Austria,Norway",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Hopfield layers yielded a new state-ofthe-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets\"\n\n\"Our approach has set a new state-of-the-art and has outperformed other methods <..> on the datasets Tiger, Elephant and UCSB Breast Cancer (see Table 1).\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "copyleft-like license, derivative works must retain this license. code here:\nhttps://github.com/ml-jku/hopfield-layers/blob/master/LICENSE",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "SemExp",
      "Organization": "Carnegie Mellon University (CMU),Facebook AI Research",
      "Publication date": "2020-07-02",
      "Domain": "Robotics",
      "Task": "Object detection",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Gibson,Matterport3D (MP3D)",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "\"Our training and test set consists of a total of 86 scenes (25 Gibson tiny and 61 MP3D) and 16 scenes (5 Gibson tiny and 11 MP3D), respectively\"",
      "Confidence": "Unknown",
      "Link": "https://proceedings.neurips.cc/paper/2020/file/2c75cf2681788adaca63aa95ae028b22-Paper.pdf\nhttps://arxiv.org/abs/2007.00643",
      "Reference": "Object Goal Navigation using Goal-Oriented Semantic Exploration",
      "Citations": "632.0",
      "Authors": "Devendra Singh Chaplot, Dhiraj Gandhi, Abhinav Gupta, Ruslan Salakhutdinov",
      "Abstract": "This work studies the problem of object goal navigation which involves navigating to an instance of the given object category in unseen environments. End-to-end learning-based navigation methods struggle at this task as they are ineffective at exploration and long-term planning. We propose a modular system called, \u2018GoalOriented Semantic Exploration\u2019 which builds an episodic semantic map and uses it to explore the environment efficiently based on the goal object category. Empirical results in visually realistic simulation environments show that the proposed model outperforms a wide range of baselines including end-to-end learning-based methods as well as modular map-based methods and led to the winning entry of the CVPR2020 Habitat ObjectNav Challenge. Ablation analysis indicates that the proposed model learns semantic priors of the relative arrangement of objects in a scene, and uses them to explore efficiently. Domain-agnostic module design allows us to transfer our model to a mobile robot platform and achieve similar performance for object goal navigation in the real-world.",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "United States of America,United States of America,France",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Our method achieves state-of-the-art performance on the object goal navigation task and won the CVPR2020 Habitat ObjectNav challenge\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT code/weights: https://github.com/devendrachaplot/Object-Goal-Navigation",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GShard (dense)",
      "Organization": "Google",
      "Publication date": "2020-06-30",
      "Domain": "Language",
      "Task": "Translation",
      "Parameters": "2300000000.0",
      "Parameters notes": "\"Our best quality dense single Transformer model (2.3B parameters) achieving \u2206BLEU of 6.1, was trained with GPipe [15] on 2048 TPU v3 cores for 6 weeks or total of 235.5 TPU v3 core-years.\"",
      "Training compute (FLOP)": "4.765e+22",
      "Training compute notes": "Trained for a total of 235.5 TPU v3 core-years.\nHardware estimate: 235.5 * 365.25 * 24 * 3600 * (1.23e14 / 2) * 0.3 = 1.371e23\n\nFootnote 10 indicates 300k steps and 4M tokens/step -> 1.2T tokens\nArithmetic estimate: 6 * 2.3B * 1.2T = 1.656e22 FLOPs\n\nGeometric mean: sqrt(1.371e23 * 1.656e22) = 4.765e22",
      "Training dataset": "",
      "Training dataset size (gradients)": "346666666667",
      "Dataset size notes": "\"We focus on improving the translation quality (measured in terms of BLEU score [48]) from all 100 languages to English. This resulted in approximately 13 billion training examples to be used for model training\"\n\nEach example is a sentence pair. Assuming 20 words per sentence and 4/3 tokens per word, that is 13*20*4/3 billion tokens",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2006.16668",
      "Reference": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding",
      "Citations": "1584.0",
      "Authors": "Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen",
      "Abstract": "Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art\"\n\nI don't see any standard benchmark that they claim SOTA on",
      "Epochs": "",
      "Training time (hours)": "1008.0",
      "Training time notes": "6 weeks = 1008 hours",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "1024.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "266200.0315979732",
      "Compute cost notes": "",
      "Training power draw (W)": "939481.3420790087",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "4000000.0",
      "Batch size notes": "Table 3, bolded row is best model",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Unreleased",
      "Accessibility notes": "training code is open, Apache: https://github.com/tensorflow/lingvo/tree/master/lingvo/tasks/lm",
      "Numerical format": "FP32",
      "Frontier model": "True",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "1006832.2819141106",
      "Training compute cost (upfront)": "11855440.802498134"
    },
    {
      "Model": "GPT-3 175B (davinci)",
      "Organization": "OpenAI",
      "Publication date": "2020-05-28",
      "Domain": "Language",
      "Task": "Text autocompletion,Language modeling/generation",
      "Parameters": "174600000000.0",
      "Parameters notes": "\"we train GPT-3, an autoregressive language model with 175 billion parameters\"\nRather, it's 174.6 billion.\nTable D.1: 174,600 million parameters",
      "Training compute (FLOP)": "3.14e+23",
      "Training compute notes": "Table D.1\nhttps://arxiv.org/abs/2005.14165",
      "Training dataset": "Common Crawl,WebText2,Wikipedia,Books1,Books2",
      "Training dataset size (gradients)": "238000000000",
      "Dataset size notes": "From table 2.2, we determine that there are 410 + 19 + 12 + 55 + 3 = 499 billion tokens. \n\nWe multiply this by 0.75 to give 374B words. \n\n3.74e11\n\n========================\n[Anson: I think the calculation below doesn't look at all the data, the CommonCrawl data only constitutes 60% of the data. Multiplying by 5/3 gives 4.75e11]\n\n\"The CommonCrawl data was downloaded from 41 shards of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before filtering and 570GB after filtering, roughly equivalent to 400 billion byte-pair-encoded tokens. \"\n\nConverted to words using \nhttp://extraconversion.com/data-storage/gigabits/gigabits-to-words.html\n\n2.85e11",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2005.14165",
      "Reference": "Language Models are Few-Shot Learners",
      "Citations": "51687.0",
      "Authors": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei",
      "Abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,Training cost",
      "Notability criteria notes": "",
      "Epochs": "0.6",
      "Training time (hours)": "355.2",
      "Training time notes": "14.8 days according to https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf",
      "Training hardware": "NVIDIA Tesla V100 DGXS 32 GB",
      "Hardware quantity": "10000.0",
      "Hardware utilization (MFU)": "0.1966",
      "Training compute cost (2023 USD)": "2116866.2428390156",
      "Compute cost notes": "",
      "Training power draw (W)": "5100759.605963441",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "3200000.0",
      "Batch size notes": "3.2M, per table 2.1",
      "Model accessibility": "API access",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "https://openai.com/blog/openai-api\n",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "4493219.860317459",
      "Training compute cost (upfront)": "232236674.505193"
    },
    {
      "Model": "DETR",
      "Organization": "Facebook",
      "Publication date": "2020-05-26",
      "Domain": "Vision",
      "Task": "Object detection",
      "Parameters": "60000000.0",
      "Parameters notes": "60M per Table 1",
      "Training compute (FLOP)": "4e+20",
      "Training compute notes": "\"Training the baseline model for 300 epochs on 16 V100 GPUs takes 3 days, with 4 images per GPU (hence a total batch size of 64). For the longer schedule used to compare with Faster R-CNN we train for 500 epochs with learning rate drop after 400 epochs. This schedule adds 1.5 AP compared to the shorter schedule.\"\n\n48 V100-days for baseline DETR model. Larger model had 1.5x the params and 5/3 as many epochs, so required ~2.5x as much training compute.\n\n125 teraflop/s * 2.5 * 48 * 24 * 3600 * 0.3 (assumed utilization) ~ 4e20",
      "Training dataset": "COCO 2017",
      "Training dataset size (gradients)": "826000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2005.12872",
      "Reference": "End-to-End Object Detection with Transformers",
      "Citations": "16219.0",
      "Authors": "Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko",
      "Abstract": "Abstract. We present a new method that views object detection as a\ndirect set prediction problem. Our approach streamlines the detection\npipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation\nthat explicitly encode our prior knowledge about the task. The main\ningredients of the new framework, called DEtection TRansformer or\nDETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given\na fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output\nthe final set of predictions in parallel. The new model is conceptually\nsimple and does not require a specialized library, unlike many other\nmodern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation\nin a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at\nhttps://github.com/facebookresearch/detr.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "500.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "959.9097627206426",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "64.0",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache 2.0: https://github.com/facebookresearch/detr",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Retrieval-Augmented Generator",
      "Organization": "Facebook,New York University (NYU),University College London (UCL)",
      "Publication date": "2020-05-22",
      "Domain": "Language",
      "Task": "Question answering,Retrieval-augmented generation",
      "Parameters": "626000000.0",
      "Parameters notes": "\"Our RAG models contain the trainable parameters for the BERT-base query and document encoder of DPR, with 110M parameters each (although we do not train the document encoder ourselves) and 406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable parameters\"",
      "Training compute (FLOP)": "",
      "Training compute notes": "not enough info, e.g. no training time reported:\n\n\"We train with mixed precision floating point arithmetic [40], distributing training across 8, 32GB NVIDIA V100 GPUs, though training and inference can be run on one GPU\"",
      "Training dataset": "Wikipedia,NQ (Natural Questions)",
      "Training dataset size (gradients)": "3074560",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2005.11401v4",
      "Reference": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
      "Citations": "10066.0",
      "Authors": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, Douwe Kiela",
      "Abstract": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
      "Organization categorization": "Industry,Academia,Academia",
      "Country (of organization)": "United States of America,United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\"Our RAG models achieve state-of-the-art results on open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] \"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA Tesla V100 PCIe 32 GB",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "It's in HF transformers library:\nhttps://huggingface.co/docs/transformers/en/model_doc/rag\n\nthis library has an apache license: https://github.com/huggingface/transformers/blob/main/LICENSE",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Conformer",
      "Organization": "Google",
      "Publication date": "2020-05-16",
      "Domain": "Speech",
      "Task": "Speech recognition (ASR)",
      "Parameters": "118800000.0",
      "Parameters notes": "118.8M for Conformer(L)",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "LibriSpeech",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2005.08100v1",
      "Reference": "Conformer: Convolution-augmented Transformer for Speech Recognition",
      "Citations": "3740.0",
      "Authors": "Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, Ruoming Pang",
      "Abstract": "Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs). Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively. In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way. To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer. Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external language model on test/testother. We also observe competitive performance of 2.7%/6.3% with a small model of only 10M parameters.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\"Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external language model on test/testother\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ContextNet",
      "Organization": "Google",
      "Publication date": "2020-05-07",
      "Domain": "Speech",
      "Task": "Speech recognition (ASR)",
      "Parameters": "112700000.0",
      "Parameters notes": "Table 5",
      "Training compute (FLOP)": "",
      "Training compute notes": "Uses pre-trained joint network from https://arxiv.org/pdf/1811.06621, so total training compute should factor this in.",
      "Training dataset": "LibriSpeech",
      "Training dataset size (gradients)": "349200000",
      "Dataset size notes": "970 hours of speech in the LibreSpeech experiments. There is mention of a \"large scale experiment\" which trained on audio from YouTube videos. Tracing the references, it appears to be similar to the dataset used in https://arxiv.org/abs/1610.09975, which has 125,000 hours of transcribed audio for training. However, they mention in footnote 2 that the training and evaluation set have changed from previous experiments.\n125k hours at 13,680 words per hour = 1.71B words",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2005.03191v3",
      "Reference": "ContextNet: Improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context",
      "Citations": "290.0",
      "Authors": "Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming Pang, Yonghui Wu",
      "Abstract": "Convolutional neural networks (CNN) have shown promising results for end-to-end speech recognition, albeit still behind other state-of-the-art methods in performance. In this paper, we study how to bridge this gap and go beyond with a novel CNN-RNN-transducer architecture, which we call ContextNet. ContextNet features a fully convolutional encoder that incorporates global context information into convolution layers by adding squeeze-and-excitation modules. In addition, we propose a simple scaling method that scales the widths of ContextNet that achieves good trade-off between computation and accuracy. We demonstrate that on the widely used LibriSpeech benchmark, ContextNet achieves a word error rate (WER) of 2.1%/4.6% without external language model (LM), 1.9%/4.1% with LM and 2.9%/7.0% with only 10M parameters on the clean/noisy LibriSpeech test sets. This compares to the previous best published system of 2.0%/4.6% with LM and 3.9%/11.3% with 20M parameters. The superiority of the proposed ContextNet model is also verified on a much larger internal dataset.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We demonstrate that on the widely used Librispeech benchmark, ContextNet achieves a word error rate (WER) of 2.1%/4.6% without external language model (LM), 1.9%/4.1% with LM and 2.9%/7.0% with only 10M parameters on the clean/noisy LibriSpeech test sets. This compares to the best previously published model of 2.0%/4.6% with LM and 3.9%/11.3% with 20M parameters\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "NAS+ESS (23M)",
      "Organization": "Northeastern University (China),NiuTrans Research,Kingsoft",
      "Publication date": "2020-05-06",
      "Domain": "Language",
      "Task": "Neural Architecture Search - NAS,Language modeling",
      "Parameters": "23000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Penn TreeBank (PTB)",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/pdf/2005.02593",
      "Reference": "Learning Architectures from an Extended Search Space for Language Modeling",
      "Citations": "",
      "Authors": "Yinqiao Li, Chi Hu, Yuhao Zhang, Nuo Xu, Yufan Jiang, Tong Xiao, Jingbo Zhu, Tongran Liu, Changliang Li",
      "Abstract": "Neural architecture search (NAS) has advanced significantly in recent years but most NAS systems restrict search to learning architectures of a recurrent or convolutional cell. In this paper, we extend the search space of NAS. In particular, we present a general approach to learn both intra-cell and inter-cell architectures (call it ESS). For a better search result, we design a joint learning method to perform intra-cell and inter-cell NAS simultaneously. We implement our model in a differentiable architecture search system. For recurrent neural language modeling, it outperforms a strong baseline significantly on the PTB and WikiText data, with a new state-of-the-art on PTB. Moreover, the learned architectures show good transferability to other systems. E.g., they improve state-of-the-art systems on the CoNLL and WNUT named entity recognition (NER) tasks and CoNLL chunking task, indicating a promising line of research on large-scale pre-learned architectures.",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "China,China,China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Our ESS method achieves state-of-the-art result on the PTB task\"",
      "Epochs": "3000.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA GeForce GTX 1080 Ti",
      "Hardware quantity": "1.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "280.3988579029138",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "523.2496386408993",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "UnifiedQA",
      "Organization": "Allen Institute for AI,University of Washington",
      "Publication date": "2020-05-02",
      "Domain": "Language",
      "Task": "Question answering",
      "Parameters": "11000000000.0",
      "Parameters notes": "11B from appendix A.2 : Model sizes: \"Most of the experiments are done on T5(11B) which has 11 billion parameters. We also report experiments with BART (large) with 440 million parameters.\"",
      "Training compute (FLOP)": "1.65e+19",
      "Training compute notes": "A.2: \"In the experiments, we use v3-8 TPUs for T5 models... pretraining UNIFIEDQA approximately takes about 36 hours on T5(11B)\"\n(Note that v3-8 refers to 8 TPUv3 cores, or 4 TPUv3 chips. From the JAX github repo: \"In a TPU v3-8 you have 4 chips, 2 cores per chip, and each core has 16GB of HBM. So this looks to JAX like 8 devices with 16GB each.\")\n\n4 * 1.23e14 * 36 * 3600 * 0.3 = 1.91e19\n\nAlternatively, input (ouput) size of 512 (100) tokens, batch size of 8, trained for 100k steps. Input tokens only need the forward pass.\n((2 * 11B * 512 * 8) + (6 * 11B * 100 * 8)) * 100k = 1.43e19\n\nTook geometric mean of these estimates:\nsqrt(1.91e19*1.43e19) = 1.65e19",
      "Training dataset": "SQuAD 1.1",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "Table 2:\nSQuAD 1.1: 87k examples, avg total length of 136.2 + 3.0\nSQuAD 2.0: 130k examples, avg total length of 139.9 + 2.6\nNarrativeQA: 65k examples, avg total length of 563.6 + 6.2\nRACE: 87k examples, avg total length of 317.9 + 6.9\nARC (easy): 2k examples, avg total length of 39.4 + 3.7\nARC (hard): 1k examples, avg total length of  47.4 + 5.0\nOBQA: 4k examples, avg total length of 28.7 + 3.6\nMCTest: 1.4k examples, avg total length of 245.4 + 4.0\nBoolQ: 9k examples, avg total length of 105.1 + 1.0\n\nTotal tokens: 97,309,860",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2005.00700v3",
      "Reference": "UnifiedQA: Crossing Format Boundaries With a Single QA System",
      "Citations": "793.0",
      "Authors": "Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, Hannaneh Hajishirzi",
      "Abstract": "Question answering (QA) tasks have been posed using a variety of formats, such as extractive span selection, multiple choice, etc. This has led to format-specialized models, and even to an implicit division in the QA community. We argue that such boundaries are artificial and perhaps unnecessary, given the reasoning abilities we seek to teach are not governed by the format. As evidence, we use the latest advances in language modeling to build a single pre-trained QA model, UnifiedQA, that performs surprisingly well across 17 QA datasets spanning 4 diverse formats. UnifiedQA performs on par with 9 different models that were trained on individual datasets themselves. Even when faced with 12 unseen datasets of observed formats, UnifiedQA performs surprisingly well, showing strong generalization from its out-of-format training data. Finally, simply fine-tuning this pre-trained QA model into specialized models results in a new state of the art on 6 datasets, establishing UnifiedQA as a strong starting point for building QA systems.",
      "Organization categorization": "Research collective,Academia",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Table 5\n\"We then introduce UNIFIEDQA (\u00a73.2) that is a QA system trained on datasets in multiple formats, indicating new state-of-the-art results on 10 datasets and generalization to unseen datasets.\"",
      "Epochs": "1.88",
      "Training time (hours)": "36.0",
      "Training time notes": "Appendix A.2: \"pretraining UNIFIEDQA approximately takes about 36 and 55 hours, on T5(11B) and BART models, respectively.\"",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "8.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "59.122086907352475",
      "Compute cost notes": "",
      "Training power draw (W)": "7349.34790019072",
      "Base model": "T5-11B",
      "Finetune compute (FLOP)": "3.8e+19",
      "Finetune compute notes": "\"\u2022 Infrastructure: In the experiments, we use v3-8 TPUs for T5 models, and eight 32GB GPUs for\nBART models.\n\u2022 Time spent to build UNIFIEDQA: pretraining UNIFIEDQA approximately takes about 36 and 55\nhours, on T5(11B) and BART models, respectively.\"\n\n8 * 123 TFLOPS * 36 * 3600 * 0.3 (utilization assumption) = 3.8e19",
      "Batch size": "27.0",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache 2.0 license, includes models and training code: https://github.com/allenai/unifiedqa",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Once for All",
      "Organization": "MIT-IBM Watson AI Lab,Massachusetts Institute of Technology (MIT),IBM",
      "Publication date": "2020-04-29",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "7700000.0",
      "Parameters notes": "\"Since\nall of these sub-networks share the same weights (i.e., Wo) (Cheung et al., 2019), we only require\n7.7M parameters to store all of them. Without sharing, the total model size will be prohibitive\"",
      "Training compute (FLOP)": "6.237e+20",
      "Training compute notes": "4.2k V100-hours (table 1)\n0.33 utilization rate\nV100 FP16 Tensor FLOPs: 125000000000000\n\n4200 hours*60*60* 125000000000000 FLOP/s *0.33 utilization =623700000000000000000\n",
      "Training dataset": "ImageNet",
      "Training dataset size (gradients)": "1280000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1908.09791",
      "Reference": "Once for all: Train one network and specialize it for efficient deployment.",
      "Citations": "1416.0",
      "Authors": "Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han",
      "Abstract": "We address the challenging problem of efficient inference across many devices and resource constraints, especially on edge devices. Conventional approaches either manually design or use neural architecture search (NAS) to find a specialized neural network and train it from scratch for each case, which is computationally prohibitive (causing CO2 emission as much as 5 cars' lifetime) thus unscalable. In this work, we propose to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. We can quickly get a specialized sub-network by selecting from the OFA network without additional training. To efficiently train OFA networks, we also propose a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution). It can obtain a surprisingly large number of sub-networks (>1019) that can fit different hardware platforms and latency constraints while maintaining the same level of accuracy as training independently. On diverse edge devices, OFA consistently outperforms state-of-the-art (SOTA) NAS methods (up to 4.0% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and CO2 emission. In particular, OFA achieves a new SOTA 80.0% ImageNet top-1 accuracy under the mobile setting (<600M MACs). OFA is the winning solution for the 3rd Low Power Computer Vision Challenge (LPCVC), DSP classification track and the 4th LPCVC, both classification track and detection track. Code and 50 pre-trained models (for many devices & many latency constraints) are released at this https URL.",
      "Organization categorization": "Academia,Industry,Academia,Industry",
      "Country (of organization)": "United States of America,United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"In particular, OFA achieves a new SOTA 80.0% ImageNet top-1 accuracy under the mobile setting\"",
      "Epochs": "180.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "1753.9255676777682",
      "Compute cost notes": "from Table 1",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT license: https://github.com/mit-han-lab/once-for-all\n\nrepo contains inference and training code. models available via library",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Go-explore",
      "Organization": "Uber AI,OpenAI",
      "Publication date": "2020-04-27",
      "Domain": "Games",
      "Task": "Atari",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "40000000000",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/2004.12919",
      "Reference": "First return, then explore",
      "Citations": "405.0",
      "Authors": "Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O. Stanley, Jeff Clune",
      "Abstract": "The promise of reinforcement learning is to solve complex sequential decision problems autonomously by specifying a high-level reward function only. However, reinforcement learning algorithms struggle when, as is often the case, simple and intuitive rewards provide sparse and deceptive feedback. Avoiding these pitfalls requires thoroughly exploring the environment, but creating algorithms that can do so remains one of the central challenges of the field. We hypothesise that the main impediment to effective exploration originates from algorithms forgetting how to reach previously visited states (\"detachment\") and from failing to first return to a state before exploring from it (\"derailment\"). We introduce Go-Explore, a family of algorithms that addresses these two challenges directly through the simple principles of explicitly remembering promising states and first returning to such states before intentionally exploring. Go-Explore solves all heretofore unsolved Atari games and surpasses the state of the art on all hard-exploration games, with orders of magnitude improvements on the grand challenges Montezuma's Revenge and Pitfall. We also demonstrate the practical potential of Go-Explore on a sparse-reward pick-and-place robotics task. Additionally, we show that adding a goal-conditioned policy can further improve Go-Explore's exploration efficiency and enable it to handle stochasticity throughout training. The substantial performance gains from Go-Explore suggest that the simple principles of remembering states, returning to them, and exploring from them are a powerful and general approach to exploration, an insight that may prove critical to the creation of truly intelligent learning agents.",
      "Organization categorization": "Industry,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "not an absolute SOTA\n\n\"GoExplore solves all heretofore unsolved Atari games (meaning those for which algorithms could not previously outperform humans when evaluated following current community standards for Atari3) and surpasses the state of the art on all hard-exploration games\"\n\n\"the final mean performance of Go-Explore is both superhuman and surpasses the state of the art in all eleven games (except in Freeway where both Go-Explore and the state of the art reach the maximum score; Fig. 2b). These games include the grand challenges of Montezuma\u2019s Revenge, where Go-Explore quadruples the state-of-the-art score, and Pitfall, where Go-Explore surpasses average human performance while previous algorithms were unable to score any points\u2020. Also noteworthy is the performance on Private Eye, where Go-Explore is able to reliably achieve the highest possible score in 4 out of 5 runs, and the performance on Skiing, where Go-Explore outperforms human performance despite the fact that the reward structure of this game makes it notoriously hard to train on\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "Open access (non-commercial)",
      "Accessibility notes": "non-commercial code: https://github.com/uber-research/go-explore/blob/master/LICENSE",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "CURL",
      "Organization": "University of California (UC) Berkeley",
      "Publication date": "2020-04-08",
      "Domain": "Games",
      "Task": "Atari",
      "Parameters": "907264.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/2004.04136v4",
      "Reference": "CURL: Contrastive Unsupervised Representations for Reinforcement Learning",
      "Citations": "978.0",
      "Authors": "A Srinivas, M Laskin, P Abbeel",
      "Abstract": "We present CURL: Contrastive Unsupervised Representations for Reinforcement Learning. CURL extracts high-level features from raw pixels using contrastive learning and performs off-policy control on top of the extracted features. CURL outperforms prior pixel-based methods, both model-based and model-free, on complex tasks in the DeepMind Control Suite and Atari Games showing 1.9x and 1.2x performance gains at the 100K environment and interaction steps benchmarks respectively. On the DeepMind Control Suite, CURL is the first image-based algorithm to nearly match the sample-efficiency of methods that use state-based features. Our code is open-sourced and available at this https URL.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Table 1 and Table 2\n\n\"CURL achieves state-of-the-art performance on the majority (5 out of 6) environments benchmarked on DMControl500k\"\n\n\nNot an absolute SOTA on ALE (Atari 2600)\n\"CURL achieves state-of-the-art performance on 7 out of 26 environments. \"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT: https://github.com/MishaLaskin/curl\n\n",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Agent57",
      "Organization": "DeepMind",
      "Publication date": "2020-03-30",
      "Domain": "Games",
      "Task": "Atari",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/2003.13350",
      "Reference": "Agent57: Outperforming the Atari Human Benchmark",
      "Citations": "564.0",
      "Authors": "AP Badia, B Piot, S Kapturowski",
      "Abstract": "Atari games have been a long-standing benchmark in the reinforcement learning (RL) community for the past decade. This benchmark was proposed to test general competency of RL algorithms. Previous work has achieved good average performance by doing outstandingly well on many games of the set, but very poorly in several of the most challenging games. We propose Agent57, the first deep RL agent that outperforms the standard human benchmark on all 57 Atari games. To achieve this result, we train a neural network which parameterizes a family of policies ranging from very exploratory to purely exploitative. We propose an adaptive mechanism to choose which policy to prioritize throughout the training process. Additionally, we utilize a novel parameterization of the architecture that allows for more consistent and stable learning.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We propose Agent57, the first deep RL agent that outperforms the standard human benchmark on all 57 Atari games\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "MetNet",
      "Organization": "Google",
      "Publication date": "2020-03-24",
      "Domain": "Earth science",
      "Task": "Weather forecasting",
      "Parameters": "225000000.0",
      "Parameters notes": "\" In total this setting for MetNet has 225M parameters.\"",
      "Training compute (FLOP)": "9.510912e+18",
      "Training compute notes": "6 FLOP / parameter / token * 7045120000 updates [see training dataset size notes, \"Likely\" confidence] * 225000000 parameters = 9.510912e+18 FLOP\n\ncheck: they report training in 256 TPUs (TPU v3 most likely) \n9.510912e+18 FLOP / (123000000000000 FLOP / sec / chip * 256 TPUs * 3600 sec / hour * 0.3 [assumed utilization]) = 0.27 hours (seems to be an underestimation) -> 'Speculative'  confidence",
      "Training dataset": "MRMS,GOES-16",
      "Training dataset size (gradients)": "7045120000",
      "Dataset size notes": "\"we randomly extracted 13,717 test and validation samples and kept increasing the training set size\nuntil we observed no over-fitting at 1.72 million training samples\"",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/2003.12140",
      "Reference": "MetNet: A Neural Weather Model for Precipitation Forecasting",
      "Citations": "323.0",
      "Authors": "Casper Kaae S\u00f8nderby, Lasse Espeholt, Jonathan Heek, Mostafa Dehghani, Avital Oliver, Tim Salimans, Shreya Agrawal, Jason Hickey, Nal Kalchbrenner",
      "Abstract": "Weather forecasting is a long standing scientific challenge with direct social and economic impact. The task is suitable for deep neural networks due to vast amounts of continuously collected data and a rich spatial and temporal structure that presents long range dependencies. We introduce MetNet, a neural network that forecasts precipitation up to 8 hours into the future at the high spatial resolution of 1 km^2 and at the temporal resolution of 2 minutes with a latency in the order of seconds. MetNet takes as input radar and satellite data and forecast lead time and produces a probabilistic precipitation map. The architecture uses axial self-attention to aggregate the global context from a large input patch corresponding to a million square kilometers. We evaluate the performance of MetNet at various precipitation thresholds and find that MetNet outperforms Numerical Weather Prediction at forecasts of up to 7 to 8 hours on the scale of the continental United States.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"MetNet improves upon the current operational NWP system HRRR for up to 8 hours of lead time\"\n... \n\"Numerical Weather Prediction is the most successful framework to perform medium- and longrange (up to 6 days with high confidence) forecast to date (Bauer et al., 2015).\"\n\n\"MetNet outperforms HRRR up to 400 to 480 minutes and outperforms a strong optical flow method and the persistence baseline throughout the 480 minute range.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "256.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "235383.4758953672",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ELECTRA",
      "Organization": "Stanford University,Google,Google Brain",
      "Publication date": "2020-03-23",
      "Domain": "Language",
      "Task": "Text autocompletion",
      "Parameters": "335000000.0",
      "Parameters notes": "https://github.com/google-research/electra",
      "Training compute (FLOP)": "3.1e+21",
      "Training compute notes": "Table 8: \"ELECTRA-1.75M\" used 3.1e21 train FLOPs. Note that the actual parameter count is 335M. The 1.75M refers to the number of training steps.\n\nThis doesn't quite line up with a 6ND estimate, \n6 * 335M * (1.75M * 2048 * 128) = 9.22e20 FLOPs\nI'm inferring 128 sequence length, possibly this is 256 or 512?",
      "Training dataset": "BookCorpus (BooksCorpus, Toronto Book Corpus),Wikipedia,ClueWeb,Gigaword",
      "Training dataset size (gradients)": "33000000000",
      "Dataset size notes": "33B tokens or ~25B words\n\n\"For most experiments we pre-train on the same data as BERT, which consists\nof 3.3 Billion tokens from Wikipedia and BooksCorpus (Zhu et al., 2015). However, for our Large\nmodel we pre-trained on the data used for XLNet (Yang et al., 2019), which extends the BERT\ndataset to 33B tokens by including data from ClueWeb (Callan et al., 2009), CommonCrawl, and\nGigaword (Parker et al., 2011).\"",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2003.10555v1",
      "Reference": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators",
      "Citations": "2968.0",
      "Authors": "Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning",
      "Abstract": "Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.",
      "Organization categorization": "Academia,Industry,Industry",
      "Country (of organization)": "United States of America,United States of America,United States of America",
      "Notability criteria": "Discretionary",
      "Notability criteria notes": "",
      "Epochs": "13.9",
      "Training time (hours)": "",
      "Training time notes": "table 1",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "262144.0",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "models and training code, Apache 2.0: https://github.com/google-research/electra",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Tensor-Transformer(1core)+PN (WT103)",
      "Organization": "University of California (UC) Berkeley",
      "Publication date": "2020-03-17",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "85300000.0",
      "Parameters notes": "six layers tensorized transformer core-1 for Wikitext-103, following (Ma et al., 2019).\n\nhttps://arxiv.org/abs/1906.09777",
      "Training compute (FLOP)": "1.58e+18",
      "Training compute notes": "6 FLOP / parameter / token * 85300000 parameters * 103000000 tokens * 30 epochs = 1.581462e+18 FLOP",
      "Training dataset": "WikiText-103",
      "Training dataset size (gradients)": "103000000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2003.07845",
      "Reference": "PowerNorm: Rethinking Batch Normalization in Transformers",
      "Citations": "60.0",
      "Authors": "Sheng Shen, Zhewei Yao, Amir Gholami, Michael W. Mahoney, Kurt Keutzer",
      "Abstract": "The standard normalization method for neural network (NN) models used in Natural Language Processing (NLP) is layer normalization (LN). This is different than batch normalization (BN), which is widely-adopted in Computer Vision. The preferred use of LN in NLP is principally due to the empirical observation that a (naive/vanilla) use of BN leads to significant performance degradation for NLP tasks; however, a thorough understanding of the underlying reasons for this is not always evident. In this paper, we perform a systematic study of NLP transformer models to understand why BN has a poor performance, as compared to LN. We find that the statistics of NLP data across the batch dimension exhibit large fluctuations throughout training. This results in instability, if BN is naively implemented. To address this, we propose Power Normalization (PN), a novel normalization scheme that resolves this issue by (i) relaxing zero-mean normalization in BN, (ii) incorporating a running quadratic mean instead of per batch statistics to stabilize fluctuations, and (iii) using an approximate backpropagation for incorporating the running statistics in the forward pass. We show theoretically, under mild assumptions, that PN leads to a smaller Lipschitz constant for the loss, compared with BN. Furthermore, we prove that the approximate backpropagation scheme leads to bounded gradients. We extensively test PN for transformers on a range of NLP tasks, and we show that it significantly outperforms both LN and BN. In particular, PN outperforms LN by 0.4/0.6 BLEU on IWSLT14/WMT14 and 5.6/3.0 PPL on PTB/WikiText-103. We make our code publicly available at \\url{this https URL}.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"The results are reported in Table 1. In the first section of rows, we report state-of-the-art results for these two tasks with comparable model sizes\"",
      "Epochs": "30.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "copyleft license: https://github.com/sIncerass/powernorm/blob/master/LICENSE",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Routing Transformer (WT-103)",
      "Organization": "Google Research",
      "Publication date": "2020-03-12",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "79500000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "WikiText-103",
      "Training dataset size (gradients)": "103000000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2003.05997",
      "Reference": "Efficient Content-Based Sparse Attention with Routing Transformers",
      "Citations": "679.0",
      "Authors": "Aurko Roy, Mohammad Saffar, Ashish Vaswani, David Grangier",
      "Abstract": "Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic compute and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: it combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity) as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "code/weights: https://github.com/google-research/google-research/tree/master/routing_transformer\n\nrepo is Apache 2.0: https://github.com/google-research/google-research/blob/master/LICENSE",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "TransformerXL + spectrum control",
      "Organization": "University of California Los Angeles (UCLA),JD.com",
      "Publication date": "2020-03-11",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "151000000.0",
      "Parameters notes": "151M (Table 2)\n\n\" On the large WikiText-103 dataset, we implement our method based on the state-of-the-art Transformer-XL based models (Dai et al., 2019). We follow the same settings reported in (Dai et al., 2019), and our implementation is based on the\nofficial code for Transformer-XL.\"",
      "Training compute (FLOP)": "2.6289761e+19",
      "Training compute notes": "6 FLOP / parameter / token *  151000000 parameters * 103000000 tokens * 250 epochs = 2.33295e+19 FLOP\n\n31330000000000 FLOP / sec / GPU [fp16] * 4 GPUs * 3152 sec per epoch * 250 epochs * 0.3 [assumed precision] = 2.9625648e+19 FLOP\n\nsqrt(2.33295e+19*2.9625648e+19) = 2.6289761e+19 FLOP\n\n'speculative' confidence due to the assumption of the amount of epochs",
      "Training dataset": "WikiText-103",
      "Training dataset size (gradients)": "103000000",
      "Dataset size notes": "\"For WikiText-103 dataset, we use four NVIDIA Tesla V100\nGPU and set the batch size to be 40.\" \nunknown amount of epochs and training steps but they say they follow original Transformer-XL setup (400K steps + 16K warmup steps, 128 batch size, 384 sequence length) \n\n400000*4*40*384 / 103000000 = 248 epochs  ",
      "Confidence": "Speculative",
      "Link": "https://openreview.net/forum?id=ByxY8CNtvr",
      "Reference": "Improving Neural Language Generation with Spectrum Control",
      "Citations": "90.0",
      "Authors": "Lingxiao Wang, Jing Huang, Kevin Huang, Ziniu Hu, Guangtao Wang, Quanquan Gu",
      "Abstract": "Recent Transformer-based models such as Transformer-XL and BERT have achieved huge success on various natural language processing tasks. However, contextualized embeddings at the output layer of these powerful models tend to degenerate and occupy an anisotropic cone in the vector space, which is called the representation degeneration problem. In this paper, we propose a novel spectrum control approach to address this degeneration problem. The core idea of our method is to directly guide the spectra training of the output embedding matrix with a slow-decaying singular value prior distribution through a reparameterization framework. We show that our proposed method encourages isotropy of the learned word representations while maintains the modeling power of these contextual neural models. We further provide a theoretical analysis and insight on the benefit of modeling singular value distribution. We demonstrate that our spectrum control method outperforms the state-of-the-art Transformer-XL modeling for language model, and various Transformer-based models for machine translation, on common benchmark datasets for these tasks.",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "United States of America,China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We demonstrate that our spectrum control method outperforms the state-of-the-art Transformer-XL modeling for language model\"\n\n\" Our spectrum control method outperforms the latest state-of-the-art TransformerXL model on WikiText-103 dataset for language modeling; and obtains close to 1.5 BLEU improvement on IWSLT 2014 German-English translation task compared to the Transformer baseline model.\"",
      "Epochs": "250.0",
      "Training time (hours)": "219.0",
      "Training time notes": "3152 sec per epoch (Table 7) * 250 epochs / 3600 sec/hour = 219 hours",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "4.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "2452.6211427753756",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "61440.0",
      "Batch size notes": "4*40*384",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "TCAN (WT2)",
      "Organization": "Nanjing University,Ant Group",
      "Publication date": "2020-02-28",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "33000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "WikiText-2",
      "Training dataset size (gradients)": "2000000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2002.12530",
      "Reference": "Temporal Convolutional Attention-based Network For Sequence Modeling",
      "Citations": "47.0",
      "Authors": "Hongyan Hao, Yan Wang, Yudi Xia, Jian Zhao, Furao Shen",
      "Abstract": "With the development of feed-forward models, the default model for sequence modeling has gradually evolved to replace recurrent networks. Many powerful feed-forward models based on convolutional networks and attention mechanism were proposed and show more potential to handle sequence modeling tasks. We wonder that is there an architecture that can not only achieve an approximate substitution of recurrent network, but also absorb the advantages of feed-forward models. So we propose an exploratory architecture referred to Temporal Convolutional Attention-based Network (TCAN) which combines temporal convolutional network and attention mechanism. TCAN includes two parts, one is Temporal Attention (TA) which captures relevant features inside the sequence, the other is Enhanced Residual (ER) which extracts shallow layer's important information and transfers to deep layers. We improve the state-of-the-art results of bpc/perplexity to 30.28 on word-level PTB, 1.092 on character-level PTB, and 9.20 on WikiText-2.",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "China,China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We improve the state-of-theart results of ... 9.20 on WikiText-2\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT license for code: https://github.com/haohy/TCAN",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Feedback Transformer",
      "Organization": "LORIA,University of Lorraine,Facebook AI Research",
      "Publication date": "2020-02-21",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "126000000.0",
      "Parameters notes": "Table 3 shows 126M.\n\nThere is another instance of the Feedback Transformer mentioned in Table 9 with 139M parameters.",
      "Training compute (FLOP)": "7.690547e+18",
      "Training compute notes": "6 FLOP / token / parameter * 126*10^6 parameters * 256 tokens per sequences * 512 sequences per batch * 210000 steps = 2.0808991e+19 FLOP\n\nassuming V100 GPU fp16:\n\n31330000000000 FLOP/sec/GPU * 1 GPU * 84 hours * 3600 sec / hour * 0.3 [assumed utilization] = 2.8422576e+18 FLOP\n\nsqrt(2.0808991e+19*2.8422576e+18) = 7.690547e+18 FLOP\n\n___________\nin the Algorithmic progress paper they used estimation of 4.41e+19 FLOP also with low confidence",
      "Training dataset": "WikiText-103",
      "Training dataset size (gradients)": "103000000",
      "Dataset size notes": "\"The models are trained for 200k steps and the finetuned for additional 10k steps.\"\n\nfrom Table 9 (describes 139M model)\nbatch size 512\nsequence length 256\n\n256*512*210000 / 103000000 = 267 epochs\n\n\"speculative\" confidence since there is no clear description of the 126M model and numbers are assumed based on 139M model description",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/2002.09402",
      "Reference": "Addressing Some Limitations of Transformers with Feedback Memory",
      "Citations": "41.0",
      "Authors": "Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, Sainbayar Sukhbaatar",
      "Abstract": "Transformers have been successfully applied to sequential, auto-regressive tasks despite being feedforward networks. Unlike recurrent neural networks, Transformers use attention to capture temporal relations while processing input tokens in parallel. While this parallelization makes them computationally efficient, it restricts the model from fully exploiting the sequential nature of the input. The representation at a given layer can only access representations from lower layers, rather than the higher level representations already available. In this work, we propose the Feedback Transformer architecture that exposes all previous representations to all future representations, meaning the lowest representation of the current timestep is formed from the highest-level abstract representation of the past. We demonstrate on a variety of benchmarks in language modeling, machine translation, and reinforcement learning that the increased representation capacity can create small, shallow models with much stronger performance than comparable Transformers.",
      "Organization categorization": "Academia,Academia,Industry",
      "Country (of organization)": "France,France,United States of America,France",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"As shown in Table 4, the Feedback\nTransformer model achieves a new SOTA performance (on Enwiki8) of 0.96 bit-per-byte despite its small size.\"",
      "Epochs": "267.23",
      "Training time (hours)": "84.0",
      "Training time notes": "\" Our Feedback architecture takes 3.5 days to train\"\n3.5*24 = 84 hours",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "131072.0",
      "Batch size notes": "256*512",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "FFN SwiGLU",
      "Organization": "Google",
      "Publication date": "2020-02-14",
      "Domain": "Language",
      "Task": "Language modeling,Question answering",
      "Parameters": "220000000.0",
      "Parameters notes": "\"We use the same code base, model architecture, and training task as the base model from [Raffel et al.,2019]. The encoder and decoder each consist of 12 layers, with dmodel = 768. For the attention layers, h = 12 and dk = dv = 64. The FFN layers have hidden size df f = 3072.\"\n\n[Raffel et al.,2019]: \"Specifically, both the encoder and decoder consist of 12 blocks (each block comprising self-attention, optional encoder-decoder attention, and a feed-forward network). The feed-forward networks in each block consist of a dense layer with an output dimensionality of dff = 3072 followed by a\nReLU nonlinearity and another dense layer. The \u201ckey\u201d and \u201cvalue\u201d matrices of all attention mechanisms have an inner dimensionality of dkv = 64 and all attention mechanisms have 12 heads. All other sub-layers and embeddings have a dimensionality of dmodel = 768. In total, this results in a model with about 220 million parameters.\"",
      "Training compute (FLOP)": "3.3687317e+19",
      "Training compute notes": "6 FLOP/parameter/token * 220000000 parameters * 50600083456 tokens = 66792110161920000000 FLOP (10^19)\n\n45000000000000 FLOP/GPU/sec * 21.85 hours * 3600 sec / hour * 16 GPUs * 0.3 [assumed utilization] = 16990560000000002000 FLOP (10^19)\n\nsqrt(66792110161920000000*16990560000000002000) = 3.3687317e+19",
      "Training dataset": "C4,SQuAD,GLUE",
      "Training dataset size (gradients)": "50600083456",
      "Dataset size notes": "pre-training: \"Identically to [Raffel et al., 2019], we pre-train for 524,288 steps on the span-filling objective on the C4 dataset. Each training batch consists of 128 examples, each of which has an input of 512 tokens and an output of 114 tokens\"\n\n524288*128*(512+114) = 42010148864\n\nfine-tuning: \"Fine-tuning consists of 131072 steps <..>, the input sequences for each step have a combined length of approximately 65,536 tokens.\"\n\n131072*65536 = 8589934592\n\n42010148864+8589934592 = 50600083456",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2002.05202",
      "Reference": "GLU Variants Improve Transformer",
      "Citations": "",
      "Authors": "Noam Shazeer",
      "Abstract": "Gated Linear Units (arXiv:1612.08083) consist of the component-wise product of two linear projections, one of which is first passed through a sigmoid function. Variations on GLU are possible, using different nonlinear (or even linear) functions in place of sigmoid. We test these variants in the feed-forward sublayers of the Transformer (arXiv:1706.03762) sequence-to-sequence model, and find that some of them yield quality improvements over the typically-used ReLU or GELU activations.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "the paper introduced SwiGLU",
      "Epochs": "",
      "Training time (hours)": "21.85",
      "Training time notes": "\"Each training step took approximately 0.15 seconds on a 32-core TPUv2 cluster\" -> 16 chips\n\n\"we pre-train for 524,288 steps\"\n\n524288*0.15/3600 ~ 21.85 hours",
      "Training hardware": "Google TPU v2",
      "Hardware quantity": "16.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "9161.75542079835",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Turing-NLG",
      "Organization": "Microsoft",
      "Publication date": "2020-02-13",
      "Domain": "Language",
      "Task": "Text autocompletion,Language generation,Text summarization",
      "Parameters": "17000000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "1.57e+22",
      "Training compute notes": "source: https://lair.lighton.ai/akronomicon/\n157 PF-days * 3600 * 24 * 10^15  = 1.35648e+22\n\narchived: https://github.com/lightonai/akronomicon/tree/main/akrodb\n\n6ND=6*17000000000*46400000000=4.7328e+21 (confidence regarding dataset size - likely)\n\nAuthors of \"AI and Memory Wall\" (https://github.com/amirgholami/ai_and_memory_wall) estimated model's training compute as 28,000,000 PFLOP = 2.8*10^22 FLOP",
      "Training dataset": "",
      "Training dataset size (gradients)": "46400000000",
      "Dataset size notes": "Authors say they pretrain on the same data as for Megatron-LM. \n\nFrom the Megatron-LM paper: https://arxiv.org/pdf/1909.08053.pdf\n\n\"The resulting aggregate corpus contains 174 GB of deduplicated text.\"\n\n174GB * 2e8words/GB = 3.48e10 words\n3.48e10 words (if english) *4/3 = 46400000000 tokens\n\nconfidence - likely",
      "Confidence": "Likely",
      "Link": "https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/",
      "Reference": "Turing-NLG: A 17-billion-parameter language model by Microsoft",
      "Citations": "114.0",
      "Authors": "Corby Rosset",
      "Abstract": "Turing Natural Language Generation (T-NLG) is a 17 billion parameter language model by Microsoft that outperforms the state of the art on many downstream NLP tasks. We present a demo of the model, including its freeform generation, question answering, and summarization capabilities, to academics for feedback and research purposes. <|endoftext|>",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "from paper: \"Turing Natural Language Generation (T-NLG) is a 17 billion parameter language model by Microsoft that outperforms the state of the art on many downstream NLP tasks\"",
      "Epochs": "3.39",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA Tesla V100 DGXS 32 GB",
      "Hardware quantity": "256.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "51659.713290894986",
      "Compute cost notes": "",
      "Training power draw (W)": "130885.13499433055",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "524288.0",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "SimCLR",
      "Organization": "Google Brain",
      "Publication date": "2020-02-13",
      "Domain": "Vision",
      "Task": "Image completion",
      "Parameters": "375000000.0",
      "Parameters notes": "source: https://openai.com/blog/image-gpt/",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "ILSVRC 2012 subset of ImageNet",
      "Training dataset size (gradients)": "10467704832",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/2002.05709",
      "Reference": "A Simple Framework for Contrastive Learning of Visual Representations",
      "Citations": "16017.0",
      "Authors": "Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton",
      "Abstract": "This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "1000.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache 2.0: https://github.com/google-research/simclr",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ALBERT-xxlarge",
      "Organization": "Toyota Technological Institute at Chicago,Google",
      "Publication date": "2020-02-09",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering",
      "Parameters": "235000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "2.39e+21",
      "Training compute notes": "32 hours of training\n512 TPU V3s\n0.33 utilization rate\n\n123000000000000 FLOP / chip / sec * 512 TPUs * 32 hours * 3600 sec / hour * 0.33 [assumed utilization] = 2.3940956e+21 FLOP\n\n\"We train all models for 125,000 steps unless otherwise specified\"\n\"All the model updates use a batch size of 4096 \"\n\"We always limit the maximum input length to 512, and randomly generate input sequences shorter than 512 with a probability of 10%.\"\n\n6 FLOP / parameter / token * 235000000 parameters * 512 tokens per sequence * 4096 sequences per batch * 125000 steps =  3.6962304e+20 FLOP\n\nAuthors of \"AI and Memory Wall\" (https://github.com/amirgholami/ai_and_memory_wall) estimated model's training compute as 31,000,000 PFLOP = 3.1*10^22 FLOP",
      "Training dataset": "Wikipedia,BookCorpus (BooksCorpus, Toronto Book Corpus)",
      "Training dataset size (gradients)": "3300000000",
      "Dataset size notes": "Pretraining same as for BERT - Wikipedia and BookCorpus\n\n\"For the pre-training corpus we use the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words)\"",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/1909.11942",
      "Reference": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.",
      "Citations": "7071.0",
      "Authors": "Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut",
      "Abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at this https URL.",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "79.4",
      "Training time (hours)": "32.0",
      "Training time notes": "",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "512.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "4439.921298510215",
      "Compute cost notes": "",
      "Training power draw (W)": "471228.45995621517",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "2097152.0",
      "Batch size notes": "Sequences are capped at 512 tokens; 10% of the time they'll use an input less than 512 long. Batches are over 4096 sequences. Tokens per batch: 2,097,152",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache 2.0 code/weights. repo includes training code: https://github.com/google-research/ALBERT",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "TaLK Convolution",
      "Organization": "Carleton University",
      "Publication date": "2020-02-08",
      "Domain": "Language",
      "Task": "Language modeling,Translation,Text summarization",
      "Parameters": "240000000.0",
      "Parameters notes": "Table 5\n\n\"For the language model, we followed the same configuration\nas Baevski & Auli (2019). We used 17 decoding layers, each\nlayer with a 1024 hidden size, a 4096 feed-forward hidden\nsize and 8 heads. The adaptive input factor was set to 4.\"",
      "Training compute (FLOP)": "2.6990346e+19",
      "Training compute notes": "6 FLOP / parameter / token * 240000000 parameters * 286000 steps * 65536 tokens per batch = 2.6990346e+19 FLOP",
      "Training dataset": "WikiText-103",
      "Training dataset size (gradients)": "103000000",
      "Dataset size notes": "\" We replicated their setup and partition the training data into blocks of 512 contiguous tokens\"\n\n\" same setup as in Baevski & Auli (2019)\"  https://arxiv.org/abs/1809.10853\nin that paper they trained for 286k steps in batches of 65,536 tokens.\n\n286000*65536 / 103000000= 182 epochs (same as in as in Baevski & Auli (2019))",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/2002.03184",
      "Reference": "Time-aware Large Kernel Convolutions",
      "Citations": "30.0",
      "Authors": "Vasileios Lioutas, Yuhong Guo",
      "Abstract": "To date, most state-of-the-art sequence modeling architectures use attention to build generative models for language based tasks. Some of these models use all the available sequence tokens to generate an attention distribution which results in time complexity of O(n2). Alternatively, they utilize depthwise convolutions with softmax normalized kernels of size k acting as a limited-window self-attention, resulting in time complexity of O(k\u22c5n). In this paper, we introduce Time-aware Large Kernel (TaLK) Convolutions, a novel adaptive convolution operation that learns to predict the size of a summation kernel instead of using a fixed-sized kernel matrix. This method yields a time complexity of O(n), effectively making the sequence encoding process linear to the number of tokens. We evaluate the proposed method on large-scale standard machine translation, abstractive summarization and language modeling datasets and show that TaLK Convolutions constitute an efficient improvement over other attention/convolution based approaches.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Canada",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"[We] set a new state-of-the-art result on the IWSLT De-En and CNN-DailyMail datasets\"",
      "Epochs": "182.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA GeForce RTX 2080 Ti 11GB",
      "Hardware quantity": "8.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "4090.615920439178",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "65536.0",
      "Batch size notes": "same setup as in Baevski & Auli (2019)",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Unreleased",
      "Accessibility notes": "MIT, code and weights (though this repo is for translation not WT-103):\n\nhttps://github.com/lioutasb/TaLKConvolutions?tab=readme-ov-file",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "9524.968213430311",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Perceiver IO (optical flow)",
      "Organization": "DeepMind",
      "Publication date": "2020-02-08",
      "Domain": "Multimodal,Language,Vision",
      "Task": "Language modeling/generation,Image captioning",
      "Parameters": "27900000.0",
      "Parameters notes": "Optical flow model (SOTA) was 27.9M params. There are other, larger models described in this paper, e.g. for language.\n\n\"For the pixel- and patch-based models, total computational\ncomplexity for a forward pass on a 368 \u00d7 496 image is roughly 987 billion FLOPs, and there are\nroughly 27.9 million parameters.\"",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "AutoFlow",
      "Training dataset size (gradients)": "146022400000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2107.14795",
      "Reference": "Perceiver IO: A General Architecture for Structured Inputs & Outputs",
      "Citations": "713.0",
      "Authors": "Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier H\u00e9naff,\nMatthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, Jo\u00e3o Carreira",
      "Abstract": "A central goal of machine learning is the development of systems that can solve many problems in as many data domains as possible. Current architectures, however, cannot be applied beyond a small set of stereotyped settings, as they bake in domain & task assumptions or scale poorly to large inputs or outputs. In this work, we propose Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs. Our model augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering. The same architecture achieves strong results on tasks spanning natural language and visual understanding, multi-task and multi-modal reasoning, and StarCraft II. As highlights, Perceiver IO outperforms a Transformer-based BERT baseline on the GLUE language benchmark despite removing input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation with no explicit mechanisms for multiscale correspondence.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Perceiver IO... achieves state-of-the-art performance on Sintel optical flow estimation\"",
      "Epochs": "480.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Theseus 6/768",
      "Organization": "University of California San Diego,Beihang University,Microsoft",
      "Publication date": "2020-02-07",
      "Domain": "Language",
      "Task": "Text autocompletion",
      "Parameters": "66000000.0",
      "Parameters notes": "66M, Table 1",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "GLUE",
      "Training dataset size (gradients)": "393000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2002.02925",
      "Reference": "BERT-of-Theseus: Compressing BERT by Progressive Module Replacing",
      "Citations": "217.0",
      "Authors": "Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, Ming Zhou",
      "Abstract": "In this paper, we propose a novel model compression approach to effectively compress BERT by progressive module replacing. Our approach first divides the original BERT into several modules and builds their compact substitutes. Then, we randomly replace the original modules with their substitutes to train the compact modules to mimic the behavior of the original modules. We progressively increase the probability of replacement through the training. In this way, our approach brings a deeper level of interaction between the original and compact models. Compared to the previous knowledge distillation approaches for BERT compression, our approach does not introduce any additional loss function. Our approach outperforms existing knowledge distillation approaches on GLUE benchmark, showing a new perspective of model compression.",
      "Organization categorization": "Academia,Academia,Industry",
      "Country (of organization)": "United States of America,China,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Table 2\n\n\"Our approach outperforms existing knowledge distillation approaches on GLUE benchmark\"\n\nIt seems that they compared only with models of the same size.",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "BERT-Large",
      "Finetune compute (FLOP)": "2.7e+18",
      "Finetune compute notes": "Actually BERT-base, 110M params. Up to 20 V100-hours depending on task. \n\n125 trillion * 20 * 3600 * 0.3 (utilization assumption) = 2.7e18",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache 2.0: https://github.com/JetRunner/BERT-of-Theseus",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Meena",
      "Organization": "Google Brain",
      "Publication date": "2020-01-28",
      "Domain": "Language",
      "Task": "Text autocompletion,Chat",
      "Parameters": "2600000000.0",
      "Parameters notes": "\"We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token.\"",
      "Training compute (FLOP)": "1.12e+23",
      "Training compute notes": "https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf\nTable 4\n\nIn the paper: \"We trained our best model for 30 days on a TPUv3 Pod (2,048 TPU cores) on the Meena dataset containing 40B words (or 61B BPE tokens) [...] by the end of training, the model had traversed the full\ntraining set 164 times (or epochs) and observed a total of about 10T tokens\"\n\nHardware: 30 * 24 * 3600 * (2048/2) * 1.23e14 * 0.3 = 9.794e22\nOps counting: 6 * 10T * 2.6B = 1.56E23\nGeometric mean: sqrt(9.79e22*1.56E23) = 1.24e23, very close to the figure in the link above.",
      "Training dataset": "",
      "Training dataset size (gradients)": "53333333333.333336",
      "Dataset size notes": "\"The final Meena dataset contains 341GB of text\n(40B words)\"\n\nConverting from GB to words yields 6.8e10, which is in the same OOM",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2001.09977",
      "Reference": "Towards a Human-like Open-Domain Chatbot",
      "Citations": "991.0",
      "Authors": "Dongling Xiao, Han Zhang, Yukun Li, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang",
      "Abstract": "We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token. We also propose a human evaluation metric called Sensibleness and Specificity Average (SSA), which captures key elements of a human-like multi-turn conversation. Our experiments show strong correlation between perplexity and SSA. The fact that the best perplexity end-to-end trained Meena scores high on SSA (72% on multi-turn evaluation) suggests that a human-level SSA of 86% is potentially within reach if we can better optimize perplexity. Additionally, the full version of Meena (with a filtering mechanism and tuned decoding) scores 79% SSA, 23% higher in absolute SSA than the existing chatbots we evaluated.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We also propose a human evaluation metric called Sensibleness and Specificity Average (SSA)... the full version of Meena (with a filtering mechanism and tuned decoding) scores 79% SSA, 23% higher in absolute SSA than the existing chatbots we evaluated\"",
      "Epochs": "164.0",
      "Training time (hours)": "720.0",
      "Training time notes": "We trained our best model for 30 days on a TPUv3 Pod (2,048 TPU cores)",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "1024.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "214809.91213508433",
      "Compute cost notes": "",
      "Training power draw (W)": "942708.8086440804",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "82655.0",
      "Batch size notes": "61B tokens over 738k training steps, or 82655 tokens per batch on average. Not certain about warmup, etc",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "0.34306622",
      "Training compute cost (cloud)": "724243.8744571933",
      "Training compute cost (upfront)": "11939150.88759962"
    },
    {
      "Model": "ContextNet + Noisy Student",
      "Organization": "Google",
      "Publication date": "2020-01-19",
      "Domain": "Speech",
      "Task": "Speech recognition (ASR)",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "8.16e+21",
      "Training compute notes": "\"We train 6 generations of models numbered 0 to 5, where\nwe count the baseline model trained with the supervised set\nas the zeroth generation. Each generation is trained ... on 32 Google\nCloud TPU chips for 10 days.\"\n\nThe TPU version is likely v3 given this is a 2020 paper.\n\nwe get 6 * 10 * 24 * 3600 * 32 * 123 tflops * 0.4  (assumed utilization) = 8.16e21",
      "Training dataset": "LibriSpeech,LibriLight",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/2005.09629v2",
      "Reference": "Improved Noisy Student Training for Automatic Speech Recognition",
      "Citations": "257.0",
      "Authors": "Daniel S. Park, Yu Zhang, Ye Jia, Wei Han, Chung-Cheng Chiu, Bo Li, Yonghui Wu, Quoc V. Le",
      "Abstract": "Recently, a semi-supervised learning method known as \"noisy student training\" has been shown to improve image classification performance of deep networks significantly. Noisy student training is an iterative self-training method that leverages augmentation to improve network performance. In this work, we adapt and improve noisy student training for automatic speech recognition, employing (adaptive) SpecAugment as the augmentation method. We find effective methods to filter, balance and augment the data generated in between self-training iterations. By doing so, we are able to obtain word error rates (WERs) 4.2%/8.6% on the clean/noisy LibriSpeech test sets by only using the clean 100h subset of LibriSpeech as the supervised set and the rest (860h) as the unlabeled set. Furthermore, we are able to achieve WERs 1.7%/3.4% on the clean/noisy LibriSpeech test sets by using the unlab-60k subset of LibriLight as the unlabeled set for LibriSpeech 960h. We are thus able to improve upon the previous state-of-the-art clean/noisy test WERs achieved on LibriSpeech 100h (4.74%/12.20%) and LibriSpeech (1.9%/4.1%).",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We are thus able to improve upon the previous state-of-the-art clean/noisy test WERs achieved on LibriSpeech 100h (4.74%/12.20%) and LibriSpeech (1.9%/4.1%)\"",
      "Epochs": "",
      "Training time (hours)": "1440.0",
      "Training time notes": "roughly 10 days",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "14226.054462994534",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "AlphaFold",
      "Organization": "DeepMind",
      "Publication date": "2020-01-15",
      "Domain": "Biology",
      "Task": "Protein folding prediction,Proteins",
      "Parameters": "16340840.0",
      "Parameters notes": "\"Neural network hyperparameters\" section of https://www.nature.com/articles/s41586-019-1923-7:\n\u201c7 \u00d7 4 Blocks with 256 channels, cycling through dilations 1, 2, 4, 8\u201d\n\u201c48 \u00d7 4 Blocks with 128 channels, cycling through dilations 1, 2, 4, 8\u201d\n\n\"Distogram prediction\" section:\n\"For the final layer, a position-specific bias was used\"\n\nExtended Data Fig.1 (b): \nShows that each block consists of 9 layers:\n(1) Batch norm\n(2) Elu\n(3) Project down (halves number of dimensions)\n(4) Batch norm\n(5) Elu\n(6) 3x3 kernel with dilation\n(7) Batch norm\n(8) Elu\n(9) Project up (doubles number of dimensions)\n\nDilations don't change the number of parameters in each filter\nAssuming that projection layers are convolutional layers with 1x1 kernels\n\nParameter estimate for each layer in a 256 channel block:\n(1) 256*2            = 512\n(2) 0\n(3) 1*1*256*128 = 32768\n(4) 128*2            = 256 \n(5) 0\n(6) 3*3*128*128 = 147456\n(7) 128*2            = 256 \n(8) 0\n(9) 1*1*128*256 + 256 = 33024\nTotal                             = 214272\n\nParameter estimate for each layer in a 128 channel block:\n(1) 128*2            = 256\n(2) 0\n(3) 1*1*128*64   = 8192\n(4) 64*2              = 128\n(5) 0\n(6) 3*3*64*64     = 36864\n(7) 64*2              = 128\n(8) 0\n(9) 1*1*64*128 + 128 = 8320\nTotal                   = 53897\n\nEstimate total network = 7*4*214272 + 48*4*53897 = 5992616 + 10348224\n                                     = 16340840\n                                     ~ 16e6\n\nWithin a factor of 2 of the estimate of 21M parameters stated in: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7305407/\n\n[Previous approximation: 7 * 4 * 256 * 3 * 3 * 256 + 48 * 4 * 128 * 3 * 3 * 128 = 44826624]",
      "Training compute (FLOP)": "1e+20",
      "Training compute notes": "Estimated in the blogpost below\n\nhttps://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening\n\n\"AlphaFold: they say they trained on GPU and not TPU. Assuming V100 GPU, it's 5 days * 24 hours/day * 3600 sec/hour * 8 V100 GPU * 100*10^12 FLOP/s * 33% actual GPU utilization = 10^20 FLOP.\"",
      "Training dataset": "PDB (Protein Data Bank),UniRef30 (FKA UniClust30)",
      "Training dataset size (gradients)": "6622252080",
      "Dataset size notes": "Training Domains: 29,427\nAverage Residues per Domain: 100\nData Points per Domain: 100 \u00d7 100 = 10,000\nTotal Data Points = 29,427 \u00d7 10,000 = 294,270,000 \u2248 3.0 \u00d7 10\u2078",
      "Confidence": "Speculative",
      "Link": "https://www.nature.com/articles/s41586-019-1923-7",
      "Reference": "Improved protein structure prediction using potentials from deep learning",
      "Citations": "2773.0",
      "Authors": "Andrew W. Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin \u017d\u00eddek, Alexander W. R. Nelson, Alex Bridgland, Hugo Penedones, Stig Petersen, Karen Simonyan, Steve Crossan, Pushmeet Kohli, David T. Jones, David Silver, Koray Kavukcuoglu, Demis Hassabis",
      "Abstract": "Protein structure prediction can be used to determine the three-dimensional shape of a protein from its amino acid sequence. This problem is of fundamental importance as the structure of a protein largely determines its function; however, protein structures can be difficult to determine experimentally. Considerable progress has recently been made by leveraging genetic information. It is possible to infer which amino acid residues are in contact by analysing covariation in homologous sequences, which aids in the prediction of protein structures. Here we show that we can train a neural network to make accurate predictions of the distances between pairs of residues, which convey more information about the structure than contact predictions. Using this information, we construct a potential of mean force that can accurately describe the shape of a protein. We find that the resulting potential can be optimized by a simple gradient descent algorithm to generate structures without complex sampling procedures. The resulting system, named AlphaFold, achieves high accuracy, even for sequences with fewer homologous sequences. In the recent Critical Assessment of Protein Structure Prediction (CASP13)\u2014a blind assessment of the state of the field\u2014AlphaFold created high-accuracy structures (with template modelling (TM) scores of 0.7 or higher) for 24 out of 43 free modelling domains, whereas the next best method, which used sampling and contact information, achieved such accuracy for only 14 out of 43 domains. AlphaFold represents a considerable advance in protein-structure prediction. We expect this increased accuracy to enable insights into the function and malfunction of proteins, especially in cases for which no structures for homologous proteins have been experimentally determined.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement,Highly cited",
      "Notability criteria notes": "\"AlphaFold represents a considerable advance in protein-structure prediction.\" [Abstract]",
      "Epochs": "",
      "Training time (hours)": "120.0",
      "Training time notes": "\"Training time: about 5 days for 600,000 steps\"",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Big Transfer (BiT-L)",
      "Organization": "Google Brain",
      "Publication date": "2019-12-24",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "928000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "JFT-300M",
      "Training dataset size (gradients)": "300000000",
      "Dataset size notes": "\"For BiT-L, we train for 40 epochs\"",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/1912.11370",
      "Reference": "Big Transfer (BiT): General Visual Representation Learning",
      "Citations": "1300.0",
      "Authors": "Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby",
      "Abstract": "Transfer of pre-trained representations improves sample efficiency and simplifies hyperparameter tuning when training deep neural networks for vision. We revisit the paradigm of pre-training on large supervised datasets and fine-tuning the model on a target task. We scale up pre-training, and propose a simple recipe that we call Big Transfer (BiT). By combining a few carefully selected components, and transferring using a simple heuristic, we achieve strong performance on over 20 datasets. BiT performs well across a surprisingly wide range of data regimes -- from 1 example per class to 1M total examples. BiT achieves 87.5% top-1 accuracy on ILSVRC-2012, 99.4% on CIFAR-10, and 76.3% on the 19 task Visual Task Adaptation Benchmark (VTAB). On small datasets, BiT attains 76.8% on ILSVRC-2012 with 10 examples per class, and 97.0% on CIFAR-10 with 10 examples per class. We conduct detailed analysis of the main components that lead to high transfer performance.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We transfer BiT to many diverse tasks... These tasks include ImageNet\u2019s ILSVRC-2012 [10], CIFAR-10/100 [27], Oxford-IIIT Pet [41], Oxford Flowers-102 [39] (including few-shot variants), and the 1000-sample VTAB-1k benchmark [66], which consists of 19 diverse datasets. BiT-L attains state-ofthe-art performance on many of these tasks",
      "Epochs": "40.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DD-PPO",
      "Organization": "Georgia Institute of Technology,Facebook AI Research,Oregon State University,Simon Fraser University",
      "Publication date": "2019-12-19",
      "Domain": "Robotics",
      "Task": "Object detection",
      "Parameters": "",
      "Parameters notes": "no parameter count but some architecture details: \"The policy is parameterized by a 2-layer LSTM with a 512-dimensional hidden state. It takes three inputs: the previous action, the target relative to the current state, and the output of the visual encoder. The LSTM\u2019s output is used to produce a softmax distribution over the action space and an estimate of the value function. See Appendix C for full details.\"",
      "Training compute (FLOP)": "7.8e+20",
      "Training compute notes": "\"Using DD-PPO, we train agents for 2.5 Billion steps of experience with 64 Tesla V100 GPUs in 2.75 days \u2013 180 GPU-days of training\"\n\n125 teraFLOP/s (exact V100 model not specified) * 180 * 24 * 3600 * 0.4 (assumed utilization) = 7.8e20",
      "Training dataset": "",
      "Training dataset size (gradients)": "2500000000",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://openreview.net/forum?id=H1gX8C4YPr\nhttps://arxiv.org/abs/1911.00357",
      "Reference": "DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames",
      "Citations": "549.0",
      "Authors": "Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, Dhruv Batra",
      "Abstract": "We present Decentralized Distributed Proximal Policy Optimization (DD-PPO), a method for distributed reinforcement learning in resource-intensive simulated environments. DD-PPO is distributed (uses multiple machines), decentralized (lacks a centralized server), and synchronous (no computation is ever \"stale\"), making it conceptually simple and easy to implement. In our experiments on training virtual robots to navigate in Habitat-Sim, DD-PPO exhibits near-linear scaling -- achieving a speedup of 107x on 128 GPUs over a serial implementation. We leverage this scaling to train an agent for 2.5 Billion steps of experience (the equivalent of 80 years of human experience) -- over 6 months of GPU-time training in under 3 days of wall-clock time with 64 GPUs. ",
      "Organization categorization": "Academia,Industry,Academia,Academia",
      "Country (of organization)": "United States of America,United States of America,France,United States of America,Canada",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"This agent achieves state-of-art on the Habitat Challenge 2019 RGB track (rank 2 entry has 0.89 SPL).\"",
      "Epochs": "",
      "Training time (hours)": "66.0",
      "Training time notes": "2.75 days",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "64.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "1926.8992900057376",
      "Compute cost notes": "",
      "Training power draw (W)": "39314.53850266432",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "MIT license for environment used to train. doesn't seem like it has training code for this model. https://github.com/facebookresearch/habitat-lab",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "OpenAI Five Rerun",
      "Organization": "OpenAI",
      "Publication date": "2019-12-13",
      "Domain": "Games",
      "Task": "Dota 2",
      "Parameters": "159000000.0",
      "Parameters notes": "\"We define a policy (\u03c0) as a function from the history of observations to a probability distribution\nover actions, which we parameterize as a recurrent neural network with approximately 159 million\nparameters (\u03b8).\" pg. 3 of paper\n\nsource: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",
      "Training compute (FLOP)": "1.3e+22",
      "Training compute notes": "THIS CALCULATION IS FOR RERUN\n\n\"Rerun took 2 months and 150 \u00b1 5 PFlops/s\u00b7days of compute (see Figure 4)\"\n\n\n\nsource: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",
      "Training dataset": "",
      "Training dataset size (gradients)": "53084160000",
      "Dataset size notes": "54k iterations (Fig 7)\nwith a batch size of 983040 (Table 2)",
      "Confidence": "Confident",
      "Link": "https://cdn.openai.com/dota-2.pdf",
      "Reference": "Dota 2 with Large Scale Deep Reinforcement Learning",
      "Citations": "2020.0",
      "Authors": "Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung,\nPrzemys\u0142aw \u201cPsyho\" D\u0119biak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal J\u00f3zefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique Pond\u00e9 de Oliveira Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, Susan Zhang",
      "Abstract": "On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\"On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA P100",
      "Hardware quantity": "512.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "321105.9055",
      "Compute cost notes": "",
      "Training power draw (W)": "262131.94609302413",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "1899934.841842503",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "OpenAI Five",
      "Organization": "OpenAI",
      "Publication date": "2019-12-13",
      "Domain": "Games",
      "Task": "Dota 2",
      "Parameters": "159000000.0",
      "Parameters notes": "\"We define a policy (\u03c0) as a function from the history of observations to a probability distribution over actions, which we parameterize as a recurrent neural network with approximately 159 million parameters (\u03b8).\" pg. 3 of paper\n\nsource: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",
      "Training compute (FLOP)": "6.7e+22",
      "Training compute notes": "\"770\u00b150 PFlops/s\u00b7days of compute\" for the model that played against world champions. They did a single training run that took 10 months.\n\nWhile the model was playing against world champions, they continued training for a few days, so that the resulting model used even more training compute: 820\u00b150 PFlops/s\u00b7days.\n\nFinally, they also trained a Rerun model with 150\u00b15 PFlops/s\u00b7days of compute.\n\nSource: Dota 2 with Large Scale Deep Reinforcement Learning\nhttps://arxiv.org/abs/1912.06680\n\nYou cannot multiply the hardware quantity by training time to get the quantity of GPU-hours! Page 5: \" the number of GPUs (up to 1536 at the peak)\"\n\nFrom this NVIDIA blogpost, it appears they were using P100s:\nhttps://developer.nvidia.com/blog/ai-learns-to-play-dota-2-with-human-precision/#:~:text=AI%20Learns%20to%20Play%20Dota,The%20neural",
      "Training dataset": "",
      "Training dataset size (gradients)": "454164480000",
      "Dataset size notes": "\"Although the Dota 2 engine runs at 30 frames per second, OpenAI Five only acts on every 4th\nframe which we call a timestep\"\n--> 7.5 timesteps/s\n\n\"OpenAI Five is a single training run that ran from June 30th, 2018 to April 22nd, 2019. \" --> 296 days\n\n296 * 24*3600 * 7.5 = 1.92e8\n\nThis number seems a little low? The DQN paper had 1e7 timesteps. Might be to do with sample efficiency?\n\nEDIT 14/06/2022\nMultiple copies of OpenAI Five were trained in parallel, so the total training time is much higher than 296 days.\nTable 1 shows 220,000 GPU iterations, each iteration has a batch size of between 1M and 3M timesteps (Table 2), so the total number of episodes is on the order of 2e11",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1912.06680",
      "Reference": "Dota 2 with Large Scale Deep Reinforcement Learning",
      "Citations": "2020.0",
      "Authors": "Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemys\u0142aw D\u0119biak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal J\u00f3zefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique P. d.O. Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, Susan Zhang",
      "Abstract": "On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.\n",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "no standard benchmark results\n\n\"On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game.\"\n\n\" it defeated the Dota 2 world champions\nin a best-of-three match and 99.4% of human players during a multi-day online showcase.\"\n\n\"we ran OpenAI Five Arena, in which we opened OpenAI Five to the public for\ncompetitive online games from April 18-21, 2019. In total, Five played 3,193 teams in 7,257 total games, winning 99.4%\"",
      "Epochs": "",
      "Training time (hours)": "7104.0",
      "Training time notes": "\"OpenAI Five is a single training run that ran from June 30th, 2018 to April 22nd, 2019. \" --> 296 days",
      "Training hardware": "NVIDIA P100",
      "Hardware quantity": "1536.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "4328311.470477086",
      "Compute cost notes": "Cannot multiply the hardware quantity by training time to get the quantity of GPU-hours! Page 5: \" the number of GPUs (up to 1536 at the peak)\"",
      "Training power draw (W)": "786395.8382790724",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "5699804.525527508",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "11072136.60189257",
      "Training compute cost (upfront)": "33761961.904761896"
    },
    {
      "Model": "MMLSTM (WT-2)",
      "Organization": "Beijing University of Posts and Telecommunications,University of West London",
      "Publication date": "2019-12-05",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "32300000.0",
      "Parameters notes": "Table III",
      "Training compute (FLOP)": "1.938e+17",
      "Training compute notes": "6 FLOP / token / parameter * 32300000 parameters * 2000000 tokens * 500 epochs [assumption] = 1.938e+17 FLOP",
      "Training dataset": "WikiText-2",
      "Training dataset size (gradients)": "2000000",
      "Dataset size notes": "size of WT-2",
      "Confidence": "Likely",
      "Link": "http://repository.uwl.ac.uk/id/eprint/6490/1/Loo_etal_IEEE_TNNLS_2019_Major-minor_long_short-term_memory_for_word-level_language_model.pdf",
      "Reference": "Major\u2013Minor Long Short-Term Memory for Word-Level Language Model",
      "Citations": "19.0",
      "Authors": "Kai Shuang, Rui Li, Mengyu Gu, Jonathan Loo, Sen Su",
      "Abstract": "Abstract\u2014Language model plays an important role in natural language processing (NLP) systems like machine translation,\nspeech recognition, learning token embeddings, natural language generation and text classification. Recently, the multi-layer Long Short-Term Memory (LSTM) models have been demonstrated to achieve promising performance on word-level language modeling.\nFor each LSTM layer, larger hidden size usually means more diverse semantic features, which enables the language model to perform better. However, we have observed that when a certain LSTM layer reaches a sufficiently large scale, the promotion\nof overall effect will slow down as its hidden size increases. In this paper, we analyze that an important factor leading to this\nphenomenon is the high correlation between the newly extended hidden states and original hidden states, which hinders diverse feature expression of the LSTM. As a result, when the scale is large enough, simply lengthening the LSTM hidden states will cost tremendous extra parameters but has little effect. We propose a simple yet effective improvement on each LSTM layer consisting of a large-scale Major LSTM and a smallscale Minor LSTM to break the high correlation between the two parts of hidden states, which we call Major-Minor LSTMs (MMLSTMs). In experiments, we demonstrate the language model with MMLSTMs surpasses the existing state-of-the-art model on Penn Treebank (PTB) and WikiText-2 (WT2) datasets, and outperforms the baseline by 3.3 points in perplexity on WikiText-103 dataset without increasing model parameter counts.",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "China,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"In experiments, we demonstrate the language model with MMLSTMs surpasses the existing state-of-the-art model on Penn Treebank (PTB) and WikiText-2 (WT2) datasets\"",
      "Epochs": "500.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "MMLSTM (PTB)",
      "Organization": "Beijing University of Posts and Telecommunications,University of West London",
      "Publication date": "2019-12-05",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "21300000.0",
      "Parameters notes": "Table II",
      "Training compute (FLOP)": "5.8298782e+16",
      "Training compute notes": "6 FLOP / token / parameter * 21300000 parameters * 912344 tokens * 500 epochs [assumption] = 5.8298782e+16 FLOP",
      "Training dataset": "Penn TreeBank (PTB)",
      "Training dataset size (gradients)": "929000",
      "Dataset size notes": "size of PTB",
      "Confidence": "Likely",
      "Link": "http://repository.uwl.ac.uk/id/eprint/6490/1/Loo_etal_IEEE_TNNLS_2019_Major-minor_long_short-term_memory_for_word-level_language_model.pdf",
      "Reference": "Major\u2013Minor Long Short-Term Memory for Word-Level Language Model",
      "Citations": "19.0",
      "Authors": "Kai Shuang, Rui Li, Mengyu Gu, Jonathan Loo, Sen Su",
      "Abstract": "Abstract\u2014Language model plays an important role in natural\nlanguage processing (NLP) systems like machine translation,\nspeech recognition, learning token embeddings, natural language generation and text classification. Recently, the multi-layer Long Short-Term Memory (LSTM) models have been demonstrated to achieve promising performance on word-level language modeling.\nFor each LSTM layer, larger hidden size usually means more\ndiverse semantic features, which enables the language model to\nperform better. However, we have observed that when a certain\nLSTM layer reaches a sufficiently large scale, the promotion\nof overall effect will slow down as its hidden size increases. In\nthis paper, we analyze that an important factor leading to this\nphenomenon is the high correlation between the newly extended hidden states and original hidden states, which hinders diverse feature expression of the LSTM. As a result, when the scale is large enough, simply lengthening the LSTM hidden states will cost tremendous extra parameters but has little effect. We propose a simple yet effective improvement on each LSTM layer consisting of a large-scale Major LSTM and a smallscale Minor LSTM to break the high correlation between the two parts of hidden states, which we call Major-Minor LSTMs (MMLSTMs). In experiments, we demonstrate the language model with MMLSTMs surpasses the existing state-of-the-art model on Penn Treebank (PTB) and WikiText-2 (WT2) datasets, and outperforms the baseline by 3.3 points in perplexity on WikiText-103 dataset without increasing model parameter counts.",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "China,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"In experiments, we demonstrate the language model with MMLSTMs surpasses the existing state-of-the-art model on Penn Treebank (PTB) and WikiText-2 (WT2) datasets\"",
      "Epochs": "500.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "StarGAN v2",
      "Organization": "NAVER,Yonsei University,Swiss Federal Institute of Technology",
      "Publication date": "2019-12-04",
      "Domain": "Vision,Image generation",
      "Task": "Image generation,Image-to-image",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "CelebA,AFHQ",
      "Training dataset size (gradients)": "400000",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1912.01865",
      "Reference": "StarGAN v2: Diverse Image Synthesis for Multiple Domains",
      "Citations": "1939.0",
      "Authors": "Yunjey Choi, Youngjung Uh, Jaejun Yoo, Jung-Woo Ha",
      "Abstract": "A good image-to-image translation model should learn a mapping between different visual domains while satisfying the following properties: 1) diversity of generated images and 2) scalability over multiple domains. Existing methods address either of the issues, having limited diversity or multiple models for all domains. We propose StarGAN v2, a single framework that tackles both and shows significantly improved results over the baselines. Experiments on CelebA-HQ and a new animal faces dataset (AFHQ) validate our superiority in terms of visual quality, diversity, and scalability. To better assess image-to-image translation models, we release AFHQ, high-quality animal faces with large inter- and intra-domain differences. The code, pretrained models, and dataset can be found at this https URL.",
      "Organization categorization": "Industry,Academia,Academia",
      "Country (of organization)": "Korea (Republic of),Korea (Republic of),Switzerland",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\"Votes from AMT workers for the most preferred method regarding visual quality and style reflection (%). StarGAN v2 outperforms the baselines with remarkable margins in all aspects.\"\n\n\"As shown in Table 2, our method outperforms all the baselines by a large margin in terms of visual quality. For both CelebA-HQ and AFHQ, our method achieves FIDs of 13.7 and 16.2, respectively, which are more than two times improvement over the previous leading method.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "Open access (non-commercial)",
      "Accessibility notes": "https://github.com/clovaai/stargan-v2?tab=readme-ov-file non-commercial",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "StyleGAN2",
      "Organization": "NVIDIA,Aalto University",
      "Publication date": "2019-12-03",
      "Domain": "Image generation",
      "Task": "Image generation",
      "Parameters": "30000000.0",
      "Parameters notes": "30M for the generator, 29M for the discriminator",
      "Training compute (FLOP)": "",
      "Training compute notes": "318000000 * 30,000,000.0 * 6 = 5.724e+16. But not sure if that applies to this architecture.",
      "Training dataset": "FFHQ,LSUN,LSUN Church",
      "Training dataset size (gradients)": "114000000",
      "Dataset size notes": "Datasets trained on (with multiple epochs): 88+48+100+57+25 ~= 318M",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/pdf/1912.04958",
      "Reference": "Analyzing and Improving the Image Quality of StyleGAN",
      "Citations": "7908.0",
      "Authors": "Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila",
      "Abstract": "The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,Finland",
      "Notability criteria": "Historical significance,Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "2016.0",
      "Training time notes": "~84 GPU-days * 24 hours/day",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "8.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "4916.068644931902",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "32.0",
      "Batch size notes": "",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Transformer-XL DeFINE (141M)",
      "Organization": "University of Washington,Allen Institute for AI",
      "Publication date": "2019-11-27",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "141000000.0",
      "Parameters notes": "Table 2b",
      "Training compute (FLOP)": "1.74276e+18",
      "Training compute notes": "6 FLOP / token / parameter * 141000000 parameters * 103000000 tokens * 20 epochs [assumption based on number of epochs for LSTMs] =  1.74276e+18 FLOP\n\n_______________\nolder estimation:  6.2 \u00d7 10^18 (no explantion how it was calculated)",
      "Training dataset": "WikiText-103",
      "Training dataset size (gradients)": "103000000",
      "Dataset size notes": "",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/1911.12385",
      "Reference": "DeFINE: DEep Factorized INput Token Embeddings for Neural Sequence Modeling",
      "Citations": "27.0",
      "Authors": "Sachin Mehta, Rik Koncel-Kedziorski, Mohammad Rastegari, Hannaneh Hajishirzi",
      "Abstract": "For sequence models with large vocabularies, a majority of network parameters lie in the input and output layers. In this work, we describe a new method, DeFINE, for learning deep token representations efficiently. Our architecture uses a hierarchical structure with novel skip-connections which allows for the use of low dimensional input and output layers, reducing total parameters and training time while delivering similar or better performance versus existing methods. DeFINE can be incorporated easily in new or existing sequence models. Compared to state-of-the-art methods including adaptive input representations, this technique results in a 6% to 20% drop in perplexity. On WikiText-103, DeFINE reduces the total parameters of Transformer-XL by half with minimal impact on performance. On the Penn Treebank, DeFINE improves AWD-LSTM by 4 points with a 17% reduction in parameters, achieving comparable performance to state-of-the-art methods with fewer parameters. For machine translation, DeFINE improves the efficiency of the Transformer model by about 1.4 times while delivering similar performance.",
      "Organization categorization": "Academia,Research collective",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Compared to state-of-the-art methods including adaptive input representations,\nthis technique results in a 6% to 20% drop in perplexity\"\n\nTable 2a",
      "Epochs": "20.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Photo-Geometric Autoencoder",
      "Organization": "University of Oxford",
      "Publication date": "2019-11-25",
      "Domain": "3D modeling,Vision",
      "Task": "3D reconstruction",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "CelebA,3DFAW,BFM",
      "Training dataset size (gradients)": "819200000",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1911.11130",
      "Reference": "Unsupervised Learning of Probably Symmetric Deformable 3D Objects From Images in the Wild",
      "Citations": "328.0",
      "Authors": "Shangzhe Wu, Christian Rupprecht, Andrea Vedaldi\n",
      "Abstract": "We propose a method to learn 3D deformable object categories from raw single-view images, without external supervision. The method is based on an autoencoder that factors each input image into depth, albedo, viewpoint and illumination. In order to disentangle these components without supervision, we use the fact that many object categories have, at least in principle, a symmetric structure. We show that reasoning about illumination allows us to exploit the underlying object symmetry even if the appearance is not symmetric due to shading. Furthermore, we model objects that are probably, but not certainly, symmetric by predicting a symmetry probability map, learned end-to-end with the other components of the model. Our experiments show that this method can recover very accurately the 3D shape of human faces, cat faces and cars from single-view images, without any supervision or a prior shape model. On benchmarks, we demonstrate superior accuracy compared to another method that uses supervision at the level of 2D image correspondences.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Our model outperforms a\ncurrent state-of-the-art 3D reconstruction method that uses 2D keypoint supervision\"\n\nThey don't claim absolute SOTA, only SOTA among unsupervised methods",
      "Epochs": "30.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT license: https://github.com/elliottwu/unsup3d",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Transformer - LibriVox + Decoding/Rescoring",
      "Organization": "Facebook",
      "Publication date": "2019-11-19",
      "Domain": "Speech",
      "Task": "Speech recognition (ASR)",
      "Parameters": "296000000.0",
      "Parameters notes": "Table 2",
      "Training compute (FLOP)": "",
      "Training compute notes": "\"Models are trained on 64 GPUs each with an overall batch size of 256 for ResNet and TDS and 320 for Transformer. With only LIBRISPEECH, all models converged in under a week; with pseudo-labels from LIBRIVOX, training required 2-3 weeks\"\n\nGPU not specified",
      "Training dataset": "LibriSpeech,LibriVox",
      "Training dataset size (gradients)": "981312000",
      "Dataset size notes": "\"the resulting audio corpus contains 53.8K hours of read speech\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1911.08460v3",
      "Reference": "End-to-end ASR: from Supervised to Semi-Supervised Learning with Modern Architectures",
      "Citations": "260.0",
      "Authors": "Gabriel Synnaeve, Qiantong Xu, Jacob Kahn, Tatiana Likhomanenko, Edouard Grave, Vineel Pratap, Anuroop Sriram, Vitaliy Liptchinsky, Ronan Collobert",
      "Abstract": "We study pseudo-labeling for the semi-supervised training of ResNet, Time-Depth Separable ConvNets, and Transformers for speech recognition, with either CTC or Seq2Seq loss functions. We perform experiments on the standard LibriSpeech dataset, and leverage additional unlabeled data from LibriVox through pseudo-labeling. We show that while Transformer-based acoustic models have superior performance with the supervised dataset alone, semi-supervision improves all models across architectures and loss functions and bridges much of the performance gaps between them. In doing so, we reach a new state-of-the-art for end-to-end acoustic models decoded with an external language model in the standard supervised learning setting, and a new absolute state-of-the-art with semi-supervised training. Finally, we study the effect of leveraging different amounts of unlabeled audio, propose several ways of evaluating the characteristics of unlabeled audio which improve acoustic modeling, and show that acoustic models trained with more audio rely less on external language models.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Results with decoding/rescoring are shown in Table 2, where we reach 2.09% and 4.11% on test-clean and test-other , respectively, and are further improvements on the state-of-the-art.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "BSD license: https://github.com/jakeju/wav2letter?tab=License-1-ov-file#readme",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "MuZero",
      "Organization": "DeepMind",
      "Publication date": "2019-11-19",
      "Domain": "Games",
      "Task": "Atari",
      "Parameters": "36864000.0",
      "Parameters notes": "Both the representation and dynamics function use the same architecture asAlphaZero, but with 16 instead of20 residual blocks [15]. We use 3x3 kernels and 256 hidden planes for each convolution.\n\nPrevious downsampling:\n\u2022  1 convolution with stride 2 and 128 output planes, output resolution 48x48.\u2022  2 residual blocks with 128 planes\u2022  1 convolution with stride 2 and 256 output planes, output resolution 24x24.\u2022  3 residual blocks with 256 planes.\u2022  Average pooling with stride 2, output resolution 12x12.\u2022  3 residual blocks with 256 planes.\u2022  Average pooling with stride 2, output resolution 6x6.",
      "Training compute (FLOP)": "4.8e+19",
      "Training compute notes": "third-generation Google Cloud TPU\n(For each board game, we used 16 TPUs for training and 1000 TPUs for self-play)\nFor each game in Atari, we used 8 TPUs for training and 32 TPUs for self-play\nTraining for 12 hours (for Atari)\nData from Parameter, Compute and Data Trends in Machine Learning\nGoogle v3 TPU: 1.23E+14 FLOP/s (although with the caveat that it might be not applicable)\nUtilization rate \nIn LaMDA: Language Models for Dialog Applications, they report for TPU V3: 56.5%\nCalculations for Atari:\n12 hours \u2192 43200 seconds\n(8 TPUs for training) * (1.23*10^14 FLOP/s) * (43.2 *10^3 s) * (0.565 utilization rate) = 2.4017472 * 10^19 FLOP\nTraining time missing for boardgames\nAssumption also 12 hours \nAlso: 2.4017472 * 10^19 FLOP\nTotal cost \u2248 4.8 * 10^19 FLOP",
      "Training dataset": "",
      "Training dataset size (gradients)": "12288000000",
      "Dataset size notes": "Table 1\nhttps://arxiv.org/pdf/1911.08265.pdf",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/1911.08265v2",
      "Reference": "Mastering Atari Go Chess and Shogi by Planning with a Learned Model",
      "Citations": "2296.0",
      "Authors": "J Schrittwieser, I Antonoglou, T Hubert, K Simonyan",
      "Abstract": "Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\"When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. \"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "MoCo",
      "Organization": "Facebook AI",
      "Publication date": "2019-11-13",
      "Domain": "Vision,Image generation",
      "Task": "Image completion",
      "Parameters": "375000000.0",
      "Parameters notes": "https://openai.com/blog/image-gpt/#rfref53",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "ImageNet,Instagram-1B",
      "Training dataset size (gradients)": "940000000",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/1911.05722",
      "Reference": "Momentum Contrast for Unsupervised Visual Representation Learning",
      "Citations": "10711.0",
      "Authors": "Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xe, Ross Girshick",
      "Abstract": "We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.\n",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "Open access (non-commercial)",
      "Accessibility notes": "non-commercial. the released models seem to be trained on ImageNet, not Instagram\nhttps://github.com/facebookresearch/moco",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Noisy Student (L2)",
      "Organization": "Carnegie Mellon University (CMU),Google",
      "Publication date": "2019-11-11",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "480000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "2.612e+22",
      "Training compute notes": "\"Our largest model, EfficientNet-L2, needs to be trained for 6 days on a Cloud TPU v3 Pod, which has 2048 cores, if the unlabeled batch size is 14x the labeled batch size\"\nTPU v3 gets 1.23e14 FLOP/s per chip, with 2 cores per chip\n\n1024 * 1.23e14 * 6 * 24 * 3600 * 0.4 = 2.612e22",
      "Training dataset": "ImageNet,JFT",
      "Training dataset size (gradients)": "81000000",
      "Dataset size notes": "\"Due to duplications, there are only 81M unique images among these 130M images.\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1911.04252v4",
      "Reference": "Self-training with Noisy Student improves ImageNet classification",
      "Citations": "2600.0",
      "Authors": "Q Xie, MT Luong, E Hovy, QV Lee",
      "Abstract": "We present Noisy Student Training, a semi-supervised learning approach that works well even when labeled data is abundant. Noisy Student Training achieves 88.4% top-1 accuracy on ImageNet, which is 2.0% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0% to 83.7%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean flip rate from 27.8 to 12.2.\nNoisy Student Training extends the idea of self-training and distillation with the use of equal-or-larger student models and noise added to the student during learning. On ImageNet, we first train an EfficientNet model on labeled images and use it as a teacher to generate pseudo labels for 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the learning of the student, we inject noise such as dropout, stochastic depth, and data augmentation via RandAugment to the student so that the student generalizes better than the teacher. Models are available at this https URL. Code is available at this https URL.",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\"Noisy Student Training achieves 88.4% top-1 accuracy on ImageNet, which is 2.0% better than the state-of-the-art model\"",
      "Epochs": "",
      "Training time (hours)": "144.0",
      "Training time notes": "6 days",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "1024.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "45609.73119622589",
      "Compute cost notes": "",
      "Training power draw (W)": "944347.7271739373",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "apache 2.0 license: https://github.com/google-research/noisystudent train script: https://github.com/google-research/noisystudent/blob/master/local_scripts/imagenet/train.sh ",
      "Numerical format": "FP32",
      "Frontier model": "True",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "146923.60067323185",
      "Training compute cost (upfront)": "12110168.268263536"
    },
    {
      "Model": "Sandwich Transformer",
      "Organization": "Allen Institute for AI,Facebook AI Research",
      "Publication date": "2019-11-10",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "209000000.0",
      "Parameters notes": "209M\n\"All of our experiments use the same hyperparameters as Baevski and Auli\u2019s original model.\"",
      "Training compute (FLOP)": "2.3504093e+19",
      "Training compute notes": "6 FLOP / token / parameter * 209000000 parameters * 286000 steps * 65536 tokens per batch [same as Baevski and Auli (2019) = 2.3504093e+19 FLOP\n\n__________\n\nin the Algorithmic progress paper they assumed 180 epochs (same as Baevski and Auli 2019 Transformer, but that one was trained on WT 103 not the book corpus) -> training compute was estimated to be 1.58E+20 FLOP",
      "Training dataset": "BookCorpus (BooksCorpus, Toronto Book Corpus)",
      "Training dataset size (gradients)": "700000000",
      "Dataset size notes": "\"while retaining the other architectural aspects and hyperparameter settings from Baevski and Auli (2019)\"\n\nBaevski and Auli (2019): 286k steps in batches of 65,536 tokens.\n\n286000*65536/700000000 = 27 epochs",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/1911.03864",
      "Reference": "Improving Transformer Models by Reordering their Sublayers",
      "Citations": "93.0",
      "Authors": "Ofir Press, Noah A. Smith, Omer Levy",
      "Abstract": "Multilayer transformer networks consist of interleaved self-attention and feedforward sublayers. Could ordering the sublayers in a different pattern lead to better performance? We generate randomly ordered transformers and train them with the language modeling objective. We observe that some of these models are able to achieve better performance than the interleaved baseline, and that those successful variants tend to have more self-attention at the bottom and more feedforward sublayers at the top. We propose a new transformer pattern that adheres to this property, the sandwich transformer, and show that it improves perplexity on multiple word-level and character-level language modeling benchmarks, at no cost in parameters, memory, or training time. However, the sandwich reordering pattern does not guarantee performance gains across every task, as we demonstrate on machine translation models. Instead, we suggest that further exploration of task-specific sublayer reorderings is needed in order to unlock additional gains.",
      "Organization categorization": "Research collective,Industry",
      "Country (of organization)": "United States of America,United States of America,France",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Sandwich transformers achieve state-of-the-art results on the enwik8 character-level language modeling dataset and on an additional word-level corpus, but have no significant effect on machine translation\"",
      "Epochs": "27.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "65536.0",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "Open access (non-commercial)",
      "Accessibility notes": "non-commercial training and inference code: https://github.com/ofirpress/sandwich_transformer",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "CamemBERT",
      "Organization": "Facebook,INRIA,Sorbonne University",
      "Publication date": "2019-11-10",
      "Domain": "Language",
      "Task": "Language modeling/generation,Part-of-speech tagging,Named entity recognition (NER)",
      "Parameters": "335000000.0",
      "Parameters notes": "CamemBERT Large, Table 4",
      "Training compute (FLOP)": "8.3e+20",
      "Training compute notes": "\"Unless otherwise specified, our models use the BASE architecture, and are pretrained for 100k backpropagation steps on 256 Nvidia V100 GPUs (32GB each) for a day\"\n\n256 V100-days\n\n256 * 125 teraflops * 24 * 3600 * 0.3 (assumed utilization)\n= 8.3e20\n\n\n\"Following (Liu et al., 2019), we optimize the model using Adam (Kingma and Ba, 2014) (\u03b21 = 0.9, \u03b22 = 0.98) for 100k steps with large batch sizes of 8192 sequences, each sequence containing at most 512 tokens\"\n\nUsing compute = 6*N*D, that's 6 * (100k * 8192 * 512) * 335M= 8.43e20",
      "Training dataset": "CCNet",
      "Training dataset size (gradients)": "28615771779.58404",
      "Dataset size notes": " 31.9B tokens, Table 6.",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1911.03894",
      "Reference": "CamemBERT: a Tasty French Language Model",
      "Citations": "1046.0",
      "Authors": "Louis Martin, Benjamin Muller, Pedro Javier Ortiz Su\u00e1rez, Yoann Dupont, Laurent Romary, \u00c9ric Villemonte de la Clergerie, Djam\u00e9 Seddah, Beno\u00eet Sagot",
      "Abstract": "Pretrained language models are now ubiquitous in Natural Language Processing. Despite their success, most available models have either been trained on English data or on the concatenation of data in multiple languages. This makes practical use of such models --in all languages except English-- very limited. In this paper, we investigate the feasibility of training monolingual Transformer-based language models for other languages, taking French as an example and evaluating our language models on part-of-speech tagging, dependency parsing, named entity recognition and natural language inference tasks. We show that the use of web crawled data is preferable to the use of Wikipedia data. More surprisingly, we show that a relatively small web crawled dataset (4GB) leads to results that are as good as those obtained using larger datasets (130+GB). Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks.",
      "Organization categorization": "Industry,Academia,Academia",
      "Country (of organization)": "United States of America,France,France",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks.\" (part-of-speech tagging, dependency parsing, named entity recognition and natural language inference tasks)",
      "Epochs": "13.0",
      "Training time (hours)": "24.0",
      "Training time notes": "1 day for each model (may not have been a full 24 hours)",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "2319.5419478533995",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "MIT: \nhttps://camembert-model.fr/",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "XLM-RoBERTa",
      "Organization": "Facebook AI",
      "Publication date": "2019-11-05",
      "Domain": "Language",
      "Task": "Named entity recognition (NER),Question answering,Text classification",
      "Parameters": "550000000.0",
      "Parameters notes": "The number of parameters in the model is specified as \"550M params\" for XLM-R.",
      "Training compute (FLOP)": "2.076e+22",
      "Training compute notes": "\"We use the multilingual MLM loss and train our XLM-R model for\n1.5 Million updates on five-hundred 32GB Nvidia\nV100 GPUs with a batch size of 8192. \"\n\n6ND:\nSequence length was probably 512, based on follow up paper (XLM-R XXL)\n6 * 550e6 * 1.5e6 * 8192 * 512 = 2.076e22\n",
      "Training dataset": "CC100",
      "Training dataset size (gradients)": "167000000000",
      "Dataset size notes": "size of CC100 - copied from other rows",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1911.02116",
      "Reference": "Unsupervised Cross-lingual Representation Learning at Scale",
      "Citations": "7541.0",
      "Authors": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov\n",
      "Abstract": "This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "citation \"which obtains state-of-the-art performance on cross-lingual classification, sequence labeling and question answering\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA Tesla V100 DGXS 32 GB",
      "Hardware quantity": "500.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "77861.31819548723",
      "Compute cost notes": "",
      "Training power draw (W)": "256204.94677329363",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "Open access (non-commercial)",
      "Accessibility notes": "non-commercial: https://github.com/facebookresearch/XLM?tab=License-1-ov-file#readme\n\ndata is wikipedia",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": "11788558.141848203"
    },
    {
      "Model": "Base LM + kNN LM + Continuous Cache",
      "Organization": "Stanford University,Facebook AI Research",
      "Publication date": "2019-11-01",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "247000000.00000003",
      "Parameters notes": "\"we take the exact architecture and optimization described by Baevski & Auli (2019) and use it to create a kNN-LM for inference. This model consists of 16 layers, each with 16 self-attention heads, 1024 dimensional hidden states, and 4096 dimensional feedforward layers, amounting to 247M trainable parameters.\"",
      "Training compute (FLOP)": "3.05292e+19",
      "Training compute notes": "6 FLOP / parameter / token * 247*10^6 parameters * 103000000 tokens * 200 epochs = 3.05292e+19 FLOP\n\n\n__________\nfor the Algorithmic progress paper 7.3 \u00d7 10^18 FLOP was estimated similar to supposedly base model (transformer) ",
      "Training dataset": "WikiText-103",
      "Training dataset size (gradients)": "103000000",
      "Dataset size notes": "\" During this forward pass, each target token is provided a minimum of 1536 tokens of prior context for WIKITEXT-103\"\n\n200 epochs - figure 8",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/1911.00172",
      "Reference": "Generalization through Memorization: Nearest Neighbor Language Models",
      "Citations": "962.0",
      "Authors": "Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, Mike Lewis",
      "Abstract": "We introduce kNN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a k-nearest neighbors (kNN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our kNN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail.",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "United States of America,United States of America,France",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"GNN-LM achieves a new state-of-the-art perplexity of 14.8 on WikiText-103\"",
      "Epochs": "200.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Training code, MIT: https://github.com/urvashik/knnlm",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "AlphaStar",
      "Organization": "DeepMind",
      "Publication date": "2019-10-30",
      "Domain": "Games",
      "Task": "StarCraft",
      "Parameters": "139000000.0",
      "Parameters notes": "AlphaStar has 139 million weights, but only 55 million weights are required during inference.",
      "Training compute (FLOP)": "1.0773400001e+23",
      "Training compute notes": "(Estimate by James Sanders, checked by Robi Rahman)\nFig 6 indicates that the learner uses 16 TPU \"devices\" which are 128 TPU cores total, which matches 4 TPUs per device, and 2 cores per TPU. Fig 6 indicates that 64 TPUs are used for training, and 64 are used for inference. (128 TPUs)*(12 agents)*(44 days)*(123 TFLOPS)*(0.3 utilization) = 2.15e23 FLOP (total), of which 1.07e23 FLOP are for training.",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "Multiple data types. First supervised learning, then other stuff",
      "Confidence": "Confident",
      "Link": "https://www.deepmind.com/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning",
      "Reference": "Grandmaster level in StarCraft II using multi-agent reinforcement learning",
      "Citations": "4093.0",
      "Authors": "Oriol Vinyals,Igor Babuschkin,Wojciech M. Czarnecki,Micha\u00ebl Mathieu,Andrew Dudzik,Junyoung Chung,David H. Choi,Richard Powell,Timo Ewalds,Petko Georgiev,Junhyuk Oh,Dan Horgan,Manuel Kroiss,Ivo Danihelka,Aja Huang,Laurent Sifre,Trevor Cai,John P. Agapiou,Max Jaderberg,Alexander S. Vezhnevets,R\u00e9mi Leblond,Tobias Pohlen,Valentin Dalibard,David Budden,Yury Sulsky,James Molloy,Tom L. Paine,Caglar Gulcehre,Ziyu Wang,Tobias Pfaff,Yuhuai Wu,Roman Ring,Dani Yogatama,Dario W\u00fcnsch,Katrina McKinney,Oliver Smith,Tom Schaul,Timothy Lillicrap,Koray Kavukcuoglu,Demis Hassabis,Chris Apps,David Silver",
      "Abstract": "Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1\u20133, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using generalpurpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8% of officially ranked human players.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Discretionary",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "1056.0",
      "Training time notes": "\"Each agent was trained using 32 third-generation tensor\nprocessing units (TPUs) over 44 days\"",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "384.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "130654.07330431018",
      "Compute cost notes": "",
      "Training power draw (W)": "354225.04547181545",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache 2.0, training tools: https://github.com/google-deepmind/alphastar\n\ntraining instructions here: https://github.com/google-deepmind/alphastar/blob/main/alphastar/unplugged/README.md ",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "395061.2373658012",
      "Training compute cost (upfront)": "4541313.100598825"
    },
    {
      "Model": "BART-large",
      "Organization": "Facebook AI",
      "Publication date": "2019-10-29",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering",
      "Parameters": "406291456.0",
      "Parameters notes": "\"In total, BART contains roughly 10% more parameters than the equivalently sized BERT model.\"\n\nI counted the parameters in the huggingface model\nhttps://huggingface.co/facebook/bart-large/tree/main\n\nfrom transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\nmodel = AutoModel.from_pretrained(\"facebook/bart-large\")\nsum(p.numel() for p in model.parameters() if p.requires_grad)",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Wikipedia",
      "Training dataset size (gradients)": "42666666666",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/1910.13461",
      "Reference": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
      "Citations": "11996.0",
      "Authors": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer",
      "Abstract": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Models:\nhttps://github.com/facebookresearch/fairseq/blob/main/examples/bart/README.md\n\nMIT license:\nhttps://github.com/facebookresearch/fairseq/blob/main/LICENSE",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "T5-11B",
      "Organization": "Google",
      "Publication date": "2019-10-23",
      "Domain": "Language",
      "Task": "Text autocompletion,Language modeling/generation",
      "Parameters": "11000000000.0",
      "Parameters notes": "The full 11-billion parameter model",
      "Training compute (FLOP)": "3.3e+22",
      "Training compute notes": "https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf\nTable 4, 4.05e22\n\nupdate: 3.3e22 per FLAN paper from Google \nhttps://arxiv.org/pdf/2210.11416.pdf\n\n6ND rule suggests somewhat more FLOPs:\n6 * 1T * 11B = 6.6e22",
      "Training dataset": "C4",
      "Training dataset size (gradients)": "34000000000",
      "Dataset size notes": "\"This produces a collection of text that is not only\norders of magnitude larger than most data sets used for pre-training (about 750 GB) but also comprises reasonably clean and natural English text. We dub this data set the \u201cColossal Clean Crawled Corpus\u201d (or C4 for short) and release it as part of TensorFlow Datasets\"\n\n750GB * 200M word/GB * 4/3 tokens per word = 2e11.\n\nTotal tokens seen is about 1T:  \"We therefore pre-train our models for 1 million steps on a batch size of 2^11 sequences of length 512, corresponding to a total of about 1 trillion pre-training tokens\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1910.10683",
      "Reference": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
      "Citations": "23572.0",
      "Authors": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu",
      "Abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "481.9",
      "Training time notes": "4.05*10^22 FLOP at 37.073% utilization on 512 TPU v3 chips (123 TFLOPS) -> 482 hours\nhttps://www.wolframalpha.com/input?i=4.05*10%5E22+seconds+%2F+%28512*123*10%5E12%29+*%28123%2F45.6%29",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "512.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "78464.68282814459",
      "Compute cost notes": "",
      "Training power draw (W)": "472373.69114573154",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "65536.0",
      "Batch size notes": "\"We use a maximum sequence length of 512 and a batch size of 128 sequences. Whenever possible, we \u201cpack\u201d multiple sequences into each entry of the batch10 so that our batches contain roughly 2^16 = 65,536 tokens\"",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache for code and weights:\nhttps://github.com/google-research/text-to-text-transfer-transformer\n\nData is C4 which is open",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "0.3707",
      "Training compute cost (cloud)": "240378.8008668934",
      "Training compute cost (upfront)": "6055084.134131768"
    },
    {
      "Model": "T5-3B",
      "Organization": "Google",
      "Publication date": "2019-10-23",
      "Domain": "Language",
      "Task": "Text autocompletion",
      "Parameters": "2800000000.0",
      "Parameters notes": "page 37, 3B and 11B. \"To further explore what kind of performance is possible when using larger models, we consider two additional variants. In both cases, we use d_model = 1024, a 24 layer encoder and decoder, and dkv = 128. For the \u201c3B\u201d variant, we use dff = 16,384 with 32-headed attention, which results in around 2.8 billion parameters; for \u201c11B\u201d we use dff = 65,536 with 128-headed attention producing a model with about 11 billion parameters\"",
      "Training compute (FLOP)": "9.0000000001e+21",
      "Training compute notes": "Akronomicon states 1.04e+22 FLOP. Archived source: https://github.com/lightonai/akronomicon/tree/main/akrodb\nHowever, this seems dubiously high.\n\n\"We pre-train each model for 2^19 = 524,288 steps on C4 before fine-tuning.\"\n\"In total, this batch size and number of steps corresponds to pre-training on 2^35 \u2248 34B tokens.\"\n\"To compare these mixing strategies on equal footing with our baseline pre-train-then-fine-tune results, we train multi-task models for the same total number of steps: 2^19 + 2^18 = 786,432\"\nUsing the 6DN approximation gives: 6 FLOP/token/param * 2^35 pretrain tokens * (1+1/2 finetune tokens per pretrain token) * 1 iteration of training data* 2.8 billion parameters = 8.659e20 FLOP\nhttps://www.wolframalpha.com/input?i=6+*+2%5E35+*+2.8+billion+*+1.5\n\nupdate: 9.0E+21 per FLAN paper from Google \nhttps://arxiv.org/pdf/2210.11416.pdf",
      "Training dataset": "C4",
      "Training dataset size (gradients)": "5100000000",
      "Dataset size notes": "\"This produces a collection of text that is not only orders of magnitude larger than most data sets used for pre-training (about 750 GB) but also\ncomprises reasonably clean and natural English text. We dub this data set the \u201cColossal Clean Crawled Corpus\u201d (or C4 for short) and release it as part of TensorFlow Datasets\"\n750GB * 200M word/GB = 1.5e11\n\n\"In total, this batch size and number of steps corresponds to pre-training on 2^35 \u2248 34B tokens.\"\n\"Note that 2^35 tokens only covers a fraction of the entire C4 data set, so we never repeat any data during pre-training.\"\nThe fraction is 25.5 billion / 150 billion = 0.17 epochs.",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1910.10683",
      "Reference": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
      "Citations": "23572.0",
      "Authors": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu",
      "Abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "17420.709393371122",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache for code and weights:\nhttps://github.com/google-research/text-to-text-transfer-transformer\n\nData is C4 which is open\ntraining script: https://github.com/google-research/text-to-text-transfer-transformer?tab=readme-ov-file#training ",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "M4-50B",
      "Organization": "Google",
      "Publication date": "2019-10-11",
      "Domain": "Language",
      "Task": "Translation",
      "Parameters": "50000000000.0",
      "Parameters notes": "(sparse architecture)\n\n\"By modifying the Transformer architecture through the substitution of the vanilla feed-forward layers with sparsely-gated mixture of experts, we drastically scale up the model capacity, allowing us to successfully train and pass 50 billion parameters, which further improved translation quality across the board.\"",
      "Training compute (FLOP)": "",
      "Training compute notes": "Sparse architecture, so training compute is uncertain",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "25+ billion sentence pairs",
      "Confidence": "Confident",
      "Link": "https://blog.research.google/2019/10/exploring-massively-multilingual.html",
      "Reference": "Exploring Massively Multilingual, Massive Neural Machine Translation",
      "Citations": "",
      "Authors": "Ankur Bapna, Orhan Firat",
      "Abstract": "Over the last few years there has been enormous progress in the quality of machine translation (MT) systems, breaking language barriers around the world thanks to the developments in neural machine translation (NMT). The success of NMT however, owes largely to the great amounts of supervised training data. But what about languages where data is scarce, or even absent? Multilingual NMT, with the inductive bias that \u201cthe learning signal from one language should benefit the quality of translation to other languages\u201d, is a potential remedy.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "all evaluations are performed on internal benchmarks, I don't see any standard benchmarks",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DistilBERT",
      "Organization": "Hugging Face",
      "Publication date": "2019-10-02",
      "Domain": "Language",
      "Task": "Text autocompletion",
      "Parameters": "66000000.0",
      "Parameters notes": "Table 3",
      "Training compute (FLOP)": "1.24416e+19",
      "Training compute notes": "Section 3: DistilBERT was trained on 8 16GB V100 GPUs for approximately 90 hours.\n\n1.6e13*8*60**2*90*0.3 = 1.2e19",
      "Training dataset": "Wikipedia,BookCorpus (BooksCorpus, Toronto Book Corpus)",
      "Training dataset size (gradients)": "495000000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1910.01108",
      "Reference": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
      "Citations": "8789.0",
      "Authors": "Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA Tesla V100 DGXS 16 GB",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "code, including train: https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation\n\nweights: https://huggingface.co/distilbert/distilbert-base-uncased\n\nrepo license is apache: https://github.com/huggingface/transformers/blob/main/LICENSE\n\nWikipedia is open, BookCorpus is not",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "AlphaX-1",
      "Organization": "Facebook AI Research,Brown University",
      "Publication date": "2019-10-02",
      "Domain": "Vision",
      "Task": "Neural architecture search for computer vision,Image classification,Object detection,Image captioning",
      "Parameters": "5400000.0",
      "Parameters notes": "Table 3: multiadds for AlphaX-1 579M, parameters 5.4M",
      "Training compute (FLOP)": "8.89344e+17",
      "Training compute notes": "\" Our models for ImageNet use polynomial learning rate\nschedule, starting with 0.05 and decay through 200 epochs.\"\n\n1280000 images * 200 epochs *3 forward-backward adjustment * 1158000000 forward FLOP =889344000000000000",
      "Training dataset": "ImageNet,COCO",
      "Training dataset size (gradients)": "61280000",
      "Dataset size notes": "Standard image net training size, not otherwise specified",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1903.11059",
      "Reference": "AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search",
      "Citations": "97.0",
      "Authors": "Linnan Wang, Yiyang Zhao, Yuu Jinnai, Yuandong Tian, Rodrigo Fonseca1",
      "Abstract": "Neural Architecture Search (NAS) has shown great success in automating the design of neural networks, but the prohibitive amount of computations behind current NAS methods requires further investigations in improving the sample efficiency and the network evaluation cost to get better results in a shorter time. In this paper, we present a novel scalable Monte Carlo Tree Search (MCTS) based NAS agent, named AlphaX, to tackle these two aspects. AlphaX improves the search efficiency by adaptively balancing the exploration and exploitation at the state level, and by a Meta-Deep Neural Network (DNN) to predict network accuracies for biasing the search toward a promising region. To amortize the network evaluation cost, AlphaX accelerates MCTS rollouts with a distributed design and reduces the number of epochs in evaluating a network by transfer learning guided with the tree structure in MCTS. In 12 GPU days and 1000 samples, AlphaX found an architecture that reaches 97.84\\% top-1 accuracy on CIFAR-10, and 75.5\\% top-1 accuracy on ImageNet, exceeding SOTA NAS methods in both the accuracy and sampling efficiency. Particularly, we also evaluate AlphaX on NASBench-101, a large scale NAS dataset; AlphaX is 3x and 2.8x more sample efficient than Random Search and Regularized Evolution in finding the global optimum. Finally, we show the searched architecture improves a variety of vision applications from Neural Style Transfer, to Image Captioning and Object Detection.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,France,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"In 12 GPU days and 1000 samples, AlphaX found an architecture that reaches 97.84\\% top-1 accuracy on CIFAR-10, and 75.5\\% top-1 accuracy on ImageNet, exceeding SOTA NAS methods in both the accuracy and sampling efficiency\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA GeForce GTX 1080 Ti",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "Open access (non-commercial)",
      "Accessibility notes": "code, no license specified: https://github.com/linnanwang/AlphaX-NASBench101\ntraining: https://github.com/linnanwang/AlphaX-NASBench101/blob/master/net_training.py ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ALBERT",
      "Organization": "Toyota Technological Institute at Chicago,Google Research",
      "Publication date": "2019-09-26",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering",
      "Parameters": "18000000.0",
      "Parameters notes": "Section 3.2 of paper",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "BookCorpus (BooksCorpus, Toronto Book Corpus),Wikipedia",
      "Training dataset size (gradients)": "3300000000",
      "Dataset size notes": "Pretraining same as for BERT - Wikipedia and BookCorpus\n\n\"For the pre-training corpus we\nuse the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words)\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1909.11942",
      "Reference": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
      "Citations": "7071.0",
      "Authors": "Z Lan, M Chen, S Goodman, K Gimpel",
      "Abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at this https URL.",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "79.4",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "2097152.0",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache 2.0: https://github.com/google-research/ALBERT",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Adaptive Inputs + LayerDrop",
      "Organization": "Facebook AI Research,LORIA",
      "Publication date": "2019-09-25",
      "Domain": "Language",
      "Task": "Language modeling/generation,Translation,Question answering,Text summarization,Language modeling",
      "Parameters": "423000000.00000006",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "WikiText-103",
      "Training dataset size (gradients)": "103000000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1909.11556",
      "Reference": "Reducing Transformer Depth on Demand with Structured Dropout",
      "Citations": "655.0",
      "Authors": "Angela Fan, Edouard Grave, Armand Joulin",
      "Abstract": "Overparameterized transformer networks have obtained state of the art results in various natural language processing tasks, such as machine translation, language modeling, and question answering. These models contain hundreds of millions of parameters, necessitating a large amount of computation and making them prone to overfitting. In this work, we explore LayerDrop, a form of structured dropout, which has a regularization effect during training and allows for efficient pruning at inference time. In particular, we show that it is possible to select sub-networks of any depth from one large network without having to finetune them and with limited impact on performance. We demonstrate the effectiveness of our approach by improving the state of the art on machine translation, language modeling, summarization, question answering, and language understanding benchmarks. Moreover, we show that our approach leads to small BERT-like models of higher quality compared to training from scratch or using distillation.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,France,France",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"In neural machine translation on newstest2014, our 12 encoder layer Transformer model with LayerDrop further improves the state of the art, reaching 30.2 BLEU\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "https://github.com/facebookresearch/fairseq/blob/main/examples/layerdrop/README.md\nRepo has MIT license\nWT training: https://github.com/facebookresearch/fairseq/tree/main/examples/language_model ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Megatron-LM (8.3B)",
      "Organization": "NVIDIA",
      "Publication date": "2019-09-17",
      "Domain": "Language",
      "Task": "Language modeling/generation",
      "Parameters": "8300000000.0",
      "Parameters notes": "Source: https://lair.lighton.ai/akronomicon/\n\nArchived source: https://web.archive.org/web/20211220142906/https://lair.lighton.ai/akronomicon/\n\nData also available on GitHub: https://github.com/lightonai/akronomicon/blob/main/akrodb/NVIDIA/Megatron-LM.json",
      "Training compute (FLOP)": "9.1e+21",
      "Training compute notes": "source: https://lair.lighton.ai/akronomicon/\n\narchived: https://github.com/lightonai/akronomicon/tree/main/akrodb\n\nother estimates:\n\n8.3B is a GPT-2-based model (Table 2). \"For GPT-2 models, all training is performed with sequences of 1024 subword units at a batch size of 512 for 300k iterations\" \n\nI interpret the above as 1024*512*300k = 157B training tokens \n\n6 * 157 billion * 8.3 billion  = 7.8e21\n\nAlso, their training setup achieved 15.1 petaFLOPS or 1.5e16 FLOPS.\n(512 V100s is 512 * 125 teraflops = 64 petaFLOPS so they had ~25% utilization)\n2.1 days per epoch, ~4.4 epochs\n2.1 * 4.4 * 24 * 3600 * 1.5e16 = 1.197e22\n\nThese are both close to the akronomicon estimate\n\nAuthors of \"AI and Memory Wall\" (https://github.com/amirgholami/ai_and_memory_wall) estimated model's training compute as 8,100,000 PFLOP = 8.1*10^21 FLOP",
      "Training dataset": "",
      "Training dataset size (gradients)": "46400000000",
      "Dataset size notes": "\"The resulting aggregate\ncorpus contains 174 GB of deduplicated text.\"",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/1909.08053",
      "Reference": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism",
      "Citations": "2375.0",
      "Authors": "Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro",
      "Abstract": "Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\"Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA\" \n\nGPT-2 model here meaning model similar to GPT-2",
      "Epochs": "4.4",
      "Training time (hours)": "327.0",
      "Training time notes": "Reported throughput is 15.1 teraFLOPS per GPU on 512 GPUs\nAssume total compute is 9.1e21 FLOP.\nThen training time is 327 hours.\nhttps://www.wolframalpha.com/input?i=9.1*10%5E21+FLOP+%2F+%28512*15.1+teraFLOPS%29",
      "Training hardware": "NVIDIA Tesla V100 DGXS 32 GB",
      "Hardware quantity": "512.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "109287.55758939347",
      "Compute cost notes": "327 hours * 512 GPUs * $0.55/V100 GPU-hour = $92,083\nConvert to 2020 dollars: $78,689",
      "Training power draw (W)": "262640.30207328824",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Unreleased",
      "Accessibility notes": "code (2.5B model is a GPT model): https://github.com/NVIDIA/Megatron-LM?tab=readme-ov-file#megatron-overview  \nopen license: https://github.com/NVIDIA/Megatron-LM?tab=License-1-ov-file#readme ",
      "Numerical format": "FP16",
      "Frontier model": "True",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "0.2359",
      "Training compute cost (cloud)": "224480.52397419358",
      "Training compute cost (upfront)": "12082300.278773397"
    },
    {
      "Model": "Megatron-LM (1.2B)",
      "Organization": "NVIDIA",
      "Publication date": "2019-09-17",
      "Domain": "Language",
      "Task": "Language modeling/generation",
      "Parameters": "1200000000.0",
      "Parameters notes": "Table 1 in https://arxiv.org/pdf/1909.08053",
      "Training compute (FLOP)": "1.13e+22",
      "Training compute notes": "300,000*512*1,024 = 1.57e11 tokens\n6*1.57*10e11*1.2e9 = 1.13e22 FLOPs",
      "Training dataset": "Wikipedia,CC-Stories,Realnews,OPENWEBTEXT",
      "Training dataset size (gradients)": "157000000000",
      "Dataset size notes": "300,000 *512*1,024 = 1.57e11 tokens",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1909.08053",
      "Reference": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism",
      "Citations": "2375.0",
      "Authors": "Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro",
      "Abstract": "Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA Tesla V100 SXM3 32 GB",
      "Hardware quantity": "1.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "338.2215428484996",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "",
      "Accessibility notes": "https://github.com/NVIDIA/Megatron-LM",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Megatron-BERT",
      "Organization": "NVIDIA",
      "Publication date": "2019-09-17",
      "Domain": "Language",
      "Task": "Language modeling/generation",
      "Parameters": "3900000000.0",
      "Parameters notes": "Table 4",
      "Training compute (FLOP)": "2.2e+22",
      "Training compute notes": "A third-party source: https://lair.lighton.ai/akronomicon/ claims 5.7e22\n\nThe authors report experimenting on 1 V100 GPU and achieving throughput of 39 TFLOPS which is 30% of the peak throughput. Therefore the GPU has a peak throughput of 130 TFLOPS so it is specifically the NVIDIA V100S PCIe.\nhttps://images.nvidia.com/content/technologies/volta/pdf/volta-v100-datasheet-update-us-1165301-r5.pdf\n\nParam-based calculation:\n6ND = 6*3.9e9*(2e6+1e4)*1024*512 = 2.5e22 FLOP\n\n1024 is the batch size, 512 is the sequence length (not explicitly stated but they say non-specified hyperparameters follow cited papers).\n\nTime-based calculation:\nThe 8.3B GPT-like arch took 2.1 days per epoch on 512 GPUs, batch size 512. An epoch was 68.5k iterations with sequence length 1024.\n\nHalving the model size should ~halve the iteration time.\nDoubling the batch size should ~double the iteration time.\nHalving the sequence length should ~quarter the iteration time (quadratic scaling).\n\nHence 3.1e-5 days/iteration * 2 * 1/2 * 1/4 = 7.8e-6 days/iteration.\n\n2e6 iterations => seems like 15.6 days training.\n\nOn 512 GPUs they achieve a peak throughput of 15.1 PFLOPS.\nC=15.1 PFLOPS * 58 days = 2.0e22 FLOP.\n\nIf we disregard the Akronomicon estimate and aggregate our two, geometric mean is 2.2e22 FLOP",
      "Training dataset": "",
      "Training dataset size (gradients)": "6960000000",
      "Dataset size notes": "\"The resulting aggregate corpus contains 174 GB of deduplicated text.\"\n174e9 bytes * (1 word / 5 bytes) * (4 tokens / 3 words) = 4.64e10 tokens",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1909.08053",
      "Reference": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism",
      "Citations": "2375.0",
      "Authors": "Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro",
      "Abstract": "Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\"Our BERT model achieves SOTA results on the RACE dataset\"",
      "Epochs": "",
      "Training time (hours)": "374.0",
      "Training time notes": "The 8.3B GPT-like arch took 2.1 days per epoch on 512 GPUs, batch size 512, sequence length 1024. An epoch was 68.5k iterations.\n\nBERT: batch size 1024, sequence length 512, 2e6 iterations total.\n\nHalving the model size should ~halve the iteration time.\nDoubling the batch size should ~double the iteration time.\nHalving the sequence length should ~quarter the iteration time (quadratic scaling).\n\nHence 3.1e-5 days/iteration * 2 * 1/2 * 1/4 = 7.8e-6 days/iteration.\n\n2e6 iterations => seems like 15.6 days training.",
      "Training hardware": "NVIDIA Tesla V100S PCIe 32 GB",
      "Hardware quantity": "512.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "171818.64196707975",
      "Compute cost notes": "",
      "Training power draw (W)": "262640.30207328824",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "524288.0",
      "Batch size notes": "\"we set the batch size to 1024 and use a learning rate of 1.0e4 warmed up over 10,000 iterations and decayed linearly\nover 2 million iterations. Other training parameters are kept\nthe same as (Devlin et al., 2018).\"\n\nin Devlin et al (BERT), sequences are 512 tokens",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Unreleased",
      "Accessibility notes": "training code \n\nhttps://github.com/NVIDIA/Megatron-LM/blob/main/pretrain_bert.py \n\nMIT-like license:\nhttps://github.com/NVIDIA/Megatron-LM/blob/main/LICENSE ",
      "Numerical format": "FP16",
      "Frontier model": "True",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "260989.03287741935",
      "Training compute cost (upfront)": "12082300.278773397"
    },
    {
      "Model": "UDSMProt",
      "Organization": "Fraunhofer Heinrich Hertz Institute",
      "Publication date": "2019-09-04",
      "Domain": "Biology",
      "Task": "Proteins,Protein or nucleotide language model (pLM/nLM),Enzyme function prediction",
      "Parameters": "28303800.0",
      "Parameters notes": "Python code:  \n# Given LSTM parameters\nemb_sz = 400  # embedding size, typically equal to the input size for the first layer\nnh = 1150     # number of hidden units\nnl = 3        # number of layers\n\n# The formula for a single LSTM layer parameters is:\n# P = 4 * ((input_dim + hidden_dim) * hidden_dim + hidden_dim)\n\n# First layer parameters (input_dim is the embedding size)\nfirst_layer_params = 4 * ((emb_sz + nh) * nh + nh)\n\n# For subsequent layers, input_dim is equal to hidden_dim (nh)\nsubsequent_layer_params = 4 * ((nh + nh) * nh + nh)\n\n# Total parameters for all layers\ntotal_params = first_layer_params + (nl - 1) * subsequent_layer_params\n\nprint(total_params)",
      "Training compute (FLOP)": "6.37e+17",
      "Training compute notes": "Pretraining:\nTable 7 gives max of 499k sequences each at (seemingly) L=1024:\n499k * 1024 * 28.3M * 6 = 8.7e16\n\nFinetuning:\nLargest downstream task has 104940 sequences (Table 5), each sequence has L=1024 residues, 28.3M parameters, and 30 epochs.\n105k * 1024 * 30 * 28.3 * 6 = 5.5e17.",
      "Training dataset": "SwissProt,a subset of UniProtKB",
      "Training dataset size (gradients)": "149700000",
      "Dataset size notes": "560K proteins",
      "Confidence": "Likely",
      "Link": "https://www.biorxiv.org/content/10.1101/704874v2.full.pdf",
      "Reference": "UDSMProt: Universal Deep Sequence Models for Protein Classification",
      "Citations": "",
      "Authors": "Nils Strodthoff, Patrick Wagner, Markus Wenzel, and Wojciech Samek",
      "Abstract": "Motivation: Inferring the properties of a protein from its amino acid sequence is one of the key problems in bioinformatics. Most state-of-the-art approaches for protein classification tasks are tailored to single classi- fication tasks and rely on handcrafted features such as position-specific-scoring matrices from expensive database searches. We argue that this level of performance can be reached or even be surpassed by learning a task-agnostic representation once, using self-supervised language modeling, and transferring it to specific tasks by a simple finetuning step.\nResults: We put forward a universal deep sequence model that is pretrained on unlabeled protein se- quences from Swiss-Prot and finetuned on protein classification tasks. We apply it to three prototypical tasks, namely enzyme class prediction, gene ontology prediction and remote homology and fold detection. The proposed method performs on par with state-of-the-art algorithms that were tailored to these specific tasks or, for two out of three tasks, even outperforms them. These results stress the possibility of inferring protein properties from the sequence alone and, on more general grounds, the prospects of modern natural language processing methods in omics.",
      "Organization categorization": "Research collective",
      "Country (of organization)": "Germany",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"The proposed method performs on par with state-of-the-art algorithms that were tailored to these specific tasks or, for two out of three tasks, even outperforms them.\"",
      "Epochs": "30.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "BSD license, models and code\nhttps://github.com/nstrodt/UDSMProt ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Mogrifier (d2, MoS2, MC) + dynamic eval",
      "Organization": "DeepMind,University of Oxford",
      "Publication date": "2019-09-04",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "35000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "WikiText-2",
      "Training dataset size (gradients)": "2000000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1909.01792",
      "Reference": "Mogrifier LSTM",
      "Citations": "109.0",
      "Authors": "G\u00e1bor Melis, Tom\u00e1\u0161 Ko\u010disk\u00fd, Phil Blunsom",
      "Abstract": "Many advances in Natural Language Processing have been based upon more expressive models for how inputs interact with the context in which they occur. Recurrent networks, which have enjoyed a modicum of success, still lack the generalization and systematicity ultimately required for modelling language. In this work, we propose an extension to the venerable Long Short-Term Memory in the form of mutual gating of the current input and the previous output. This mechanism affords the modelling of a richer space of interactions between inputs and their context. Equivalently, our model can be viewed as making the transition function given by the LSTM context-dependent. Experiments demonstrate markedly improved generalization on language modelling in the range of 3-4 perplexity points on Penn Treebank and Wikitext-2, and 0.01-0.05 bpc on four character-based datasets. We establish a new state of the art on all datasets with the exception of Enwik8, where we close a large gap between the LSTM and Transformer models.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We establish a new state of the art on all datasets with the exception of Enwik8\"",
      "Epochs": "145.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "Github has dead link: https://github.com/google-deepmind/lamb/blob/master/experiment/mogrifier/README.md",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "trRosetta",
      "Organization": "Nankai University,University of Washington,Tianjin University,Harvard University",
      "Publication date": "2019-08-22",
      "Domain": "Biology",
      "Task": "Protein folding prediction",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "3.8047968e+19",
      "Training compute notes": "32620000000000 FLOP / GPU / sec [Nvidia Titan RTX, FP16 assumed] * 1 GPU * 1080 hours [see training time notes] * 3600 sec / hour * 0.3 [assumed utilization] = 3.8047968e+19 FLOP",
      "Training dataset": "PDB (Protein Data Bank)",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "\"resulting in a set of 16,047 proteinchains with the average length of 250 amino acids \"\n\n16047*250 = 4011750\n\n\"Each trainingepoch runs through the whole training set, and 100 epochs are performed intotal.\"",
      "Confidence": "Confident",
      "Link": "https://www.pnas.org/doi/10.1073/pnas.1914677117",
      "Reference": "Improved protein structure prediction using predictedinterresidue orientations",
      "Citations": "",
      "Authors": "Jianyi Yanga, Ivan Anishchenko, Hahnbeom Parkb, Zhenling Peng, Sergey Ovchinnikov, David Baker",
      "Abstract": "he prediction of interresidue contacts and distances from coevo-lutionary data using deep learning has considerably advancedprotein structure prediction. Here, we build on these advances bydeveloping a deep residual network for predicting interresidueorientations, in addition to distances, and a Rosetta-constrainedenergy-minimization protocol for rapidly and accurately generat-ing structure models guided by these restraints. In benchmarktests on 13th Community-Wide Experiment on the Critical Assess-ment of Techniques for Protein Structure Prediction (CASP13)-and Continuous Automated Model Evaluation (CAMEO)-derivedsets, the method outperforms all previously described structure-prediction methods. Although trained entirely on native proteins,the network consistently assigns higher probability to de novo-designed proteins, identifying the key fold-determining residuesand providing an independent quantitative measure of the \u201cide-ality\u201d of a protein structure. The method promises to be useful fora broad range of protein structure prediction and design problems.",
      "Organization categorization": "Academia,Academia,Academia,Academia",
      "Country (of organization)": "China,United States of America,China,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"On benchmarktests on 13th Community-Wide Experiment on the Critical Assess-ment of Techniques for Protein Structure Prediction (CASP13)-and Continuous Automated Model Evaluation (CAMEO)-derivedsets, the method outperforms all previously described structure-prediction methods.\"",
      "Epochs": "100.0",
      "Training time (hours)": "1080.0",
      "Training time notes": "\"We train 5 networks with random 95/5%training/validation splits and use the average over the 5 networks as the finalprediction. Training a single network takes \u223c9 d on one NVIDIA Titan RTX GPU\"\n\n9 days * 24 hours * 5 networks = 1080 hours",
      "Training hardware": "NVIDIA TITAN RTX",
      "Hardware quantity": "1.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "315.8562689911153",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "",
      "Accessibility notes": "MIT license",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "EN^2AS with performance reward",
      "Organization": "Beijing Institute of Technology,University of Technology Sydney,Monash University",
      "Publication date": "2019-07-22",
      "Domain": "Language",
      "Task": "Neural Architecture Search - NAS,Language modeling",
      "Parameters": "23000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/1907.09109",
      "Reference": "Efficient Novelty-Driven Neural Architecture Search",
      "Citations": "1.0",
      "Authors": "Miao Zhang, Huiqi Li, Shirui Pan, Taoping Liu, Steven Su",
      "Abstract": "One-Shot Neural architecture search (NAS) attracts broad attention recently due to its capacity to reduce the computational hours through weight sharing. However, extensive experiments on several recent works show that there is no positive correlation between the validation accuracy with inherited weights from the supernet and the test accuracy after re-training for One-Shot NAS. Different from devising a controller to find the best performing architecture with inherited weights, this paper focuses on how to sample architectures to train the supernet to make it more predictive. A single-path supernet is adopted, where only a small part of weights are optimized in each step, to reduce the memory demand greatly. Furthermore, we abandon devising complicated reward based architecture sampling controller, and sample architectures to train supernet based on novelty search. An efficient novelty search method for NAS is devised in this paper, and extensive experiments demonstrate the effectiveness and efficiency of our novelty search based architecture sampling method. The best architecture obtained by our algorithm with the same search space achieves the state-of-the-art test error rate of 2.51\\% on CIFAR-10 with only 7.5 hours search time in a single GPU, and a validation perplexity of 60.02 and a test perplexity of 57.36 on PTB. We also transfer these search cell structures to larger datasets ImageNet and WikiText-2, respectively.",
      "Organization categorization": "Academia,Academia,Academia",
      "Country (of organization)": "China,Australia,Australia",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Table 2",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Pluribus",
      "Organization": "Facebook AI Research",
      "Publication date": "2019-07-11",
      "Domain": "Games",
      "Task": "Poker",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "6.6e+16",
      "Training compute notes": "Trained in 8 days on a 64 core CPU\nhttps://ai.facebook.com/blog/pluribus-first-ai-to-beat-pros-in-6-player-poker/\n\n\"We trained the blueprint strategy for Pluribus in eight days on a 64-core server and required less than 512 GB of RAM. No GPUs were used. At typical cloud computing instance rates, it would cost less than $150 to train.\"\n\nGuess: trained on i7 Intel CPU, approx 5e9 FLOP/s for each core.\n\n https://epoch.ai/blog/estimating-training-compute\n8 days, 64 cores, 5e9 FLOP/s, 30% utilization",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://www.science.org/cms/asset/910714a7-ee2a-486e-9970-42fb893b08d9/pap.pdf",
      "Reference": "Superhuman AI for multiplayer poker",
      "Citations": "797.0",
      "Authors": "Noam Brown, Tuomas Sandholm",
      "Abstract": "In recent years there have been great strides in artificial intelligence (AI), with games often serving as\nchallenge problems, benchmarks, and milestones for progress. Poker has served for decades as such a\nchallenge problem. Past successes in such benchmarks, including poker, have been limited to two-player\ngames. However, poker in particular is traditionally played with more than two players. Multiplayer games\npresent fundamental additional issues beyond those in two-player games, and multiplayer poker is a\nrecognized AI milestone. In this paper we present Pluribus, an AI that we show is stronger than top human\nprofessionals in six-player no-limit Texas hold\u2019em poker, the most popular form of poker played by\nhumans.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,France",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "first to beat humans at multiplayer poker: \"Developing a superhuman AI for multiplayer poker was the widely,recognized main remaining milestone. In this paper we describe Pluribus, an AI capable of defeating elite human professionals in six-player no-limit Texas hold\u2019em poker, the most commonly played poker format in the world.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "BigBiGAN",
      "Organization": "Google",
      "Publication date": "2019-07-04",
      "Domain": "Vision,Image generation",
      "Task": "Image completion",
      "Parameters": "86000000.0",
      "Parameters notes": "https://openai.com/blog/image-gpt/#rfref53",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "ImageNet",
      "Training dataset size (gradients)": "2560000",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/1907.02544",
      "Reference": "Large Scale Adversarial Representation Learning",
      "Citations": "525.0",
      "Authors": "Jeff Donahue, Karen Simonyan",
      "Abstract": "Adversarially trained generative models (GANs) have recently achieved compelling image synthesis results. But despite early successes in using GANs for unsupervised representation learning, they have since been superseded by approaches based on self-supervision. In this work we show that progress in image generation quality translates to substantially improved representation learning performance. Our approach, BigBiGAN, builds upon the state-of-the-art BigGAN model, extending it to representation learning by adding an encoder and modifying the discriminator. We extensively evaluate the representation learning and generation capabilities of these BigBiGAN models, demonstrating that these generation-based models achieve the state of the art in unsupervised representation learning on ImageNet, as well as in unconditional image generation. Pretrained BigBiGAN models -- including image generators and encoders -- are available on TensorFlow Hub (this https URL).",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"BigBiGAN, an unsupervised learning approach based purely on generative models, achieves state-of-the-art results in image representation learning on ImageNet\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "model (Apache 2.0 license): https://www.kaggle.com/models/deepmind/bigbigan\n\nthey share a notebook but with a broken link",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "RoBERTa Large",
      "Organization": "Facebook,University of Washington",
      "Publication date": "2019-07-01",
      "Domain": "Language",
      "Task": "Question answering,Language modeling/generation",
      "Parameters": "355000000.0",
      "Parameters notes": "355M \nhttps://github.com/facebookresearch/fairseq/blob/main/examples/roberta/README.md",
      "Training compute (FLOP)": "8.5067e+21",
      "Training compute notes": "Section 5: We pretrain our model using 1024 V100 GPUs for approximately one day.\n\nNote this is the base pretraining comparable to BERT, 100k steps. Subsequently they do more: \"increasing the number of pretraining steps\nfrom 100K to 300K, and then further to 500K\".\n\nSo assume 5x the 1024 V100 GPUs for 1d estimate. Mixed precision tensor cores get 1.25e14 FLOP/s.\n\n1024 * 1.25e14 * 5 * 24 * 3600 * 0.3 = 1.65888e22\n\n6ND estimate: batches are 8k sequences of 512 tokens; 500k updates means the model saw 500k * 8k * 512 = 2.048T tokens\n6 * 2.048T * 355M = 4.36224e21\n\ngeometric mean: sqrt(1.65888e22 * 4.36224e21) = 8.5067e21\n\nAuthors of \"AI and Memory Wall\" estimated model's training compute as 4,300,000 PFLOP = 4.3*10^21 FLOP\n(https://github.com/amirgholami/ai_and_memory_wall)",
      "Training dataset": "CC-News,BookCorpus (BooksCorpus, Toronto Book Corpus),WebText2,Wikipedia",
      "Training dataset size (gradients)": "42666666666",
      "Dataset size notes": "160GB*200M words/GB * (4 tokens / 3 words) = 3.2e10 tokens\n\nmax steps 500k\nbatch size  8k\n\"We pretrain with sequences of at most T = 512 tokens.\"\n\n500000*8000*512 = 2.048e+12 tokens",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1907.11692",
      "Reference": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "Citations": "27640.0",
      "Authors": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov",
      "Abstract": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\"Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD\"",
      "Epochs": "48.0",
      "Training time (hours)": "120.0",
      "Training time notes": "First the model is pretrained for 100k steps on 1024 GPUs for 1 day, then pretraining is increased to 500k steps, so assuming they used the same number of GPUs, this would have taken 5 days.",
      "Training hardware": "NVIDIA Tesla V100 DGXS 32 GB",
      "Hardware quantity": "1024.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "85350.2350127364",
      "Compute cost notes": "",
      "Training power draw (W)": "526193.8152119832",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "code and weights: https://github.com/facebookresearch/fairseq/blob/main/examples/roberta/README.md\npretrain code: https://github.com/facebookresearch/fairseq/blob/main/examples/roberta/README.pretraining.md \n\nrepo is MIT license\n\n",
      "Numerical format": "FP16",
      "Frontier model": "True",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "166246.0074358047",
      "Training compute cost (upfront)": "24383087.000200927"
    },
    {
      "Model": "Walking Minotaur robot",
      "Organization": "University of California (UC) Berkeley,Google Brain",
      "Publication date": "2019-06-19",
      "Domain": "Robotics",
      "Task": "Animal (human/non-human) imitation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1812.11103",
      "Reference": "Learning to Walk via Deep Reinforcement Learning",
      "Citations": "472.0",
      "Authors": "Tuomas Haarnoja, Sehoon Ha, Aurick Zhou, Jie Tan, George Tucker, Sergey Levine",
      "Abstract": "Deep reinforcement learning (deep RL) holds the promise of automating the acquisition of complex controllers that can map sensory inputs directly to low-level actions. In the domain of robotic locomotion, deep RL could enable learning locomotion skills with minimal engineering and without an explicit model of the robot dynamics. Unfortunately, applying deep RL to real-world robotic tasks is exceptionally difficult, primarily due to poor sample complexity and sensitivity to hyperparameters. While hyperparameters can be easily tuned in simulated domains, tuning may be prohibitively expensive on physical systems, such as legged robots, that can be damaged through extensive trial-and-error learning. In this paper, we propose a sample-efficient deep RL algorithm based on maximum entropy RL that requires minimal per-task tuning and only a modest number of trials to learn neural network policies. We apply this method to learning walking gaits on a real-world Minitaur robot. Our method can acquire a stable gait from scratch directly in the real world in about two hours, without relying on any model or simulation, and the resulting policy is robust to moderate variations in the environment. We further show that our algorithm achieves state-of-the-art performance on simulated benchmarks with a single set of hyperparameters. Videos of training and the learned policy can be found on the project website.",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Fig. 3: (a) \u2013 (d) Standard benchmark training results. Our method (blue) achieves similar or better performance compared to other\nalgorithms. Note that all other algorithms except ours went through dense hyperparameter tuning to achieve the above learning curves.",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "LaNet-L (CIFAR-10)",
      "Organization": "Brown University,Facebook",
      "Publication date": "2019-06-17",
      "Domain": "Vision",
      "Task": "Image classification,Neural Architecture Search - NAS",
      "Parameters": "44100000.0",
      "Parameters notes": "44.1M",
      "Training compute (FLOP)": "",
      "Training compute notes": "LaNet-L was trained on 150 GPU-days, however the GPU was not specified",
      "Training dataset": "CIFAR-10",
      "Training dataset size (gradients)": "60000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1906.06832",
      "Reference": "Sample-Efficient Neural Architecture Search by Learning Action Space",
      "Citations": "48.0",
      "Authors": "Linnan Wang, Saining Xie, Teng Li, Rodrigo Fonseca, Yuandong Tian",
      "Abstract": "Neural Architecture Search (NAS) has emerged as a promising technique for automatic neural network design. However, existing MCTS based NAS approaches often utilize manually designed action space, which is not directly related to the performance metric to be optimized (e.g., accuracy), leading to sample-inefficient explorations of architectures. To improve the sample efficiency, this paper proposes Latent Action Neural Architecture Search (LaNAS), which learns actions to recursively partition the search space into good or bad regions that contain networks with similar performance metrics. During the search phase, as different action sequences lead to regions with different performance, the search efficiency can be significantly improved by biasing towards the good regions. On three NAS tasks, empirical results demonstrate that LaNAS is at least an order more sample efficient than baseline methods including evolutionary algorithms, Bayesian optimizations, and random search. When applied in practice, both one-shot and regular LaNAS consistently outperform existing results. Particularly, LaNAS achieves 99.0% accuracy on CIFAR-10 and 80.8% top1 accuracy at 600 MFLOPS on ImageNet in only 800 samples, significantly outperforming AmoebaNet with 33x fewer samples. Our code is publicly available at this https URL.",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"In practice, LaNAS finds a network that achieves SOTA 99.0% accuracy on CIFAR-10\"",
      "Epochs": "600.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "Open access (non-commercial)",
      "Accessibility notes": "code and weights here, non-commercial license: https://github.com/facebookresearch/LaMCTS/tree/main/LaNAS/LaNet/CIFAR10",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "PG-SWGAN",
      "Organization": "ETH Zurich",
      "Publication date": "2019-06-15",
      "Domain": "Image generation",
      "Task": "Image generation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "CIFAR-10,LSUN,CelebA",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://openaccess.thecvf.com/content_CVPR_2019/html/Wu_Sliced_Wasserstein_Generative_Models_CVPR_2019_paper.html",
      "Reference": "Sliced Wasserstein Generative Models",
      "Citations": "136.0",
      "Authors": "Jiqing Wu, Zhiwu Huang, Dinesh Acharya, Wen Li, Janine Thoma, Danda Pani Paudel, Luc Van Gool",
      "Abstract": "In generative modeling, the Wasserstein distance (WD) has emerged as a useful metric to measure the discrepancy between generated and real data distributions. Unfortunately, it is challenging to approximate the WD of high-dimensional distributions. In contrast, the sliced Wasserstein distance (SWD) factorizes high-dimensional distributions into their multiple one-dimensional marginal distributions and is thus easier to approximate. In this paper, we introduce novel approximations of the primal and dual SWD. Instead of using a large number of random projections, as it is done by conventional SWD approximation methods, we propose to approximate SWDs with a small number of parameterized orthogonal projections in an end-to-end deep learning fashion. As concrete applications of our SWD approximations, we design two types of differentiable SWD blocks to equip modern generative frameworks---Auto-Encoders (AE) and Generative Adversarial Networks (GAN). In the experiments, we not only show the superiority of the proposed generative models on standard image synthesis benchmarks, but also demonstrate the state-of-the-art performance on challenging high resolution image and video generation in an unsupervised manner.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Switzerland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"For fair comparison, we equip the same progressive growing architecture with our proposed SWGAN objective and its dual\nSWD blocks (PG-SWGAN). As shown in Fig. 3 (Right)\nand Fig. 5, our PG-SWGAN can outperform PG-WGAN in\nterms of both qualitative and quantitative comparison on the\nCelebA-HQ and LSUN datasets\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "Open access (non-commercial)",
      "Accessibility notes": "looks like code but no weights, no license specified: https://github.com/musikisomorphie/swd",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "FixRes ResNeXt-101 WSL",
      "Organization": "Facebook AI",
      "Publication date": "2019-06-14",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "829000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "ImageNet",
      "Training dataset size (gradients)": "940000000",
      "Dataset size notes": "\"Conversely, when training a ResNeXt-101 32x48d pre-trained in weakly-supervised fashion on 940 million public images at resolution 224x224 and further optimizing for test resolution 320x320, we obtain a test top-1 accuracy of 86.4% (top-5: 98.0%) (single-crop)\"",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/1906.06423",
      "Reference": "Fixing the train-test resolution discrepancy",
      "Citations": "405.0",
      "Authors": "Hugo Touvron, Andrea Vedaldi, Matthijs Douze, Herv\u00e9 J\u00e9gou",
      "Abstract": "Data-augmentation is key to the training of neural networks for image classification. This paper first shows that existing augmentations induce a significant discrepancy between the typical size of the objects seen by the classifier at train and test time. We experimentally validate that, for a target test resolution, using a lower train resolution offers better classification at test time.\nWe then propose a simple yet effective and efficient strategy to optimize the classifier performance when the train and test resolutions differ. It involves only a computationally cheap fine-tuning of the network at the test resolution. This enables training strong classifiers using small training images. For instance, we obtain 77.1% top-1 accuracy on ImageNet with a ResNet-50 trained on 128x128 images, and 79.8% with one trained on 224x224 image. In addition, if we use extra training data we get 82.5% with the ResNet-50 train with 224x224 images.\nConversely, when training a ResNeXt-101 32x48d pre-trained in weakly-supervised fashion on 940 million public images at resolution 224x224 and further optimizing for test resolution 320x320, we obtain a test top-1 accuracy of 86.4% (top-5: 98.0%) (single-crop). To the best of our knowledge this is the highest ImageNet single-crop, top-1 and top-5 accuracy to date.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"To the best of our knowledge our ResNeXt-101 32x48d surpasses all other models available in the literature\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "https://medium.com/swlh/deepmind-achieved-starcraft-ii-grandmaster-level-but-at-what-cost-32891dd990e4#:~:text=According%20to%20the%20analysis%20by,Source%3A%20DeepMind.",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "Open access (non-commercial)",
      "Accessibility notes": "code/weights with non-commercial license: https://github.com/facebookresearch/FixRes?tab=License-1-ov-file#readme",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Char-CNN-BiLSTM",
      "Organization": "Capital One",
      "Publication date": "2019-06-13",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Penn TreeBank (PTB)",
      "Training dataset size (gradients)": "929000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1906.05678",
      "Reference": "Telephonetic: Making Neural Language Models Robust to ASR and Semantic Noise",
      "Citations": "2.0",
      "Authors": "Chris Larson, Tarek Lahlou, Diana Mingels, Zachary Kulis, Erik Mueller",
      "Abstract": "Speech processing systems rely on robust feature extraction to handle phonetic and semantic variations found in natural language. While techniques exist for desensitizing features to common noise patterns produced by Speech-to-Text (STT) and Text-to-Speech (TTS) systems, the question remains how to best leverage state-of-the-art language models (which capture rich semantic features, but are trained on only written text) on inputs with ASR errors. In this paper, we present Telephonetic, a data augmentation framework that helps robustify language model features to ASR corrupted inputs. To capture phonetic alterations, we employ a character-level language model trained using probabilistic masking. Phonetic augmentations are generated in two stages: a TTS encoder (Tacotron 2, WaveGlow) and a STT decoder (DeepSpeech). Similarly, semantic perturbations are produced by sampling from nearby words in an embedding space, which is computed using the BERT language model. Words are selected for augmentation according to a hierarchical grammar sampling strategy. Telephonetic is evaluated on the Penn Treebank (PTB) corpus, and demonstrates its effectiveness as a bootstrapping technique for transferring neural language models to the speech domain. Notably, our language model achieves a test perplexity of 37.49 on PTB, which to our knowledge is state-of-the-art among models trained only on PTB.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Notably, our language model achieves a test perplexity of 37.49 on PTB, which to our knowledge is state-of-the-art among models trained only on PTB.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "AWD-LSTM + MoS + Partial Shuffled",
      "Organization": "University of Texas at Austin",
      "Publication date": "2019-06-10",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "35000000.0",
      "Parameters notes": "35M (Table 2)",
      "Training compute (FLOP)": "3.15e+17",
      "Training compute notes": "6 FLOP / parameter / token * 35000000 parameters * 2000000 tokens * 750 epochs = 3.15e+17 FLOP",
      "Training dataset": "WikiText-2",
      "Training dataset size (gradients)": "2000000",
      "Dataset size notes": "750 epochs (figure 1c)",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1906.03805",
      "Reference": "Improving Neural Language Modeling via Adversarial Training",
      "Citations": "122.0",
      "Authors": "Dilin Wang, Chengyue Gong, Qiang Liu",
      "Abstract": "Recently, substantial progress has been made in language modeling by using deep neural networks. However, in practice, large scale neural language models have been shown to be prone to overfitting. In this paper, we present a simple yet highly effective adversarial training mechanism for regularizing neural language models. The idea is to introduce adversarial noise to the output embedding layer while training the models. We show that the optimal adversarial noise yields a simple closed-form solution, thus allowing us to develop a simple and time efficient algorithm. Theoretically, we show that our adversarial mechanism effectively encourages the diversity of the embedding vectors, helping to increase the robustness of models. Empirically, we show that our method improves on the single model state-of-the-art results for language modeling on Penn Treebank (PTB) and Wikitext-2, achieving test perplexity scores of 46.01 and 38.07, respectively. When applied to machine translation, our method improves over various transformer-based translation baselines in BLEU scores on the WMT14 English-German and IWSLT14 German-English tasks.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"our method improves on the single model state-of-the-art results for language modeling on Penn Treebank (PTB) and Wikitext-2, achieving test perplexity scores of 46.01 and 38.07, respectively\"",
      "Epochs": "750.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "Open access (non-commercial)",
      "Accessibility notes": "code and weights. no license provided:\nhttps://github.com/ChengyueGongR/advsoft",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Transformer-XL Large + Phrase Induction",
      "Organization": "Massachusetts Institute of Technology (MIT),University of Illinois Urbana-Champaign (UIUC)",
      "Publication date": "2019-06-04",
      "Domain": "Language",
      "Task": "Language modeling/generation",
      "Parameters": "256999999.99999997",
      "Parameters notes": "",
      "Training compute (FLOP)": "3.7848651e+20",
      "Training compute notes": "Fine-tuned from pre-trained Transformer-XL Large (upd 3.7832771e+20 FLOP, old estimation 1.09e19 FLOP). \n\nTotal: 3.7832771e20 + 1.588e17 = 3.7848651e+20 FLOP (Speculative confidence same as Transformer XL)",
      "Training dataset": "WikiText-103",
      "Training dataset size (gradients)": "103000000",
      "Dataset size notes": "",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/1906.01702",
      "Reference": "Improving Neural Language Models by Segmenting, Attending, and Predicting the Future",
      "Citations": "14.0",
      "Authors": "Hongyin Luo, Lan Jiang, Yonatan Belinkov, James Glass",
      "Abstract": "Common language models typically predict the next word given the context. In this work, we propose a method that improves language modeling by learning to align the given context and the following phrase. The model does not require any linguistic annotation of phrase segmentation. Instead, we define syntactic heights and phrase segmentation rules, enabling the model to automatically induce phrases, recognize their task-specific heads, and generate phrase embeddings in an unsupervised learning manner. Our method can easily be applied to language models with different network architectures since an independent module is used for phrase induction and context-phrase alignment, and no change is required in the underlying language modeling network. Experiments have shown that our model outperformed several strong baseline models on different data sets. We achieved a new state-of-the-art performance of 17.4 perplexity on the Wikitext-103 dataset. Additionally, visualizing the outputs of the phrase induction module showed that our model is able to learn approximate phrase-level structural knowledge without any annotation.",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We achieved a new state-of-the-art performance of 17.4 perplexity on the Wikitext-103 dataset\"",
      "Epochs": "1.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "Transformer-XL (257M)",
      "Finetune compute (FLOP)": "1.588e+17",
      "Finetune compute notes": "Additional 1.6e17 FLOP of fine-tuning from one epoch on WikiText-103:\n6 * 257M * 103M = 1.588e17 FLOP.",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "code license, BSD-3: https://github.com/luohongyin/PILM?tab=BSD-3-Clause-1-ov-file#readme\n\ntraining: https://github.com/luohongyin/PILM/blob/master/train_span_wt103.sh ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "XLNet",
      "Organization": "Carnegie Mellon University (CMU),Google Brain",
      "Publication date": "2019-06-01",
      "Domain": "Language",
      "Task": "Language modeling/generation,Question answering,Sentiment classification",
      "Parameters": "340000000.0",
      "Parameters notes": "Same size as BERT-Large, which was 340M",
      "Training compute (FLOP)": "6.19e+21",
      "Training compute notes": "\"Specifically, we train on 512 TPU v3 chips for 500K steps with an Adam weight decay optimizer, linear learning rate decay, and a batch size of 8192, which takes about 5.5 days.\"\n\n123 teraflops * 5.5 days * 24 * 3600 * 512 * 0.3 utilization (assumption) ~= 8977858560*10^12=8.9*10^21\n\nAlternatively, 500k steps * batch size 8192 * sequence length 512 = 2.1T training passes. 340 million * 6 * 2 trillion = 4.3e21 FLOP. \n\nGeometric mean: sqrt(8.9e21 * 4.3e21) = 6.19e21",
      "Training dataset": "Wikipedia,BookCorpus (BooksCorpus, Toronto Book Corpus)",
      "Training dataset size (gradients)": "32890000000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1906.08237",
      "Reference": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
      "Citations": "9044.0",
      "Authors": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le",
      "Abstract": "With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "63.7626026148",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "13644.959135438661",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "8192.0",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache 2.0 for code and weights: https://github.com/zihangdai/xlnet",
      "Numerical format": "FP32",
      "Frontier model": "True",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "XLM",
      "Organization": "Facebook",
      "Publication date": "2019-06-01",
      "Domain": "Language",
      "Task": "Translation,Language modeling",
      "Parameters": "665000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/1901.07291",
      "Reference": "Cross-lingual Language Model Pretraining",
      "Citations": "2588.0",
      "Authors": "Guillaume Lample, Alexis Conneau",
      "Abstract": "Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT'16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT'16 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models will be made publicly available.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\"On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT\u201916 Romanian-English, outperforming the previous best approach by more than 4 BLEU\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "weights/code non-commercial: https://github.com/facebookresearch/XLM?tab=License-1-ov-file#readme",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DLRM-2020",
      "Organization": "Facebook AI",
      "Publication date": "2019-05-31",
      "Domain": "Recommendation",
      "Task": "Recommender system",
      "Parameters": "100000000000.0",
      "Parameters notes": "Figure 1\n\nhttps://arxiv.org/abs/2104.05158",
      "Training compute (FLOP)": "4e+18",
      "Training compute notes": "Figure 1\n\nhttps://arxiv.org/abs/2104.05158",
      "Training dataset": "",
      "Training dataset size (gradients)": "38571428",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1906.00091",
      "Reference": "Deep Learning Recommendation Model for Personalization and Recommendation Systems",
      "Citations": "832.0",
      "Authors": "Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu, Alisson G. Azzolini, Dmytro Dzhulgakov, Andrey Mallevich, Ilia Cherniavskii, Yinghai Lu, Raghuraman Krishnamoorthi, Ansha Yu, Volodymyr Kondratenko, Stephanie Pereira, Xianjie Chen, Wenlin Chen, Vijay Rao, Bill Jia, Liang Xiong, Misha Smelyanskiy",
      "Abstract": "With the advent of deep learning, neural network-based recommendation models have emerged as an important tool for tackling personalization and recommendation tasks. These networks differ significantly from other deep learning networks due to their need to handle categorical features and are not well studied or understood. In this paper, we develop a state-of-the-art deep learning recommendation model (DLRM) and provide its implementation in both PyTorch and Caffe2 frameworks. In addition, we design a specialized parallelization scheme utilizing model parallelism on the embedding tables to mitigate memory constraints while exploiting data parallelism to scale-out compute from the fully-connected layers. We compare DLRM against existing recommendation models and characterize its performance on the Big Basin AI platform, demonstrating its usefulness as a benchmark for future algorithmic experimentation and system co-design.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"In this paper, we develop a state-of-the-art deep learning recommendation model (DLRM)\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT, training/inference code: https://github.com/facebookresearch/dlrm",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "EfficientNet-L2",
      "Organization": "Google",
      "Publication date": "2019-05-28",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "480000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "ImageNet",
      "Training dataset size (gradients)": "1280000",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/1905.11946",
      "Reference": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
      "Citations": "15327.0",
      "Authors": "Mingxing Tan, Quoc V. Le",
      "Abstract": "Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.\nTo go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at this https URL.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache license: https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "CPC v2",
      "Organization": "DeepMind,University of California (UC) Berkeley",
      "Publication date": "2019-05-22",
      "Domain": "Vision",
      "Task": "Image completion,Object detection,Image classification",
      "Parameters": "303000000.0",
      "Parameters notes": "source: https://openai.com/blog/image-gpt/#rfref25d",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "ImageNet",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/1905.09272",
      "Reference": "Data-Efficient Image Recognition with Contrastive Predictive Coding",
      "Citations": "491.0",
      "Authors": "Olivier J. H\u00e9naff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, S. M. Ali Eslami, Aaron van den Oord",
      "Abstract": "Human observers can learn to recognize new categories of images from a handful of examples, yet doing so with artificial ones remains an open challenge. We hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable. We therefore revisit and improve Contrastive Predictive Coding, an unsupervised objective for learning such representations. This new implementation produces features which support state-of-the-art linear classification accuracy on the ImageNet dataset. When used as input for non-linear classification with deep neural networks, this representation allows us to use 2-5x less labels than classifiers trained directly on image pixels. Finally, this unsupervised representation substantially improves transfer learning to object detection on the PASCAL VOC dataset, surpassing fully supervised pre-trained ImageNet classifiers.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"this unsupervised representation substantially improves transfer learning to object detection on the PASCAL VOC dataset, surpassing fully supervised pre-trained ImageNet classifiers\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "AWD-LSTM-DRILL + dynamic evaluation\u2020 (WT2)",
      "Organization": "IDIAP",
      "Publication date": "2019-05-14",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "34000000.0",
      "Parameters notes": "34M, Table 2",
      "Training compute (FLOP)": "4.08e+17",
      "Training compute notes": "6 FLOP / parameter / token * 34000000 parameters * 2000000 tokens * 1000 epochs = 4.08e+17 FLOP",
      "Training dataset": "WikiText-2",
      "Training dataset size (gradients)": "2000000",
      "Dataset size notes": "max epochs - 1000 (from http://github.com/idiap/drill)",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/1905.05513",
      "Reference": "Deep Residual Output Layers for Neural Language Generation",
      "Citations": "7.0",
      "Authors": "Nikolaos Pappas, James Henderson",
      "Abstract": "Many tasks, including language generation, benefit from learning the structure of the output space, particularly when the space of output labels is large and the data is sparse. State-of-the-art neural language models indirectly capture the output space structure in their classifier weights since they lack parameter sharing across output labels. Learning shared output label mappings helps, but existing methods have limited expressivity and are prone to overfitting. In this paper, we investigate the usefulness of more powerful shared mappings for output labels, and propose a deep residual output mapping with dropout between layers to better capture the structure of the output space and avoid overfitting. Evaluations on three language generation tasks show that our output label mapping can match or improve state-of-the-art recurrent and self-attention architectures, and suggest that the classifier does not necessarily need to be high-rank to better model natural language if it is better at capturing the structure of the output space.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Switzerland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"our models improve over the state-of-the-art by +1.6 perplexity on PennTreebank and by +3.9 perplexity on\nWikitext-2\"",
      "Epochs": "1000.0",
      "Training time (hours)": "29.0",
      "Training time notes": "106 sec per epoch (Table 3) -> 106000 seconds = 29 hours",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "copyleft license (restricts derivative works to be open)\nhttps://github.com/idiap/drill?tab=GPL-3.0-1-ov-file#readme\n\ntrain/eval script: https://github.com/idiap/drill/blob/master/main.py ",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ResNeXt-101 Billion-scale",
      "Organization": "Facebook AI",
      "Publication date": "2019-05-02",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "193000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "YFCC-100M",
      "Training dataset size (gradients)": "90000000",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/1905.00546",
      "Reference": "Billion-scale semi-supervised learning for image classification",
      "Citations": "436.0",
      "Authors": "I. Zeki Yalniz, Herv\u00e9 J\u00e9gou, Kan Chen, Manohar Paluri, Dhruv Mahajan",
      "Abstract": "This paper presents a study of semi-supervised learning with large convolutional networks. We propose a pipeline, based on a teacher/student paradigm, that leverages a large collection of unlabelled images (up to 1 billion). Our main goal is to improve the performance for a given target architecture, like ResNet-50 or ResNext. We provide an extensive analysis of the success factors of our approach, which leads us to formulate some recommendations to produce high-accuracy models for image classification with semi-supervised learning. As a result, our approach brings important gains to standard architectures for image, video and fine-grained classification. For instance, by leveraging one billion unlabelled images, our learned vanilla ResNet-50 achieves 81.2% top-1 accuracy on the ImageNet benchmark.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We demonstrate the performance of our method on popular classification benchmarks for both images and videos and significantly outperforms the state of the art.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "non-commercial for weights: \nhttps://github.com/facebookresearch/semi-supervised-ImageNet1K-models",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "RaptorX-Contact",
      "Organization": "Toyota Technological Institute at Chicago",
      "Publication date": "2019-05-02",
      "Domain": "Biology",
      "Task": "Protein folding prediction,Proteins,Protein contact and distance prediction",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "PDB25 and UniProt,PDB (Protein Data Bank),UniProtKB",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "Calculation steps:\n1. Training proteins = 11,410 - 900 = 10,510 proteins\n2. Residue pairs per protein = (300 \u00d7 299)/2 = 44,850 pairs\n3. Total data points = 10,510 \u00d7 44,850 = 4.73 \u00d7 10\u2078\nFinal estimate \u2248 4.5 \u00d7 10\u2078 data points",
      "Confidence": "Unknown",
      "Link": "https://www.biorxiv.org/content/biorxiv/early/2019/05/02/624460.full.pdf",
      "Reference": "Analysis of distance-based protein structure prediction by deep learning in CASP13",
      "Citations": "",
      "Authors": "Jinbo Xu, Sheng Wang",
      "Abstract": "This paper reports the CASP13 results of distance-based contact prediction, threading and folding methods implemented in three RaptorX servers, which are built upon the powerful deep convolutional residual neural network (ResNet) method initiated by us for contact prediction in CASP12. On the 32 CASP13 FM (free-modeling) targets with a median MSA (multiple sequence alignment) depth of 36, RaptorX yielded the best contact prediction among 46 groups and almost the best 3D structure modeling among all server groups without time-consuming conformation sampling. In particular, RaptorX achieved top L/5, L/2 and L long-range contact precision of 70%, 58% and 45%, respectively, and predicted correct folds (TMscore>0.5) for 18 of 32 targets. Although on average underperforming AlphaFold in 3D modeling, RaptorX predicted correct folds for all FM targets with >300 residues (T0950-D1, T0969-D1 and T1000-D2) and generated the best 3D models for T0950-D1 and T0969-D1 among all groups. This CASP13 test confirms our previous findings: (1) predicted distance is more useful than contacts for both template-based and free modeling; and (2) structure modeling may be improved by integrating alignment and co- evolutionary information via deep learning. This paper will discuss progress we have made since CASP12, the strength and weakness of our methods, and why deep learning performed much better in CASP13.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"On the 32 CASP13 FM (free-modeling) targets with a median MSA (multiple sequence alignment) depth of 36, RaptorX yielded the best contact prediction among 46 groups and almost the best 3D structure modeling among all server groups without time-consuming conformation sampling.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open (restricted use)",
      "Inference code accessibility": "",
      "Accessibility notes": "this license for code:\nhttps://github.com/j3xugit/RaptorX-Contact?tab=GPL-3.0-1-ov-file",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Neuro-Symbolic Concept Learner",
      "Organization": "Massachusetts Institute of Technology (MIT),Tsinghua University,MIT-IBM Watson AI Lab,DeepMind",
      "Publication date": "2019-04-26",
      "Domain": "Vision,Language",
      "Task": "Visual question answering,Semantic segmentation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "CLEVR,VQS,ImageNet",
      "Training dataset size (gradients)": "100000",
      "Dataset size notes": "CLEVR, ImageNet, VQS\n5000 in CLEVR\n64509 in VQS\nand whole ImageNet for pretraining\n\"We train NS-CL on 5K images (<10% of CLEVR\u2019s 70K training images). We generate 20 questions for each image for the entire curriculum learning process\"\n\nsection 4.3 \"All models use a pre-trained semantic parser on the full CLEVR dataset\"\n\n\"The only extra supervision of the visual perception module comes from the pre-training of the perception modules on ImageNet (Deng et al., 2009). To quantify the influence of this pre-training\"\n\nIn appendix G.2 (VQS Dataset):\n\"All models are trained on the first 63,509 images of the training set, and tested on the test split. For hyper-parameter tuning and model selection, the rest 5,000 images from the training set are used for validation.",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1904.12584",
      "Reference": "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision",
      "Citations": "772.0",
      "Authors": "Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B. Tenenbaum, Jiajun Wu",
      "Abstract": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.",
      "Organization categorization": "Academia,Academia,Academia,Industry,Industry",
      "Country (of organization)": "United States of America,China,United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"NS-CL\u2019s modularized design enables interpretable, robust, and accurate visual reasoning: it achieves state-of-the-art performance on the CLEVR datase\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT code: https://github.com/vacancy/NSCL-PyTorch-Release",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "MuseNet",
      "Organization": "OpenAI",
      "Publication date": "2019-04-25",
      "Domain": "Audio",
      "Task": "Audio generation",
      "Parameters": "2038431744.0",
      "Parameters notes": "From article:\n72 Layers\n24 Attention heads\n\nAssumptions: \nD_head = 64 (GPT-2)\nD_model = 24*64=1536 (based on D_head)\nD_mlp = 4*1536=6144\n\nAttention: H*(W*(2*D+N)+N*M)\n24*(1536*(2*64+64)+64*1536)=9437184\nMLP: 1536*6144+6144*1536=18874368\nTotal parameters: 72*(9437184+18874368)=2038431744\n",
      "Training compute (FLOP)": "2.208301056e+20",
      "Training compute notes": "From article:\n72 Layers\n24 Attention heads\n4096 ctx length\n\nAssumptions: \nD_head = 64 (GPT-2)\nD_model = 24*64=1536 (based on D_head)\nD_mlp = 4*1536=6144\nAverage sequence length: 5000\nEpochs: 5\nTraining data: 500000\n\nForward FLOP:\nAttention: 2*H*(W*(2*D+N)+L*(D+N)+N*M)\n2*24*(1536*(2*64+64)+4096*(64+64)+64*1536)=44040192\nMLP: 2*(1536*6144+6144*1536)=37748736\nTotal forward flop: 72*(44040192+37748736)=5888802816\nTraining compute\n5888802816*3*500000*5000*5=220830105600000000000=2.2e20\n",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "\u201chundreds of thousands of MIDI files\u201d",
      "Confidence": "Likely",
      "Link": "https://openai.com/index/musenet/",
      "Reference": "MuseNet",
      "Citations": "",
      "Authors": "OpenAI",
      "Abstract": "We\u2019ve created MuseNet, a deep neural network that can generate 4-minute musical compositions with 10 different instruments, and can combine styles from country to Mozart to the Beatles. MuseNet was not explicitly programmed with our understanding of music, but instead discovered patterns of harmony, rhythm, and style by learning to predict the next token in hundreds of thousands of MIDI files. MuseNet uses the same general-purpose unsupervised technology as GPT-2\u2060, a large-scale transformer\u2060(opens in a new window) model trained to predict the next token in a sequence, whether audio or text.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Significant use",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "BERT-Large-CAS (PTB+WT2+WT103)",
      "Organization": "Amazon",
      "Publication date": "2019-04-20",
      "Domain": "Language",
      "Task": "Neural Architecture Search - NAS,Language modeling/generation",
      "Parameters": "395000000.0",
      "Parameters notes": "395M (Table 6)",
      "Training compute (FLOP)": "1.5405e+20",
      "Training compute notes": "6 FLOP / token / parameter * 395000000 parameters * 1300000000 parameters * 50 epochs = 1.5405e+20 FLOP\n\n________\nin the Algorithmic progress paper, the estimation was 5.21E+20 FLOP",
      "Training dataset": "Penn TreeBank (PTB),WikiText-2,WikiText-103",
      "Training dataset size (gradients)": "1300000000",
      "Dataset size notes": "Table 7:\n0.1B (PTB)+ 0.2B (WT-2) + 1.0B (WT-103) = 1.3B\n\nWe pick 128 as sequence length and 16 as minibatch size\n\n\"We use NT-ASGD (Merity et al., 2017) to train 50 epochs on training datasets\"",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/1904.09408",
      "Reference": "Language Models with Transformers",
      "Citations": "130.0",
      "Authors": "Chenguang Wang, Mu Li, Alexander J. Smola",
      "Abstract": "The Transformer architecture is superior to RNN-based models in computational efficiency. Recently, GPT and BERT demonstrate the efficacy of Transformer models on various NLP tasks using pre-trained language models on large-scale corpora. Surprisingly, these Transformer architectures are suboptimal for language model itself. Neither self-attention nor the positional encoding in the Transformer is able to efficiently incorporate the word-level sequential context crucial to language modeling.\nIn this paper, we explore effective Transformer architectures for language model, including adding additional LSTM layers to better capture the sequential context while still keeping the computation efficient. We propose Coordinate Architecture Search (CAS) to find an effective architecture through iterative refinement of the model. Experimental results on the PTB, WikiText-2, and WikiText-103 show that CAS achieves perplexities between 20.42 and 34.11 on all problems, i.e. on average an improvement of 12.0 perplexity units compared to state-of-the-art LSTMs. The source code is publicly available.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"CAS achieves perplexities between 20.42 and 34.11 on all problems, i.e. on average an improvement of 12.0 perplexity units compared to state-of-the-art LSTMs\"",
      "Epochs": "50.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache 2.0 license: https://github.com/cgraywang/gluon-nlp-1/blob/lmtransformer/scripts/language_model/train/transformer_lm.py",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Transformer-XL + RMS dynamic eval",
      "Organization": "University of Edinburgh",
      "Publication date": "2019-04-17",
      "Domain": "Language",
      "Task": "Language modeling/generation",
      "Parameters": "256999999.99999997",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "WikiText-103",
      "Training dataset size (gradients)": "103000000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1904.08378",
      "Reference": "Dynamic Evaluation of Transformer Language Models",
      "Citations": "45.0",
      "Authors": "Ben Krause, Emmanuel Kahembwe, Iain Murray, Steve Renals",
      "Abstract": "This research note combines two methods that have recently improved the state of the art in language modeling: Transformers and dynamic evaluation. Transformers use stacked layers of self-attention that allow them to capture long range dependencies in sequential data. Dynamic evaluation fits models to the recent sequence history, allowing them to assign higher probabilities to re-occurring sequential patterns. By applying dynamic evaluation to Transformer-XL models, we improve the state of the art on enwik8 from 0.99 to 0.94 bits/char, text8 from 1.08 to 1.04 bits/char, and WikiText-103 from 18.3 to 16.4 perplexity points.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"By applying dynamic evaluation to Transformer-XL models, we improve the state of the art on enwik8 from 0.99 to 0.94 bits/char, text8 from 1.08 to 1.04 bits/char, and WikiText-103 from 18.3 to 16.4 perplexity points.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache for code: https://github.com/benkrause/dynamiceval-transformer\nwt103 train script: https://github.com/benkrause/dynamiceval-transformer/blob/master/tf/sota/wt103.sh ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "WeNet (Penn Treebank)",
      "Organization": "Amazon",
      "Publication date": "2019-04-08",
      "Domain": "Language",
      "Task": "Neural Architecture Search - NAS,Language modeling",
      "Parameters": "23000000.0",
      "Parameters notes": "Table 1",
      "Training compute (FLOP)": "7.30000001e+17",
      "Training compute notes": "PTB has 912344 tokens. The model has 23M parameters and was trained for 6k epochs. If the model was dense, 6 FLOP/token/param/epoch * 6k epochs * 23M params * 912k tokens = 1.05e18 FLOP.\n\nAlternatively, the model was trained on 1 V100 GPU and \"In terms of efficiency, the overall cost... is within 1 GPU day\" so the training time was around or below 24 hours. Half precision and 30% utilization would be a pretty good match for the arithmetic estimate: 24 hours * 30% * 28 TFLOPS = 7.3e17 FLOP.",
      "Training dataset": "Penn TreeBank (PTB)",
      "Training dataset size (gradients)": "929000",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/1904.03819",
      "Reference": "WeNet: Weighted Networks for Recurrent Network Architecture Search",
      "Citations": "5.0",
      "Authors": "Zhiheng Huang, Bing Xiang",
      "Abstract": "In recent years, there has been increasing demand for automatic architecture search in deep learning. Numerous approaches have been proposed and led to state-of-the-art results in various applications, including image classification and language modeling. In this paper, we propose a novel way of architecture search by means of weighted networks (WeNet), which consist of a number of networks, with each assigned a weight. These weights are updated with back-propagation to reflect the importance of different networks. Such weighted networks bear similarity to mixture of experts. We conduct experiments on Penn Treebank and WikiText-2. We show that the proposed WeNet can find recurrent architectures which result in state-of-the-art performance.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We show that an architecture found by WeNets arXiv:1904.03819v1 [cs.NE] 8 Apr 2019 WeNet: Weighted Networks for Recurrent Network Architecture Search achieves state-of-the-art results on the Penn Treebank language dataset\"",
      "Epochs": "6000.0",
      "Training time (hours)": "24.0",
      "Training time notes": "",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "1.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "339.4439274233134",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "70000.0",
      "Batch size notes": "They use BPTT with length 35. During architecture search data batch size is 20 and network batch size is 100. While training the architecture they end up finding, batch size is 64. So effective batch size is 35 * 20 * 100 = 70,000 during architecture search, and 35 * 64 = 2,240 during final training. ",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "PTB dataset",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "True-Regularization+Finetune+Dynamic-Eval",
      "Organization": "Mobvoi,Williams College",
      "Publication date": "2019-04-08",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "7000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Penn TreeBank (PTB)",
      "Training dataset size (gradients)": "929000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1904.04163",
      "Reference": "Knowledge Distillation For Recurrent Neural Network Language Modeling With Trust Regularization",
      "Citations": "25.0",
      "Authors": "Yangyang Shi, Mei-Yuh Hwang, Xin Lei, Haoyu Sheng",
      "Abstract": "Recurrent Neural Networks (RNNs) have dominated language modeling because of their superior performance over traditional N-gram based models. In many applications, a large Recurrent Neural Network language model (RNNLM) or an ensemble of several RNNLMs is used. These models have large memory footprints and require heavy computation. In this paper, we examine the effect of applying knowledge distillation in reducing the model size for RNNLMs. In addition, we propose a trust regularization method to improve the knowledge distillation training for RNNLMs. Using knowledge distillation with trust regularization, we reduce the parameter size to a third of that of the previously published best model while maintaining the state-of-the-art perplexity result on Penn Treebank data. In a speech recognition N-bestrescoring task, we reduce the RNNLM model size to 18.5% of the baseline system, with no degradation in word error rate(WER) performance on Wall Street Journal data set.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "China,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"In the first experiment, the student model achieves state-of-the-art perplexity results on the Penn Treebank dataset [1] with a model size one third of that of the\npreviously published best model\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Cross-lingual alignment",
      "Organization": "Tel Aviv University,Massachusetts Institute of Technology (MIT)",
      "Publication date": "2019-04-04",
      "Domain": "Language",
      "Task": "Translation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "2.56e+18",
      "Training compute notes": "From author communication:\n\nPrecision: float32\n\nHardware: 4 GPU  NVIDIA 1080Ti\n\nNVIDIA 1080Ti: 1.06E+13\n\nCompute: 7 GPU-days\n\n0.4 * 1.06E+13 FLOP/s * 7 days * 24h/day * 3600s/h\n= 2.56E+18",
      "Training dataset": "Wikipedia,CoNLL2017",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/1902.09492",
      "Reference": "Cross-lingual alignment of contextual word embeddings, with applications to zero- shot dependency parsing.",
      "Citations": "218.0",
      "Authors": "Tal Schuster, Ori Ram, Regina Barzilay, and Amir Globerson.",
      "Abstract": "We introduce a novel method for multilingual transfer that utilizes deep contextual embeddings, pretrained in an unsupervised fashion. While contextual embeddings have been shown to yield richer representations of meaning compared to their static counterparts, aligning them poses a challenge due to their dynamic nature. To this end, we construct context-independent variants of the original monolingual spaces and utilize their mapping to derive an alignment for the context-dependent spaces. This mapping readily supports processing of a target language, improving transfer by context-aware embeddings. Our experimental results demonstrate the effectiveness of this approach for zero-shot and few-shot learning of dependency parsing. Specifically, our method consistently outperforms the previous state-of-the-art on 6 tested languages, yielding an improvement of 6.8 LAS points on average.",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "Israel,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"our method consistently outperforms the previous state-of-the-art on 6 tested languages\"\n\n\"Table 3 summarizes the results for our zero-shot, multi-source experiments on six languages from Google universal dependency treebank version 2.0.\"\n\n\"Table 5 summarizes the results, showing that our algorithm outperforms the best model from the shared task by 5.05 LAS points and improves by over 10 points over a FASTTEXT baseline\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA GeForce GTX 1080 Ti",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "ELMo",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT license\nhttps://github.com/TalSchuster/CrossLingualContextualEmb",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "SciBERT",
      "Organization": "Allen Institute for AI",
      "Publication date": "2019-03-26",
      "Domain": "Language",
      "Task": "Relation extraction,Sentiment classification,Text classification,Named entity recognition (NER)",
      "Parameters": "110000000.0",
      "Parameters notes": "110M\nsize of bert base from https://huggingface.co/google-bert/bert-base-uncased\nrelevant citation: \n\"We use the original BERT code to\ntrain SCIBERT on our corpus with the same con-\nfiguration and size as BERT-Base. We train 4\ndifferent versions of SCIBERT: (i) cased or un-\ncased and (ii) BASEVOCAB or SCIVOCAB. The\ntwo models that use BASEVOCAB are finetuned\nfrom the corresponding BERT-Base models. The\nother two models that use the new SCIVOCAB are\ntrained from scratch.\"",
      "Training compute (FLOP)": "8.926848e+19",
      "Training compute notes": "4*123e12*0.3*(7*24*3600) = 8.926848e+19\n(num gpu) * (peak compute) * (assumed utilization rate) * (time in seconds)\nWe have:\n 4 TPUv3 chips.123teraFLOPS per chip.\n7 days of training\n\"We use a single TPU v3 with 8 cores. Training the SCIVOCAB models from scratch on our corpus takes 1 week (5 days with max length 128, then 2 days with max length 512). \"\n\nIf this compute estimate is accurate and BERT is approximately dense, then C=6eND -> e=C/6ND ~= 40 epochs.",
      "Training dataset": "",
      "Training dataset size (gradients)": "3170000000",
      "Dataset size notes": "\"The average paper length is 154 sentences (2,769 tokens) resulting in a corpus size of 3.17B tokens, similar to the 3.3B tokens on which BERT was trained.\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1903.10676",
      "Reference": "SciBERT: A Pretrained Language Model for Scientific Text",
      "Citations": "3432.0",
      "Authors": "Iz Beltagy, Kyle Lo, Arman Cohan",
      "Abstract": "Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at this https://github.com/allenai/scibert/",
      "Organization categorization": "Research collective",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\"We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks\"",
      "Epochs": "",
      "Training time (hours)": "168.0",
      "Training time notes": "1 week",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "4.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "247.26289010271603",
      "Compute cost notes": "",
      "Training power draw (W)": "3707.8009472465747",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "apache 2.0, code and weights: https://github.com/allenai/scibert/",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "NMT Transformer 437M",
      "Organization": "Google,Bar-Ilan University",
      "Publication date": "2019-02-28",
      "Domain": "Language",
      "Task": "Translation",
      "Parameters": "437700000.0",
      "Parameters notes": "\"Regarding the model, for these experiments we\nuse a larger Transformer model with 6 layers in\nboth the encoder and the decoder, model dimension set to 1024, hidden dimension size of 8192,\nand 16 attention heads. This results in a model\nwith approximately 473.7M parameters.\"",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "96M total examples, per Table 4. One sentence per example?",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1903.00089",
      "Reference": "Massively Multilingual Neural Machine Translation",
      "Citations": "520.0",
      "Authors": "Roee Aharoni, Melvin Johnson, Orhan Firat",
      "Abstract": "Multilingual neural machine translation (NMT) enables training a single model that supports translation from multiple source languages into multiple target languages. In this paper, we push the limits of multilingual NMT in terms of number of languages being used. We perform extensive experiments in training massively multilingual NMT models, translating up to 102 languages to and from English within a single model. We explore different setups for training such models and analyze the trade-offs between translation quality and various modeling decisions. We report results on the publicly available TED talks multilingual corpus where we show that massively multilingual many-to-many models are effective in low resource settings, outperforming the previous state-of-the-art while supporting up to 59 languages. Our experiments on a large-scale dataset with 102 languages to and from English and up to one million examples per direction also show promising results, surpassing strong bilingual baselines and encouraging future work on massively multilingual NMT.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,Israel",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We report results on the publicly available TED talks multilingual corpus where we show that massively multilingual many-to-many models are effective in low resource settings, outperforming the previous state-of-the-art while supporting up to 59 languages.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "KataGo",
      "Organization": "Jane Street",
      "Publication date": "2019-02-27",
      "Domain": "Games",
      "Task": "Go",
      "Parameters": "2500000.0",
      "Parameters notes": "https://arxiv.org/abs/2210.00849 gives parameter count for AlphaZero in Fig 1b.",
      "Training compute (FLOP)": "2.32e+19",
      "Training compute notes": "\"[KataGo] surpasses the strength of ELF OpenGo after training on about 27 V100 GPUs for 19 days\"\n14.13 teraFLOP/s * 19 days = 2.32e+19 FLOP",
      "Training dataset": "",
      "Training dataset size (gradients)": "241000000",
      "Dataset size notes": "241 million training samples across 4.2 million games",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/1902.10565",
      "Reference": "Accelerating Self-Play Learning in Go",
      "Citations": "104.0",
      "Authors": "David J. Wu",
      "Abstract": "By introducing several improvements to the AlphaZero process and architecture, we greatly accelerate self-play learning in Go, achieving a 50x reduction in computation over comparable methods. Like AlphaZero and replications such as ELF OpenGo and Leela Zero, our bot KataGo only learns from neural-net-guided Monte Carlo tree search self-play. But whereas AlphaZero required thousands of TPUs over several days and ELF required thousands of GPUs over two weeks, KataGo surpasses ELF's final model after only 19 days on fewer than 30 GPUs. Much of the speedup involves non-domain-specific improvements that might directly transfer to other problems. Further gains from domain-specific techniques reveal the remaining efficiency gap between the best methods and purely general methods such as AlphaZero. Our work is a step towards making learning in state spaces as large as Go possible without large-scale computational resources.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Better than ELF OpenGo while using 1/50th the compute.\n\nnot an absolute SOTA, hey compare only against ELF OpenGo and Leela Zero, not against AlphaGo Zero/AlphaZero",
      "Epochs": "",
      "Training time (hours)": "456.0",
      "Training time notes": "27 processors for 19 days",
      "Training hardware": "NVIDIA Tesla V100 DGXS 16 GB",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "104.91425851608678",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "permissive license https://github.com/lightvector/KataGo/blob/master/LICENSE\n\ntraining here: https://github.com/lightvector/KataGo/blob/master/SelfplayTraining.md ",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GPT-2 (1.5B)",
      "Organization": "OpenAI",
      "Publication date": "2019-02-14",
      "Domain": "Language",
      "Task": "Language modeling/generation",
      "Parameters": "1500000000.0",
      "Parameters notes": "\"GPT-2 is a large transformer-based language model with 1.5 billion parameters\"",
      "Training compute (FLOP)": "1.920000000001e+21",
      "Training compute notes": "Estimating based on compute = 6 FLOP/token/param * epochs * parameters * tokens.\n\n40GB dataset is approximately 8B words, or 1/0.75 * 8B = 10.66B tokens.\n\nThe number of epochs is not reported, but another paper [1] claims in table 1 that it is 20 or 100 epochs, and another paper [2] claims 12 epochs based on communication with the GPT-2 authors (page 4).\n\n12 epochs is the modal, most credible value. Mean of probability mass is probably around 20 epochs, so calculating from that value:\n\n6 * (40 * 200 million * 1/0.75 * 20) * 1.5 billion parameters = 1.92e21\nhttps://www.wolframalpha.com/input?i=6+FLOP+*+20+*+%2840+billion+%2F+5+*+%284%2F3%29%29+*+1.5+billion\n\n[1] https://arxiv.org/abs/1906.06669 One Epoch Is All You Need\n[2] https://www.usenix.org/system/files/sec21-carlini-extracting.pdf Extracting Data From Large Language Models\n\nIt also appears the model was trained on TPU v3 chips:\nhttps://huggingface.co/openai-community/gpt2",
      "Training dataset": "WebText",
      "Training dataset size (gradients)": "10666666666.666666",
      "Dataset size notes": "\u201cAll results presented in this paper use a preliminary version of WebText which does not include links created after Dec 2017 and which after de-duplication and some heuristic based cleaning contains slightly over 8 million documents for a total of 40 GB of text.\u201d\n40GB is approximately 8e9 words.\n",
      "Confidence": "Speculative",
      "Link": "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf",
      "Reference": "Language Models are Unsupervised Multitask Learners",
      "Citations": "26463.0",
      "Authors": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever",
      "Abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "20.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "4348.443645",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "modified MIT\nhttps://github.com/openai/gpt-2?tab=License-1-ov-file#readme",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "SDE",
      "Organization": "Carnegie Mellon University (CMU),Google Brain,Monash University",
      "Publication date": "2019-02-09",
      "Domain": "Language",
      "Task": "Translation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Unspecified unreleased",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "\"The batch size is set to be 1500 words. We evaluate by development set BLEU score for every 2500 training batches\"",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1902.03499",
      "Reference": "Multilingual Neural Machine Translation With Soft Decoupled Encoding",
      "Citations": "",
      "Authors": "Xinyi Wang, Hieu Pham, Philip Arthur, Graham Neubig",
      "Abstract": "Multilingual training of neural machine translation (NMT) systems has led to impressive accuracy improvements on low-resource languages. However, there are still significant challenges in efficiently learning word representations in the face of paucity of data. In this paper, we propose Soft Decoupled Encoding (SDE), a multilingual lexicon encoding framework specifically designed to share lexical-level information intelligently without requiring heuristic preprocessing such as pre-segmenting the data. SDE represents a word by its spelling through a character encoding, and its semantic meaning through a latent embedding space shared by all languages. Experiments on a standard dataset of four low-resource languages show consistent improvements over strong multilingual NMT baselines, with gains of up to 2 BLEU on one of the tested languages, achieving the new state-of-the-art on all four language pairs.\n",
      "Organization categorization": "Academia,Industry,Academia",
      "Country (of organization)": "United States of America,United States of America,Australia",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Experiments on a standard dataset of four low-resource languages show consistent improvements over strong multilingual NMT baselines, with gains of up to 2 BLEU on one of the tested languages, achieving the new state-of-the-art on all four language pairs\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "",
      "Accessibility notes": "https://github.com/cindyxinyiwang/SDE",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Hanabi 4 player",
      "Organization": "DeepMind,University of Oxford,Carnegie Mellon University (CMU),Google Brain",
      "Publication date": "2019-02-01",
      "Domain": "Games",
      "Task": "Hanabi",
      "Parameters": "764000.0",
      "Parameters notes": "source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",
      "Training compute (FLOP)": "4.3e+18",
      "Training compute notes": "14.13e+12 FLOP/s * 7 days * 86400 s/day * 0.50 utilization = 4.3e+18 FLOP",
      "Training dataset": "",
      "Training dataset size (gradients)": "20000000000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1902.00506",
      "Reference": "The Hanabi Challenge: A New Frontier for AI Research",
      "Citations": "229.0",
      "Authors": "",
      "Abstract": "",
      "Organization categorization": "Industry,Academia,Academia,Industry",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland,United States of America,United States of America",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "Adapted some SOTA RL algorithms to a new task that posed research challenges",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "7 days on V100 \u2013> 7 * 24 * $0.55 = $92.40\nAdjust to 2020 dollars: $78.32",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "MT-DNN",
      "Organization": "Microsoft",
      "Publication date": "2019-01-31",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "330000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "GLUE,SciTail",
      "Training dataset size (gradients)": "1000000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1901.11504",
      "Reference": "Multi-Task Deep Neural Networks for Natural Language Understanding",
      "Citations": "1324.0",
      "Authors": "X Liu, P He, W Chen, J Gao",
      "Abstract": "In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks. MT-DNN not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations in order to adapt to new tasks and domains. MT-DNN extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al., 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7% (2.2% absolute improvement). We also demonstrate using the SNLI and SciTail datasets that the representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations. The code and pre-trained models are publicly available at this https URL.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\"MT-DNN extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al., 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7% (2.2% absolute improvement)\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT for code/weights: https://github.com/namisan/mt-dnn",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Transformer-XL (257M)",
      "Organization": "Carnegie Mellon University (CMU),Google Brain",
      "Publication date": "2019-01-09",
      "Domain": "Language",
      "Task": "Language modeling/generation",
      "Parameters": "256999999.99999997",
      "Parameters notes": "Transformer-XL Large, Table 1",
      "Training compute (FLOP)": "3.7832771e+20",
      "Training compute notes": "6 FLOP / token / parameter * 257000000 parameters * 103000000 tokens * 1908 epochs [see dataset size notes] = 3.0304001e+20 FLOP\n\nfrom training code (https://github.com/kimiyoung/transformer-xl/blob/master/tf/scripts/wt103_large_tpu.sh) they used 64 tpv3 cores \n\n123000000000000 FLOP/s/chip* (64 cores / 2 cores per chip) * 4000000 steps *  0.1 sec / step [assumption] * 0.3 [assumed utilization] = 4.7232e+20 FLOP\n\ngeometric mean\nsqrt(3.0304001e+20 * 4.7232e+20) = 3.7832771e+20\n\nspeculative confidence given assumptions used\n_________\nprevious estimation in the algorithmic progress paper was 1.09 \u00d7 10^19 FLOP without explanation \n",
      "Training dataset": "WikiText-103",
      "Training dataset size (gradients)": "103000000",
      "Dataset size notes": "from the training code (https://github.com/kimiyoung/transformer-xl/blob/master/pytorch/run_wt103_large.sh):\n\n--tgt_len 384\n--batch_size 128\n--max_step 4000000 \n\n384*128*4000000 / 103000000 = 1908 epochs",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/1901.02860",
      "Reference": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context",
      "Citations": "4090.0",
      "Authors": "Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov",
      "Abstract": "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "Discretionary",
      "Notability criteria notes": "",
      "Epochs": "1908.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "32.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "29712.65288608572",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "49152.0",
      "Batch size notes": "384*128 --tgt_len 384\n--batch_size 128",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache 2.0, includes train code\nhttps://github.com/kimiyoung/transformer-xl?tab=Apache-2.0-1-ov-file#readme",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Transformer ELMo",
      "Organization": "Allen Institute for AI,University of Washington",
      "Publication date": "2019-01-01",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "56000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "2000000000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/Dissecting-Contextual-Word-Embeddings%3A-Architecture-Peters-Neumann/ac11062f1f368d97f4c826c317bf50dcc13fdb59",
      "Reference": "Dissecting Contextual Word Embeddings: Architecture and Representation",
      "Citations": "459.0",
      "Authors": "ME Peters, M Neumann, L Zettlemoyer, W Yih",
      "Abstract": "Contextual word representations derived from pre-trained bidirectional language models (biLMs) have recently been shown to provide significant improvements to the state of the art for a wide range of NLP tasks. However, many questions remain as to how and why these models are so effective. In this paper, we present a detailed empirical study of how the choice of neural architecture (e.g. LSTM, CNN, or self attention) influences both end task accuracy and qualitative properties of the representations that are learned. We show there is a tradeoff between speed and accuracy, but all architectures learn high quality contextual representations that outperform word embeddings for four challenging NLP tasks. Additionally, all architectures learn representations that vary with network depth, from exclusively morphological based at the word embedding layer through local syntax based in the lower contextual layers to longer range semantics such coreference at the upper layers. Together, these results suggest that unsupervised biLMs, independent of architecture, are learning much more about the structure of language than previously appreciated.",
      "Organization categorization": "Research collective,Academia",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Our model is the Reconciled Span Parser (RSP; Joshi et al., 2018), which, using ELMo representations, achieved state of the art performance for this task. As shown in Table 2, the LSTM based models demonstrate the best performance with a 0.2% and 1.0% improvement over the Transformer and CNN models, respectively\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "StyleGAN",
      "Organization": "NVIDIA",
      "Publication date": "2018-12-12",
      "Domain": "Image generation",
      "Task": "Image generation",
      "Parameters": "26200000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "3.93e+16",
      "Training compute notes": "6 * 26.2M * 25M ~= 39300000000000000",
      "Training dataset": "CelebA",
      "Training dataset size (gradients)": "50000000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1812.04948",
      "Reference": "A Style-Based Generator Architecture for Generative Adversarial Networks",
      "Citations": "14496.0",
      "Authors": "Tero Karras, Samuli Laine, Timo Aila",
      "Abstract": "We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance,Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "168.0",
      "Training time notes": "\"Approximately 1 week on an NVIDIA DGX-1 with 8 Tesla V100 GPUs\"",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "8.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "4955.197627044293",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "SPN (ImageNet 128)",
      "Organization": "Google Brain,DeepMind",
      "Publication date": "2018-12-04",
      "Domain": "Image generation",
      "Task": "Image generation",
      "Parameters": "250000000.0",
      "Parameters notes": "250M (Table 4)",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "ImageNet",
      "Training dataset size (gradients)": "251658240000",
      "Dataset size notes": "batch size: 2048",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1812.01608",
      "Reference": "Generating High Fidelity Images with Subscale Pixel Networks and Multidimensional Upscaling",
      "Citations": "",
      "Authors": "Jacob Menick, Nal Kalchbrenner",
      "Abstract": "The unconditional generation of high fidelity images is a longstanding benchmark for testing the performance of image decoders. Autoregressive image models have been able to generate small images unconditionally, but the extension of these methods to large images where fidelity can be more readily assessed has remained an open problem. Among the major challenges are the capacity to encode the vast previous context and the sheer difficulty of learning a distribution that preserves both global semantic coherence and exactness of detail. To address the former challenge, we propose the Subscale Pixel Network (SPN), a conditional decoder architecture that generates an image as a sequence of sub-images of equal size. The SPN compactly captures image-wide spatial dependencies and requires a fraction of the memory and the computation required by other fully autoregressive models. To address the latter challenge, we propose to use Multidimensional Upscaling to grow an image in both size and depth via intermediate stages utilising distinct SPNs. We evaluate SPNs on the unconditional generation of CelebAHQ of size 256 and of ImageNet from size 32 to 256. We achieve state-of-the-art likelihood results in multiple settings, set up new benchmark results in previously unexplored settings and are able to generate very high fidelity large scale samples on the basis of both datasets.",
      "Organization categorization": "Industry,Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"state-of-the-art log-likelihoods at 128x128 by a large margin\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "Google TPU v3",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GPipe (Transformer)",
      "Organization": "Google",
      "Publication date": "2018-11-16",
      "Domain": "Language",
      "Task": "Translation",
      "Parameters": "6000000000.0",
      "Parameters notes": "Section 5: ",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "1450000000000",
      "Dataset size notes": "[WORDS]\n\nSection 5: \"We use a\ncorpus of parallel documents over 102 languages and English, containing a total of 25 billion training examples, ranging from 10^4 to 10^9 per language\"\n\n10^9 sentences * 20 words per sentence",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/1811.06965",
      "Reference": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism",
      "Citations": "1218.0",
      "Authors": "Y Huang, Y Cheng, A Bapna, O Firat",
      "Abstract": "Scaling up deep neural network capacity has been known as an effective approach to improving model quality for several different machine learning tasks. In many cases, increasing model capacity beyond the memory limit of a single accelerator has required developing special algorithms or infrastructure. These solutions are often architecture-specific and do not transfer to other tasks. To address the need for efficient and task-independent model parallelism, we introduce GPipe, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers. By pipelining different sub-sequences of layers on separate accelerators, GPipe provides the flexibility of scaling a variety of different networks to gigantic sizes efficiently. Moreover, GPipe utilizes a novel batch-splitting pipelining algorithm, resulting in almost linear speedup when a model is partitioned across multiple accelerators. We demonstrate the advantages of GPipe by training large-scale neural networks on two different tasks with distinct network architectures: (i) Image Classification: We train a 557-million-parameter AmoebaNet model and attain a top-1 accuracy of 84.4% on ImageNet-2012, (ii) Multilingual Neural Machine Translation: We train a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages and achieve better quality than all bilingual models.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\"We train a single 6-billion-parameter,\n128-layer Transformer model on a corpus spanning over 100 languages and achieve better quality than all bilingual models.\"\n\nI don't see any standard benchmark that they claim SOTA on for a Transforemer model",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "4000000.0",
      "Batch size notes": "\"Starting from 260K tokens per batch, we increase the effective batch size to 4M\"",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Multi-cell LSTM",
      "Organization": "University of Hyderabad",
      "Publication date": "2018-11-15",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "7200000.0",
      "Parameters notes": "Based on the details in the paper, the number of parameters in the Multi-cell LSTM model can be calculated as follows:\n\nThe model has 2 hidden LSTM layers, each with 1500 hidden units\n\nEach LSTM unit is a multi-cell LSTM with 10 memory cells per unit\n\nFor a standard LSTM layer with n hidden units:\n\nW matrix: n x input_size\nU matrix: n x n\n4 bias vectors of size n (for input, forget, cell, output gates)\nSo for each multi-cell LSTM layer with 1500 units and 10 cells per unit:\n\nW matrix: 1500 x input_size\nU matrix: 1500 x 1500\n4 bias vectors of size 1500\nNumber of parameters is same as standard LSTM layer\n\nFor the 2 hidden layers:\n\nInput size for Layer 1: embedding dimension (estimated 300 in paper)\n\nInput size for Layer 2: 1500 (output of layer 1)\n\nTotal params =\nLayer 1: 1500 x (300 + 1500 + 4) = 2,706,000\nLayer 2: 1500 x (1500 + 1500 + 4) = 4,506,000\n\nTotal Parameters = 2,706,000 + 4,506,000 = 7,212,000\n\nSo the total number of parameters for the Multi-cell LSTM model with 2 layers of 1500 units and 10 cells per unit is approximately 7.2 million.",
      "Training compute (FLOP)": "2006640000000000.0",
      "Training compute notes": "6 FLOP / parameter / token * 7200000 parameters * 929000 tokens * 50 epochs = 2.00664e+15 FLOP",
      "Training dataset": "Penn TreeBank (PTB)",
      "Training dataset size (gradients)": "929000",
      "Dataset size notes": "50 epochs (from Figure 4)",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/1811.06477",
      "Reference": "Multi-cell LSTM Based Neural Language Model",
      "Citations": "6.0",
      "Authors": "Thomas Cherian, Akshay Badola, Vineet Padmanabhan",
      "Abstract": "Language models, being at the heart of many NLP problems, are always of great interest to researchers. Neural language models come with the advantage of distributed representations and long range contexts. With its particular dynamics that allow the cycling of information within the network, `Recurrent neural network' (RNN) becomes an ideal paradigm for neural language modeling. Long Short-Term Memory (LSTM) architecture solves the inadequacies of the standard RNN in modeling long-range contexts. In spite of a plethora of RNN variants, possibility to add multiple memory cells in LSTM nodes was seldom explored. Here we propose a multi-cell node architecture for LSTMs and study its applicability for neural language modeling. The proposed multi-cell LSTM language models outperform the state-of-the-art results on well-known Penn Treebank (PTB) setup.",
      "Organization categorization": "Academia",
      "Country (of organization)": "India",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"The proposed multi-cell LSTM language models outperform the state-of-the-art results on well-known Penn Treebank (PTB) setup\"",
      "Epochs": "50.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Fine-tuned-AWD-LSTM-DOC (fin)",
      "Organization": "Samsung R&D Institute Russia",
      "Publication date": "2018-11-12",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "46000000.0",
      "Parameters notes": "This is the model trained on Penn Treebank, which uses as a base model the 23M model from Table 7 in https://aclanthology.org/D18-1489.pdf\n\nThey additionally train a discriminator with the same architecture, so total parameters is 2*23M = 46M",
      "Training compute (FLOP)": "5.188e+16",
      "Training compute notes": "Base model uses 4.323e16 FLOPs.\nThey then train a discriminator using the same architecture for 30 epochs, and then use the discriminator to fine-tune the base model for another 15 epochs. Both of these latter training steps require running forward passes on both the discriminator and the language model, but only doing a backward pass on one of them.\n\nDiscriminator training: \n2*23M*30*1044112 + 6*23M*30*1044112 = 5.763e15\n\nLM fine-tuning:\n2*23M*15*1044112 + 6*23M*15*1044112 = 2.882e15\n\nTotal:\n4.323e16 + 5.763e15 + 2.882e15 = 5.188e16",
      "Training dataset": "Penn TreeBank (PTB)",
      "Training dataset size (gradients)": "1044112",
      "Dataset size notes": "Per https://arxiv.org/pdf/1904.04733:\n\"The most common split of this corpus, where sections from 0 to 18 are used for training (38 219 sentences, 912 344 tokens), sections from 19 to 21 are used for validation (5 527 sentences, 131 768 tokens), and sections from 22 to 24 are used for testing (5 462 sentences, 129 654 tokens).\"\n\nSo dev set is 912,344 + 131768 = 1,044,112",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1811.04623",
      "Reference": "Fine-tuning of Language Models with Discriminator",
      "Citations": "2.0",
      "Authors": "Vadim Popov, Mikhail Kudinov",
      "Abstract": "Cross-entropy loss is a common choice when it comes to multiclass classification tasks and language modeling in particular. Minimizing this loss results in language models of very good quality. We show that it is possible to fine-tune these models and make them perform even better if they are fine-tuned with sum of cross-entropy loss and reverse Kullback-Leibler divergence. The latter is estimated using discriminator network that we train in advance. During fine-tuning probabilities of rare words that are usually underestimated by language models become bigger. The novel approach that we propose allows us to reach state-of-the-art quality on Penn Treebank: perplexity decreases from 52.4 to 52.1. Our fine-tuning algorithm is rather fast, scales well to different architectures and datasets and requires almost no hyperparameter tuning: the only hyperparameter that needs to be tuned is learning rate.",
      "Organization categorization": "Industry",
      "Country (of organization)": "Russia",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"The novel approach that we propose allows us to reach state-of-theart quality on Penn Treebank: perplexity decreases from 52.4 to 52.1.\"",
      "Epochs": "15.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "AWD-LSTM-DOC (fin) (23M)",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Mesh-TensorFlow Transformer 4.9B (language)",
      "Organization": "Google Brain",
      "Publication date": "2018-11-05",
      "Domain": "Language",
      "Task": "Language modeling/generation,Translation",
      "Parameters": "4900000000.0",
      "Parameters notes": "4.9B from section 9.1 : ''The largest model (4.9B parameters) took 13 hours to train on a 512-core TPUv2 cluster.'",
      "Training compute (FLOP)": "1.617408e+20",
      "Training compute notes": "flops = (256) * ( 45 * 10**12) * (13 * 3600) * (0.3) = 1.6e20\n(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)\n\nfrom section 9.1 : ''The largest model (4.9B parameters) took 13 hours to train on a 512-core TPUv2 cluster.'\nfrom https://en.wikipedia.org/wiki/Tensor_Processing_Unit \n45TFLOPs per chips",
      "Training dataset": "Wikipedia,One Billion Word benchmark",
      "Training dataset size (gradients)": "5000000000",
      "Dataset size notes": "from section 9.1. Experiments done on a \"billion word benchmark\" and a 5B token wikipedia dataset. At 4/3 tokens per word, 1.3B tokens in the first.",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1811.02084",
      "Reference": "Mesh-TensorFlow: Deep Learning for Supercomputers",
      "Citations": "421.0",
      "Authors": "Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, Hyoukjoong Mingsheng Lee, Cliff Hong, Ryan Young, Blake Sepassi,  Hechtman",
      "Abstract": "Batch-splitting (data-parallelism) is the dominant distributed Deep Neural Network (DNN) training strategy, due to its universal applicability and its amenability to Single-Program-Multiple-Data (SPMD) programming. However, batch-splitting suffers from problems including the inability to train very large models (due to memory constraints), high latency, and inefficiency at small batch sizes. All of these can be solved by more general distribution strategies (model-parallelism). Unfortunately, efficient model-parallel algorithms tend to be complicated to discover, describe, and to implement, particularly on large clusters. We introduce Mesh-TensorFlow, a language for specifying a general class of distributed tensor computations. Where data-parallelism can be viewed as splitting tensors and operations along the \"batch\" dimension, in Mesh-TensorFlow, the user can specify any tensor-dimensions to be split across any dimensions of a multi-dimensional mesh of processors. A Mesh-TensorFlow graph compiles into a SPMD program consisting of parallel operations coupled with collective communication primitives such as Allreduce. We use Mesh-TensorFlow to implement an efficient data-parallel, model-parallel version of the Transformer sequence-to-sequence model. Using TPU meshes of up to 512 cores, we train Transformer models with up to 5 billion parameters, surpassing state of the art results on WMT'14 English-to-French translation task and the one-billion-word language modeling benchmark. Mesh-Tensorflow is available at this https URL .",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "'Using TPU meshes of up to 512 cores, we train Transformer models with up to 5 billion parameters, surpassing state of the art results on WMT'14 English-to-French translation task and the one-billion-word language modeling benchmark.'",
      "Epochs": "10.0",
      "Training time (hours)": "13.0",
      "Training time notes": "from section 9.1 \"For the billion-word language modeling benchmark, we trained the models for 10 epochs. The largest model (4.9B parameters) took 13 hours to train on a 512-core TPUv2 cluster.\"",
      "Training hardware": "Google TPU v2",
      "Hardware quantity": "256.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "935.3300509163912",
      "Compute cost notes": "",
      "Training power draw (W)": "148117.2291982981",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "code here, apache license: https://github.com/tensorflow/mesh/tree/master/mesh_tensorflow/transformer \n\nhttps://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/transformer.py ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Mesh-TensorFlow Transformer 2.9B (translation)",
      "Organization": "Google Brain",
      "Publication date": "2018-11-05",
      "Domain": "Language",
      "Task": "Language modeling/generation,Translation",
      "Parameters": "2900000000.0",
      "Parameters notes": "2.9B from section 9.1 : \"On the WMT14 En-Fr translation tasks (3), we trained the models for 3 epochs. The largest model\n(2.9B parameters) was trained for 22 hours on a 128-core TPUv2 cluster.\"",
      "Training compute (FLOP)": "6.84288e+19",
      "Training compute notes": "flops = (64) * ( 45 * 10**12) * (22 * 3600) * (0.3) = 6.8e19\n(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate)\n\nfrom section 9.1 : \"On the WMT14 En-Fr translation tasks (3), we trained the models for 3 epochs. The largest model\n(2.9B parameters) was trained for 22 hours on a 128-core TPUv2 cluster.\"\nfrom https://en.wikipedia.org/wiki/Tensor_Processing_Unit \n45TFLOPs per chips",
      "Training dataset": "WMT14",
      "Training dataset size (gradients)": "1550000000",
      "Dataset size notes": "Per Attention is All You Need, WMT 2014 En-Fr is ~36 million sentence pairs. If the average sentence is ~25 tokens (ballpark), dataset size is \n36M * 25 * 2 = 1.8B tokens",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/1811.02084",
      "Reference": "Mesh-TensorFlow: Deep Learning for Supercomputers",
      "Citations": "421.0",
      "Authors": "Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, Hyoukjoong Mingsheng Lee, Cliff Hong, Ryan Young, Blake Sepassi,  Hechtman",
      "Abstract": "Batch-splitting (data-parallelism) is the dominant distributed Deep Neural Network (DNN) training strategy, due to its universal applicability and its amenability to Single-Program-Multiple-Data (SPMD) programming. However, batch-splitting suffers from problems including the inability to train very large models (due to memory constraints), high latency, and inefficiency at small batch sizes. All of these can be solved by more general distribution strategies (model-parallelism). Unfortunately, efficient model-parallel algorithms tend to be complicated to discover, describe, and to implement, particularly on large clusters. We introduce Mesh-TensorFlow, a language for specifying a general class of distributed tensor computations. Where data-parallelism can be viewed as splitting tensors and operations along the \"batch\" dimension, in Mesh-TensorFlow, the user can specify any tensor-dimensions to be split across any dimensions of a multi-dimensional mesh of processors. A Mesh-TensorFlow graph compiles into a SPMD program consisting of parallel operations coupled with collective communication primitives such as Allreduce. We use Mesh-TensorFlow to implement an efficient data-parallel, model-parallel version of the Transformer sequence-to-sequence model. Using TPU meshes of up to 512 cores, we train Transformer models with up to 5 billion parameters, surpassing state of the art results on WMT'14 English-to-French translation task and the one-billion-word language modeling benchmark. Mesh-Tensorflow is available at this https URL .",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "'Using TPU meshes of up to 512 cores, we train Transformer models with up to 5 billion parameters, surpassing state of the art results on WMT'14 English-to-French translation task and the one-billion-word language modeling benchmark.'",
      "Epochs": "10.0",
      "Training time (hours)": "22.0",
      "Training time notes": "from section 9.1 \"On the WMT14 En-Fr translation tasks (3), we trained the models for 3 epochs. The largest model\n(2.9B parameters) was trained for 22 hours on a 128-core TPUv2 cluster.\"",
      "Training hardware": "Google TPU v2",
      "Hardware quantity": "64.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "395.71656000308855",
      "Compute cost notes": "",
      "Training power draw (W)": "37029.307299574524",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "code here, apache license https://github.com/tensorflow/mesh/tree/master/mesh_tensorflow/transformer \n\nhttps://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/transformer.py  ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "MemoReader",
      "Organization": "Samsung,Korea University",
      "Publication date": "2018-10-31",
      "Domain": "Language",
      "Task": "Question answering",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "\"Our model does require more memory than existing methods, but a single GPU (e.g., M40 with 12GB memory) was enough to train model within a reasonable amount of time\"\n\n\"Reasonable\" could mean anything, maybe hours to a few days.",
      "Training dataset": "TriviaQA",
      "Training dataset size (gradients)": "1057958",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://aclanthology.org/D18-1237/",
      "Reference": "MemoReader: Large-Scale Reading Comprehension through Neural Memory Controller\n",
      "Citations": "17.0",
      "Authors": "Seohyun Back, Seunghak Yu, Sathish Indurthi, Jihie Kim, Jaegul Choo",
      "Abstract": "Machine reading comprehension helps machines learn to utilize most of the human knowledge written in the form of text. Existing approaches made a significant progress comparable to human-level performance, but they are still limited in understanding, up to a few paragraphs, failing to properly comprehend lengthy document. In this paper, we propose a novel deep neural network architecture to handle a long-range dependency in RC tasks. In\ndetail, our method has two novel aspects: (1) an advanced memory-augmented architecture and (2) an expanded gated recurrent unit with dense connections that mitigate potential information distortion occurring in the memory.\nOur proposed architecture is widely applicable to other models. We have performed extensive experiments with well-known benchmark datasets such as TriviaQA, QUASAR-T, and SQuAD. The experimental results demonstrate that the proposed method outperforms existing\nmethods, especially for lengthy documents.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "Korea (Republic of),Korea (Republic of)",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"TriviaQA. As shown in Table 2, our model,\neven without DEBS, outperforms the existing\nstate-of-the-art method such as \u2018BiDAF + SA +\nSN\u2019 by a large margin in all the cases\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "\"reasonable amount of time\" with a single GPU",
      "Training hardware": "NVIDIA M40",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "TrellisNet",
      "Organization": "Carnegie Mellon University (CMU),Bosch Center for Artificial Intelligence,Intel Labs",
      "Publication date": "2018-10-15",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "180000000.0",
      "Parameters notes": "180M, Table 2",
      "Training compute (FLOP)": "2.78e+18",
      "Training compute notes": "6 FLOP / parameter / token * 180000000 parameters * 103000000 tokens * 25 epochs = 2.781e+18 FLOP",
      "Training dataset": "WikiText-103",
      "Training dataset size (gradients)": "103000000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1810.06682",
      "Reference": "Trellis Networks for Sequence Modeling",
      "Citations": "155.0",
      "Authors": "Shaojie Bai, J. Zico Kolter, Vladlen Koltun",
      "Abstract": "We present trellis networks, a new architecture for sequence modeling. On the one hand, a trellis network is a temporal convolutional network with special structure, characterized by weight tying across depth and direct injection of the input into deep layers. On the other hand, we show that truncated recurrent networks are equivalent to trellis networks with special sparsity structure in their weight matrices. Thus trellis networks with general weight matrices generalize truncated recurrent networks. We leverage these connections to design high-performing trellis networks that absorb structural and algorithmic elements from both recurrent and convolutional models. Experiments demonstrate that trellis networks outperform the current state of the art methods on a variety of challenging benchmarks, including word-level language modeling and character-level language modeling tasks, and stress tests designed to evaluate long-term memory retention. The code is available at this https URL .",
      "Organization categorization": "Academia,Industry,Industry",
      "Country (of organization)": "United States of America,Germany,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Experiments demonstrate that trellis networks outperform the current state of the art methods on a variety of challenging benchmarks, including word-level language modeling and character-level language modeling\ntasks\"\n\n\"On word-level WikiText-103, a trellis network outperforms by 7.6% in perplexity the contemporaneous self-attention-based Relational Memory Core (Santoro et al., 2018), and by 11.5% the work of Merity et al. (2018a). (Concurrently with our work, Dai et al. (2019) employ a transformer and achieve even better results on WikiText-103.)\"",
      "Epochs": "25.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT license for code: https://github.com/locuslab/trellisnet/tree/master/TrellisNet/word_WT103 ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "MetaMimic",
      "Organization": "Google",
      "Publication date": "2018-10-11",
      "Domain": "Robotics",
      "Task": "Robotic manipulation",
      "Parameters": "22000000.0",
      "Parameters notes": "\"This representational demand motivates the introduction of high-capacity deep neural networks. We found the architecture, shown in Figure 3, with residual connections, 20 convolution layers with 512 channels\nfor a total of 22 million parameters, and instance normalization to drastically improve performance, as shown in Figure 6 of the Experiments section.\"",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/1810.05017",
      "Reference": "One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL",
      "Citations": "26.0",
      "Authors": "Tom Le Paine, Sergio Gomez",
      "Abstract": "Humans are experts at high-fidelity imitation -- closely mimicking a demonstration, often in one attempt. Humans use this ability to quickly solve a task instance, and to bootstrap learning of new tasks. Achieving these abilities in autonomous agents is an open problem. In this paper, we introduce an off-policy RL algorithm (MetaMimic) to narrow this gap. MetaMimic can learn both (i) policies for high-fidelity one-shot imitation of diverse novel skills, and (ii) policies that enable the agent to solve tasks more efficiently than the demonstrators. MetaMimic relies on the principle of storing all experiences in a memory and replaying these to learn massive deep neural network policies by off-policy RL. This paper introduces, to the best of our knowledge, the largest existing neural networks for deep RL and shows that larger networks with normalization are needed to achieve one-shot high-fidelity imitation on a challenging manipulation task. The results also show that both types of policy can be learned from vision, in spite of the task rewards being sparse, and without access to demonstrator actions.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"By retaining and taking advantage of all its experiences, MetaMimic also substantially outperforms the state-of-the-art D4PG RL agent, when D4PG uses only the current task experiences.\"\n\nI haven't found any standard benchmarks or metrics that they claim SOTA on",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "BERT-Large",
      "Organization": "Google",
      "Publication date": "2018-10-11",
      "Domain": "Language",
      "Task": "Question answering,Text autocompletion",
      "Parameters": "340000000.0",
      "Parameters notes": "340M",
      "Training compute (FLOP)": "2.85e+20",
      "Training compute notes": "more info here https://docs.google.com/document/d/1B8x6XYcmB1u6Tmq3VcbAtj5bzhDaj2TcIPyK6Wpupx4/edit?usp=sharing\n285000000000000000000 = 2.85 \u00d7 10^20\n\n\"AI and Memory Wall\" paper (https://github.com/amirgholami/ai_and_memory_wall) made an estimation of 250,000 PFLOPS = 2.5*10^20 FLOP",
      "Training dataset": "BookCorpus (BooksCorpus, Toronto Book Corpus),English Wikipedia",
      "Training dataset size (gradients)": "2649900000",
      "Dataset size notes": "\"For the pre-training corpus we\nuse the BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words)\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1810.04805",
      "Reference": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "Citations": "107323.0",
      "Authors": "J Devlin, MW Chang, K Lee, K Toutanova",
      "Abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "40.0",
      "Training time (hours)": "96.0",
      "Training time notes": "from appendix A.2: \"Training of BERTLARGE was performed\non 16 Cloud TPUs (64 TPU chips total). Each pre-\ntraining took 4 days to complete.\"",
      "Training hardware": "Google TPU v2",
      "Hardware quantity": "64.0",
      "Hardware utilization (MFU)": "0.2801",
      "Training compute cost (2023 USD)": "1751.4770087736404",
      "Compute cost notes": "",
      "Training power draw (W)": "37049.928524121315",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "128000.0",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "apache 2.0\ntrain+inference code and models here: https://github.com/google-research/bert ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Transformer (Adaptive Input Embeddings) WT103",
      "Organization": "Facebook AI Research",
      "Publication date": "2018-09-28",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "247000000.0",
      "Parameters notes": "Table 2",
      "Training compute (FLOP)": "4.47e+19",
      "Training compute notes": "8 V100s * 67 hours per Table 2.\n125e12 FLOP/sec * 8 * 67 * 3600 * 0.3 (utilization assumption) = 7.2e19 FLOP\n\nThey also say they trained for 286k steps in batches of 65,536 tokens.\n6 * 247M * (286k * 65536) = 2.78e19\n\ngeometric mean: sqrt(7.2e19 * 2.78e19) = 4.47e19",
      "Training dataset": "WikiText-103",
      "Training dataset size (gradients)": "100000000",
      "Dataset size notes": "\"The training data of WIKITEXT-103 comprises about 100M tokens\"\nDatasets are not combined but used to train separate models",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1809.10853",
      "Reference": "Adaptive Input Representations for Neural Language Modeling",
      "Citations": "422.0",
      "Authors": "Alexei Baevski, Michael Auli",
      "Abstract": "We introduce adaptive input representations for neural language modeling which extend the adaptive softmax of Grave et al. (2017) to input representations of variable capacity. There are several choices on how to factorize the input and output layers, and whether to model words, characters or sub-word units. We perform a systematic comparison of popular choices for a self-attentional architecture. Our experiments show that models equipped with adaptive embeddings are more than twice as fast to train than the popular character input CNN while having a lower number of parameters. On the WIKITEXT-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5 perplexity compared to the previously best published result and on the BILLION WORD benchmark, we achieve 23.02 perplexity.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,France",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"On the WikiText-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5 perplexity compared to the previously best published result\"",
      "Epochs": "180.0",
      "Training time (hours)": "67.0",
      "Training time notes": "",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "8.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "2880.917278699733",
      "Compute cost notes": "",
      "Training power draw (W)": "4963.480727525225",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT for code and weights: https://github.com/facebookresearch/fairseq/blob/main/examples/language_model/README.adaptive_inputs.md \n\ninference in other readme: https://github.com/facebookresearch/fairseq/blob/main/examples/language_model/README.md ",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "LSTM+NeuralCache",
      "Organization": "KU Leuven,ESAT - PSI,Apple",
      "Publication date": "2018-09-24",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "2100000.0",
      "Parameters notes": "Given:\n\nHidden size (H) = 512\nNumber of hidden layers (L) = 1\nInput size (I) is not mentioned, so let's denote it as I\nThe total number of parameters P in an LSTM can be calculated as follows:\n\nP = 4 * ((I * H) + (H * H) + H)\n\nThis is for one layer of LSTM cells. Since the LSTM model described has only one layer, we don't need to multiply by the number of layers.\n\nTo calculate the exact number of parameters, we would need to know the input size I. However, if you are looking for the number of parameters just within a single LSTM cell (assuming I is equal to H), then you can substitute I with H in the above formula:\n\nP = 4 * ((H * H) + (H * H) + H)\n= 4 * (2 * (H^2) + H)\n\nFor H = 512, this becomes:\n\nP = 4 * (2 * (512^2) + 512)\n= 4 * (2 * 262144 + 512)\n= 4 * (524288 + 512)\n= 4 * 524800\n\u2248 2,099,200",
      "Training compute (FLOP)": "982800000000000.0",
      "Training compute notes": "6 FLOP / parameter / token * 2100000 parameters * 2000000 tokens * 39 epochs = 9.828e+14 FLOP",
      "Training dataset": "WikiText-2",
      "Training dataset size (gradients)": "2000000",
      "Dataset size notes": "\" we stop training anyway after 39 epochs\"",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/1809.08826",
      "Reference": "Information-Weighted Neural Cache Language Models for ASR",
      "Citations": "3.0",
      "Authors": "Lyan Verwimp, Joris Pelemans, Hugo Van hamme, Patrick Wambacq",
      "Abstract": "Neural cache language models (LMs) extend the idea of regular cache language models by making the cache probability dependent on the similarity between the current context and the context of the words in the cache. We make an extensive comparison of 'regular' cache models with neural cache models, both in terms of perplexity and WER after rescoring first-pass ASR results. Furthermore, we propose two extensions to this neural cache model that make use of the content value/information weight of the word: firstly, combining the cache probability and LM probability with an information-weighted interpolation and secondly, selectively adding only content words to the cache. We obtain a 29.9%/32.1% (validation/test set) relative improvement in perplexity with respect to a baseline LSTM LM on the WikiText-2 dataset, outperforming previous work on neural cache LMs. Additionally, we observe significant WER reductions with respect to the baseline model on the WSJ ASR task.",
      "Organization categorization": "Academia,Academia,Industry",
      "Country (of organization)": "Belgium,Belgium,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We obtain a 29.9%/32.1% (validation/test set) relative improvement in perplexity with respect to a baseline LSTM LM on the WikiText-2 dataset, outperforming previous work on neural cache LMs\" \n... \n\n\"we observe that neural cache models consistently outperform regular cache models on this dataset.\"",
      "Epochs": "39.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "AWD-LSTM-MoS + dynamic evaluation (WT2, 2018)",
      "Organization": "Peking University,Microsoft Research Asia",
      "Publication date": "2018-09-18",
      "Domain": "Language",
      "Task": "Language modeling,Translation,Text classification",
      "Parameters": "35000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "WikiText-2",
      "Training dataset size (gradients)": "2000000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1809.06858",
      "Reference": "FRAGE: Frequency-Agnostic Word Representation",
      "Citations": "152.0",
      "Authors": "Chengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang, Tie-Yan Liu",
      "Abstract": "Continuous word representation (aka word embedding) is a basic building block in many neural network-based models used in natural language processing tasks. Although it is widely accepted that words with similar semantics should be close to each other in the embedding space, we find that word embeddings learned in several tasks are biased towards word frequency: the embeddings of high-frequency and low-frequency words lie in different subregions of the embedding space, and the embedding of a rare word and a popular word can be far from each other even if they are semantically similar. This makes learned word embeddings ineffective, especially for rare words, and consequently limits the performance of these neural network models. In this paper, we develop a neat, simple yet effective way to learn \\emph{FRequency-AGnostic word Embedding} (FRAGE) using adversarial training. We conducted comprehensive studies on ten datasets across four natural language processing tasks, including word similarity, language modeling, machine translation and text classification. Results show that with FRAGE, we achieve higher performance than the baselines in all tasks.",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "China,China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Specifically, in language modeling and machine translation, we achieve better performance than the state-of-the-art results on PTB, WT2 and WMT14 English-German datasets.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "code, no license: https://github.com/ChengyueGongR/Frequency-Agnostic ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Transformer + Simple Recurrent Unit",
      "Organization": "ASAPP,Cornell University,Google,Princeton University",
      "Publication date": "2018-09-17",
      "Domain": "Language",
      "Task": "Translation",
      "Parameters": "90000000.0",
      "Parameters notes": "5-layer model, Table 3",
      "Training compute (FLOP)": "1.1e+19",
      "Training compute notes": "\"We use a single NVIDIA Tesla V100 GPU for each model. The published results were obtained\nusing 8 GPUs in parallel, which provide a large effective batch size during training. To approximate\nthe setup, we update the model parameters every 5\u00d75120 tokens and use 16,000 warm-up steps\nfollowing OpenNMT suggestions. We train each\nmodel for 40 epochs (250,000 steps), and perform\n3 independent trials for each model configuration.\nA single run takes about 3.5 days with a Tesla V100 GPU.\"\n\n125 trillion * 3.5 * 24 * 3600 * 0.3 = 1.1e19",
      "Training dataset": "WMT English-German",
      "Training dataset size (gradients)": "112500000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1709.02755v5",
      "Reference": "Simple Recurrent Units for Highly Parallelizable Recurrence",
      "Citations": "294.0",
      "Authors": "Tao Lei, Yu Zhang, Sida I. Wang, Hui Dai, Yoav Artzi",
      "Abstract": "Common recurrent neural architectures scale poorly due to the intrinsic difficulty in parallelizing their state computations. In this work, we propose the Simple Recurrent Unit (SRU), a light recurrent unit that balances model capacity and scalability. SRU is designed to provide expressive recurrence, enable highly parallelized implementation, and comes with careful initialization to facilitate training of deep models. We demonstrate the effectiveness of SRU on multiple NLP tasks. SRU achieves 5--9x speed-up over cuDNN-optimized LSTM on classification and question answering datasets, and delivers stronger results than LSTM and convolutional models. We also obtain an average of 0.7 BLEU improvement over the Transformer model on translation by incorporating SRU into the architecture.",
      "Organization categorization": "Industry,Academia,Industry,Academia",
      "Country (of organization)": "United States of America,United States of America,United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We use the state-of-the-art Transformer\nmodel of Vaswani et al. (2017) as our base architecture... When SRU is incorporated into the architecture,\nboth the 4-layer and 5-layer model outperform the\nTransformer base model\"",
      "Epochs": "40.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "8.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "45.373219244858774",
      "Compute cost notes": "",
      "Training power draw (W)": "4964.696746005496",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "repo, but no training code for translation: https://github.com/taolei87/sru ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "NetSurfP-2.0",
      "Organization": "Technical University of Denmark,University of Copenhagen,Universidad Nacional de San Mart\u00edn,AIMST University",
      "Publication date": "2018-09-10",
      "Domain": "Biology",
      "Task": "Protein folding prediction,Protein property prediction",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "PDB (Protein Data Bank)",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "\"A structural dataset consisting of 12,185 crystal structures was obtained from the Protein Data Bank (PDB) (23), culled and selected by the PISCES server (24) with 25% sequence similarity clustering threshold and a resolution of 2.5 \u00c5 or better. To avoid overfitting, any sequence that had more than 25% identity to any sequences in the test datasets (see \u201cEvaluation\u201d section for details) was removed, as well as peptide chains with less than 20 residues, leaving 10,837 sequences. Finally, we\nrandomly selected 500 sequences (validation set) left out for early stopping and\nparameter optimization, leaving 10,337 sequences for training.\"",
      "Confidence": "Unknown",
      "Link": "https://www.biorxiv.org/content/10.1101/311209v3.full",
      "Reference": "NetSurfP-2.0: improved prediction of protein structural features by integrated deep learning",
      "Citations": "",
      "Authors": "Michael Schantz Klausen, Martin Closter Jespersen, Henrik Nielsen, Kamilla Kj\u00e6rgaard Jensen, Vanessa Isabell Jurtz, Casper Kaae S\u00f8nderby, Morten Otto Alexander Sommer, Ole Winther, Morten Nielsen, Bent Petersen,  View ORCID ProfilePaolo Marcatili",
      "Abstract": "The ability to predict local structural features of a protein from the primary sequence is of paramount importance for unravelling its function in absence of experimental structural information. Two main factors affect the utility of potential prediction tools: their accuracy must enable extraction of reliable structural information on the proteins of interest, and their runtime must be low to keep pace with sequencing data being generated at a constantly increasing speed.\n\nHere, we present an updated and extended version of the NetSurfP tool (http://www.cbs.dtu.dk/services/NetSurfP-2.0/), that can predict the most important local structural features with unprecedented accuracy and runtime. NetSurfP-2.0 is sequence-based and uses an architecture composed of convolutional and long short-term memory neural networks trained on solved protein structures. Using a single integrated model, NetSurfP-2.0 predicts solvent accessibility, secondary structure, structural disorder, and backbone dihedral angles for each residue of the input sequences.\n\nWe assessed the accuracy of NetSurfP-2.0 on several independent test datasets and found it to consistently produce state-of-the-art predictions for each of its output features. We observe a correlation of 80% between predictions and experimental data for solvent accessibility, and a precision of 85% on secondary structure 3-class predictions. In addition to improved accuracy, the processing time has been optimized to allow predicting more than 1,000 proteins in less than 2 hours, and complete proteomes in less than 1 day.",
      "Organization categorization": "Academia,Academia,Academia,Academia",
      "Country (of organization)": "Denmark,Denmark,Argentina,Malaysia",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We assessed the accuracy of NetSurfP-2.0 on several independent test datasets and found it to consistently produce state-of-the-art predictions for each of its output features.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Hosted access (no API)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "NetSurfP-2.0 is available both as a web-server, and as an independent software (http://www.cbs.dtu.dk/services/NetSurfP-2.0/). The web-server version accepts up to 4,000 sequences or 4,000,000 residues per job. ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "(ensemble): AWD-LSTM-DOC (fin) \u00d7 5 (WT2)",
      "Organization": "NTT Communication Science Laboratories,Tohoku University",
      "Publication date": "2018-08-30",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "185000000.0",
      "Parameters notes": "185M (table 8)",
      "Training compute (FLOP)": "6.66e+17",
      "Training compute notes": "6 FLOP / parameter / token * 185000000 parameters * 2000000 tokens * 300 epochs = 6.66e+17 FLOP",
      "Training dataset": "WikiText-2",
      "Training dataset size (gradients)": "2000000",
      "Dataset size notes": "batch size 15\n300 epochs (figure 2)",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1808.10143",
      "Reference": "Direct Output Connection for a High-Rank Language Model",
      "Citations": "37.0",
      "Authors": "Sho Takase, Jun Suzuki, Masaaki Nagata",
      "Abstract": "This paper proposes a state-of-the-art recurrent neural network (RNN) language model that combines probability distributions computed not only from a final RNN layer but also from middle layers. Our proposed method raises the expressive power of a language model based on the matrix factorization interpretation of language modeling introduced by Yang et al. (2018). The proposed method improves the current state-of-the-art language model and achieves the best score on the Penn Treebank and WikiText-2, which are the standard benchmark datasets. Moreover, we indicate our proposed method contributes to two application tasks: machine translation and headline generation. Our code is publicly available at: this https URL.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "Japan,Japan",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"The proposed method improves the current state-of-the-art language model and achieves the best score on the Penn Treebank and WikiText-2, which are the standard benchmark datasets\"",
      "Epochs": "300.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "code and weights, MIT: https://github.com/nttcslab-nlp/doc_lm?tab=readme-ov-file ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Big Transformer for Back-Translation",
      "Organization": "Facebook AI Research,Google Brain",
      "Publication date": "2018-08-28",
      "Domain": "Language",
      "Task": "Translation",
      "Parameters": "",
      "Parameters notes": "\"We re-implemented the Transformer model in py-\ntorch using the fairseq toolkit.1 All experiments\nare based on the Big Transformer architecture with\n6 blocks in the encoder and decoder. We use the\nsame hyper-parameters for all experiments, i.e.,\nword representations of size 1024, feed-forward\nlayers with inner dimension 4096. \"\n\nI am not sure what authors mean by 'Big Transformer architecture'",
      "Training compute (FLOP)": "4.7808e+20",
      "Training compute notes": "(128) * (1.25e14) * (27*3600 + 40*60) * (0.3)  = 4.7808e20\n(number of gpus) * (peak flops) * (seconds) * (assumed utilization rate)  \n\n\"We run experiments on DGX-1 machines with 8Nvidia V100 GPUs and machines are interconnected by Infiniband. Experiments are run on 16\nmachines and we perform 30K synchronous updates.\"\n\"We also use the NCCL2 library [...] with 16-bit floating point\noperations\"\n\nNCCL2 supported tensor core operations at 1.25e14 FLOP/s on a V100 for FP16 \n\nin section 5.6 we have\n\n\"train this system we perform 300K training up-\ndates in 27h 40min on 128 GPUs;\"",
      "Training dataset": "WMT English-German",
      "Training dataset size (gradients)": "4520000000",
      "Dataset size notes": "\"Finally, for WMT English-German we train on all 226M available monolingual training sentences and perform 250K updates in 22.5 hours on 128 GPUs.\"\n\nWe assume that 1 sentence have 15 words",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/1808.09381",
      "Reference": "Understanding Back-Translation at Scale",
      "Citations": "1155.0",
      "Authors": "Sergey Edunov, Myle Ott, Michael Auli, David Grangier",
      "Abstract": "An effective method to improve neural machine translation with monolingual data is to augment the parallel training corpus with back-translations of target language sentences. This work broadens the understanding of back-translation and investigates a number of methods to generate synthetic source sentences. We find that in all but resource poor settings back-translations obtained via sampling or noised beam outputs are most effective. Our analysis shows that sampling or noisy synthetic data gives a much stronger training signal than data generated by beam or greedy search. We also compare how synthetic data compares to genuine bitext and study various domain effects. Finally, we scale to hundreds of millions of monolingual sentences and achieve a new state of the art of 35 BLEU on the WMT'14 English-German test set. ",
      "Organization categorization": "Industry,Industry",
      "Country (of organization)": "United States of America,France,United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\"Finally, we scale to hundreds of millions of monolingual sentences and achieve a new state of the art of 35 BLEU on the WMT'14 English-German test set. \"",
      "Epochs": "",
      "Training time (hours)": "27.666",
      "Training time notes": "\"training updates in 27h 40min on 128 GPUs\"",
      "Training hardware": "NVIDIA Tesla V100 DGXS 16 GB",
      "Hardware quantity": "128.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "2442.1618775733145",
      "Compute cost notes": "",
      "Training power draw (W)": "66225.44602672113",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Code and weights, MIT license: https://github.com/facebookresearch/fairseq/blob/main/examples/backtranslation/README.md ",
      "Numerical format": "FP16",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "AWD-LSTM-MoS+PDR + dynamic evaluation (WT2)",
      "Organization": "IBM",
      "Publication date": "2018-08-14",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "35000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "WikiText-2",
      "Training dataset size (gradients)": "2000000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1808.05908",
      "Reference": "Improved Language Modeling by Decoding the Past",
      "Citations": "6.0",
      "Authors": "Siddhartha Brahma",
      "Abstract": "Highly regularized LSTMs achieve impressive results on several benchmark datasets in language modeling. We propose a new regularization method based on decoding the last token in the context using the predicted distribution of the next token. This biases the model towards retaining more contextual information, in turn improving its ability to predict the next token. With negligible overhead in the number of parameters and training time, our Past Decode Regularization (PDR) method achieves a word level perplexity of 55.6 on the Penn Treebank and 63.5 on the WikiText-2 datasets using a single softmax. We also show gains by using PDR in combination with a mixture-of-softmaxes, achieving a word level perplexity of 53.8 and 60.5 on these datasets. In addition, our method achieves 1.169 bits-per-character on the Penn Treebank Character dataset for character level language modeling. These results constitute a new state-of-the-art in their respective settings.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"our Past Decode Regularization (PDR) method achieves a word level perplexity of 55.6 on the Penn Treebank and 63.5 on the WikiText-2 datasets using a single softmax. We also show gains by using PDR in combination with a mixture-of-softmaxes, achieving a word level perplexity of 53.8 and 60.5 on these datasets. In addition, our method achieves 1.169 bits-per-character on the Penn Treebank Character dataset for character level language modeling. These results constitute a new state-of-the-art in their respective settings.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Big-Little Net (speech)",
      "Organization": "IBM",
      "Publication date": "2018-07-10",
      "Domain": "Speech",
      "Task": "Speech recognition (ASR)",
      "Parameters": "3320000.0",
      "Parameters notes": "table 3",
      "Training compute (FLOP)": "4.290048e+17",
      "Training compute notes": "980000000 (number of FLOPs from table 3) * 27360000 (dataset size) * 16 (number of epochs from appendix B.1) = 429004800000000000",
      "Training dataset": "Switchboard,Fisher",
      "Training dataset size (gradients)": "720000000",
      "Dataset size notes": "\"We train ResNet style acoustic models in the hybrid framework on Switchboard+Fisher (2000h) and provide results on Hub5 (Switchboard and Call Home portions). Switchboard is a large dataset with 2000 hours of transcribed speech from 28, 000 speakers\"\n\n2000h * 13680 words per hour = 27360000\n\nhttps://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.3pbt0hfgv7pq",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/1807.03848",
      "Reference": "Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition",
      "Citations": "99.0",
      "Authors": "Chun-Fu (Richard) Chen, Quanfu Fan, Neil Mallinar, Tom Sercu, Rogerio Feris",
      "Abstract": "In this paper, we propose a novel Convolutional Neural Network (CNN) architecture for learning multi-scale feature representations with good tradeoffs between speed and accuracy. This is achieved by using a multi-branch network, which has different computational complexity at different branches. Through frequent merging of features from branches at distinct scales, our model obtains multi-scale features while using less computation. The proposed approach demonstrates improvement of model efficiency and performance on both object recognition and speech recognition tasks,using popular architectures including ResNet and ResNeXt. For object recognition, our approach reduces computation by 33% on object recognition while improving accuracy with 0.9%. Furthermore, our model surpasses state-of-the-art CNN acceleration approaches by a large margin in accuracy and FLOPs reduction. On the task of speech recognition, our proposed multi-scale CNNs save 30% FLOPs with slightly better word error rates, showing good generalization across domains. The codes are available at https://github.com/IBM/BigLittleNet.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Furthermore, our model surpasses state-of-the-art CNN acceleration approaches by a large margin in accuracy and FLOPs reduction. On the task of speech recognition, our proposed multi-scale CNNs save 30% FLOPs with slightly better word error rates, showing good generalization across domains.\"",
      "Epochs": "16.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "apache for code/weights: \nhttps://github.com/IBM/BigLittleNet",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Big-Little Net",
      "Organization": "IBM",
      "Publication date": "2018-07-10",
      "Domain": "Vision",
      "Task": "Image classification,Object recognition",
      "Parameters": "77360000.0",
      "Parameters notes": "Table 2",
      "Training compute (FLOP)": "2.46048e+17",
      "Training compute notes": "Using the 6ND formula: \n6\u00d7number of tokens\u00d7number of parameters\u00d7number of epochs\n6\u00d71.28\u00d710^6\u00d777360000\u00d7110=6.5353728e+16 FLOPs\n\n9.32*10^9 (flops per inference)*1.28\u00d710^6(dataset size)/16 (batch size) * 110 epochs * 3 (to account for backpropagation)= 2.46048e+17 FLOPs",
      "Training dataset": "ImageNet",
      "Training dataset size (gradients)": "1280000",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/1807.03848",
      "Reference": "Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition",
      "Citations": "99.0",
      "Authors": "Chun-Fu Chen, Quanfu Fan, Neil Mallinar, Tom Sercu, and Rogerio Feris",
      "Abstract": "In this paper, we propose a novel Convolutional Neural Network (CNN) architecture for learning multi-scale feature representations with good tradeoffs between speed and accuracy. This is achieved by using a multi-branch network, which has different computational complexity at different branches. Through frequent merging of features from branches at distinct scales, our model obtains multi-scale features while using less computation. The proposed approach demonstrates improvement of model efficiency and performance on both object recognition and speech recognition tasks,using popular architectures including ResNet and ResNeXt. For object recognition, our approach reduces computation by 33% on object recognition while improving accuracy with 0.9%. Furthermore, our model surpasses state-of-the-art CNN acceleration approaches by a large margin in accuracy and FLOPs reduction. On the task of speech recognition, our proposed multi-scale CNNs save 30% FLOPs with slightly better word error rates, showing good generalization across domains.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"On object recognition task, we demonstrated that our approach provides approximately 2\u00d7 speedup over baselines while improving accuracy, and the result significantly outperforms the state-of-the-art networks by a large margin in terms of accuracy and FLOPs reduction\"",
      "Epochs": "110.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA Tesla K80",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "256.0",
      "Batch size notes": "\"All the models were trained with 110 epochs, batch size 256\"",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "Apache 2 license\nhttps://github.com/IBM/BigLittleNet",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "FTW (For The Win)",
      "Organization": "DeepMind",
      "Publication date": "2018-07-03",
      "Domain": "Games",
      "Task": "Capture the flag",
      "Parameters": "126001330.0",
      "Parameters notes": "Architecture described in figure S11 of the supplement\n\nThe architecture includes modules for visual embedding, reward prediction, recurrent processing, policy, baseline and pixel control.\n\nInput is 84x84x3 pixels as seen in figure S10 of the supplement\n\n\"We elected to use a resolution of 84x84 pixels as in previous related work in this environment. Each pixel is represented by a triple of three bytes\"\n\nVisual embedding (84x84x3 -> 256)\n32*(8*8*3+1)+64*(4*4*32+1)+64*(3*3*64+1)+64*(3*3*64+1) + (84/(S^4)*84/(S^4)*64+1)*256\nNote there is no information about the stride S used in the convolutions; we assume S = 1\n\nReward prediction (256 -> 3)\n(256+1)*128 + (128+1)*3\n\nRecurrent processing (n-> 512)\nVU1 (256 -> 512)\n4*(799+2*32)*((512+(32*2) + 3*32 + 5*2 + 3)+(799+2*32)+1) + 2*(256+1)*256\n\nVU2 (512 -> 512)\n4*(512+2*32)*((512+(32*2) + 3*32 + 5*2 + 3)+(512+2*32)+1) + 2*(256+1)*256\n\nLSTMs usually have 4*(n*m+n*n+n) parameters, where n=input size and m=output size.\n\nThis DNS + LSTM takes as input the concatenation of the previous layer of size n and R read vectors of size W=32; and outputs m units plus an interface vector of size (W*R) + 3*W + 5*R + 3, for a total of about 4*(n+R*W)*((m+(W*R) + 3*W + 5*R + 3)+(n+R*32)+1) parameters\n\nI assume R=2 since that seems implied by the previous paper (?)\n\nThe first VU has as input the visual embedding (size 256), the previous action (size 540) and the previous reward (size 3), for a total size of 256+540+3 = 799. The output is size 512.\n\nThe second VU has input size 512 and output size 512\n\nThe DNC memory architecture is described in https://www.nature.com/articles/nature20101.epdf\n\nPolicy (512 -> 5x3x3x3x2x2)\n6*(512+1)*256 + (256+1)*5 + 3*(256+1)*3 + 2*(256+1)*2\n\nBaseline\n(512+1)*256 + (256+1)*1\n\nPixel control\n(512+1)*32*7*7 + 32*(9*9+1) + 5*(4*4+1) + 3*2*(4*4+1) + 2*2*(4*4+1) + 1*(4*4+1)\n\"we trained independent pixel control policies for each of the six action groups\"",
      "Training compute (FLOP)": "3.49e+19",
      "Training compute notes": "Source: \nhttps://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389\n\n\nAdditional estimate based on parameters (low confidence, had to make some assumptions about the architecture)\nForward pass operations\nVision\n84*84*32*8*8*3+20*20*32*64*4*4+7*7*64*64*3*3+7*7*64*64*3*3+7*7*64*256=60874752\n60874752*2=121749504\nPixel control (guess on the internal dimensions)\n256*32*7*7+7*7*32*32*9*9+20*20*32*4*4*(1+2+3+4+5)=7537664\nRemaining 2*(463616+33152+1310720)=3614976\nTotal: 121749504+7537664+3614976\nTraining FLOP estimate: 3*3614976*2000000000=21689856000000000\n",
      "Training dataset": "",
      "Training dataset size (gradients)": "2000000000",
      "Dataset size notes": "Agents were trained for two billion steps, corresponding to approximately 450K games.",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/1807.01281",
      "Reference": "Human-level performance in first-person multiplayer games with population-based deep reinforcement learning",
      "Citations": "769.0",
      "Authors": "Max Jaderberg, Wojciech M. Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil C. Rabinowitz, Ari S. Morcos, Avraham Ruderman, Nicolas Sonnerat, Tim Green, Louise Deason, Joel Z. Leibo, David Silver, Demis Hassabis, Koray Kavukcuoglu, Thore Graepel",
      "Abstract": "Recent progress in artificial intelligence through reinforcement learning (RL) has shown great success on increasingly complex single-agent environments and two-player turn-based games. However, the real-world contains multiple agents, each learning and acting independently to cooperate and compete with other agents, and environments reflecting this degree of complexity remain an open challenge. In this work, we demonstrate for the first time that an agent can achieve human-level in a popular 3D multiplayer first-person video game, Quake III Arena Capture the Flag, using only pixels and game points as input. These results were achieved by a novel two-tier optimisation process in which a population of independent RL agents are trained concurrently from thousands of parallel matches with agents playing in teams together and against each other on randomly generated environments. Each agent in the population learns its own internal reward signal to complement the sparse delayed reward from winning, and selects actions using a novel temporally hierarchical representation that enables the agent to reason at multiple timescales. During game-play, these agents display human-like behaviours such as navigating, following, and defending based on a rich learned representation that is shown to encode high-level game knowledge. In an extensive tournament-style evaluation the trained agents exceeded the win-rate of strong human players both as teammates and opponents, and proved far stronger than existing state-of-the-art agents. These results demonstrate a significant jump in the capabilities of artificial agents, bringing us closer to the goal of human-level intelligence.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"In this work, we demonstrate for the first time that an agent can achieve human-level in a popular 3D multiplayer first-person video game, Quake III Arena Capture the Flag (28), using only pixels and game points as input.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "MobileNetV2",
      "Organization": "Google",
      "Publication date": "2018-06-18",
      "Domain": "Vision",
      "Task": "Object detection,Image classification,Image segmentation",
      "Parameters": "3400000.0",
      "Parameters notes": "Rados",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "1280000",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://ieeexplore.ieee.org/document/8578572",
      "Reference": "MobileNetV2: Inverted Residuals and Linear Bottlenecks",
      "Citations": "16899.0",
      "Authors": "M Sandler, A Howard, M Zhu",
      "Abstract": "In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. is based on an inverted residual structure where the shortcut connections are between the thin bottleneck layers. The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on ImageNet [1] classification, COCO object detection [2], VOC image segmentation [3]. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as actual latency, and the number of parameters.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Relational Memory Core",
      "Organization": "DeepMind,University College London (UCL)",
      "Publication date": "2018-06-05",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "WikiText-103",
      "Training dataset size (gradients)": "4000000000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1806.01822",
      "Reference": "Relational recurrent neural networks",
      "Citations": "235.0",
      "Authors": "Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Theophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, Timothy Lillicrap",
      "Abstract": "Memory-based neural networks model temporal data by leveraging an ability to remember information for long periods. It is unclear, however, whether they also have an ability to perform complex relational reasoning with the information they remember. Here, we first confirm our intuitions that standard memory architectures may struggle at tasks that heavily involve an understanding of the ways in which entities are connected -- i.e., tasks involving relational reasoning. We then improve upon these deficits by using a new memory module -- a \\textit{Relational Memory Core} (RMC) -- which employs multi-head dot product attention to allow memories to interact. Finally, we test the RMC on a suite of tasks that may profit from more capable relational reasoning across sequential information, and show large gains in RL domains (e.g. Mini PacMan), program evaluation, and language modeling, achieving state-of-the-art results on the WikiText-103, Project Gutenberg, and GigaWord datasets.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Finally, we test the RMC on a suite of tasks that may profit from more capable relational reasoning across sequential information, and show large gains in RL domains (e.g. Mini PacMan), program evaluation, and language modeling, achieving state-of-the-art results on the WikiText-103, Project Gutenberg, and GigaWord datasets.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "looks like code for the architecture, but not experiment code: https://github.com/google-deepmind/sonnet/blob/v1/sonnet/python/modules/relational_memory.py ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GPT-1",
      "Organization": "OpenAI",
      "Publication date": "2018-06-01",
      "Domain": "Language",
      "Task": "Question answering,Text classification,Language modeling",
      "Parameters": "117000000.0",
      "Parameters notes": "\"The model had 117M parameters in total.\"\n\nsource: https://medium.com/walmartglobaltech/the-journey-of-open-ai-gpt-models-32d95b7b7fb2",
      "Training compute (FLOP)": "1.7578125e+19",
      "Training compute notes": "COMPUTE = FORWARD COMPUTE PER TOKEN * 3 BACKWARD FORWARD ADJUSTMENT * EPOCHS * DATASET SIZE\n\n\"We train for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens.\"\n\nAuthors of \"AI and Memory Wall\" estimated model's training compute as 57,000 PFLOPS = 5.7*10^19 FLOP\n(https://github.com/amirgholami/ai_and_memory_wall)",
      "Training dataset": "BookCorpus (BooksCorpus, Toronto Book Corpus)",
      "Training dataset size (gradients)": "1333333333",
      "Dataset size notes": "\"BookCorpus is a large collection of free novel books written by unpublished authors, which contains 11,038 books (around 74M sentences and 1G words) of 16 different sub-genres (e.g., Romance, Historical, Adventure, etc.).\"\nhttps://paperswithcode.com/dataset/bookcorpus\n\nBookCorpus seems to have about 5000MB of content\nsource: https://huggingface.co/datasets/bookcorpusopen\n\nAssuming a byte-pair encoder similar to GPT-2, there are 8 bytes / token.\n\nSo approximately 5000MB / 8 bytes / token = 5e9 / 8 tokens",
      "Confidence": "Likely",
      "Link": "https://openai.com/blog/language-unsupervised/",
      "Reference": "Improving Language Understanding by Generative Pre-Training",
      "Citations": "13799.0",
      "Authors": "A Radford, K Narasimhan, T Salimans, I Sutskever",
      "Abstract": "Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI).",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "720.0",
      "Training time notes": "\"1 month on 8 GPUs.\" from the reference link",
      "Training hardware": "NVIDIA Quadro P600",
      "Hardware quantity": "8.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "663.5535559333689",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT, code and weights\nhttps://github.com/openai/finetune-transformer-lm/blob/master/LICENSE",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "1730.4009124918987",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "aLSTM(depth-2)+RecurrentPolicy (WT2)",
      "Organization": "University of Manchester,Alan Turing Institute",
      "Publication date": "2018-05-22",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "32000000.0",
      "Parameters notes": "32M (Table 3)",
      "Training compute (FLOP)": "7.296e+16",
      "Training compute notes": "6 FLOP / token / parameter * 32000000 parameters * 2000000 tokens * 190 epochs = 7.296e+16 FLOP",
      "Training dataset": "WikiText-2",
      "Training dataset size (gradients)": "2000000",
      "Dataset size notes": "\"Both models are trained for 10 000 steps with a batch size of\n50 and a learning rate of 0.003.\"\n\n\"Without tuning for WT2, both outperform previously published results in 150 epochs (table 3) and converge to new state of the art performance in 190 epochs\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1805.08574",
      "Reference": "Breaking the Activation Function Bottleneck through Adaptive Parameterization",
      "Citations": "12.0",
      "Authors": "Sebastian Flennerhag, Hujun Yin, John Keane, Mark Elliot",
      "Abstract": "Standard neural network architectures are non-linear only by virtue of a simple element-wise activation function, making them both brittle and excessively large. In this paper, we consider methods for making the feed-forward layer more flexible while preserving its basic structure. We develop simple drop-in replacements that learn to adapt their parameterization conditional on the input, thereby increasing statistical efficiency significantly. We present an adaptive LSTM that advances the state of the art for the Penn Treebank and WikiText-2 word-modeling tasks while using fewer parameters and converging in less than half as many iterations.",
      "Organization categorization": "Academia,Government",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Without tuning for WT2, both outperform previously published results in 150 epochs (table 3) and converge to new state of the art performance in 190 epochs\"",
      "Epochs": "190.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "BSD-3 license: https://github.com/flennerhag/alstm/tree/master/examples ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Dropout-LSTM+Noise(Bernoulli) (WT2)",
      "Organization": "Columbia University,New York University (NYU),Princeton University",
      "Publication date": "2018-05-03",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "51000000.0",
      "Parameters notes": "\"The large network has 2 layers with 1500 hidden units each. This leads to a model complexity of 51 million parameters.\"",
      "Training compute (FLOP)": "1.27e+17",
      "Training compute notes": "6 FLOP / parameter / token * 51000000 parameters * 2000000 tokens * 200 epochs = 1.224e+17 FLOP\n\n'Likely' confidence because I am not very sure that 51M paramters and 200 epochs relate to WT-2 model, but it is very likely",
      "Training dataset": "WikiText-2",
      "Training dataset size (gradients)": "2000000",
      "Dataset size notes": "\"We train the models using truncated backpropagation through time with average stochastic gradient descent (Polyak & Juditsky, 1992) for a maximum of 200 epochs\"",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/1805.01500",
      "Reference": "Noisin: Unbiased Regularization for Recurrent Neural Networks",
      "Citations": "26.0",
      "Authors": "Adji B. Dieng, Rajesh Ranganath, Jaan Altosaar, David M. Blei",
      "Abstract": "Recurrent neural networks (RNNs) are powerful models of sequential data. They have been successfully used in domains such as text and speech. However, RNNs are susceptible to overfitting; regularization is important. In this paper we develop Noisin, a new method for regularizing RNNs. Noisin injects random noise into the hidden states of the RNN and then maximizes the corresponding marginal likelihood of the data. We show how Noisin applies to any RNN and we study many different types of noise. Noisin is unbiased--it preserves the underlying RNN on average. We characterize how Noisin regularizes its RNN both theoretically and empirically. On language modeling benchmarks, Noisin improves over dropout by as much as 12.2% on the Penn Treebank and 9.4% on the Wikitext-2 dataset. We also compared the state-of-the-art language model of Yang et al. 2017, both with and without Noisin. On the Penn Treebank, the method with Noisin more quickly reaches state-of-the-art performance.",
      "Organization categorization": "Academia,Academia,Academia",
      "Country (of organization)": "United States of America,United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "this is the best model in this paper per Table 4\n\"On language modeling benchmarks, Noisin improves over dropout by as much as 12.2% on the Penn Treebank and 9.4% on the Wikitext-2 dataset\"",
      "Epochs": "200.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "\"The source code is available upon request.\"",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ResNeXt-101 32x48d",
      "Organization": "Facebook",
      "Publication date": "2018-05-02",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "829000000.0",
      "Parameters notes": "Table 6\n",
      "Training compute (FLOP)": "8.74395e+21",
      "Training compute notes": "Table 6: 153e9 mult-adds.\nSection 2.4: \"minibatches of 8,064 images\".\n\nCompute = 2 * 3 * mult-adds * dataset size = 2 * 3 * 153e9 * 9525e6 = 8.74e21 FLOP\n\nLikely trained on V100s, since Facebook had just upgraded their Big Basin GPU cluster to V100s as of March 2018. The previous iteration of Big Basin had 32 clusters of 8xP100s, while Big Basin v2 had 42 clusters of 8xV100s, which matches the 336 GPUs used in this paper.",
      "Training dataset": "ImageNet,Instagram",
      "Training dataset size (gradients)": "940000000",
      "Dataset size notes": "Table 3: (300+1925+300+7000) million images",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1805.00932",
      "Reference": "Exploring the Limits of Weakly Supervised Pretraining",
      "Citations": "1425.0",
      "Authors": "Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, Laurens van der Maaten",
      "Abstract": "State-of-the-art visual perception models for a wide range of tasks rely on supervised pretraining. ImageNet classification is the de facto pretraining task for these models. Yet, ImageNet is now nearly ten years old and is by modern standards \"small\". Even so, relatively little is known about the behavior of pretraining with datasets that are multiple orders of magnitude larger. The reasons are obvious: such datasets are difficult to collect and annotate. In this paper, we present a unique study of transfer learning with large convolutional networks trained to predict hashtags on billions of social media images. Our experiments demonstrate that training for large-scale hashtag prediction leads to excellent results. We show improvements on several image classification and object detection tasks, and report the highest ImageNet-1k single-crop, top-1 accuracy to date: 85.4% (97.6% top-5). We also perform extensive experiments that provide novel empirical data on the relationship between large-scale pretraining and transfer learning performance.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\"We show improvements on several image classification and object detection tasks, and report the highest ImageNet-1k single-crop, top-1 accuracy to date: 85.4%",
      "Epochs": "",
      "Training time (hours)": "496.0",
      "Training time notes": "\"Mahajan et al. (2018) required 19 GPU years to train their ResNeXt101-32x48d\" https://arxiv.org/abs/2103.00020\nModels were trained on 336 GPUs, so that suggests 20.65 days or 496 hours",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "336.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "134077.42688552555",
      "Compute cost notes": "",
      "Training power draw (W)": "209159.05867416784",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "models, non-commercial: https://github.com/facebookresearch/WSL-Images ",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "229613.15707826085",
      "Training compute cost (upfront)": "8015194.444444443"
    },
    {
      "Model": "YOLOv3",
      "Organization": "University of Washington",
      "Publication date": "2018-04-08",
      "Domain": "Vision",
      "Task": "Object detection",
      "Parameters": "56933216.0",
      "Parameters notes": "Feature extractor (ignoring biases)\n32*3*3*3 +\n64*3*3*32 +\n32*1*1*64 +\n64*3*3*32 +\n128*3*3*64 +\n2*(64*1*1*128 +\n128*3*3*64) +\n256*3*3*128 +\n8*(128*1*1*256 +\n256*3*3*128) +\n512*3*3*256 + \n8*(256*1*1*512 + \n512*3*3*256) + \n1024*3*3*512 + \n4*(512*1*1*1024 +\n1024*3*3*512) +\n4*4*1024*1000\n\nsource: table 1\nThis is assuming the average pooling step changes the output size from 8x8 to 4x4.\n\nThe weights file is 237MB. If the weights are saved as float32, 4 bytes per weight, then there are approximately 237M/4=59M parameters, consistent with the calculation above.",
      "Training compute (FLOP)": "1.3416380824e+19",
      "Training compute notes": "We use the formula training_compute = ops_per_forward_pass * 3.5 * n_epochs * n_examples\n\nAssuming 160 epochs of training as in https://arxiv.org/pdf/1612.08242.pdf\n\nTable 2: 18700000000 operations \n\n18700000000 ops * 3.5 *160 epochs * 1281167 images",
      "Training dataset": "ImageNet",
      "Training dataset size (gradients)": "5430000",
      "Dataset size notes": "Source: https://image-net.org/download.php",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/1804.02767",
      "Reference": "YOLOv3: An Incremental Improvement",
      "Citations": "23936.0",
      "Authors": "Joseph Redmon, Ali Farhadi",
      "Abstract": "We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at this https URL",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA M40,NVIDIA GeForce GTX TITAN X",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "code and weights, unclear license: https://pjreddie.com/darknet/yolo/ ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "4 layer QRNN (h=2500)",
      "Organization": "Salesforce Research",
      "Publication date": "2018-03-22",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "151000000.0",
      "Parameters notes": "Table 6",
      "Training compute (FLOP)": "5.9158815e+17",
      "Training compute notes": "20670000000000 FLOP / sec / GPU [fp16 assumed] * 1 GPU * 12 hours * 3600 sec / hour * 0.3 [assumed utilization] = 2.678832e+17FLOP\n\n6 FLOP / token / parameter * 151000000 parameters * 103000000 tokens * 14 epochs = 1.306452e+18 FLOP\n\nsqrt(2.678832e+17*1.306452e+18) = 5.9158815e+17 FLOP\n__________________\nin the algorithmic progress paper the estimation was 2.4 \u00d7 10^17 FLOP under assumption of 26M parameters",
      "Training dataset": "WikiText-103",
      "Training dataset size (gradients)": "103000000",
      "Dataset size notes": "\"The model was trained for 12 hours (14 epochs)\"",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/1803.08240",
      "Reference": "An Analysis of Neural Language Modeling at Multiple Scales",
      "Citations": "183.0",
      "Authors": "Stephen Merity, Nitish Shirish Keskar, Richard Socher",
      "Abstract": "Many of the leading approaches in language modeling introduce novel, complex and specialized architectures. We take existing state-of-the-art word level language models based on LSTMs and QRNNs and extend them to both larger vocabularies as well as character-level granularity. When properly tuned, LSTMs and QRNNs achieve state-of-the-art results on character-level (Penn Treebank, enwik8) and word-level (WikiText-103) datasets, respectively. Results are obtained in only 12 hours (WikiText-103) to 2 days (enwik8) using a single modern GPU.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"QRNNs achieve stateof-the-art results on character-level (Penn Treebank, enwik8) and word-level (WikiText-103)\ndatasets, respectively\"",
      "Epochs": "14.0",
      "Training time (hours)": "12.0",
      "Training time notes": "\"Results are obtained in only 12 hours (WikiText-103) to 2 days (enwik8) using a single modern GPU\"",
      "Training hardware": "NVIDIA Quadro GP100",
      "Hardware quantity": "1.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "268.1693571105067",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "BSD-3 license: https://github.com/salesforce/awd-lstm-lm ",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Chinese - English translation",
      "Organization": "Microsoft",
      "Publication date": "2018-03-01",
      "Domain": "Language",
      "Task": "Translation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://www.microsoft.com/en-us/research/publication/achieving-human-parity-on-automatic-chinese-to-english-news-translation/",
      "Reference": "Achieving Human Parity on Automatic Chinese to English News Translation",
      "Citations": "623.0",
      "Authors": "H Hassan, A Aue, C Chen, V Chowdhary",
      "Abstract": "Machine translation has made rapid advances in recent years. Millions of people are using it today in online translation systems and mobile applications in order to communicate across language barriers. The question naturally arises whether such systems can approach or achieve parity with human translations. In this paper, we first address the problem of how to define and accurately measure human parity in translation. We then describe Microsoft\u2019s machine translation system and measure the quality of its translations on the widely used WMT 2017 news translation task from Chinese to English. We find that our latest neural machine translation system has reached a new state-of-the-art, and that the translation quality is at human parity when compared to professional human translations. We also find that it significantly exceeds the quality of crowd-sourced non-professional translations.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We find that our latest neural machine translation system has reached a new state-of-the-art, and that the translation quality is at human parity when compared to professional human translations\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "TCN (P-MNIST)",
      "Organization": "Carnegie Mellon University (CMU),Intel Labs",
      "Publication date": "2018-02-15",
      "Domain": "Vision",
      "Task": "Image classification,Digit recognition",
      "Parameters": "42000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "P-MNIST",
      "Training dataset size (gradients)": "60000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://openreview.net/forum?id=rk8wKk-R-",
      "Reference": "Convolutional Sequence Modeling Revisited",
      "Citations": "5796.0",
      "Authors": "Shaojie Bai, J. Zico Kolter, Vladlen Koltun",
      "Abstract": "This paper revisits the problem of sequence modeling using convolutional \narchitectures.  Although both convolutional and recurrent architectures have a\nlong history in sequence prediction, the current \"default\" mindset in much of\nthe deep learning community is that generic sequence modeling is best handled using recurrent networks.  The goal of this paper is to question this assumption. \nSpecifically, we consider a simple generic temporal convolution network (TCN), which adopts features from modern ConvNet architectures such as a dilations and residual connections.  We show that on a variety of sequence modeling tasks, including many frequently used as benchmarks for evaluating recurrent networks, the TCN outperforms baseline RNN methods (LSTMs, GRUs, and vanilla RNNs) and sometimes even highly specialized approaches.  We further show that the potential \"infinite memory\" advantage that RNNs have over TCNs is largely absent in practice: TCNs indeed exhibit longer effective history sizes than their recurrent counterparts.   As a whole, we argue that it may be time to (re)considerConvNets as the default \"go to\" architecture for sequence modeling.",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"For the permuted sequential MNIST, TCNs outperform state of the art results using recurrent nets (95.9%) with Zoneout+Recurrent BatchNorm (Cooijmans et al., 2016; Krueger et al., 2017), a highly optimized method for regularizing RNNs\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DeepLabV3+",
      "Organization": "Google",
      "Publication date": "2018-02-07",
      "Domain": "Vision",
      "Task": "Semantic segmentation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "ImageNet-1k,COCO,JFT-300M",
      "Training dataset size (gradients)": "8655843074",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1802.02611v3",
      "Reference": "Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation",
      "Citations": "15217.0",
      "Authors": "Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, Hartwig Adam",
      "Abstract": "Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89.0\\% and 82.1\\% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at \\url{this https URL}.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "IMPALA",
      "Organization": "DeepMind",
      "Publication date": "2018-02-05",
      "Domain": "Games",
      "Task": "Atari",
      "Parameters": "1600000.0",
      "Parameters notes": "\"Figure 3 in the paper states that the large architecture has 1.6 million parameters. I am using the large model because it was the only one trained on all the Atari games at once, which seems like the most impressive task in the suite.\"\n\nSource: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",
      "Training compute (FLOP)": "1.68e+20",
      "Training compute notes": "Source: Ajeya Cotra and Tom Davidson, https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",
      "Training dataset": "",
      "Training dataset size (gradients)": "11400000000",
      "Dataset size notes": "From fig 6, there were 1e10 environment frames, and 24 agents. Thus we note down 2.4e11 for the \"dataset size\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1802.01561",
      "Reference": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures",
      "Citations": "1730.0",
      "Authors": "Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, Koray Kavukcuoglu",
      "Abstract": "In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "not an absolute SOTA\n\n\"IMPALA is able to achieve better performance than previous agents with less data\"\n\n\"We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data\"",
      "Epochs": "",
      "Training time (hours)": "100.0",
      "Training time notes": "Maximum training time for IMPALA is 100 hours according to Figure 6. This seems to refer to the 1 GPU model. The 8 GPU model looks to have been trained about 1/8 as long.",
      "Training hardware": "NVIDIA P100",
      "Hardware quantity": "1.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "53.42846804494412",
      "Compute cost notes": "",
      "Training power draw (W)": "285.5725852593349",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "training code, Apache license: https://github.com/google-deepmind/scalable_agent ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "6032.510382910857",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "QRNN",
      "Organization": "Salesforce Research",
      "Publication date": "2018-02-01",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "135000000.0",
      "Parameters notes": "Based on the details provided in the paper, the number of parameters in the QRNN model can be calculated as follows:\n\nEmbedding layer size: 400\nNumber of QRNN layers: 4\nNumber of nodes per QRNN layer: 2500\nVocabulary size: 267,735 (for WikiText-103 dataset)\nAdaptive softmax layer\nParameters:\n\nEmbedding layer: 400 x 267,735 = 107,094,000\nQRNN layers:\nInput to hidden weights per layer: 400 x 2500 = 1,000,000\nHidden to hidden weights per layer: 2500 x 2500 = 6,250,000\nBiases per layer: 2500 = 2,500\nTotal QRNN layers parameters: 4 x (1,000,000 + 6,250,000 + 2,500) = 28,000,000\nAdaptive softmax layer: No extra parameters due to weight tying\nTotal Parameters = Embedding + QRNN layers\n= 107,094,000 + 28,000,000\n= 135,094,000\n\nSo the total number of parameters in the QRNN model is approximately 135 million.\n\nThe majority of parameters are in the embedding layer, while the 4 QRNN layers contribute 28 million parameters. The adaptive softmax does not add any extra parameters due to weight tying.",
      "Training compute (FLOP)": "6.8866472e+17",
      "Training compute notes": "6 FLOP / parameter / token * 135000000 parameters ['Likely' confidence] * 103000000 tokens * 14 epochs = 1.16802e+18 FLOP\n\n31330000000000 FLOP / sec / GPU [fp16 assumed] * 1 GPU * 12 hours * 3600 sec / hour * 0.3 [assumed utilization] = 4.060368e+17 FLOP \n\nsqrt(4.060368e+17*1.16802e+18) = 6.8866472e+17 FLOP \n",
      "Training dataset": "WikiText-103",
      "Training dataset size (gradients)": "103000000",
      "Dataset size notes": "\"We train with a batch size of 60 and a sequence length of 140.\"\n\n\"\"Using this approach we reduce our per-epoch time substantially and achieve a new state-of-the-art on WikiText-103 despite training for 14 epochs, a total time of only 12 hours.\"",
      "Confidence": "Likely",
      "Link": "https://mlsys.org/Conferences/doc/2018/50.pdf",
      "Reference": "Scalable Language Modeling: WikiText-103 on a Single GPU in 12 hours",
      "Citations": "5.0",
      "Authors": "Stephen Merity, Nitish Shirish Keskar, James Bradbury, Richard Socher",
      "Abstract": "Word-level language modeling (WLM) is one the foundational tasks of unsupervised natural language processing. Most modern architectures for WLM use several LSTM layers, followed by a softmax layer. Even with larger batch sizes and a multi-GPU setup, training of these networks on large-vocabulary corpora is slow due to increased computation involving the softmax and the high cost of recurrence computation. We propose a model architecture and training strategy that enables us to achieve state-of-the-art performance on the WikiText-103 data set using a single GPU while being substantially faster than an NVIDIA cuDNN LSTM-based model by utilizing the Quasi-Recurrent Neural Network (QRNN), an adaptive softmax with weight tying, and longer sequences within batches.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"we reduce our per-epoch time substantially and achieve a new state-of-the-art on WikiText-103 despite training for 14 epochs\"",
      "Epochs": "14.0",
      "Training time (hours)": "12.0",
      "Training time notes": "\"The model trains at 2980 seconds per epoch on the NVIDIA V100\nand 5460 seconds per epoch on the NVIDIA P100.\"\n\n\"Using this approach we reduce our per-epoch time substantially and achieve a new state-of-the-art on WikiText-103 despite training for 14 epochs, a total time of only 12 hours.\"",
      "Training hardware": "NVIDIA V100",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "8400.0",
      "Batch size notes": "60*140",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ELMo",
      "Organization": "University of Washington,Allen Institute for AI",
      "Publication date": "2018-02-01",
      "Domain": "Language",
      "Task": "Question answering,Sentiment classification,Language modeling",
      "Parameters": "94000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "3300100000000010.0",
      "Training compute notes": "3300e12 - https://github.com/amirgholami/ai_and_memory_wall",
      "Training dataset": "",
      "Training dataset size (gradients)": "2000000000",
      "Dataset size notes": "",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/1802.05365",
      "Reference": "Deep contextualized word representations",
      "Citations": "11949.0",
      "Authors": "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer",
      "Abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
      "Organization categorization": "Academia,Research collective",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "T-DMCA",
      "Organization": "Google Brain",
      "Publication date": "2018-01-30",
      "Domain": "Language",
      "Task": "Language modeling,Text summarization",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "WikiSum",
      "Training dataset size (gradients)": "14000000000",
      "Dataset size notes": "10^6 examples (= articles with crawlable citations)\n\"We divide the articles roughly into 80/10/10 for train/development/test subsets, resulting in 1865750, 233252, and 232998 examples respectively\"\n\nfor best performing transformer L = 7500 (sequence length)\n\nnumber of epochs is unknown\n\n1865750 articles * 7500 ~ 14B tokens per epoch",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/1801.10198",
      "Reference": "Generating Wikipedia by Summarizing Long Sequences",
      "Citations": "",
      "Authors": "Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, Noam Shazeer",
      "Abstract": "We show that generating English Wikipedia articles can be approached as a multi- document summarization of source documents. We use extractive summarization to coarsely identify salient information and a neural abstractive model to generate the article. For the abstractive model, we introduce a decoder-only architecture that can scalably attend to very long sequences, much longer than typical encoder- decoder architectures used in sequence transduction. We show that this model can generate fluent, coherent multi-sentence paragraphs and even whole Wikipedia articles. When given reference documents, we show it can extract relevant factual information as reflected in perplexity, ROUGE scores and human evaluations.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "the first decoder-only transformer model",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "\"our machines equipped with 16GB of GPU RAM (NVIDIA P100)\"",
      "Training hardware": "NVIDIA P100 PCIe 16GB",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "",
      "Accessibility notes": "Apache 2.0\nhttps://github.com/tensorflow/tensor2tensor/",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "AlphaZero",
      "Organization": "DeepMind",
      "Publication date": "2017-12-05",
      "Domain": "Games",
      "Task": "Chess,Shogi,Go",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "1.0600000001e+20",
      "Training compute notes": "The Go version (there were versions trained individually for three different games) was trained for 34 hours using\n5k TPUv1 for data generation\n64 TPUv2 for parameter updating\nI used the PCD chip entries for the lowest precision numerical format we had for each, which was INT8 for TPUv1 and FP16 for TPUv2\n\nDirect training compute: 1.06e20 FLOPs",
      "Training dataset": "",
      "Training dataset size (gradients)": "3520000000",
      "Dataset size notes": "\"We trained a separate instance of AlphaZero for each game. Training proceeded for 700,000 steps\"",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/1712.01815",
      "Reference": "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm",
      "Citations": "1968.0",
      "Authors": "David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, Demis Hassabis",
      "Abstract": "The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Highly cited,Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "24.0",
      "Training time notes": "",
      "Training hardware": "Google TPU v2,Google TPU v1",
      "Hardware quantity": "5064.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "229918.6146969874",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DL scaling speech",
      "Organization": "Baidu",
      "Publication date": "2017-12-01",
      "Domain": "Speech",
      "Task": "Speech recognition (ASR)",
      "Parameters": "193000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "2149200000",
      "Dataset size notes": "8M utterances, 11940 hours, trained for up to 2048h\nEstimated number of words: 11940*120*60=85968000\nTraining size: 2048/11940=0.172\nTraining words: 85968000*0.172=14786496",
      "Confidence": "Likely",
      "Link": "https://www.semanticscholar.org/paper/Deep-Learning-Scaling-is-Predictable%2C-Empirically-Hestness-Narang/a1c922be467d1c0c64b963e65dae41778b81b2a0",
      "Reference": "Deep Learning Scaling is Predictable, Empirically",
      "Citations": "",
      "Authors": "Joel Hestness, Sharan Narang, Newsha Ardalani, G. Diamos, Heewoo Jun, Hassan Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, Yanqi Zhou",
      "Abstract": "Deep learning (DL) creates impactful advances following a virtuous recipe: model architecture search, creating large training data sets, and scaling computation. It is widely believed that growing training sets and models should improve accuracy and result in better products. As DL application domains grow, we would like a deeper understanding of the relationships between training set size, computational scale, and model accuracy improvements to advance the state-of-the-art. This paper presents a large scale empirical characterization of generalization error and model size growth as training sets grow. We introduce a methodology for this measurement and test four machine learning domains: machine translation, language modeling, image processing, and speech recognition. Our empirical results show power-law generalization error scaling across a breadth of factors, resulting in power-law exponents---the \"steepness\" of the learning curve---yet to be explained by theoretical work. Further, model improvements only shift the error but do not appear to affect the power-law exponent. We also show that model size scales sublinearly with data size. These scaling relationships have significant implications on deep learning research, practice, and systems. They can assist model debugging, setting accuracy targets, and decisions about data set growth. They can also guide computing system design and underscore the importance of continued computational scaling.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DL scaling LM",
      "Organization": "Baidu",
      "Publication date": "2017-12-01",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "177000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "One Billion Word benchmark",
      "Training dataset size (gradients)": "400000000",
      "Dataset size notes": " We train the models on shards ranging from\n0.1% up to 40% of the Billion Word Dataset.\n",
      "Confidence": "Speculative",
      "Link": "https://www.semanticscholar.org/paper/Deep-Learning-Scaling-is-Predictable%2C-Empirically-Hestness-Narang/a1c922be467d1c0c64b963e65dae41778b81b2a0",
      "Reference": "Deep Learning Scaling is Predictable, Empirically",
      "Citations": "",
      "Authors": "Joel Hestness, Sharan Narang, Newsha Ardalani, G. Diamos, Heewoo Jun, Hassan Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, Yanqi Zhou",
      "Abstract": "Deep learning (DL) creates impactful advances following a virtuous recipe: model architecture search, creating large training data sets, and scaling computation. It is widely believed that growing training sets and models should improve accuracy and result in better products. As DL application domains grow, we would like a deeper understanding of the relationships between training set size, computational scale, and model accuracy improvements to advance the state-of-the-art. This paper presents a large scale empirical characterization of generalization error and model size growth as training sets grow. We introduce a methodology for this measurement and test four machine learning domains: machine translation, language modeling, image processing, and speech recognition. Our empirical results show power-law generalization error scaling across a breadth of factors, resulting in power-law exponents---the \"steepness\" of the learning curve---yet to be explained by theoretical work. Further, model improvements only shift the error but do not appear to affect the power-law exponent. We also show that model size scales sublinearly with data size. These scaling relationships have significant implications on deep learning research, practice, and systems. They can assist model debugging, setting accuracy targets, and decisions about data set growth. They can also guide computing system design and underscore the importance of continued computational scaling.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DL scaling Image",
      "Organization": "Baidu",
      "Publication date": "2017-12-01",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "121000000.0",
      "Parameters notes": "We test models with parameter counts ranging from 89K to 121M.",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "ImageNet",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "We train and validate ResNets on various shard sizes of ImageNet, ranging from 1 image per class (0.08% of images) up to 800 images per class (62%). ImageNet has 1,000 different object classes as outputs",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/Deep-Learning-Scaling-is-Predictable%2C-Empirically-Hestness-Narang/a1c922be467d1c0c64b963e65dae41778b81b2a0",
      "Reference": "Deep Learning Scaling is Predictable, Empirically",
      "Citations": "",
      "Authors": "Joel Hestness, Sharan Narang, Newsha Ardalani, G. Diamos, Heewoo Jun, Hassan Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, Yanqi Zhou",
      "Abstract": "Deep learning (DL) creates impactful advances following a virtuous recipe: model architecture search, creating large training data sets, and scaling computation. It is widely believed that growing training sets and models should improve accuracy and result in better products. As DL application domains grow, we would like a deeper understanding of the relationships between training set size, computational scale, and model accuracy improvements to advance the state-of-the-art. This paper presents a large scale empirical characterization of generalization error and model size growth as training sets grow. We introduce a methodology for this measurement and test four machine learning domains: machine translation, language modeling, image processing, and speech recognition. Our empirical results show power-law generalization error scaling across a breadth of factors, resulting in power-law exponents---the \"steepness\" of the learning curve---yet to be explained by theoretical work. Further, model improvements only shift the error but do not appear to affect the power-law exponent. We also show that model size scales sublinearly with data size. These scaling relationships have significant implications on deep learning research, practice, and systems. They can assist model debugging, setting accuracy targets, and decisions about data set growth. They can also guide computing system design and underscore the importance of continued computational scaling.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "TriNet",
      "Organization": "Visual Computing Institute,RWTH Aachen University",
      "Publication date": "2017-11-21",
      "Domain": "Video",
      "Task": "Person re-identification",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "509914",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1703.07737",
      "Reference": "In Defense of the Triplet Loss for Person Re-Identification",
      "Citations": "3428.0",
      "Authors": "Alexander Hermans, Lucas Beyer, Bastian Leibe",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "Germany,Germany",
      "Notability criteria": "Highly cited,Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "AWD-LSTM-MoS + dynamic evaluation (WT2, 2017)",
      "Organization": "Carnegie Mellon University (CMU)",
      "Publication date": "2017-11-10",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "35000000.0",
      "Parameters notes": "35M (Table 2)",
      "Training compute (FLOP)": "3.36e+18",
      "Training compute notes": "6 FLOP / parameter / token * 35000000 parameters * 2000000 tokens * 8000 epochs = 3.36e+18 FLOP \n\n_____________\nin the algorithmic progress paper the estimation was 4.37 \u00d7 10^17 FLOP based on 1000 epochs assumption",
      "Training dataset": "WikiText-2",
      "Training dataset size (gradients)": "2000000",
      "Dataset size notes": "max sequence length from https://github.com/zihangdai/mos/blob/master/main.py \nis 110 tokens \nbatch size 15 (table 8)\nepochs: 8000 (from https://github.com/zihangdai/mos/blob/master/main.py)\n\n",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/1711.03953",
      "Reference": "Breaking the Softmax Bottleneck: A High-Rank RNN Language Model",
      "Citations": "398.0",
      "Authors": "Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, William W. Cohen",
      "Abstract": "We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively. The proposed method also excels on the large-scale 1B Word dataset, outperforming the baseline by over 5.6 points in perplexity.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Experimental results confirm that the\nproposed method significantly improves state-of-the-art language models, achieving a perplexity of 55.31 and 62.89 on the test set of Penn Treebank and WikiText-2\"",
      "Epochs": "8000.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT code: https://github.com/zihangdai/mos ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "VQ-VAE",
      "Organization": "DeepMind",
      "Publication date": "2017-11-02",
      "Domain": "Vision",
      "Task": "Representation learning",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "CIFAR-10",
      "Training dataset size (gradients)": "62914560000",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1711.00937",
      "Reference": "Neural Discrete Representation Learning",
      "Citations": "",
      "Authors": "A\u00e4ron van den Oord, O. Vinyals, K. Kavukcuoglu",
      "Abstract": "Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of \"posterior collapse\" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "",
      "Accessibility notes": "Apache 2.0\nhttps://github.com/google-deepmind/sonnet/blob/v1/sonnet/python/modules/nets/vqvae.py",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Fraternal dropout + AWD-LSTM 3-layer (WT2)",
      "Organization": "Jagiellonian University,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),University of Montreal / Universit\u00e9 de Montr\u00e9al",
      "Publication date": "2017-10-31",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "34000000.0",
      "Parameters notes": "34M (Table 2)",
      "Training compute (FLOP)": "3.06e+17",
      "Training compute notes": "6 FLOP / token / parameter * 34000000 parameters * 2000000 tokens * 750 epochs = 3.06e+17 FLOP\n\n_________________\nIn the Algorithmic Progress paper, the compute was estimated to be 9.85 \u00d7 10\u00b9\u2076 FLOP, assuming 520 epochs reported for the PTB dataset.",
      "Training dataset": "WikiText-2",
      "Training dataset size (gradients)": "2000000",
      "Dataset size notes": "code suggests 750 epochs\nhttps://github.com/kondiz/fraternal-dropout/blob/WT2/main.py",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/1711.00066",
      "Reference": "Fraternal Dropout",
      "Citations": "55.0",
      "Authors": "Konrad Zolna, Devansh Arpit, Dendi Suhubdy, Yoshua Bengio",
      "Abstract": "Recurrent neural networks (RNNs) are important class of architectures among neural networks useful for language modeling and sequential prediction. However, optimizing RNNs is known to be harder compared to feed-forward neural networks. A number of techniques have been proposed in literature to address this problem. In this paper we propose a simple technique called fraternal dropout that takes advantage of dropout to achieve this goal. Specifically, we propose to train two identical copies of an RNN (that share parameters) with different dropout masks while minimizing the difference between their (pre-softmax) predictions. In this way our regularization encourages the representations of RNNs to be invariant to dropout mask, thus being robust. We show that our regularization term is upper bounded by the expectation-linear dropout objective which has been shown to address the gap due to the difference between the train and inference phases of dropout. We evaluate our model and achieve state-of-the-art results in sequence modeling tasks on two benchmark datasets - Penn Treebank and Wikitext-2. We also show that our approach leads to performance improvement by a significant margin in image captioning (Microsoft COCO) and semi-supervised (CIFAR-10) tasks.",
      "Organization categorization": "Academia,Academia,Academia",
      "Country (of organization)": "Poland,Canada,Canada",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We evaluate our model and achieve state-of-the-art results in sequence modeling tasks on two benchmark datasets \u2013 Penn Treebank and Wikitext-2\"",
      "Epochs": "750.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "BSD-3 license: https://github.com/kondiz/fraternal-dropout ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DCN+",
      "Organization": "Salesforce Research",
      "Publication date": "2017-10-31",
      "Domain": "Language",
      "Task": "Question answering",
      "Parameters": "",
      "Parameters notes": "Not directly repoted - It may be possible to extract number from:\nhttps://github.com/lmn-extracts/dcn_plus/tree/master/question_answering",
      "Training compute (FLOP)": "",
      "Training compute notes": "in Figure 4 we see that network was trained on 140k iterations\nfrom https://github.com/lmn-extracts/dcn_plus/tree/master we see that batch size is 64\nIt should be possible to compute inference FLOPs from repository and estimate training compute",
      "Training dataset": "SQuAD",
      "Training dataset size (gradients)": "215570",
      "Dataset size notes": "from https://paperswithcode.com/dataset/squad SQuAD have 107,785 question-answer pairs\ndownload-ed dataset from: https://www.kaggle.com/datasets/stanfordu/stanford-question-answering-dataset?resource=download\nwc -w on train-v.1.1 returns 4017471 words so around 5.4M tokens\n\nLooks like they probably trained on each token in SQuAD rather than QA pairs, but uncertain.",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1711.00106v2",
      "Reference": "DCN+: Mixed Objective and Deep Residual Coattention for Question Answering",
      "Citations": "121.0",
      "Authors": "Caiming Xiong, Victor Zhong, Richard Socher",
      "Abstract": "Traditional models for question answering optimize using cross entropy loss, which encourages exact answers at the cost of penalizing nearby or overlapping answers that are sometimes equally accurate. We propose a mixed objective that combines cross entropy loss with self-critical policy learning. The objective uses rewards derived from word overlap to solve the misalignment between evaluation metric and optimization objective. In addition to the mixed objective, we improve dynamic coattention networks (DCN) with a deep residual coattention encoder that is inspired by recent work in deep self-attention and residual networks. Our proposals improve model performance across question types and input lengths, especially for long questions that requires the ability to capture long-term dependencies. On the Stanford Question Answering Dataset, our model achieves state-of-the-art results with 75.1% exact match accuracy and 83.1% F1, while the ensemble obtains 78.9% exact match accuracy and 86.0% F1. ",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"On the Stanford Question Answering Dataset, our model achieves state-of-the-art results with 75.1% exact match accuracy and 83.1% F1, while the ensemble obtains 78.9% exact match accuracy and 86.0% F1. \"\nhttps://paperswithcode.com/paper/dcn-mixed-objective-and-deep-residual\n",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "S-Norm",
      "Organization": "University of Washington,Allen Institute for AI",
      "Publication date": "2017-10-29",
      "Domain": "Language",
      "Task": "Question answering",
      "Parameters": "",
      "Parameters notes": "Not stated. Probably obtainable from github: https://github.com/allenai/document-qa/tree/master",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "TriviaQA",
      "Training dataset size (gradients)": "1060000",
      "Dataset size notes": "\"530k question-document training pairs\"\n\naverage question length of 14 words and document length of 2895 words, per\n https://www.cs.utexas.edu/~eunsol/files/papers/acl17jcwz.pdf\n\n530,000 * 2895 words on average * 1.33 tokens/word = ~2,000,000,000",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1710.10723v2",
      "Reference": "Simple and Effective Multi-Paragraph Reading Comprehension",
      "Citations": "479.0",
      "Authors": "Christopher Clark, Matt Gardner",
      "Abstract": "We consider the problem of adapting neural paragraph-level question answering models to the case where entire documents are given as input. Our proposed solution trains models to produce well calibrated confidence scores for their results on individual paragraphs. We sample multiple paragraphs from the documents during training, and use a shared-normalization training objective that encourages the model to produce globally correct output. We combine this method with a state-of-the-art pipeline for training models on document QA data. Experiments demonstrate strong performance on several document QA datasets. Overall, we are able to achieve a score of 71.3 F1 on the web portion of TriviaQA, a large improvement from the 56.7 F1 of the previous best system.",
      "Organization categorization": "Academia,Research collective",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Overall, we are able to achieve a score of 71.3 F1 on the web portion of TriviaQA, a large improvement from the 56.7 F1 of the previous best system.\"",
      "Epochs": "1.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "PhraseCond",
      "Organization": "Carnegie Mellon University (CMU),University of Pittsburgh",
      "Publication date": "2017-10-28",
      "Domain": "Language",
      "Task": "Question answering",
      "Parameters": "",
      "Parameters notes": "Unclear how many layers they use for self-attention (N) and fusion (L and K). Could calculate if these were known.",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "SQuAD 1.1",
      "Training dataset size (gradients)": "160000",
      "Dataset size notes": "10% held out for test, so 100k * 0.9 = 90k",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1710.10504v2",
      "Reference": "Phase Conductor on Multi-layered Attentions for Machine Comprehension",
      "Citations": "22.0",
      "Authors": "Rui Liu, Wei Wei, Weiguang Mao, Maria Chikina",
      "Abstract": "Attention models have been intensively studied to improve NLP tasks such as machine comprehension via both question-aware passage attention model and self-matching attention model. Our research proposes phase conductor (PhaseCond) for attention models in two meaningful ways. First, PhaseCond, an architecture of multi-layered attention models, consists of multiple phases each implementing a stack of attention layers producing passage representations and a stack of inner or outer fusion layers regulating the information flow. Second, we extend and improve the dot-product attention function for PhaseCond by simultaneously encoding multiple question and passage embedding layers from different perspectives. We demonstrate the effectiveness of our proposed model PhaseCond on the SQuAD dataset, showing that our model significantly outperforms both state-of-the-art single-layered and multiple-layered attention models. We deepen our results with new findings via both detailed qualitative analysis and visualized examples showing the dynamic changes through multi-layered attention models.",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We demonstrate the effectiveness of our proposed model PhaseCond on the SQuAD dataset, showing that our model significantly outperforms both state-of-the-art single-layered and multiple-layered attention models.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ProgressiveGAN",
      "Organization": "NVIDIA",
      "Publication date": "2017-10-27",
      "Domain": "Vision",
      "Task": "Image generation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1710.10196",
      "Reference": "Progressive Growing of GANs for Improved Quality, Stability, and Variation",
      "Citations": "8099.0",
      "Authors": "Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "AlphaGo Master",
      "Organization": "DeepMind",
      "Publication date": "2017-10-19",
      "Domain": "Games",
      "Task": "Go",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "3.4100000001e+20",
      "Training compute notes": "This is a guess. There was no single journal publication that accompanied this model, that gave information about architecture/model training time etc. All I could find was that it has the same architecture as AlphaGo Zero, and that it had roughly the same power consumption as AGZ. See for instance: \nhttps://deepmind.com/blog/article/alphago-zero-starting-scratch\n\nSince AGZ reaches the ELO of AlphaGo Master in about 25-30 days (60-75% of the total training time), I estimate the compute to be around 60-75% that of AGZ. I round this to 2e23, and I expect this to only be accurate within an OOM.",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://www.nature.com/articles/nature24270",
      "Reference": "Mastering the game of Go without human knowledge",
      "Citations": "10021.0",
      "Authors": "D Silver, J Schrittwieser, K Simonyan, I Antonoglou",
      "Abstract": "A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo. \u00a9 2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "72.0",
      "Training time notes": "\"Training started from completely random behaviour and continued without human intervention for approximately three days.\"",
      "Training hardware": "Google TPU v1",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "471445.3247973037",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "AlphaGo Zero",
      "Organization": "DeepMind",
      "Publication date": "2017-10-18",
      "Domain": "Games",
      "Task": "Go",
      "Parameters": "46400244.0",
      "Parameters notes": "Input size: 19*19*17=6137, internal dimension 19*19\n1 conv block\n17*3*3*256=39168\n39 residual blocks\n2*3*3*256*256=1179648\nPolicy head\n256*2+2*19*19*(19*19+2)=262598\nValue head\n256*1+1*19*19*256+256*1=92928\nTotal: 39168+39*1179648+262598+92928=46400966",
      "Training compute (FLOP)": "6.49439910290227e+20",
      "Training compute notes": "Updating this compute estimate to only account for direct training compute, not synthetic data generation compute.\n\nNumber of connections (to calculate forward FLOP)\nInput size: 19*19*17, Assuming internal dimension stays at 19*19\n1 conv block\n19*19*17*3*3*256=14139648\n40 residual blocks\n19*19*2*3*3*256*256=425852928\nPolicy head\n19*19*256*2+2*19*19*(19*19+2)=446918\nValue head\n19*19*256*1+1*19*19*256+256*1=185088\nTotal: 14139648+40*425852928+446918+185088=17048888774\nForward FLOP: 2*17048888774=34097777548\nParameter updates \u201cParameters were updated from 3.1 million mini-batches of 2,048 positions each.\u201d\nTotal updates: 3100000*2048=6348800000\n\nTraining compute: 3 * 34097777548 FLOP * 6348800000 = 6.5e20 FLOP",
      "Training dataset": "",
      "Training dataset size (gradients)": "6348800000",
      "Dataset size notes": "\"Over the course of training, 29 million games of self-play were generated\"\n\nApprox 200 moves per Go game on average\n\nhttps://homepages.cwi.nl/~aeb/go/misc/gostat.html\n\nThus 200 * 29e6 = 5.8e9",
      "Confidence": "Confident",
      "Link": "https://www.nature.com/articles/nature24270",
      "Reference": "Mastering the game of Go without human knowledge",
      "Citations": "10021.0",
      "Authors": "D Silver, J Schrittwieser, K Simonyan, I Antonoglou",
      "Abstract": "A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo. \u00a9 2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "480.0",
      "Training time notes": "",
      "Training hardware": "Google TPU v1",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "1213.8707315707268",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Rainbow DQN",
      "Organization": "DeepMind",
      "Publication date": "2017-10-06",
      "Domain": "Games",
      "Task": "Atari",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1710.02298",
      "Reference": "Rainbow: Combining Improvements in Deep Reinforcement Learning",
      "Citations": "",
      "Authors": "Matteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, David Silver",
      "Abstract": "The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance.\"",
      "Epochs": "",
      "Training time (hours)": "240.0",
      "Training time notes": "Single GPU; ~10 hours to reach DQN\u2019s final score (7M frames), ~10 days for full 200M frames run",
      "Training hardware": "",
      "Hardware quantity": "1.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "AWD-LSTM+WT+Cache+IOG (WT2)",
      "Organization": "NTT Communication Science Laboratories",
      "Publication date": "2017-09-26",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "53000000.0",
      "Parameters notes": "53M (Table 3)",
      "Training compute (FLOP)": "3180000000000000.0",
      "Training compute notes": "6 FLOP / parameter / token * 53000000 parameters * 2000000 tokens * 5 epochs = 3.18e+15 FLOP",
      "Training dataset": "WikiText-2",
      "Training dataset size (gradients)": "2000000",
      "Dataset size notes": "5 epochs (Table 1)",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1709.08907",
      "Reference": "Input-to-Output Gate to Improve RNN Language Models",
      "Citations": "7.0",
      "Authors": "Sho Takase, Jun Suzuki, Masaaki Nagata",
      "Abstract": "This paper proposes a reinforcing method that refines the output layers of existing Recurrent Neural Network (RNN) language models. We refer to our proposed method as Input-to-Output Gate (IOG). IOG has an extremely simple structure, and thus, can be easily combined with any RNN language models. Our experiments on the Penn Treebank and WikiText-2 datasets demonstrate that IOG consistently boosts the performance of several different types of current topline RNN language models.",
      "Organization categorization": "Industry",
      "Country (of organization)": "Japan",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"IOG achieves comparable scores to the state-of-the-art on the Penn Treebank dataset and outperforms the WikiText-2 dataset\"",
      "Epochs": "5.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "license, looks non-commercial: https://github.com/nttcslab-nlp/iog?tab=License-1-ov-file#readme \n\nhttps://github.com/nttcslab-nlp/iog ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "LSTM + dynamic eval",
      "Organization": "University of Edinburgh",
      "Publication date": "2017-09-21",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "50000000.0",
      "Parameters notes": "table 2",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "WikiText-2",
      "Training dataset size (gradients)": "90000000",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/1709.07432",
      "Reference": "Dynamic Evaluation of Neural Sequence Models",
      "Citations": "143.0",
      "Authors": "Ben Krause, Emmanuel Kahembwe, Iain Murray, Steve Renals",
      "Abstract": "We present methodology for using dynamic evaluation to improve neural sequence models. Models are adapted to recent history via a gradient descent based mechanism, causing them to assign higher probabilities to re-occurring sequential patterns. Dynamic evaluation outperforms existing adaptation approaches in our comparisons. Dynamic evaluation improves the state-of-the-art word-level perplexities on the Penn Treebank and WikiText-2 datasets to 51.1 and 44.3 respectively, and the state-of-the-art character-level cross-entropies on the text8 and Hutter Prize datasets to 1.19 bits/char and 1.08 bits/char respectively.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Dynamic evaluation outperforms existing adaptation approaches in our comparisons. Dynamic evaluation improves the state-of-the-art word-level perplexities on the Penn Treebank and WikiText-2 datasets to 51.1 and 44.3 respectively\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "BSD-2: https://github.com/benkrause/dynamic-evaluation ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ISS",
      "Organization": "Duke University,Microsoft",
      "Publication date": "2017-09-15",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "11100000.000000002",
      "Parameters notes": "11.1M (Table 2)",
      "Training compute (FLOP)": "3400000000000000.0",
      "Training compute notes": "6 FLOP / parameter / token * 11100000 parameters * 929000 tokens * 55 epochs = 3.402927e+15 FLOP",
      "Training dataset": "Penn TreeBank (PTB)",
      "Training dataset size (gradients)": "929000",
      "Dataset size notes": "\"All models are trained from scratch for 55 epochs.\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1709.05027",
      "Reference": "Learning Intrinsic Sparse Structures within Long Short-Term Memory",
      "Citations": "146.0",
      "Authors": "Wei Wen, Yuxiong He, Samyam Rajbhandari, Minjia Zhang, Wenhan Wang, Fang Liu, Bin Hu, Yiran Chen, Hai Li",
      "Abstract": "Model compression is significant for the wide adoption of Recurrent Neural Networks (RNNs) in both user devices possessing limited resources and business clusters requiring quick responses to large-scale service requests. This work aims to learn structurally-sparse Long Short-Term Memory (LSTM) by reducing the sizes of basic structures within LSTM units, including input updates, gates, hidden states, cell states and outputs. Independently reducing the sizes of basic structures can result in inconsistent dimensions among them, and consequently, end up with invalid LSTM units. To overcome the problem, we propose Intrinsic Sparse Structures (ISS) in LSTMs. Removing a component of ISS will simultaneously decrease the sizes of all basic structures by one and thereby always maintain the dimension consistency. By learning ISS within LSTM units, the obtained LSTMs remain regular while having much smaller basic structures. Based on group Lasso regularization, our method achieves 10.59x speedup without losing any perplexity of a language modeling of Penn TreeBank dataset. It is also successfully evaluated through a compact model with only 2.69M weights for machine Question Answering of SQuAD dataset. Our approach is successfully extended to non- LSTM RNNs, like Recurrent Highway Networks (RHNs). Our source code is publicly available at this https URL",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Moreover, ISS learning can find a\nsmaller RHN model with width 726, meanwhile improve the state-of-the-art perplexity as shown by the second entry in Table 2.\"",
      "Epochs": "55.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "code (apache): https://github.com/wenwei202/iss-rnns/tree/master/ptb ",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "PyramidNet",
      "Organization": "Korea Advanced Institute of Science and Technology (KAIST)",
      "Publication date": "2017-09-06",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "26000000.0",
      "Parameters notes": "best model had 26M params",
      "Training compute (FLOP)": "2340000000000000.0",
      "Training compute notes": "6ND=6*26000000*50000*300=2.34e+15",
      "Training dataset": "CIFAR-10,CIFAR-100",
      "Training dataset size (gradients)": "1280000",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/1610.02915v4",
      "Reference": "Deep Pyramidal Residual Networks",
      "Citations": "734.0",
      "Authors": "Dongyoon Han, Jiwhan Kim, Junmo Kim",
      "Abstract": "Deep convolutional neural networks (DCNNs) have shown remarkable performance in image classification tasks in recent years. Generally, deep neural network architectures are stacks consisting of a large number of convolutional layers, and they perform downsampling along the spatial dimension via pooling to reduce memory usage. Concurrently, the feature map dimension (i.e., the number of channels) is sharply increased at downsampling locations, which is essential to ensure effective performance because it increases the diversity of high-level attributes. This also applies to residual networks and is very closely related to their performance. In this research, instead of sharply increasing the feature map dimension at units that perform downsampling, we gradually increase the feature map dimension at all units to involve as many locations as possible. This design, which is discussed in depth together with our new insights, has proven to be an effective means of improving generalization ability. Furthermore, we propose a novel residual unit capable of further improving the classification accuracy with our new network architecture. Experiments on benchmark CIFAR-10, CIFAR-100, and ImageNet datasets have shown that our network architecture has superior generalization ability compared to the original residual networks. Code is available at this https URL}",
      "Organization categorization": "Academia",
      "Country (of organization)": "Korea (Republic of)",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"In tests using CIFAR-10, CIFAR-100, and ImageNet1k datasets, our PyramidNets outperform all previous state-of-the-art deep network architectures.\"",
      "Epochs": "300.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "https://github.com/jhkim89/PyramidNet",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "SENet (ImageNet)",
      "Organization": "Chinese Academy of Sciences,University of Oxford",
      "Publication date": "2017-09-05",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "28100000.0",
      "Parameters notes": "Table 16",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "ImageNet",
      "Training dataset size (gradients)": "1280000",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/1709.01507",
      "Reference": "Squeeze-and-Excitation Networks",
      "Citations": "23150.0",
      "Authors": "Jie Hu, Li Shen, Samuel Albanie, Gang Sun, Enhua Wu",
      "Abstract": "",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "China,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (WT2)",
      "Organization": "Ben-Gurion University of the Negev",
      "Publication date": "2017-08-29",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "38000000.0",
      "Parameters notes": "38M (Table 2)",
      "Training compute (FLOP)": "4.56e+17",
      "Training compute notes": "6 FLOP / parameter / token * 38000000 parameters * 2000000 tokens * 1000 epochs = 4.56e+17 FLOP",
      "Training dataset": "WikiText-2",
      "Training dataset size (gradients)": "2000000",
      "Dataset size notes": "\" The model was trained for 1000 epochs until the validation score stopped improving.\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1708.08863",
      "Reference": "Gradual Learning of Recurrent Neural Networks",
      "Citations": "4.0",
      "Authors": "Ziv Aharoni, Gal Rattner, Haim Permuter",
      "Abstract": "Recurrent Neural Networks (RNNs) achieve state-of-the-art results in many sequence-to-sequence modeling tasks. However, RNNs are difficult to train and tend to suffer from overfitting. Motivated by the Data Processing Inequality (DPI), we formulate the multi-layered network as a Markov chain, introducing a training method that comprises training the network gradually and using layer-wise gradient clipping. We found that applying our methods, combined with previously introduced regularization and optimization methods, resulted in improvements in state-of-the-art architectures operating in language modeling tasks.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Israel",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Our GL-LSTM model overcame the state-of-the-art results with only two layers and 19M parameters, and further improved\nthe state-of-the-art results with the third layer phase\"",
      "Epochs": "1000.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Libratus",
      "Organization": "Carnegie Mellon University (CMU)",
      "Publication date": "2017-08-19",
      "Domain": "Games",
      "Task": "Poker",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "5.51e+20",
      "Training compute notes": "\"In total, Libratus used about 25 million core hours. Of those, about 13 million core hours were used for exploratory experiments and evaluation. About 6 million core hours were spent on the initial abstraction and equilibrium finding component, another 3 million were used for nested subgame solving, and about 3 million were used on the self-improvement algorithm.\"\n\n\"Like many data-centric supercomputers, Bridges offers a relatively a modest number of FLOPS, but lots of memory: 895 teraflops and 130 TB, respectively.\"\n\nI just used the first bullet point (as those are usually independent systems and you only benchmark one of them).\nThe first system has 752 nodes a 2CPUs a 14cores each.\n\nsource: https://www.top500.org/news/bridges-supercomputer-boots-up-at-pittsburgh/\n\n\n\n1. 12M core hours for 196 cores\n2. We have  895 TFLOPS for 752 nodes a 2 CPUs a 14 cores\n2.1 That's 42.5 GFLOPS per core.\n3. Running this for 12M h\n3.1 12 * 10^6 * 60 * 60 * 42.5 * 10^9 FLOP/S = 1.823e21 FLOPs\n4. Assuming 30% utilization\n 1.823e21 * 0.3\n\u2192 5.51e20 FLOPs",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://www.ijcai.org/proceedings/2017/0772.pdf",
      "Reference": "Libratus: The Superhuman AI for No-Limit Poker",
      "Citations": "104.0",
      "Authors": "N Brown, T Sandholm, S Machine",
      "Abstract": "No-limit Texas Hold\u2019em is the most popular variant of poker in the world. Heads-up no-limit Texas Hold\u2019em is the main benchmark challenge for AI in imperfect-information games. We present Libratus, the first\u2014and so far only\u2014AI to defeat top human professionals in that game. Libratus\u2019s architecture features three main modules, each of which has new algorithms: pre-computing a solution to an abstraction of the game which provides a high-level blueprint for the strategy of the AI, a new nested subgame-solving algorithm which repeatedly calculates a more detailed strategy as play progresses, and a self-improving module which augments the pre-computed blueprint over time.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Claims to be first ML system to reach superhuman level at No Limit Poker Texas Hold Em\n\n\"Heads-up nolimit Texas Hold\u2019em has long been the primary benchmark challenge for imperfect-information games.\nIn January 2017 Libratus beat a team of four top-10 headsup no-limit specialist professionals in a 120,000-hand 20-day Brains vs. AI challenge match. That is the first time an AI has beaten top humans in this game. Libratus beat the humans by a large margin (147 mbb/hand), with 99.98% statistical significance. It also beat each of the humans individually.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "\"In total, Libratus used about 25 million core hours. Of those, about 13 million core hours were used for exploratory experiments and evaluation. About 6 million core hours were spent on the initial abstraction and equilibrium finding component, another 3 million were used for nested subgame solving, and about 3 million were used on the self-improvement algorithm.\"",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "NeuMF (Pinterest)",
      "Organization": "Shandong University,Texas A&M,National University of Singapore,Columbia University",
      "Publication date": "2017-08-16",
      "Domain": "Recommendation",
      "Task": "Collaborative filtering",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "1500809",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1708.05031",
      "Reference": "Neural Collaborative Filtering",
      "Citations": "6618.0",
      "Authors": "X He, L Liao, H Zhang, L Nie, X Hu",
      "Abstract": "",
      "Organization categorization": "Academia,Academia,Academia,Academia",
      "Country (of organization)": "China,United States of America,Singapore,United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "EI-REHN-1000D",
      "Organization": "Korea Advanced Institute of Science and Technology (KAIST)",
      "Publication date": "2017-08-14",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "19000000.0",
      "Parameters notes": "19M (Table 4)",
      "Training compute (FLOP)": "1.06e+16",
      "Training compute notes": "6 FLOP / parameter / token * 19000000 parameters * 929000 tokens * 100 epochs = 1.05906e+16 FLOP",
      "Training dataset": "Penn TreeBank (PTB)",
      "Training dataset size (gradients)": "929000",
      "Dataset size notes": "\"For all the experiments in this paper, Tensorflow toolkit [15] was used. For training the network, Adam optimizer [16] was adopted with 20 mini-batch size, 100 epochs, and 0.01 learning rate\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1708.04116",
      "Reference": "Early Improving Recurrent Elastic Highway Network",
      "Citations": "6.0",
      "Authors": "Hyunsin Park, Chang D. Yoo",
      "Abstract": "To model time-varying nonlinear temporal dynamics in sequential data, a recurrent network capable of varying and adjusting the recurrence depth between input intervals is examined. The recurrence depth is extended by several intermediate hidden state units, and the weight parameters involved in determining these units are dynamically calculated. The motivation behind the paper lies on overcoming a deficiency in Recurrent Highway Networks and improving their performances which are currently at the forefront of RNNs: 1) Determining the appropriate number of recurrent depth in RHN for different tasks is a huge burden and just setting it to a large number is computationally wasteful with possible repercussion in terms of performance degradation and high latency. Expanding on the idea of adaptive computation time (ACT), with the use of an elastic gate in the form of a rectified exponentially decreasing function taking on as arguments as previous hidden state and input, the proposed model is able to evaluate the appropriate recurrent depth for each input. The rectified gating function enables the most significant intermediate hidden state updates to come early such that significant performance gain is achieved early. 2) Updating the weights from that of previous intermediate layer offers a richer representation than the use of shared weights across all intermediate recurrence layers. The weight update procedure is just an expansion of the idea underlying hypernetworks. To substantiate the effectiveness of the proposed network, we conducted three experiments: regression on synthetic data, human activity recognition, and language modeling on the Penn Treebank dataset. The proposed networks showed better performance than other state-of-the-art recurrent networks in all three experiments.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Korea (Republic of)",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"The proposed networks showed better performance than other state-of-the-art recurrent networks in all three experiments.\"",
      "Epochs": "100.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "OpenAI TI7 DOTA 1v1",
      "Organization": "OpenAI",
      "Publication date": "2017-08-11",
      "Domain": "Games",
      "Task": "Dota 2",
      "Parameters": "",
      "Parameters notes": "Section 4 states: \"we used a model with over 150 million parameters\" but this is for the 5v5 agent, not the 1v1.",
      "Training compute (FLOP)": "6.046095222592002e+20",
      "Training compute notes": "Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://openai.com/research/dota-2",
      "Reference": "Dota 2",
      "Citations": "",
      "Authors": "",
      "Abstract": "We\u2019ve created a bot which beats the world\u2019s top professionals at 1v1 matches of Dota 2 under standard tournament rules. The bot learned the game from scratch by self-play, and does not use imitation learning or tree search. This is a step towards building AI systems which accomplish well-defined goals in messy, complicated situations involving real humans.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance,SOTA improvement",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "RetinaNet-R101",
      "Organization": "Facebook AI Research",
      "Publication date": "2017-08-07",
      "Domain": "Vision",
      "Task": "Object detection",
      "Parameters": "53000000.0",
      "Parameters notes": "source: table 2 in https://arxiv.org/pdf/1911.09070.pdf",
      "Training compute (FLOP)": "2.065392e+18",
      "Training compute notes": "\"We use synchronized SGD over 8 GPUs with a total of 16 images per minibatch (2 images per GPU). Unless otherwise specified, all models are trained for 90k iterations with an initial learning rate of 0.01, which is then divided by 10 at 60k and again at 80k iterations. We use horizontal image flipping as the only form of data augmentation unless otherwise noted. Weight decay of 0.0001 and momentum of 0.9 are used. The training loss is the sum the focal loss and the standard smooth L1 loss used for box regression [10]. Training time ranges between 10 and 35 hours for the models in Table 1e.\"\n\nNVIDIA M40 GPU\n\n35*60**2*0.3*8*6.83E+12 = 2.07e18",
      "Training dataset": "COCO",
      "Training dataset size (gradients)": "115000",
      "Dataset size notes": "trainval135k split",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1708.02002",
      "Reference": "Focal loss for dense object detection",
      "Citations": "16437.0",
      "Authors": "TY Lin, P Goyal, R Girshick, K He, P Dollar",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,France",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "35.0",
      "Training time notes": "\"We use synchronized SGD over 8 GPUs with a total of 16 images per minibatch (2 images per GPU). Unless otherwise specified, all models are trained for 90k iterations with an initial learning rate of 0.01, which is then divided by 10 at 60k and again at 80k iterations. We use horizontal image flipping as the only form of data augmentation unless otherwise noted. Weight decay of 0.0001 and momentum of 0.9 are used. The training loss is the sum the focal loss and the standard smooth L1 loss used for box regression [10]. Training time ranges between 10 and 35 hours for the models in Table 1e.\"\n\nNVIDIA M40 GPU\n\n35*60**2*0.3*8*6.83E+12 = 2.07e18",
      "Training hardware": "NVIDIA M40",
      "Hardware quantity": "8.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "4174.823263154792",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "93777.5806229916",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "RetinaNet-R50",
      "Organization": "Facebook AI Research",
      "Publication date": "2017-08-07",
      "Domain": "Vision",
      "Task": "Object detection",
      "Parameters": "34000000.0",
      "Parameters notes": "source: table 2 in https://arxiv.org/pdf/1911.09070.pdf",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "11500000000",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/1708.02002",
      "Reference": "Focal loss for dense object detection",
      "Citations": "16437.0",
      "Authors": "TY Lin, P Goyal, R Girshick, K He",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,France",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "likely code and weights here, would need to sort through whether this specific model is here: https://github.com/facebookresearch/Detectron ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2)",
      "Organization": "Salesforce Research",
      "Publication date": "2017-08-07",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "33000000.0",
      "Parameters notes": "33M (Table 2)",
      "Training compute (FLOP)": "2.97e+17",
      "Training compute notes": "6 FLOP / parameter / token * 33000000 parameters * 2000000 tokens * 750 epochs = 2.97e+17 FLOP",
      "Training dataset": "WikiText-2",
      "Training dataset size (gradients)": "2000000",
      "Dataset size notes": "\"For training the models, we use the NT-ASGD algorithm discussed in the previous section for 750 epochs\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1708.02182",
      "Reference": "Regularizing and Optimizing LSTM Language Models",
      "Citations": "1176.0",
      "Authors": "Stephen Merity, Nitish Shirish Keskar, Richard Socher",
      "Abstract": "Recurrent neural networks (RNNs), such as long short-term memory networks (LSTMs), serve as a fundamental building block for many sequence learning tasks, including machine translation, language modeling, and question answering. In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on hidden-to-hidden weights as a form of recurrent regularization. Further, we introduce NT-ASGD, a variant of the averaged stochastic gradient method, wherein the averaging trigger is determined using a non-monotonic condition as opposed to being tuned by the user. Using these and other regularization strategies, we achieve state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\"we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2.\"",
      "Epochs": "750.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "bsd-3 license: https://github.com/salesforce/awd-lstm-lm \ntrain/eval code: https://github.com/salesforce/awd-lstm-lm/blob/master/main.py ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GSM",
      "Organization": "Peking University,Microsoft Research",
      "Publication date": "2017-07-30",
      "Domain": "Language",
      "Task": "Question answering",
      "Parameters": "",
      "Parameters notes": "It could be possible to estimate it from section 3.",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "SQuAD",
      "Training dataset size (gradients)": "215570",
      "Dataset size notes": "from https://paperswithcode.com/dataset/squad SQuAD have 107,785 question-answer pairs\ndownload-ed dataset from: https://www.kaggle.com/datasets/stanfordu/stanford-question-answering-dataset?resource=download\nwc -w on train-v.1.1 returns 4017471 words so around 4M words",
      "Confidence": "Likely",
      "Link": "https://aclanthology.org/P17-1018/",
      "Reference": "Gated Self-Matching Networks for Reading Comprehension and Question Answering",
      "Citations": "806.0",
      "Authors": "Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang, Ming Zhou",
      "Abstract": "In this paper, we present the gated self-matching networks for reading comprehension style question answering, which aims to answer questions from a given passage. We first match the question and passage with gated attention-based recurrent networks to obtain the question-aware passage representation. Then we propose a self-matching attention mechanism to refine the representation by matching the passage against itself, which effectively encodes information from the whole passage. We finally employ the pointer networks to locate the positions of answers from the passages. We conduct extensive experiments on the SQuAD dataset. The single model achieves 71.3% on the evaluation metrics of exact match on the hidden test set, while the ensemble model further boosts the results to 75.9%. At the time of submission of the paper, our model holds the first place on the SQuAD leaderboard for both single and ensemble model.",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "China,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"At the time of submission of the paper, our model holds the first place on the SQuAD leaderboard for both single and ensemble model.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ConvS2S (ensemble of 8 models)",
      "Organization": "Meta AI",
      "Publication date": "2017-07-25",
      "Domain": "Language",
      "Task": "Translation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "5.64e+19",
      "Training compute notes": "All models are implemented in Torch (Collobert et al., 2011) and trained on a single Nvidia M40 GPU except for WMT\u201914 English-French for which we use a multi-GPU setup on a single\nmachine. We train on up to eight GPUs synchronously by\nmaintaining copies of the model on each card and split the\nbatch so that each worker computes 1/8-th of the gradients;\nat the end we sum the gradients via Nvidia NCCL.\n\n1. English-Romanian: \"Training took between 6 and 7.5 days on a single GPU.\"\n7 days * 24 * 3600 * 6.8e12 FLOP/s (Nvidia M40, fp32) * 0.3 = 1.2e18 FLOP\n\n2. English-German: \" We trained this model on a single GPU over a\nperiod of 18.5 days with a batch size of 48\".\n18.5 days * 24 * 3600 * 6.8e12 FLOP/s (Nvidia M40, fp32) * 0.3 = 3.3e18 FLOP\n\n3. English-French: \"Our results are based on training\nwith 8 GPUs for about 37 days and batch size 32 on each\nworker.6 \"\n37 days * 24 * 3600 * 8 * 6.8e12 FLOP/s (Nvidia M40, fp32) * 0.3 = 5.2e19 FLOP\n\nthe minimum compute needed to train ensemble model: 1.2e18 FLOP + 3.3e18 FLOP + 5.2e19 FLOP = 5.65e19 FLOP\n\nI am not sure how much to add more (they say ensemble model consists of 8 models), probably summarization training takes at least 1.2e18 FLOP more. \n\n",
      "Training dataset": "WMT English-German,WMT14,Gigaword",
      "Training dataset size (gradients)": "1183333333",
      "Dataset size notes": " 2.8M + 4.5M + 35.5M + 3.8M = 46.6M\n\nWe consider three major WMT translation tasks as well as\na text summarization task.\n\nWMT\u201916 English-Romanian. We use the same data and pre-processing as Sennrich et al. (2016b) but remove sentences with more than 175 words. This results in 2.8M sentence pairs for training and we evaluate on newstest2016.2\n\nWMT\u201914 English-German. We use the same setup as Luong et al. (2015) which comprises 4.5M sentence pairs for training and we test on newstest2014.\n\nWMT\u201914 English-French. We use the full training set of\n36M sentence pairs, and remove sentences longer than 175\nwords as well as pairs with a source/target length ratio exceeding 1.5. This results in 35.5M sentence-pairs for training.\n\nAbstractive summarization. We train on the Gigaword\ncorpus (Graff et al., 2003) and pre-process it identically\nto Rush et al. (2015) resulting in 3.8M training examples\nand 190K for validation.",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/1705.03122",
      "Reference": "Convolutional Sequence to Sequence Learning",
      "Citations": "",
      "Authors": "Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann N. Dauphin",
      "Abstract": "The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\"We achieve a new state of the art on several public translation benchmark data sets. On the WMT\u201916 EnglishRomanian task we outperform the previous best result by 1.9 BLEU, on WMT\u201914 English-French translation we improve over the LSTM model of Wu et al. (2016) by 1.6 BLEU in a comparable setting, and on WMT\u201914 EnglishGerman translation we ouperform the same model by 0.5\nBLEU\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA M40",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "PSPNet",
      "Organization": "Chinese University of Hong Kong (CUHK)",
      "Publication date": "2017-07-21",
      "Domain": "Vision",
      "Task": "Image segmentation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://ieeexplore.ieee.org/document/8100143",
      "Reference": "Pyramid Scene Parsing Network",
      "Citations": "13377.0",
      "Authors": "Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, Jiaya Jia",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "Hong Kong",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "NASNet-A",
      "Organization": "Google Brain",
      "Publication date": "2017-07-21",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "89000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "1280000",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/1707.07012",
      "Reference": "Learning Transferable Architectures for Scalable Image Recognition",
      "Citations": "5306.0",
      "Authors": "B Zoph, V Vasudevan, J Shlens",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "AWD-LSTM",
      "Organization": "DeepMind,University of Oxford",
      "Publication date": "2017-07-18",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "24000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "WikiText-2",
      "Training dataset size (gradients)": "2000000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1707.05589",
      "Reference": "On the State of the Art of Evaluation in Neural Language Models",
      "Citations": "555.0",
      "Authors": "G\u00e1bor Melis, Chris Dyer, Phil Blunsom",
      "Abstract": "Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing code bases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "JFT",
      "Organization": "Google Research,Carnegie Mellon University (CMU)",
      "Publication date": "2017-07-10",
      "Domain": "Vision",
      "Task": "Image classification,Object detection,Semantic segmentation,Pose estimation",
      "Parameters": "44654504.0",
      "Parameters notes": "Uses ResNet-101 architecture, which has 44,654,504 parameters:\nhttps://resources.wolframcloud.com/NeuralNetRepository/resources/ResNet-101-Trained-on-ImageNet-Competition-Data/",
      "Training compute (FLOP)": "8.43e+20",
      "Training compute notes": "Tesla K80 performance: 8.13 TFLOP/s\n\nAssume 40% utilization\n\n60 days * 50 GPUs * 40% utilization * 8.13 TFLOP/s/GPU = 8.43*10^20 FLOP",
      "Training dataset": "JFT-300M",
      "Training dataset size (gradients)": "5487300000000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1707.02968",
      "Reference": "Revisiting Unreasonable Effectiveness of Data in Deep Learning Era.",
      "Citations": "2605.0",
      "Authors": "Chen Sun, Abhinav Shrivastava, Saurabh Singh, Abhinav Gupta",
      "Abstract": "The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10x or 100x? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between `enormous data' and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "SOTA on COCO and PASCAL VOC",
      "Epochs": "4.0",
      "Training time (hours)": "1440.0",
      "Training time notes": "",
      "Training hardware": "NVIDIA Tesla K80",
      "Hardware quantity": "50.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "17910.759507843868",
      "Compute cost notes": "",
      "Training power draw (W)": "31330.704406642573",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "32.0",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "True",
      "Hardware acquisition cost": "214066.26283322967",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "34338.364791288564",
      "Training compute cost (upfront)": "568011.3598171673"
    },
    {
      "Model": "DeepLoc",
      "Organization": "Technical University of Denmark,University of Copenhagen",
      "Publication date": "2017-07-07",
      "Domain": "Biology",
      "Task": "Protein localization prediction",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "5.781024e+17",
      "Training compute notes": "6691000000000 FLOP / GPU / sec [Titan X reported] * 1 GPU [assumed] * 80 hours * 3600 sec / hour * 0.3 [assumed utilization] = 5.781024e+17 FLOP",
      "Training dataset": "DeepLoc",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://academic.oup.com/bioinformatics/article/33/21/3387/3931857",
      "Reference": "DeepLoc: prediction of protein subcellular localization using deep learning",
      "Citations": "",
      "Authors": "Jos\u00e9 Juan Almagro Armenteros, Casper Kaae S\u00f8nderby, S\u00f8ren Kaae S\u00f8nderby, Henrik Nielsen, Ole Winther",
      "Abstract": "Motivation\nThe prediction of eukaryotic protein subcellular localization is a well-studied topic in bioinformatics due to its relevance in proteomics research. Many machine learning methods have been successfully applied in this task, but in most of them, predictions rely on annotation of homologues from knowledge databases. For novel proteins where no annotated homologues exist, and for predicting the effects of sequence variants, it is desirable to have methods for predicting protein properties from sequence information only.\n\nResults\nHere, we present a prediction algorithm using deep neural networks to predict protein subcellular localization relying only on sequence information. At its core, the prediction model uses a recurrent neural network that processes the entire protein sequence and an attention mechanism identifying protein regions important for the subcellular localization. The model was trained and tested on a protein dataset extracted from one of the latest UniProt releases, in which experimentally annotated proteins follow more stringent criteria than previously. We demonstrate that our model achieves a good accuracy (78% for 10 categories; 92% for membrane-bound or soluble), outperforming current state-of-the-art algorithms, including those relying on homology information.",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "Denmark,Denmark",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "",
      "Epochs": "150.0",
      "Training time (hours)": "80.0",
      "Training time notes": "\"The training time for the full ensemble was 80 hours, approximately five hours per model. \"",
      "Training hardware": "NVIDIA GeForce GTX TITAN X",
      "Hardware quantity": "1.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "286.9303811522887",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Hosted access (no API)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Open access (non-commercial)",
      "Accessibility notes": "The method is available as a web server at http://www.cbs.dtu.dk/services/DeepLoc. Example code is available at https://github.com/JJAlmagro/subcellular_localization. The dataset is available at http://www.cbs.dtu.dk/services/DeepLoc/data.php.",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "934.4822479265549",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ShuffleNet v1",
      "Organization": "Megvii Inc",
      "Publication date": "2017-07-03",
      "Domain": "Vision",
      "Task": "Object detection,Image classification",
      "Parameters": "2430000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "1280000",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/1707.01083",
      "Reference": "ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices",
      "Citations": "6137.0",
      "Authors": "X Zhang, X Zhou, M Lin, J Sun",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "NoisyNet-Dueling",
      "Organization": "DeepMind",
      "Publication date": "2017-06-30",
      "Domain": "Games",
      "Task": "Atari",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "320000000",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1706.10295v3",
      "Reference": "Noisy Networks for Exploration",
      "Citations": "963.0",
      "Authors": "M Fortunato, MG Azar, B Piot, J Menick",
      "Abstract": "We introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of the agent's policy can be used to aid efficient exploration. The parameters of the noise are learned with gradient descent along with the remaining network weights. NoisyNet is straightforward to implement and adds little computational overhead. We find that replacing the conventional exploration heuristics for A3C, DQN and dueling agents (entropy reward and \\epsilon-greedy respectively) with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "not an absoulte SOTA on Atari 2600",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DeepLabV3",
      "Organization": "Google",
      "Publication date": "2017-06-17",
      "Domain": "Vision",
      "Task": "Semantic segmentation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "8354563074",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1706.05587",
      "Reference": "Rethinking Atrous Convolution for Semantic Image Segmentation",
      "Citations": "9394.0",
      "Authors": "Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, Hartwig Adam",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "HRA",
      "Organization": "Maluuba,Microsoft",
      "Publication date": "2017-06-13",
      "Domain": "Games",
      "Task": "Atari game: Ms. Pac-Man",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "150000000",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1706.04208",
      "Reference": "Hybrid Reward Architecture for Reinforcement Learning",
      "Citations": "273.0",
      "Authors": "H Van Seijen, M Fatemi, J Romoff, R Laroche",
      "Abstract": "One of the main challenges in reinforcement learning (RL) is generalisation. In typical deep RL methods this is achieved by approximating the optimal value function with a low-dimensional representation using a deep network. While this approach works well in many domains, in domains where the optimal value function cannot easily be reduced to a low-dimensional representation, learning can be very slow and unstable. This paper contributes towards tackling such challenging domains, by proposing a new method, called Hybrid Reward Architecture (HRA). HRA takes as input a decomposed reward function and learns a separate value function for each component reward function. Because each component typically only depends on a subset of all features, the corresponding value function can be approximated more easily by a low-dimensional representation, enabling more effective learning. We demonstrate HRA on a toy-problem and the Atari game Ms. Pac-Man, where HRA achieves above-human performance.",
      "Organization categorization": "Industry,Industry",
      "Country (of organization)": "Canada,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Table 1\n\"With the best combination, HRA not only outperforms the state-of-the-art on both metrics, it also significantly outperforms the human score, convincingly demonstrating the strength of HRA.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Transformer",
      "Organization": "Google Research,Google Brain",
      "Publication date": "2017-06-12",
      "Domain": "Language",
      "Task": "Translation",
      "Parameters": "213000000.0",
      "Parameters notes": "This page suggests the transformer has 213M parameters.\n\n\"Although there are others architectures that make use of attention layers, none achieves so good results so fast. Not only that, but the only model that can compite against Transformer is the Slicenet22, proposed just fifteen days before. It takes much longer to train, due to the huge amount of parameters it requires (348 million against the 213 millions of Transformer), and the BLEU scores it achieves are slightly worse on average. In short, up to date it offers no profit over Transformer.\"\n\nhttps://ricardokleinklein.github.io/2017/11/16/Attention-is-all-you-need.html",
      "Training compute (FLOP)": "7.4245248e+18",
      "Training compute notes": "\"The model was trained during 300000 steps, roughly 3.5 days, using 8 NVIDIA P100 GPUs.\"\n\nsource: https://ricardokleinklein.github.io/2017/11/16/Attention-is-all-you-need.html\n\nNVIDIA Tesla P100 has 9.3 teraFLOPS single-precision performance\n\nsource: https://www.nvidia.com/en-gb/data-center/tesla-p100/\n\nWe assume 0.33 utilization performance, in line with OpenAI's \"AI and compute\" article\n\nsource: https://openai.com/blog/ai-and-compute/\n\n9.3*10^12 FLOP / GPU / sec * 8 GPUs * 3.5 days * 24 hour / day * 3600 sec / hour * 0.33 [assumed utilization] = 7.4245248e+18 FLOP\n\nIn the \"AI and Memory Wall\" paper (https://github.com/amirgholami/ai_and_memory_wall) the estimation is 23,000 PFLOPS = 2.3*10^19 FLOPs. They estimate number of parameters as 65M\n\nNote that Table 2 provides a training compute estimate, but appears not to account for utilization.",
      "Training dataset": "WMT English-German,WMT14",
      "Training dataset size (gradients)": "832000000",
      "Dataset size notes": "\"We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary \"\n\nIn total, this is 40.5 million sentence-pairs. Assuming each sentence pair is 15-20 words in each language, this is 1.2-1.6 billion words.\n\nConvert to tokens: 1.4B * 4/3 = 1.87B tokens",
      "Confidence": "Confident",
      "Link": "https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf",
      "Reference": "Attention Is All You Need",
      "Citations": "159251.0",
      "Authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin",
      "Abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature",
      "Organization categorization": "Industry,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "Highly cited,Historical significance",
      "Notability criteria notes": "The original transformer",
      "Epochs": "3.0",
      "Training time (hours)": "84.0",
      "Training time notes": "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days).",
      "Training hardware": "NVIDIA P100",
      "Hardware quantity": "8.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "438.0356518424336",
      "Compute cost notes": "",
      "Training power draw (W)": "4180.032869488383",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "57264.48696887333",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "EDSR",
      "Organization": "Seoul National University",
      "Publication date": "2017-06-10",
      "Domain": "Vision,Image generation",
      "Task": "Image super-resolution",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1707.02921",
      "Reference": "Enhanced Deep Residual Networks for Single Image Super-Resolution",
      "Citations": "6703.0",
      "Authors": "Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, Kyoung Mu Lee",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "Korea (Republic of)",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Reading Twice for NLU",
      "Organization": "DeepMind",
      "Publication date": "2017-06-08",
      "Domain": "Language",
      "Task": "Question answering",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "TriviaQA,SQuAD",
      "Training dataset size (gradients)": "200000",
      "Dataset size notes": "both datasets have around 100k training examples.\nSQuAD have around 4M words. TriviaQA is larger\n\"We use 2 recent DQAbenchmark training and evaluation datasets,\nSQuAD (Rajpurkar et al., 2016) and TriviaQA\n(Joshi et al., 2017). \"",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1706.02596v3",
      "Reference": "Dynamic Integration of Background Knowledge in Neural NLU Systems",
      "Citations": "62.0",
      "Authors": "Dirk Weissenborn, Tom\u00e1\u0161 Ko\u010disk\u00fd, Chris Dyer",
      "Abstract": "Common-sense and background knowledge is required to understand natural language, but in most neural natural language understanding (NLU) systems, this knowledge must be acquired from training corpora during learning, and then it is static at test time. We introduce a new architecture for the dynamic integration of explicit background knowledge in NLU models. A general-purpose reading module reads background knowledge in the form of free-text statements (together with task-specific text inputs) and yields refined word representations to a task-specific NLU architecture that reprocesses the task inputs with these representations. Experiments on document question answering (DQA) and recognizing textual entailment (RTE) demonstrate the effectiveness and flexibility of the approach. Analysis shows that our model learns to exploit knowledge in a semantically appropriate way. ",
      "Organization categorization": "Industry",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Our results are competitive with the best systems, achieving a new state of the art on the recent TriviaQA benchmarks.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "PointNet++",
      "Organization": "Stanford University",
      "Publication date": "2017-06-07",
      "Domain": "3D modeling",
      "Task": "3D segmentation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "60000",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1706.02413",
      "Reference": "PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space",
      "Citations": "12854.0",
      "Authors": "Charles R. Qi, Li Yi, Hao Su, Leonidas J. Guibas",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Inflated 3D ConvNet",
      "Organization": "DeepMind,University of Oxford",
      "Publication date": "2017-06-01",
      "Domain": "3D modeling",
      "Task": "Action recognition",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "240000",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1705.07750",
      "Reference": "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",
      "Citations": "8953.0",
      "Authors": "Joao Carreira, Andrew Zisserman",
      "Abstract": "",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "SRGAN",
      "Organization": "Twitter",
      "Publication date": "2017-05-25",
      "Domain": "Vision,Image generation",
      "Task": "Image super-resolution",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "700000",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://openaccess.thecvf.com/content_cvpr_2017/html/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.html",
      "Reference": "Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network",
      "Citations": "11592.0",
      "Authors": "Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Mnemonic Reader",
      "Organization": "Fudan University,Microsoft Research",
      "Publication date": "2017-05-08",
      "Domain": "Language",
      "Task": "Question answering",
      "Parameters": "",
      "Parameters notes": "may be possible to estimate architecture description",
      "Training compute (FLOP)": "",
      "Training compute notes": "may be possible to estimate from architecture description",
      "Training dataset": "SQuAD",
      "Training dataset size (gradients)": "215570",
      "Dataset size notes": "size of SQuAD",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1705.02798v6",
      "Reference": "Reinforced Mnemonic Reader for Machine Reading Comprehension",
      "Citations": "217.0",
      "Authors": "Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu, Furu Wei, Ming Zhou",
      "Abstract": "In this paper, we introduce the Reinforced Mnemonic Reader for machine reading comprehension tasks, which enhances previous attentive readers in two aspects. First, a reattention mechanism is proposed to refine current attentions by directly accessing to past attentions that are temporally memorized in a multi-round alignment architecture, so as to avoid the problems of attention redundancy and attention deficiency. Second, a new optimization approach, called dynamic-critical reinforcement learning, is introduced to extend the standard supervised method. It always encourages to predict a more acceptable answer so as to address the convergence suppression problem occurred in traditional reinforcement learning algorithms. Extensive experiments on the Stanford Question Answering Dataset (SQuAD) show that our model achieves state-of-the-art results. Meanwhile, our model outperforms previous systems by over 6% in terms of both Exact Match and F1 metrics on two adversarial SQuAD datasets. ",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "China,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "from the abstract \" Extensive experiments on the Stanford Question Answering Dataset (SQuAD) show that our model achieves state-of-the-art results. Meanwhile, our model outperforms previous systems by over 6% in terms of both Exact Match and F1 metrics on two adversarial SQuAD datasets. \"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DeepLab (2017)",
      "Organization": "Johns Hopkins University,Google,University College London (UCL)",
      "Publication date": "2017-04-27",
      "Domain": "Vision",
      "Task": "Image segmentation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "26455000",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://ieeexplore.ieee.org/abstract/document/7913730",
      "Reference": "DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs",
      "Citations": "20019.0",
      "Authors": "Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L. Yuille",
      "Abstract": "",
      "Organization categorization": "Academia,Industry,Academia",
      "Country (of organization)": "United States of America,United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "MobileNet",
      "Organization": "Google",
      "Publication date": "2017-04-17",
      "Domain": "Vision",
      "Task": "Object detection,Image classification,Face detection",
      "Parameters": "4200000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "1280000",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/1704.04861",
      "Reference": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications",
      "Citations": "18770.0",
      "Authors": "AG Howard, M Zhu, B Chen, D Kalenichenko",
      "Abstract": "We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "WGAN-GP",
      "Organization": "Courant Institute of Mathematical Sciences,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms)",
      "Publication date": "2017-03-31",
      "Domain": "Image generation",
      "Task": "Image generation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "LSUN Bedroom",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1704.00028",
      "Reference": "Improved Training of Wasserstein GANs",
      "Citations": "10355.0",
      "Authors": "Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, Aaron Courville",
      "Abstract": "Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "United States of America,Canada",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "",
      "Accessibility notes": "MIT license:\nhttps://github.com/igul222/improved_wgan_training",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Mask R-CNN",
      "Organization": "Facebook AI Research",
      "Publication date": "2017-03-30",
      "Domain": "Vision",
      "Task": "Image segmentation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "COCO",
      "Training dataset size (gradients)": "46161920000",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1703.06870",
      "Reference": "Mask R-CNN",
      "Citations": "30286.0",
      "Authors": "Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, Ross Girshick",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,France",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "Training with\nResNet-50-FPN on COCO trainval35k takes 32 hours\nin our synchronized 8-GPU implementation (0.72s per 16-\nimage mini-batch), and 44 hours with ResNet-101-FPN",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Prototypical networks",
      "Organization": "University of Toronto,Twitter",
      "Publication date": "2017-03-15",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "38400",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1703.05175",
      "Reference": "Prototypical Networks for Few-shot Learning",
      "Citations": "9204.0",
      "Authors": " Jake Snell, Kevin Swersky, Richard S. Zemel",
      "Abstract": "",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "Canada,United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DnCNN",
      "Organization": "Harbin Institute of Technology,Hong Kong Polytechnic University,ULSee Inc.,Xi\u2019an Jiaotong University",
      "Publication date": "2017-02-01",
      "Domain": "Vision,Image generation",
      "Task": "Image super-resolution",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "2560000000",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://ieeexplore.ieee.org/abstract/document/7839189",
      "Reference": "Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising",
      "Citations": "7779.0",
      "Authors": "Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, Lei Zhang",
      "Abstract": "",
      "Organization categorization": "Academia,Academia,Industry,Academia",
      "Country (of organization)": "China,Hong Kong,China,China",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "MoE-Multi",
      "Organization": "Jagiellonian University,Google Brain",
      "Publication date": "2017-01-23",
      "Domain": "Language",
      "Task": "Language modeling,Translation",
      "Parameters": "8700000000.0",
      "Parameters notes": "Table 5\n\nhttps://arxiv.org/abs/1701.06538",
      "Training compute (FLOP)": "9.393905664e+19",
      "Training compute notes": "12 days \n64 NVIDIA K40 GPUs (see hardware data sheet for performance)\n0.33 util rate\n ",
      "Training dataset": "",
      "Training dataset size (gradients)": "87000000000",
      "Dataset size notes": "\"We constructed a similar training set consisting of shuffled unique sentences from Google\u2019s internal news corpus, totalling roughly 100 billion words\"\nAssuming 100 words = 133 tokens",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1701.06538",
      "Reference": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
      "Citations": "3607.0",
      "Authors": "N Shazeer, A Mirhoseini, K Maziarz, A Davis",
      "Abstract": "The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "Poland,United States of America",
      "Notability criteria": "Highly cited,SOTA improvement,Historical significance",
      "Notability criteria notes": "\"On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost\"",
      "Epochs": "10.0",
      "Training time (hours)": "288.0",
      "Training time notes": "12 days",
      "Training hardware": "NVIDIA Tesla K40t",
      "Hardware quantity": "64.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "3874.12265",
      "Compute cost notes": "",
      "Training power draw (W)": "32873.78910003616",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "1365333.3333333333",
      "Batch size notes": "\"Training was done synchronously on a cluster of up to 64 GPUs as described in section 3. Each training batch consisted of a set of sentence pairs containing roughly 16000 words per GPU.\" Although they appear to use word-level tokenization in other experiments, here they use subword tokens: \"Similar to GNMT, to effectively deal with rare words, we used subword units (also known as \u201cwordpieces\") (Schuster & Nakajima, 2012) for inputs and outputs in our system.\" In total 64 GPUs * 16k words/GPU * 4/3 tokens/word = 1,365,333",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "367523.56208787626",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "OR-WideResNet",
      "Organization": "Duke University,University of Chinese Academy of Sciences",
      "Publication date": "2017-01-07",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "18200000.0",
      "Parameters notes": "18.2M for largest OR-WideResNet model.",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "CIFAR-10",
      "Training dataset size (gradients)": "50000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1701.01833v2",
      "Reference": "Oriented Response Networks",
      "Citations": "285.0",
      "Authors": "Yanzhao Zhou, Qixiang Ye, Qiang Qiu and Jianbin Jiao",
      "Abstract": "Deep Convolution Neural Networks (DCNNs) are capable of learning unprecedentedly effective image representations. However, their ability in handling significant local and global image rotations remains limited. In this paper, we propose Active Rotating Filters (ARFs) that actively rotate during convolution and produce feature maps with location and orientation explicitly encoded. An ARF acts as a virtual filter bank containing the filter itself and its multiple unmaterialised rotated versions. During back-propagation, an ARF is collectively updated using errors from all its rotated versions. DCNNs using ARFs, referred to as Oriented Response Networks (ORNs), can produce within-class rotation-invariant deep features while maintaining inter-class discrimination for classification tasks. The oriented response produced by ORNs can also be used for image and object orientation estimation tasks. Over multiple state-of-the-art DCNN architectures, such as VGG, ResNet, and STN, we consistently observe that replacing regular filters with the proposed ARFs leads to significant reduction in the number of network parameters and improvement in classification performance. We report the best results on several commonly used benchmarks.",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "United States of America,China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"In Sec. 4.3, we upgrade the VGG [38], ResNet [18], and the\nWideResNet [45] to ORNs, and train them on CIFAR10 and\nCIFAR100 [22], showing the state-of-the-art performance\non the natural image classification task.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA Tesla K80",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DeepStack",
      "Organization": "University of Alberta,Charles University,Czech Technical University",
      "Publication date": "2017-01-06",
      "Domain": "Games",
      "Task": "Poker",
      "Parameters": "2500000.0",
      "Parameters notes": "Figure 3, p.9\n\nsource: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",
      "Training compute (FLOP)": "1.446336e+19",
      "Training compute notes": "The largest source of compute necessary for training seems to be the data generation job on 20 GPUs. We count this towards the training compute because it requires simulation using the network. This is analogous to the AlphaGo systems simulating Go games.\n\nFrom p.26: \"For the flop network, one million poker flop situations (from after the flop cards are dealt) were generated and solved. These situations were solved using DeepStack\u2019s depth limited solver with the turn network used for the counterfactual values at public states immediately after the turn card. We used a cluster of 20 GPUS and one-half of a GPU year of computation time.\"\n\nAssume they used P100 GPUs because they were common at the time (P100 was released in 2016 and this paper was published in 2017).\n\nBut assume low utilization of 10% to hedge on (a) lower-performing GPUs being used, (b) non-FLOP computations taking up a lot of the data generation job.\n\nCalculation:\n6 months * 30 days * 24 hours * 3600 seconds * 9.3e12 FLOP/s * 0.1 utilization = 1.446336e+19 FLOP.",
      "Training dataset": "",
      "Training dataset size (gradients)": "25380000000",
      "Dataset size notes": "\"The turn network was trained by solving 10 million randomly generated poker turn\ngames. These turn games used randomly generated ranges, public cards, and a random pot\nsize (10).\"",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/1701.01724",
      "Reference": "DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker",
      "Citations": "960.0",
      "Authors": "Matej Morav\u010d\u00edk, Martin Schmid, Neil Burch, Viliam Lis\u00fd, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, Michael Bowling",
      "Abstract": "Artificial intelligence has seen several breakthroughs in recent years, with games often serving as milestones. A common feature of these games is that players have perfect information. Poker is the quintessential game of imperfect information, and a longstanding challenge problem in artificial intelligence. We introduce DeepStack, an algorithm for imperfect information settings. It combines recursive reasoning to handle information asymmetry, decomposition to focus computation on the relevant decision, and a form of intuition that is automatically learned from self-play using deep learning. In a study involving 44,000 hands of poker, DeepStack defeated with statistical significance professional poker players in heads-up no-limit Texas hold'em. The approach is theoretically sound and is shown to produce more difficult to exploit strategies than prior approaches.",
      "Organization categorization": "Academia,Academia,Academia",
      "Country (of organization)": "Canada,Czechia,Czechia",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "first human-competitive poker AI, confirmed by website: https://www.deepstack.ai/",
      "Epochs": "",
      "Training time (hours)": "218.0",
      "Training time notes": "from compute notes - around 9 days  - half a year of GPU compute using 20 GPUs",
      "Training hardware": "",
      "Hardware quantity": "20.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "YOLOv2",
      "Organization": "University of Washington,Allen Institute for AI",
      "Publication date": "2016-12-25",
      "Domain": "Vision",
      "Task": "Object detection",
      "Parameters": "51000000.0",
      "Parameters notes": "Source: https://resources.wolframcloud.com/NeuralNetRepository/resources/YOLO-V2-Trained-on-MS-COCO-Data_1",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "1280000",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/1612.08242",
      "Reference": "YOLO9000: Better, Faster, Stronger",
      "Citations": "14397.0",
      "Authors": "Joseph Redmon, Ali Farhadi",
      "Abstract": "We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.",
      "Organization categorization": "Academia,Research collective",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "weights here, no license specified: https://pjreddie.com/darknet/yolo/ ",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "EnhanceNet",
      "Organization": "Max Planck Institute for Intelligent Systems",
      "Publication date": "2016-12-23",
      "Domain": "Vision",
      "Task": "Image super-resolution",
      "Parameters": "814464.0",
      "Parameters notes": "2*3*3*3*64+22*3*3*64*64=814464\n24 CNN layers with 3x3 kernels and 64 channels and 3 input/output channels (see Table 1)\n",
      "Training compute (FLOP)": "1.3079231999999998e+17",
      "Training compute notes": "Compute: 0.3*24*60*60*5046000000000=130792319999999980\nK40 FLOPs: 5046000000000\n\"We trained all models for a maximum of 24 hours on an Nvidia K40 GPU\"",
      "Training dataset": "COCO",
      "Training dataset size (gradients)": "9830400000",
      "Dataset size notes": "\"resulting in roughly 200k images\"",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/EnhanceNet%3A-Single-Image-Super-Resolution-Through-Sajjadi-Scholkopf/fddc32f3880688238847077fd927ab3025db7a6a",
      "Reference": "EnhanceNet: Single Image Super-Resolution Through Automated Texture Synthesis",
      "Citations": "",
      "Authors": "Mehdi S. M. Sajjadi, B. Scholkopf, M. Hirsch",
      "Abstract": "Single image super-resolution is the task of inferring a high-resolution image from a single low-resolution input. Traditionally, the performance of algorithms for this task is measured using pixel-wise reconstruction measures such as peak signal-to-noise ratio (PSNR) which have been shown to correlate poorly with the human perception of image quality. As a result, algorithms minimizing these metrics tend to produce over-smoothed images that lack highfrequency textures and do not look natural despite yielding high PSNR values.,,We propose a novel application of automated texture synthesis in combination with a perceptual loss focusing on creating realistic textures rather than optimizing for a pixelaccurate reproduction of ground truth images during training. By using feed-forward fully convolutional neural networks in an adversarial training setting, we achieve a significant boost in image quality at high magnification ratios. Extensive experiments on a number of datasets show the effectiveness of our approach, yielding state-of-the-art results in both quantitative and qualitative benchmarks.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Germany",
      "Notability criteria": "SOTA improvement,Highly cited",
      "Notability criteria notes": "https://paperswithcode.com/sota/image-super-resolution-on-ffhq-256-x-256-4x \n\nTable 4. PSNR for different methods at 4x super-resolution. ENet-E achieves state-of-the-art results on all datasets.",
      "Epochs": "",
      "Training time (hours)": "24.0",
      "Training time notes": "\"We trained all models for a maximum of 24 hours on an Nvidia K40 GPU\"",
      "Training hardware": "NVIDIA Tesla K40c",
      "Hardware quantity": "1.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "282.4218009310853",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "https://webdav.tue.mpg.de/pixel/enhancenet/",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "3DMM-CNN",
      "Organization": "University of Southern California",
      "Publication date": "2016-12-15",
      "Domain": "Vision",
      "Task": "Face recognition,3D reconstruction",
      "Parameters": "44500000.0",
      "Parameters notes": "Based on ResNet 101 architecture",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "CASIA Face Dataset",
      "Training dataset size (gradients)": "500000",
      "Dataset size notes": "[images]\nMulti image 3DMM reconstruction is performed by first estimating 3DMM parameters from the 500k single images in CASIA. 3DMM estimates for images of the same subject are then aggregated into a single 3DMM per subject (\u223c10k subjects)",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1612.04904",
      "Reference": "Regressing Robust and Discriminative 3D Morphable Models with a very Deep Neural Network",
      "Citations": "",
      "Authors": "A. Tran, Tal Hassner, I. Masi, G. Medioni",
      "Abstract": "The 3D shapes of faces are well known to be discriminative. Yet despite this, they are rarely used for face recognition and always under controlled viewing conditions. We claim that this is a symptom of a serious but often overlooked problem with existing methods for single view 3D face reconstruction: when applied in the wild, their 3D estimates are either unstable and change for different photos of the same subject or they are over-regularized and generic. In response, we describe a robust method for regressing discriminative 3D morphable face models (3DMM). We use a convolutional neural network (CNN) to regress 3DMM shape and texture parameters directly from an input photo. We overcome the shortage of training data required for this purpose by offering a method for generating huge numbers of labeled examples. The 3D estimates produced by our CNN surpass state of the art accuracy on the MICC data set. Coupled with a 3D-3D face matching pipeline, we show the first competitive face recognition results on the LFW, YTF and IJB-A benchmarks using 3D face shapes as representations, rather than the opaque deep feature vectors used by other modern systems.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "https://paperswithcode.com/sota/3d-face-reconstruction-on-florence \n\n\"The 3D estimates produced by our CNN surpass state of the art accuracy on the MICC data set.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA GeForce GTX 590",
      "Hardware quantity": "1.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "420.82581181711174",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (restricted use)",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "",
      "Accessibility notes": "non commercial: https://github.com/anhttran/3dmm_cnn?tab=License-1-ov-file\n\nhttps://docs.google.com/forms/d/e/1FAIpQLSd6cwKh-CO_8Yr-VeDi27GPswyqI9Lvub6S2UYBRsLooCq9Vw/viewform",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "266.2353891115474",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "HR-ResNet101",
      "Organization": "Carnegie Mellon University (CMU)",
      "Publication date": "2016-12-13",
      "Domain": "Vision",
      "Task": "Face detection",
      "Parameters": "44500000.0",
      "Parameters notes": "ResNet 101 ",
      "Training compute (FLOP)": "7.077e+18",
      "Training compute notes": "ResNet-101 training compute: 7004000000000000000\nFinetune compute: 73422840000000000\nTotal compute: 7004000000000000000+73422840000000000=7.0774228e+18",
      "Training dataset": "ILSVRC 2012 subset of ImageNet",
      "Training dataset size (gradients)": "8243968",
      "Dataset size notes": "WIDER face dataset: 32203 images\nImageNet size: 1280000\nTotal data: 1280000+32203=1312203",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/Finding-Tiny-Faces-Hu-Ramanan/71f51e1b6691343ed031c1ed42efc96a7f1a0619",
      "Reference": "Finding Tiny Faces",
      "Citations": "",
      "Authors": "Peiyun Hu, Deva Ramanan",
      "Abstract": "Though tremendous strides have been made in object recognition, one of the remaining open challenges is detecting small objects. We explore three aspects of the problem in the context of finding small faces: the role of scale invariance, image resolution, and contextual reasoning. While most recognition approaches aim to be scale-invariant, the cues for recognizing a 3px tall face are fundamentally different than those for recognizing a 300px tall face. We take a different approach and train separate detectors for different scales. To maintain efficiency, detectors are trained in a multi-task fashion: they make use of features extracted from multiple layers of single (deep) feature hierarchy. While training detectors for large objects is straightforward, the crucial challenge remains training detectors for small objects. We show that context is crucial, and define templates that make use of massively-large receptive fields (where 99% of the template extends beyond the object of interest). Finally, we explore the role of scale in pre-trained deep networks, providing ways to extrapolate networks tuned for limited scales to rather extreme ranges. We demonstrate state-of-the-art results on massively-benchmarked face datasets (FDDB and WIDER FACE). In particular, when compared to prior art on WIDER FACE, our results reduce error by a factor of 2 (our models produce an AP of 82% while prior art ranges from 29-64%).",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "https://paperswithcode.com/sota/face-detection-on-wider-face-medium \n\n\" We demonstrate state-of-the-art results on massively-benchmarked face datasets (FDDB and WIDER FACE). \"",
      "Epochs": "170.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "ResNet-101 (ImageNet)",
      "Finetune compute (FLOP)": "7.342284e+16",
      "Finetune compute notes": "50*32203*3*15200000000=73422840000000000",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GAN-Advancer",
      "Organization": "OpenAI",
      "Publication date": "2016-12-05",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://dl.acm.org/doi/10.5555/3157096.3157346",
      "Reference": "Improved Techniques for Training GANs",
      "Citations": "9798.0",
      "Authors": "Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen",
      "Abstract": "We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "Open access (non-commercial)",
      "Accessibility notes": "code, no weights, unclear license:\nhttps://github.com/openai/improved-gan \nexperiment code: https://github.com/openai/improved-gan/tree/master/mnist_svhn_cifar10 ",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "PointNet",
      "Organization": "Stanford University",
      "Publication date": "2016-12-02",
      "Domain": "3D modeling",
      "Task": "3D segmentation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "9843",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1612.00593",
      "Reference": "PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation",
      "Citations": "16276.0",
      "Authors": "CR Qi, H Su, K Mo, LJ Guibas",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Elastic weight consolidation",
      "Organization": "DeepMind",
      "Publication date": "2016-12-02",
      "Domain": "Vision,Games",
      "Task": "Image classification,Atari",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1612.00796",
      "Reference": "Overcoming catastrophic forgetting in neural networks",
      "Citations": "8717.0",
      "Authors": "James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, Raia Hadsell",
      "Abstract": "The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Neural networks are not, in general, capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks which they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on the MNIST hand written digit dataset and by learning several Atari 2600 games sequentially.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Image-to-image cGAN",
      "Organization": "University of California (UC) Berkeley",
      "Publication date": "2016-11-21",
      "Domain": "Vision,Image generation",
      "Task": "Image generation,Image-to-image",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "2400000",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1611.07004",
      "Reference": "Image-to-Image Translation with Conditional Adversarial Networks",
      "Citations": "21322.0",
      "Authors": "Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros",
      "Abstract": "We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "PolyNet",
      "Organization": "Chinese University of Hong Kong (CUHK)",
      "Publication date": "2016-11-17",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "92000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "6.4e+19",
      "Training compute notes": "Section 5: \"ResNet-500 [has] similar computation\ncosts to our Very Deep PolyNet\".\n\nResNet-152 has 11.3e9 FLOP per forward pass (https://arxiv.org/abs/1512.03385, Table 1). Hence ResNet-500 has approx 3.7e10 = 11.3e9*500/152 FLOP per forward pass.\n\n560k iterations, batch size 512:\nTrain compute = 3.7e10*3*2*560e3 * 512 = 6.4e19",
      "Training dataset": "ImageNet",
      "Training dataset size (gradients)": "1280000",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/1611.05725",
      "Reference": "PolyNet: A Pursuit of Structural Diversity in Very Deep Networks",
      "Citations": "282.0",
      "Authors": "X Zhang, Z Li, C Change Loy",
      "Abstract": "A number of studies have shown that increasing the depth or width of convolutional networks is a rewarding approach to improve the performance of image recognition. In our study, however, we observed difficulties along both directions. On one hand, the pursuit for very deep networks is met with a diminishing return and increased training difficulty; on the other hand, widening a network would result in a quadratic growth in both computational cost and memory demand. These difficulties motivate us to explore structural diversity in designing deep networks, a new dimension beyond just depth and width. Specifically, we present a new family of modules, namely the PolyInception, which can be flexibly inserted in isolation or in a composition as replacements of different parts of a network. Choosing PolyInception modules with the guidance of architectural efficiency can improve the expressive power while preserving comparable computational cost. The Very Deep PolyNet, designed following this direction, demonstrates substantial improvements over the state-of-the-art on the ILSVRC 2012 benchmark. Compared to Inception-ResNet-v2, it reduces the top-5 validation error on single crops from 4.9% to 4.25%, and that on multi-crops from 3.7% to 3.45%.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Hong Kong",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"The Very Deep PolyNet, designed following this direction, demonstrates substantial improvements over the state-of-the-art on the ILSVRC 2012 benchmark. Compared to Inception-ResNet-v2, it reduces the top-5 validation error on single crops from 4.9% to 4.25%, and that on multi-crops from 3.7% to 3.45%.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA GeForce GTX TITAN X",
      "Hardware quantity": "32.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "617.1106542",
      "Compute cost notes": "",
      "Training power draw (W)": "16797.38519252759",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "35330.13539051488",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ResNeXt-101 (64\u00d74d)",
      "Organization": "University of California San Diego,Facebook",
      "Publication date": "2016-11-16",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "83000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "1.2e+19",
      "Training compute notes": "12,000 PFLOPs = 1.2 * 10^19 FLOPs\n\nhttps://github.com/amirgholami/ai_and_memory_wall",
      "Training dataset": "ImageNet",
      "Training dataset size (gradients)": "1280000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1611.05431",
      "Reference": "Aggregated Residual Transformations for Deep Neural Networks",
      "Citations": "11215.0",
      "Authors": "Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, Kaiming He",
      "Abstract": "We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call \"cardinality\" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "",
      "Accessibility notes": "BSD License\nhttps://github.com/facebookresearch/ResNeXt?tab=readme-ov-file",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ResNeXt-50",
      "Organization": "University of California San Diego,Facebook",
      "Publication date": "2016-11-16",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "25000000.0",
      "Parameters notes": "\"If you\u2019re thinking about ResNets, yes, they are related. ResNeXt-50 has 25M parameters (ResNet-50 has 25.5M).\"\n\nhttps://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "ImageNet",
      "Training dataset size (gradients)": "1280000",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/1611.05431",
      "Reference": "Aggregated Residual Transformations for Deep Neural Networks",
      "Citations": "11215.0",
      "Authors": "Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, Kaiming He",
      "Abstract": "We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call \"cardinality\" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "",
      "Accessibility notes": "BSD License\nhttps://github.com/facebookresearch/ResNeXt?tab=readme-ov-file",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DAC-CSR",
      "Organization": "Jiangnan University,University of Surrey",
      "Publication date": "2016-11-16",
      "Domain": "Vision",
      "Task": "Face detection",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "AFLW",
      "Training dataset size (gradients)": "20000",
      "Dataset size notes": "[images]\nAFLW-full splits the 24386 images into 20000/4386 for training/testing.",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1611.05396",
      "Reference": "Dynamic Attention-controlled Cascaded Shape Regression Exploiting Training Data Augmentation and Fuzzy-set Sample Weighting",
      "Citations": "",
      "Authors": "Zhen-Hua Feng, Josef Kittler, William Christmas, Patrik Huber, Xiao-Jun Wu",
      "Abstract": "We present a new Cascaded Shape Regression (CSR) architecture, namely Dynamic Attention-Controlled CSR (DAC-CSR), for robust facial landmark detection on unconstrained faces. Our DAC-CSR divides facial landmark detection into three cascaded sub-tasks: face bounding box refinement, general CSR and attention-controlled CSR. The first two stages refine initial face bounding boxes and output intermediate facial landmarks. Then, an online dynamic model selection method is used to choose appropriate domain-specific CSRs for further landmark refinement. The key innovation of our DAC-CSR is the fault-tolerant mechanism, using fuzzy set sample weighting for attention-controlled domain-specific model training. Moreover, we advocate data augmentation with a simple but effective 2D profile face generator, and context-aware feature extraction for better facial feature representation. Experimental results obtained on challenging datasets demonstrate the merits of our DAC-CSR over the state-of-the-art.",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "China,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "https://paperswithcode.com/sota/face-alignment-on-aflw-19 ",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DTN (Domain Transfer Network)",
      "Organization": "Facebook AI Research",
      "Publication date": "2016-11-07",
      "Domain": "Vision,Image generation",
      "Task": "Image generation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "2000000",
      "Dataset size notes": "\"For face images, we use a set s of one million random images without identity information.\"",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1611.02200",
      "Reference": "Unsupervised Cross-Domain Image Generation",
      "Citations": "",
      "Authors": "Yaniv Taigman, Adam Polyak, Lior Wolf",
      "Abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given function f, which accepts inputs in either domains, would remain unchanged. Other than the function f, the training data is unsupervised and consist of a set of samples from each domain. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f-constancy component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity. ",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,France",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DLDL (PASCAL)",
      "Organization": "University of Oxford",
      "Publication date": "2016-11-06",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "564000000.0",
      "Parameters notes": "Finetunes 4 versions of VGG\nVGG 16: 138000000\nVGG 19: 144000000\nTotal 2*(138000000+144000000)=564000000\n",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "PASCAL VOC 2012",
      "Training dataset size (gradients)": "22531",
      "Dataset size notes": "doesn't say how much training vs testing is. 22531 is total size",
      "Confidence": "Likely",
      "Link": "https://www.semanticscholar.org/paper/Deep-Label-Distribution-Learning-With-Label-Gao-Xing/02567fd428a675ca91a0c6786f47f3e35881bcbd",
      "Reference": "Deep Label Distribution Learning With Label Ambiguity",
      "Citations": "",
      "Authors": "Bin-Bin Gao, Chao Xing, Chen-Wei Xie, Jianxin Wu, Xin Geng",
      "Abstract": "Convolutional neural networks (ConvNets) have achieved excellent recognition performance in various visual recognition tasks. A large labeled training set is one of the most important factors for its success. However, it is difficult to collect sufficient training images with precise labels in some domains, such as apparent age estimation, head pose estimation, multilabel classification, and semantic segmentation. Fortunately, there is ambiguous information among labels, which makes these tasks different from traditional classification. Based on this observation, we convert the label of each image into a discrete label distribution, and learn the label distribution by minimizing a Kullback\u2013Leibler divergence between the predicted and ground-truth label distributions using deep ConvNets. The proposed deep label distribution learning (DLDL) method effectively utilizes the label ambiguity in both feature learning and classifier learning, which help prevent the network from overfitting even when the training set is small. Experimental results show that the proposed approach produces significantly better results than the state-of-the-art methods for age estimation and head pose estimation. At the same time, it also improves recognition performance for multi-label classification and semantic segmentation tasks.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "https://paperswithcode.com/sota/multi-label-classification-on-pascal-voc-2007 ",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA Tesla K40c",
      "Hardware quantity": "1.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "282.71755541057985",
      "Base model": "VGG16,VGG19",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "NASv3 (CIFAR-10)",
      "Organization": "Google Brain",
      "Publication date": "2016-11-05",
      "Domain": "Vision",
      "Task": "Image classification,Neural Architecture Search - NAS",
      "Parameters": "37400000.0",
      "Parameters notes": "Table 1",
      "Training compute (FLOP)": "2.2e+21",
      "Training compute notes": "50 epochs * 50,000 images * 10.0 GFLOPSs * 12800 networks * 2 add-multiply * 3 backward pass \n= 1.9e6 PF = 22 pfs-days\n\nsource: https://openai.com/blog/ai-and-compute/",
      "Training dataset": "",
      "Training dataset size (gradients)": "45000",
      "Dataset size notes": "CIFAR-10 (does not factor in augmentation procedures)",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/1611.01578",
      "Reference": "Neural Architecture Search with Reinforcement Learning",
      "Citations": "5683.0",
      "Authors": "Barret Zoph, Quoc V. Le",
      "Abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "800.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "21184.441090423978",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": "1835809.3798165137"
    },
    {
      "Model": "NAS with base 8 and shared embeddings",
      "Organization": "Google Brain",
      "Publication date": "2016-11-05",
      "Domain": "Language",
      "Task": "Language modeling,Neural Architecture Search - NAS",
      "Parameters": "54000000.0",
      "Parameters notes": "54M (Table 2)",
      "Training compute (FLOP)": "1.05e+16",
      "Training compute notes": "6 FLOP / parameter / token * 54000000 parameters * 929000 tokens * 35 epochs = 1.053486e+16 FLOP",
      "Training dataset": "Penn TreeBank (PTB)",
      "Training dataset size (gradients)": "929000",
      "Dataset size notes": "\"every child model is constructed and trained for 35 epochs\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1611.01578",
      "Reference": "Neural Architecture Search with Reinforcement Learning",
      "Citations": "5683.0",
      "Authors": "Barret Zoph, Quoc V. Le",
      "Abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "35.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "BIDAF",
      "Organization": "University of Washington,Allen Institute for AI",
      "Publication date": "2016-11-05",
      "Domain": "Language",
      "Task": "Question answering",
      "Parameters": "2600000.0",
      "Parameters notes": "There are two similar models described in sections \"Models details\"\ncitation from the paper about model for SQuAD \"The model has about 2.6 million parameters\"\ncitation about model for cloze test\n\"The model architecture used for this task is very similar to that for SQuAD (Section 4) with only a few small changes to adapt it to the cloze test. \"\n",
      "Training compute (FLOP)": "3.4686144e+18",
      "Training compute notes": "flops = (8) * (6691 * 10**9) * (60 * 3600) * 3 // 10\n(num gpu) * (peak flops) * (time in seconds) * (assumed utilization rate) =\n\ncitation from the section about cloze test experiments \"The entire training process takes roughly 60 hours on eight Titan X GPUs. The other hyper-parameters are identical to the model described in Section 4\" (section 4 is about SQuAD experiments and cloze test experiments require more compute and data).\nflops  6.691 TFLOPS from https://www.techpowerup.com/gpu-specs/geforce-gtx-titan-x.c2632",
      "Training dataset": "SQuAD,DMQA,GloVe",
      "Training dataset size (gradients)": "879000",
      "Dataset size notes": "\"In a cloze test, the reader is asked to fill in words that have been removed from a passage, for measuring one\u2019s ability to comprehend text. Hermann et al. (2015) have recently compiled a massive Cloze-style comprehension dataset, consisting of 300k/4k/3k and 879k/65k/53k (train/dev/test)\nexamples from CNN and DailyMail news articles, respectively. \"\nassuming 40 words per example we get around 47160000 words (SQuAD have around 40 words per example - so I think it should be similar case for this dataset)",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1611.01603v6",
      "Reference": "Bidirectional Attention Flow for Machine Comprehension",
      "Citations": "2246.0",
      "Authors": "Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi",
      "Abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test. ",
      "Organization categorization": "Academia,Research collective",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\"Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test. \"",
      "Epochs": "8.0",
      "Training time (hours)": "60.0",
      "Training time notes": "see compute notes",
      "Training hardware": "NVIDIA GeForce GTX TITAN X",
      "Hardware quantity": "8.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "41.25014556779628",
      "Compute cost notes": "",
      "Training power draw (W)": "4200.468649711526",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "apache 2.0, code + weights: https://github.com/allenai/bi-att-flow",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "8909.049936172829",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "VD-LSTM+REAL Large",
      "Organization": "Salesforce Research,Stanford University",
      "Publication date": "2016-11-04",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "51000000.0",
      "Parameters notes": "51M (Table 3)",
      "Training compute (FLOP)": "2.13e+16",
      "Training compute notes": "6 FLOP / parameter / token * 51000000 parameters * 929000 tokens * 75 epochs = 2.132055e+16 FLOP",
      "Training dataset": "Penn TreeBank (PTB)",
      "Training dataset size (gradients)": "929000",
      "Dataset size notes": "75 epochs (Figure 2b)",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1611.01462",
      "Reference": "Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling",
      "Citations": "397.0",
      "Authors": "Hakan Inan, Khashayar Khosravi, Richard Socher",
      "Abstract": "Recurrent neural networks have been very successful at predicting sequences of words in tasks such as language modeling. However, all such models are based on the conventional classification framework, where the model is trained against one-hot targets, and each word is represented both as an input and as an output in isolation. This causes inefficiencies in learning both in terms of utilizing all of the information and in terms of the number of parameters needed to train. We introduce a novel theoretical framework that facilitates better learning in language modeling, and show that our framework leads to tying together the input embedding and the output projection matrices, greatly reducing the number of trainable variables. Our framework leads to state of the art performance on the Penn Treebank with a variety of network models.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Our framework leads to state of the art performance on the Penn Treebank\"",
      "Epochs": "75.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "SPIDER2",
      "Organization": "Griffith University,University of Iowa,Dezhou University",
      "Publication date": "2016-10-28",
      "Domain": "Biology",
      "Task": "Protein folding prediction,Proteins",
      "Parameters": "409536.0",
      "Parameters notes": "Three networks, each three layers. First takes in 459 inputs and outputs 12, second and third take in 459 + (12*17) = 663 inputs.\n\nNetwork 1: (459 * 150 + 150) + (150 * 150 + 150) + (150 * 150 + 150) + (150 * 12 + 12) = 116,112\nNetworks 2 and 3: (663 * 150 + 150) + (150 * 150 + 150) + (150 * 150 + 150) + (150 * 12 + 12) = 146,712\nTotal: 116,112 + (2 * 146,712) = 409,536",
      "Training compute (FLOP)": "1.822e+16",
      "Training compute notes": "120 epochs, dataset 5789 proteins. There are about 300 residues per protein (115,479 residues / 418 proteins) according to https://www.ncbi.nlm.nih.gov/pmc/articles/PMC22960/. \nFirst network gets 27 features per residue, second and third get 39.\nFLOPs from first: 6 * 116112 * (27 * 300 * 5789 * 120) = 3.92e15\nFLOPs from 2nd and 3rd: 2 *6 * 146712 * (39 * 300 * 5789 * 120) = 1.43e16\nTotal: 1.822E16",
      "Training dataset": "Unspecified",
      "Training dataset size (gradients)": "13893600",
      "Dataset size notes": "5,789 nonredundant, high resolution structure.\nAssuming ~200 residues per protein, 5,789 * 200 = 1,157,800 residues. Each residue has 12 associated features being predicted on.\n1,157,800 * 12 = 13,893,600",
      "Confidence": "Likely",
      "Link": "https://link.springer.com/protocol/10.1007/978-1-4939-6406-2_6",
      "Reference": "SPIDER2: A Package to Predict Secondary Structure, Accessible Surface Area, and Main-Chain Torsional Angles by Deep Neural Networks",
      "Citations": "",
      "Authors": "Yuedong Yang, Rhys Heffernan, Kuldip Paliwal, James Lyons, Abdollah Dehzangi, Alok Sharma, Jihua Wang, Abdul Sattar, and Yaoqi Zhou",
      "Abstract": "Predicting one-dimensional structure properties has played an important role to improve prediction of protein three-dimensional structures and functions. The most commonly predicted properties are secondary structure and accessible surface area (ASA) representing local and nonlocal structural characteristics, respectively. Secondary structure prediction is further complemented by prediction of continuous main-chain torsional angles. Here we describe a newly developed method SPIDER2 that utilizes three iterations of deep learning neural networks to improve the prediction accuracy of several structural properties simultaneously. For an independent test set of 1199 proteins SPIDER2 achieves 82 % accuracy for secondary structure prediction, 0.76 for the correlation coefficient between predicted and actual solvent accessible surface area, 19\u00b0 and 30\u00b0 for mean absolute errors of backbone \u03c6 and \u03c8 angles, respectively, and 8\u00b0 and 32\u00b0 for mean absolute errors of C\u03b1-based \u03b8 and \u03c4 angles, respectively. The method provides state-of-the-art, all-in-one accurate prediction of local structure and solvent accessible surface area. The method is implemented, as a webserver along with a standalone package that are available in our website: http://sparks-lab.org.",
      "Organization categorization": "Academia,Academia,Academia",
      "Country (of organization)": "Australia,United States of America,China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "The method provides state-of-the-art, all-in-one accurate prediction of local structure and solvent accessible surface area. ",
      "Epochs": "120.0",
      "Training time (hours)": "",
      "Training time notes": "The authors had a website where sequences could be submitted for processing through the model: \"Each prediction is usually completed within 10 min, but may take up to a few hours depending on how busy the server is and how long the protein chain is [...] Using an external PSSM file can skip the most time consuming step of generating the evolution profile by PSIBLAST, and the executive time reduce to a few seconds\" \n\nRough estimate:\nIt looks like the PSIBLAST step only needs doing once per input, and this takes the majority of the time. If the inference server uses the same hardware that was used for training, 10 mins * 5789 sequences =  965 hours for PSIBLAST calculation. Then assume training on a sequence takes 3x as long as inference (forward + backward pass uses 6 FLOPs per parameter, vs 2 for forward only), so 120 epochs would take:\n3 seconds * 3 * 5789 * 120 = 1,737 hours\nTotal: around 2,702 hours\n(This seems on the long side \u2013 probably they had better hardware for training, or else there's an incorrect assumption here)",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (non-commercial)",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "some kind of download, unclear license\n\nhttp://zhouyq-lab.szbl.ac.cn/download/",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GAWWN",
      "Organization": "University of Michigan,Max Planck Institute for Informatics",
      "Publication date": "2016-10-08",
      "Domain": "Vision",
      "Task": "Image generation",
      "Parameters": "",
      "Parameters notes": "DCGAN-style generator/discriminator with up to 128\u00d7128 output\nChar-CNN-GRU encoder (~20\u201325M parameters) from [Reed et al. 2016a].\nDual-pathway generator/discriminator roughly doubles convolutional parameters.",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "235760",
      "Dataset size notes": "directly stated in paper",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/Learning-What-and-Where-to-Draw-Reed-Akata/cad4ac0d2389a89cf1955dd4788278c1e8ac1af9",
      "Reference": "Learning What and Where to Draw",
      "Citations": "",
      "Authors": "Scott E. Reed, Zeynep Akata, S. Mohan, Samuel Tenka, B. Schiele, Honglak Lee",
      "Abstract": "Generative Adversarial Networks (GANs) have recently demonstrated the capability to synthesize compelling real-world images, such as room interiors, album covers, manga, faces, birds, and flowers. While existing models can synthesize images based on global constraints such as a class label or caption, they do not provide control over pose or object location. We propose a new model, the Generative Adversarial What-Where Network (GAWWN), that synthesizes images given instructions describing what content to draw in which location. We show high-quality 128 x 128 image synthesis on the Caltech-UCSD Birds dataset, conditioned on both informal text descriptions and also object location. Our system exposes control over both the bounding box around the bird and its constituent parts. By modeling the conditional distributions over part locations, our system also enables conditioning on arbitrary subsets of parts (e.g. only the beak and tail), yielding an efficient interface for picking part locations.",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "United States of America,Germany",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "https://paperswithcode.com/sota/text-to-image-generation-on-cub \n\nI don't see any standard benchmarks where they would claim SOTA results",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Xception",
      "Organization": "Google",
      "Publication date": "2016-10-07",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "22855952.0",
      "Parameters notes": "Table 3",
      "Training compute (FLOP)": "4.36e+20",
      "Training compute notes": "60 K80 GPUs * 30 days * 8.5 TFLOPS/GPU * 0.33 utilization  = 4.36e20\n\nAuthors of \"AI and Memory Wall\" (https://github.com/amirgholami/ai_and_memory_wall) estimated model's training compute as 450,000 PFLOP = 4.5*10^20 FLOP",
      "Training dataset": "JFT",
      "Training dataset size (gradients)": "350000000",
      "Dataset size notes": "\"JFT is an internal Google dataset for large-scale image classification dataset, first introduced by Hinton et al. in [5], which comprises over 350 million high-resolution images annotated with labels from a set of 17,000 classes. To evaluate the performance of a model trained on JFT, we use an auxiliary dataset, FastEval14k\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1610.02357",
      "Reference": "Xception: Deep Learning with Depthwise Separable Convolutions",
      "Citations": "16557.0",
      "Authors": "Fran\u00e7ois Chollet",
      "Abstract": "We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "720.0",
      "Training time notes": "\"while the JFT experiments took over one month each.\"",
      "Training hardware": "NVIDIA Tesla K80",
      "Hardware quantity": "60.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "13108.852355728606",
      "Compute cost notes": "",
      "Training power draw (W)": "37828.64014138488",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "313248.7726351885",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "20829.84110091743",
      "Training compute cost (upfront)": "689117.635066259"
    },
    {
      "Model": "Pointer Sentinel-LSTM (medium)",
      "Organization": "MetaMind Inc,Salesforce",
      "Publication date": "2016-09-26",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "21000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "7490000000000000.0",
      "Training compute notes": "6 FLOP / parameter / token * 21000000 parameters * 929000 tokens * 64 epochs = 7.491456e+15 FLOP",
      "Training dataset": "Penn TreeBank (PTB)",
      "Training dataset size (gradients)": "929000",
      "Dataset size notes": "\"We also halve the learning rate when validation perplexity is worse than the previous iteration, stopping training when validation perplexity fails to improve for three epochs or when 64 epochs are reached\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1609.07843",
      "Reference": "Pointer Sentinel Mixture Models",
      "Citations": "3451.0",
      "Authors": "Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher",
      "Abstract": "Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.",
      "Organization categorization": "Industry,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\"Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM\"",
      "Epochs": "64.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GNMT",
      "Organization": "Google",
      "Publication date": "2016-09-26",
      "Domain": "Language",
      "Task": "Translation",
      "Parameters": "278000000.0",
      "Parameters notes": "Table 5 in 'Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer'\n\nhttps://arxiv.org/abs/1701.06538",
      "Training compute (FLOP)": "6.620000000001e+21",
      "Training compute notes": "From AI and Compute:\n\"sqrt(10 * 100) factor added because production model used 2-3 orders of magnitude more data, but only 1 epoch rather than 10.\n96 K80 GPU\u2019s * 9 days * 8.5 TFLOPS * 0.33 utilization * sqrt(10 * 100)  \n= 6.9e6 PF = 79 pfs-days\"\nsource: https://openai.com/blog/ai-and-compute/\n\nhttps://www.wolframalpha.com/input?i=96+*+9+days+*+8.5+TFLOPS+*+0.33+*+sqrt%281000%29\n",
      "Training dataset": "",
      "Training dataset size (gradients)": "720000000",
      "Dataset size notes": "[WORDS]\n\" On WMT En\u2192Fr, the training set contains 36M sentence pairs. On WMT En\u2192De, the training set contains 5M sentence pairs.\"\n\"we also test GNMT on Google\u2019s translation production corpora, which are two to three decimal orders of magnitudes bigger than the WMT corpora for a given language pair.\"\n\n41M sentence pairs * 2 sentences per pair * 15 words/sentence * 10^2.5",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/1609.08144",
      "Reference": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
      "Citations": "7072.0",
      "Authors": "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, \u0141ukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, Jeffrey Dean",
      "Abstract": "Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (\"wordpieces\") for both input and output. This method provides a good balance between the flexibility of \"character\"-delimited models and the efficiency of \"word\"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "1.0",
      "Training time (hours)": "",
      "Training time notes": "Test model used 96 K80 for 9 days, then this was scaled up by 31x for the production model, but unclear how many GPUs were used or how long it was trained for. The production run used 96 * 9 days * sqrt(1000) ~= 655730 chip-hours.",
      "Training hardware": "NVIDIA Tesla K80",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "201331.569937687",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Hosted access (no API)",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "presumably deployed via Google translate",
      "Numerical format": "FP32",
      "Frontier model": "True",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Wide Residual Network",
      "Organization": "Universit\u00e9 Paris-Est",
      "Publication date": "2016-09-19",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "1280000",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1605.07146",
      "Reference": "Wide Residual Networks",
      "Citations": "8554.0",
      "Authors": "Sergey Zagoruyko, Nikos Komodakis",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "France",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ResNet-1001",
      "Organization": "Microsoft",
      "Publication date": "2016-09-17",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "10200000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "\"On CIFAR, ResNet-1001 takes about 27 h to train on 2 GPUs\"",
      "Training dataset": "CIFAR-10,CIFAR-100",
      "Training dataset size (gradients)": "50000",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://link.springer.com/chapter/10.1007/978-3-319-46493-0_38",
      "Reference": "Identity Mappings in Deep Residual Networks",
      "Citations": "10822.0",
      "Authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ResNet-200",
      "Organization": "Microsoft Research Asia",
      "Publication date": "2016-09-17",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "2.9741645e+19",
      "Training compute notes": "\"ResNet-200 takes about 3 weeks to train on 8 GPUs\". didn't specify which GPU\nupd: \ncommon GPU performance for 2016 is 6.83E+12 FLOPs/s (https://epoch.ai/blog/estimating-training-compute#forward-pass-compute-and-parameter-counts-of-common-layers) \nthen 6.83E+12*3*7*24*3600*8*0.3=2.9741645e+19 (Speculative)",
      "Training dataset": "ImageNet",
      "Training dataset size (gradients)": "1281167",
      "Dataset size notes": "",
      "Confidence": "Speculative",
      "Link": "https://link.springer.com/chapter/10.1007/978-3-319-46493-0_38",
      "Reference": "Identity Mappings in Deep Residual Networks",
      "Citations": "10822.0",
      "Authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",
      "Abstract": "Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR10 (4.62 % error) and CIFAR-100, and a 200-layer ResNet on ImageNet.",
      "Organization categorization": "Industry",
      "Country (of organization)": "China",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "500.0",
      "Training time notes": "\"about 3 weeks\"",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "Open access (non-commercial)",
      "Accessibility notes": " https://github.com/KaimingHe/resnet-1k-layers\nno definite license",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Youtube recommendation model",
      "Organization": "Google",
      "Publication date": "2016-09-15",
      "Domain": "Recommendation",
      "Task": "Recommender system",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://research.google/pubs/pub45530/",
      "Reference": "Deep Neural Networks for YouTube Recommendations",
      "Citations": "3531.0",
      "Authors": "Paul Covington, Jay Adams, and Emre Sargin",
      "Abstract": "YouTube represents one of the largest scale and most sophisticated industrial recommendation systems in existence. In this paper, we describe the system at a high level and focus on the dramatic performance improvements brought by deep learning. The paper is split according to the classic two-stage information retrieval dichotomy: first, we detail a deep candidate generation model and then describe a separate deep ranking model. We also provide practical lessons and insights derived from designing, iterating and maintaining a massive recommendation system with enormous user-facing impact.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,Historical significance,Significant use",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "WaveNet",
      "Organization": "Google DeepMind",
      "Publication date": "2016-09-12",
      "Domain": "Speech",
      "Task": "Text-to-speech (TTS),Speech synthesis,Audio generation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "11520000000",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1609.03499",
      "Reference": "WaveNet: A Generative Model for Raw Audio",
      "Citations": "7904.0",
      "Authors": "A Oord, S Dieleman, H Zen, K Simonyan",
      "Abstract": "This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "MS-ensemble-speech-recognition",
      "Organization": "Microsoft",
      "Publication date": "2016-09-12",
      "Domain": "Speech",
      "Task": "Speech recognition (ASR)",
      "Parameters": "3172117056.0",
      "Parameters notes": "Large ensemble\nCNNs: 85000000 + 38000000 + 65000000 = 188000000 = 188M\nLSTMs: 40 / 140 input - 6*512 layers - 9000 / 27000 output\n4*(140+512)*512 + 5*4*(512+512)*512 + 4*(512+27000)*27000=2983117056\nLM: 1000*1000 = 1000000 + Embedding\nTotal: 1000000+2983117056+188000000=3172117056 (underestimate because it doesn't account for embeddings)\n",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Fisher,Switchboard",
      "Training dataset size (gradients)": "11140000000",
      "Dataset size notes": "2000h of audio data + 85M words of text data",
      "Confidence": "Likely",
      "Link": "https://www.semanticscholar.org/paper/The-microsoft-2016-conversational-speech-system-Xiong-Droppo/ac94ef90be9b0c3bf744d6744e47b38855f9a4c7",
      "Reference": "The microsoft 2016 conversational speech recognition system",
      "Citations": "",
      "Authors": "Wayne Xiong, J. Droppo, Xuedong Huang, F. Seide, M. Seltzer, A. Stolcke, Dong Yu, G. Zweig",
      "Abstract": "We describe Microsoft's conversational speech recognition system, in which we combine recent developments in neural-network-based acoustic and language modeling to advance the state of the art on the Switchboard recognition task. Inspired by machine learning ensemble techniques, the system uses a range of convolutional and recurrent neural networks. I-vector modeling and lattice-free MMI training provide significant gains for all acoustic model architectures. Language model rescoring with multiple forward and backward running RNNLMs, and word posterior-based system combination provide a 20% boost. The best single system uses a ResNet architecture acoustic model with RNNLM rescoring, and achieves a word error rate of 6.9% on the NIST 2000 Switchboard task. The combined system has an error rate of 6.2%, representing an improvement over previously reported results on this benchmark task.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "https://paperswithcode.com/sota/speech-recognition-on-switchboard-hub500 ",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "LF-MMI",
      "Organization": "Johns Hopkins University,Cornell University",
      "Publication date": "2016-09-08",
      "Domain": "Speech",
      "Task": "Speech recognition (ASR)",
      "Parameters": "16600000.0",
      "Parameters notes": "Largest model: TDNN-A: 16.6 million parameters (Table 2)",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Switchboard",
      "Training dataset size (gradients)": "720000",
      "Dataset size notes": "300hr of audio, number of words unclear",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/Purely-Sequence-Trained-Neural-Networks-for-ASR-on-Povey-Peddinti/6ce6a9a30cd69bd2842a4b581cf48c6815bdfdd8",
      "Reference": "Purely sequence-trained neural networks for ASR based on lattice-free MMI",
      "Citations": "",
      "Authors": "Daniel Povey, Vijayaditya Peddinti, Daniel Galvez, Pegah Ghahremani, Vimal Manohar, Xingyu Na, Yiming Wang, S. Khudanpur",
      "Abstract": "In this paper we describe a method to perform sequencediscriminative training of neural network acoustic models without the need for frame-level cross-entropy pre-training. We use the lattice-free version of the maximum mutual information (MMI) criterion: LF-MMI. To make its computation feasible we use a phone n-gram language model, in place of the word language model. To further reduce its space and time complexity we compute the objective function using neural network outputs at one third the standard frame rate. These changes enable us to perform the computation for the forward-backward algorithm on GPUs. Further the reduced output frame-rate also provides a significant speed-up during decoding. We present results on 5 different LVCSR tasks with training data ranging from 100 to 2100 hours. Models trained with LFMMI provide a relative word error rate reduction of \u223c11.5%, over those trained with cross-entropy objective function, and \u223c8%, over those trained with cross-entropy and sMBR objective functions. A further reduction of \u223c2.5%, relative, can be obtained by fine tuning these models with the word-lattice based sMBR objective function.",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "SOTA on Speech Recognition on WSJ eval92",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DenseNet-264",
      "Organization": "Tsinghua University,Facebook AI Research,Cornell University",
      "Publication date": "2016-08-25",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "34000000.0",
      "Parameters notes": "Figure 3",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "ILSVRC 2012 subset of ImageNet",
      "Training dataset size (gradients)": "",
      "Dataset size notes": " On ImageNet,\nwe train models for 90 epochs with a batch size of 256.",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1608.06993",
      "Reference": "Densely Connected Convolutional Networks",
      "Citations": "40834.0",
      "Authors": "Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger",
      "Abstract": "Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at this https URL .",
      "Organization categorization": "Academia,Industry,Academia",
      "Country (of organization)": "China,United States of America,France,United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "90.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "",
      "Accessibility notes": "BSD-3-Clause license\nhttps://github.com/liuzhuang13/DenseNet",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "SimpleNet",
      "Organization": "Sensifai,Islamic Azad University,Technicolor R&I,Institute for Research in Fundamental Sciences (IPM)",
      "Publication date": "2016-08-22",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "5480000.0",
      "Parameters notes": "SOTA CIFAR-10 model was 5.48m params",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "CIFAR-10,ImageNet",
      "Training dataset size (gradients)": "1280000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1608.06037",
      "Reference": "Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures",
      "Citations": "126.0",
      "Authors": "Seyyed Hossein Hasanpour, Mohammad Rouhani, Mohsen Fayyaz, Mohammad Sabokrou",
      "Abstract": "Major winning Convolutional Neural Networks (CNNs), such as AlexNet, VGGNet, ResNet, GoogleNet, include tens to hundreds of millions of parameters, which impose considerable computation and memory overhead. This limits their practical use for training, optimization and memory efficiency. On the contrary, light-weight architectures, being proposed to address this issue, mainly suffer from low accuracy. These inefficiencies mostly stem from following an ad hoc procedure. We propose a simple architecture, called SimpleNet, based on a set of designing principles, with which we empirically show, a well-crafted yet simple and reasonably deep architecture can perform on par with deeper and more complex architectures. SimpleNet provides a good tradeoff between the computation/memory efficiency and the accuracy. Our simple 13-layer architecture outperforms most of the deeper and complex architectures to date such as VGGNet, ResNet, and GoogleNet on several well-known benchmarks while having 2 to 25 times fewer number of parameters and operations. This makes it very handy for embedded systems or systems with computational and memory limitations. We achieved state-of-theart result on CIFAR10 outperforming several heavier architectures, near state of the art on MNIST and highly competitive results on CIFAR100 and SVHN. We also outperformed the much larger and deeper architectures such as VGGNet and popular variants of ResNets among others on the ImageNet dataset. Models are made available at: https://github.com/Coderx7/SimpleNet",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "Belgium,Iran (Islamic Republic of),France,Iran (Islamic Republic of)",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We achieved state-of-theart result on CIFAR10 outperforming several heavier architectures\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA GeForce GTX 980",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Character-enriched word2vec",
      "Organization": "Facebook AI Research",
      "Publication date": "2016-07-15",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1607.04606",
      "Reference": "Enriching Word Vectors with Subword Information",
      "Citations": "10426.0",
      "Authors": "Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov",
      "Abstract": "Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,France",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "repo here, unclear how it corresponds to models in this paper:\nhttps://github.com/facebookresearch/fastText ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "CCL",
      "Organization": "SenseTime,Chinese University of Hong Kong (CUHK),Chinese Academy of Sciences",
      "Publication date": "2016-06-27",
      "Domain": "Vision",
      "Task": "Face detection",
      "Parameters": "",
      "Parameters notes": "The paper trains multiple regressions but the total parameter count is unclear.",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "AFLW",
      "Training dataset size (gradients)": "20000",
      "Dataset size notes": "[images]\nTable 1",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/Unconstrained-Face-Alignment-via-Cascaded-Learning-Zhu-Li/4396563ee3ec167932071e7936111f6a8899e905",
      "Reference": "Unconstrained Face Alignment via Cascaded Compositional Learning",
      "Citations": "",
      "Authors": "Shizhan Zhu, Cheng Li, Chen Change Loy, Xiaoou Tang",
      "Abstract": "We present a practical approach to address the problem of unconstrained face alignment for a single image. In our unconstrained problem, we need to deal with large shape and appearance variations under extreme head poses and rich shape deformation. To equip cascaded regressors with the capability to handle global shape variation and irregular appearance-shape relation in the unconstrained scenario, we partition the optimisation space into multiple domains of homogeneous descent, and predict a shape as a composition of estimations from multiple domain-specific regressors. With a specially formulated learning objective and a novel tree splitting function, our approach is capable of estimating a robust and meaningful composition. In addition to achieving state-of-the-art accuracy over existing approaches, our framework is also an efficient solution (350 FPS), thanks to the on-the-fly domain exclusion mechanism and the capability of leveraging the fast pixel feature.",
      "Organization categorization": "Industry,Academia,Academia",
      "Country (of organization)": "Hong Kong,Hong Kong,China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "https://paperswithcode.com/sota/face-alignment-on-aflw-19 ",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "project page (no code or weights are released): https://mmlab.ie.cuhk.edu.hk/projects/compositional.html",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "R-FCN",
      "Organization": "Tsinghua University,Microsoft Research",
      "Publication date": "2016-06-21",
      "Domain": "Vision",
      "Task": "Object detection",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "7.1935776e+17",
      "Training compute notes": "1,464  images in 2012 VOC (https://paperswithcode.com/dataset/pascal-voc)/\n9,963 images in 2007 VOC (https://www.tensorflow.org/datasets/catalog/voc)\n83K training images in MS COCO  (https://paperswithcode.com/dataset/coco)\n\nThey used a Nvidia K40 GPU and report training time/image in seconds (table 3)\n\nAssumed a 0.33 util rate\n\nSection 4.2 (MS COCO):\n\"Next we evaluate on the MS COCO dataset [ 13 ] that has 80 object categories. Our experiments involve the 80k train set, 40k val set, and 20k test-dev set. We set the learning rate as 0.001 for 90k iterations and 0.0001 for next 30k iterations, with an effective mini-batch size of 8.\"\n0.45s K40 training time per image (table 3)\nK40 has 5046000000000 FLOP/s\n\nTotal examples: 120000*8=960000 (12 epochs)\n5046000000000 FLOP/s * 960000 images * 0.45 s/image * 0.33 utilization = 719357760000000000 FLOP",
      "Training dataset": "PASCAL VOC 2007,PASCAL VOC 2012,COCO",
      "Training dataset size (gradients)": "10624000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1605.06409",
      "Reference": "R-fcn: Object detection via region-based fully convolutional networks.",
      "Citations": "5916.0",
      "Authors": "Jifeng Dai, Y. Li, Kaiming He, and Jian Sun",
      "Abstract": "",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "China,United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Segmental RNN",
      "Organization": "University of Edinburgh,Carnegie Mellon University (CMU),University of Washington",
      "Publication date": "2016-06-20",
      "Domain": "Speech",
      "Task": "Speech recognition (ASR)",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "TIMIT",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1603.00223",
      "Reference": "Segmental Recurrent Neural Networks for End-to-end Speech Recognition",
      "Citations": "",
      "Authors": "Liang Lu, Lingpeng Kong, Chris Dyer, Noah A. Smith, Steve Renals",
      "Abstract": "We study the segmental recurrent neural network for end-to-end acoustic modelling. This model connects the segmental conditional random field (CRF) with a recurrent neural network (RNN) used for feature extraction. Compared to most previous CRF-based acoustic models, it does not rely on an external system to provide features or segmentation boundaries. Instead, this model marginalises out all the possible segmentations, and features are extracted from the RNN trained together with the segmental CRF. In essence, this model is self-contained and can be trained end-to-end. In this paper, we discuss practical training and decoding issues as well as the method to speed up the training in the context of speech recognition. We performed experiments on the TIMIT dataset. We achieved 17.3 phone error rate (PER) from the first-pass decoding --- the best reported result using CRFs, despite the fact that we only used a zeroth-order CRF and without using any language model. ",
      "Organization categorization": "Academia,Academia,Academia",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland,United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "https://paperswithcode.com/sota/speech-recognition-on-timit",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "CMS-RCNN",
      "Organization": "IEEE",
      "Publication date": "2016-06-17",
      "Domain": "Vision",
      "Task": "Face detection",
      "Parameters": "138000000.0",
      "Parameters notes": "Based on VGG16 architecture, actual parameter count is slightly larger since the model contains some small added layers which are not described in detail.",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "159,424 annotated faces collected in 12,880 images",
      "Confidence": "Likely",
      "Link": "https://www.semanticscholar.org/paper/CMS-RCNN%3A-Contextual-Multi-Scale-Region-based-CNN-Zhu-Zheng/e8b2a98f87b7b2593b4a046464c1ec63bfd13b51",
      "Reference": "CMS-RCNN: Contextual Multi-Scale Region-based CNN for Unconstrained Face Detection",
      "Citations": "",
      "Authors": "Chenchen Zhu, Yutong Zheng, Khoa Luu, M. Savvides",
      "Abstract": "Robust face detection in the wild is one of the ultimate components to support various facial related problems, i.e., unconstrained face recognition, facial periocular recognition, facial landmarking and pose estimation, facial expression recognition, 3D facial model construction, etc. Although the face detection problem has been intensely studied for decades with various commercial applications, it still meets problems in some real-world scenarios due to numerous challenges, e.g., heavy facial occlusions, extremely low resolutions, strong illumination, exceptional pose variations, image or video compression artifacts, etc. In this paper, we present a face detection approach named Contextual Multi-Scale Region-based Convolution Neural Network (CMS-RCNN) to robustly solve the problems mentioned above. Similar to the region-based CNNs, our proposed network consists of the region proposal component and the region-of-interest (RoI) detection component. However, far apart of that network, there are two main contributions in our proposed network that play a significant role to achieve the state-of-the-art performance in face detection. First, the multi-scale information is grouped both in region proposal and RoI detection to deal with tiny face regions. Second, our proposed network allows explicit body contextual reasoning in the network inspired from the intuition of human vision system. The proposed approach is benchmarked on two recent challenging face detection databases, i.e., the WIDER FACE Dataset which contains high degree of variability, as well as the Face Detection Dataset and Benchmark (FDDB). The experimental results show that our proposed approach trained on WIDER FACE Dataset outperforms strong baselines on WIDER FACE Dataset by a large margin, and consistently achieves competitive results on FDDB against the recent state-of-the-art face detection methods.",
      "Organization categorization": "Industry",
      "Country (of organization)": "Multinational",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "https://paperswithcode.com/sota/face-detection-on-wider-face-medium \n\n\"The experimental results show that our proposed approach trained on WIDER FACE Dataset outperforms strong baselines on WIDER FACE Dataset by a large margin, and consistently achieves competitive results on FDDB against the recent state-of-the-art face detection methods.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "VGG16",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "PixelCNN",
      "Organization": "Google DeepMind",
      "Publication date": "2016-06-16",
      "Domain": "Vision",
      "Task": "Image generation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "ImageNet",
      "Training dataset size (gradients)": "15728640000",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1606.05328",
      "Reference": "Conditional Image Generation with PixelCNN Decoders\n",
      "Citations": "3079.0",
      "Authors": "Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, Koray Kavukcuoglu",
      "Abstract": "This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-ofthe-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "Best performance on NLL test.",
      "Epochs": "",
      "Training time (hours)": "60.0",
      "Training time notes": "We were able to achieve similar performance to the PixelRNN (Row LSTM [30]) in less than half the training time (60 hours using 32 GPUs).",
      "Training hardware": "",
      "Hardware quantity": "32.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "LRR-4X",
      "Organization": "UC Irvine",
      "Publication date": "2016-05-08",
      "Domain": "Vision",
      "Task": "Semantic segmentation",
      "Parameters": "138000000.0",
      "Parameters notes": "VGG16 with some added layers which are not specified in detail.",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Cityscapes,PASCAL VOC 2007,COCO",
      "Training dataset size (gradients)": "152231040",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://www.semanticscholar.org/paper/Laplacian-Pyramid-Reconstruction-and-Refinement-for-Ghiasi-Fowlkes/2796c448023b78fd77f3a4b57966f257c5e654c2",
      "Reference": "Laplacian Pyramid Reconstruction and Refinement for Semantic Segmentation",
      "Citations": "",
      "Authors": "Golnaz Ghiasi, Charless C. Fowlkes",
      "Abstract": "CNN architectures have terrific recognition performance but rely on spatial pooling which makes it difficult to adapt them to tasks that require dense, pixel-accurate labeling. This paper makes two contributions: (1) We demonstrate that while the apparent spatial resolution of convolutional feature maps is low, the high-dimensional feature representation contains significant sub-pixel localization information. (2) We describe a multi-resolution reconstruction architecture based on a Laplacian pyramid that uses skip connections from higher resolution feature maps and multiplicative gating to successively refine segment boundaries reconstructed from lower-resolution maps. This approach yields state-of-the-art semantic segmentation results on the PASCAL VOC and Cityscapes segmentation benchmarks without resorting to more complex random-field inference or instance detection driven architectures.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "https://paperswithcode.com/sota/semantic-segmentation-on-cityscapes\n\n\"This approach yields state-of-the-art semantic segmentation results on the PASCAL VOC and Cityscapes segmentation benchmarks without resorting to more complex random-field inference or instance detection driven architectures.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "1.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "VGG16",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Gated HORNN (3rd order)",
      "Organization": "York University",
      "Publication date": "2016-04-30",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "8970000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Penn TreeBank (PTB)",
      "Training dataset size (gradients)": "22400000",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/1605.00064",
      "Reference": "Higher Order Recurrent Neural Networks",
      "Citations": "77.0",
      "Authors": "Rohollah Soltani, Hui Jiang",
      "Abstract": "In this paper, we study novel neural network structures to better model long term dependency in sequential data. We propose to use more memory units to keep track of more preceding states in recurrent neural networks (RNNs), which are all recurrently fed to the hidden layers as feedback through different weighted paths. By extending the popular recurrent structure in RNNs, we provide the models with better short-term memory mechanism to learn long term dependency in sequences. Analogous to digital filters in signal processing, we call these structures as higher order RNNs (HORNNs). Similar to RNNs, HORNNs can also be learned using the back-propagation through time method. HORNNs are generally applicable to a variety of sequence modelling tasks. In this work, we have examined HORNNs for the language modeling task using two popular data sets, namely the Penn Treebank (PTB) and English text8 data sets. Experimental results have shown that the proposed HORNNs yield the state-of-the-art performance on both data sets, significantly outperforming the regular RNNs as well as the popular LSTMs.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Canada",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Both FOFEbased pooling and gated HORNNs have achieved the stateof-the-art performance, i.e., 100 in perplexity on this task.\nTo the best of our knowledge, this is the best reported performance on PTB under the same training condition.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Dueling DQN",
      "Organization": "Google DeepMind",
      "Publication date": "2016-04-05",
      "Domain": "Games",
      "Task": "Atari",
      "Parameters": "1700000.0",
      "Parameters notes": "Same parameter count as DQN",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1511.06581",
      "Reference": "Dueling Network Architectures for Deep Reinforcement Learning",
      "Citations": "",
      "Authors": "Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, Nando de Freitas",
      "Abstract": "In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"the dueling architecture enables our RL agent to\noutperform the state-of-the-art on the Atari 2600\ndomain\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Template Adaptation\n",
      "Organization": "University of Oxford",
      "Publication date": "2016-03-12",
      "Domain": "Vision",
      "Task": "Face recognition",
      "Parameters": "138000000.0",
      "Parameters notes": "Trains SVM on top of VGG-Face (VGG16 architecture). SVM parameters are not included in the estimate.",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "7797",
      "Dataset size notes": "\"IJB-A contains 5712 images and 2085 videos of 500 subjects, for an average of 11.4 images and 4.2 videos per subject.\"\nUnclear how the videos were converted to training images.",
      "Confidence": "Likely",
      "Link": "https://www.semanticscholar.org/paper/Template-Adaptation-for-Face-Verification-and-Crosswhite-Byrne/42515cd370f5edf4ac6c8d4bc3f6e0d848f6b73a",
      "Reference": "Template Adaptation for Face Verification and Identification",
      "Citations": "",
      "Authors": "Nate Crosswhite, J. Byrne, C. Stauffer, Omkar M. Parkhi, Qiong Cao, Andrew Zisserman",
      "Abstract": "Face recognition performance evaluation has traditionally focused on one-to-one verification, popularized by the Labeled Faces in the Wild dataset [1] for imagery and the YouTubeFaces dataset [2] for videos. In contrast, the newly released IJB-A face recognition dataset [3] unifies evaluation of one-to-many face identification with one-to-one face verification over templates, or sets of imagery and videos for a subject. In this paper, we study the problem of template adaptation, a form of transfer learning to the set of media in a template. Extensive performance evaluations on IJB-A show a surprising result, that perhaps the simplest method of template adaptation, combining deep convolutional network features with template specific linear SVMs, outperforms the state-of-the-art by a wide margin. We study the effects of template size, negative set construction and classifier fusion on performance, then compare template adaptation to convolutional networks with metric learning, 2D and 3D alignment. Our unexpected conclusion is that these other methods, when combined with template adaptation, all achieve nearly the same top performance on IJB-A for templatebased face verification and identification.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "https://paperswithcode.com/sota/face-verification-on-ijb-a \n\n\" Extensive performance evaluations on IJB-A show a surprising result, that perhaps the simplest method of template adaptation, combining deep convolutional network features with template specific linear SVMs, outperforms the state-of-the-art by a wide margin.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "VGG-Face",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Named Entity Recognition model",
      "Organization": "Carnegie Mellon University (CMU)",
      "Publication date": "2016-03-04",
      "Domain": "Language",
      "Task": "Named entity recognition (NER),Language modeling",
      "Parameters": "",
      "Parameters notes": "Architecture in Table 1",
      "Training compute (FLOP)": "9.69408e+16",
      "Training compute notes": "8 hours of training for NER\nGeForce GTX TITAN X GPU\n0.33 utilization rate\n",
      "Training dataset": "CoNLL2003",
      "Training dataset size (gradients)": "204567",
      "Dataset size notes": "Table 2. 204567 tokens",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1603.01354",
      "Reference": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF",
      "Citations": "3100.0",
      "Authors": "Xuezhe Ma, Eduard Hovy",
      "Abstract": "State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of handcrafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data preprocessing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets or two sequence labeling tasks \u2014 Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both datasets \u2014 97.55% accuracy for POS tagging and 91.21% F1 for NER.\n",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Discretionary",
      "Notability criteria notes": "",
      "Epochs": "50.0",
      "Training time (hours)": "8.0",
      "Training time notes": "\"the model training requires about 12 hours for POS tagging and 8\nhours for NER\"",
      "Training hardware": "NVIDIA GeForce GTX TITAN X",
      "Hardware quantity": "1.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "290.0785080846343",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "1329.0340383037474",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Double DQN",
      "Organization": "Google DeepMind",
      "Publication date": "2016-03-02",
      "Domain": "Games",
      "Task": "Atari",
      "Parameters": "1500000.0",
      "Parameters notes": "\"approximately 1.5M parameters\nin total\"",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://ojs.aaai.org/index.php/AAAI/article/view/10295",
      "Reference": "Deep Reinforcement Learning with Double Q-Learning",
      "Citations": "",
      "Authors": "Hado van Hasselt, Arthur Guez, \nDavid Silver",
      "Abstract": "The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Table 1 reports summary statistics for this evaluation\n(under human starts) on the 49 games from Mnih et al. (2015). Double DQN obtains clearly higher median and mean scores. Again\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "SqueezeNet",
      "Organization": "DeepScale,University of California (UC) Berkeley,Stanford University",
      "Publication date": "2016-02-24",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "1200000.0",
      "Parameters notes": "The paper says \"SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters.\"\n\nAlexNet has 60 million parameters.",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "1280000",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/1602.07360",
      "Reference": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size",
      "Citations": "6976.0",
      "Authors": "Forrest N. Iandola, Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, Kurt Keutzer",
      "Abstract": "",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "United States of America,United States of America,United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Inceptionv4",
      "Organization": "Google",
      "Publication date": "2016-02-23",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "43000000.0",
      "Parameters notes": "\"The folks from Google strike again with Inception-v4, 43M parameters.\"\n\nhttps://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "1280000",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/1602.07261",
      "Reference": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",
      "Citations": "13208.0",
      "Authors": "Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Inception-ResNet-V2",
      "Organization": "Google",
      "Publication date": "2016-02-23",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "56000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "1280000",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/1602.07261",
      "Reference": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning",
      "Citations": "13208.0",
      "Authors": "Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "A3C FF hs",
      "Organization": "Google,University of Montreal / Universit\u00e9 de Montr\u00e9al",
      "Publication date": "2016-02-04",
      "Domain": "Games",
      "Task": "Atari",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "200000000",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "http://arxiv.org/abs/1602.01783v2",
      "Reference": "Asynchronous Methods for Deep Reinforcement Learning",
      "Citations": "9583.0",
      "Authors": "V Mnih, AP Badia, M Mirza, A Graves",
      "Abstract": "We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,Canada",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "Table 1 \nSOTA by Mean human-normalized score in 57 Atari games",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "AlphaGo Lee",
      "Organization": "DeepMind",
      "Publication date": "2016-01-27",
      "Domain": "Games",
      "Task": "Go",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "1.9e+21",
      "Training compute notes": "This number is pretty uncertain. I expect it to be right to around a factor of 3, at least compared to AlphaGo Fan.\n\nThe architecture used was pretty much the same as AlphaGo Fan, but it was \"trained for longer\" and had around 5.33x the number of convolutional layers of AlphaGo Fan (256/48 = 5.33). \n\nThe convolutional layers are the major contributor to the training compute, so I somewhat arbitrarily just multiply the compute for AlphaGo Fan by 5. Thus 3.8e20 * 5 = 1.9e21\n\nOtherwise there has been little said about this model specifically - I've mainly relied on the source for AlphaGo Zero and AlphaGo Fan, linked below\n\nAlphaGo Fan: https://www.nature.com/articles/nature16961\n\nAlphaGo Zero: https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ",
      "Training dataset": "",
      "Training dataset size (gradients)": "300000000",
      "Dataset size notes": "We trained the policy network p\u03c3 to classify positions according to expert moves played in the KGS data set. This data set contains 29.4 million positions from 160,000 games played by KGS 6 to 9 dan human players; 35.4% of the games are handicap games.",
      "Confidence": "Speculative",
      "Link": "https://www.nature.com/articles/nature16961",
      "Reference": "Mastering the game of Go with deep neural networks and tree search",
      "Citations": "18175.0",
      "Authors": "David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, Demis Hassabis",
      "Abstract": "The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses \u2018value networks\u2019 to evaluate board positions and \u2018policy networks\u2019 to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "696.0",
      "Training time notes": "Training times are given for several components:\n- Policy network classifier: 3 weeks\n- Policy network RL: 1 day\n- Value network regression: 1 week\n- Rollout policy: \"Similar to the policy network, the weights \u03c0 of the rollout policy are trained from 8 million positions from human games on the Tygem server to maximize log likelihood by stochastic gradient descent. Rollouts execute at approximately 1,000 simulations per second per CPU thread on an empty board.\" could suggest (8M sims / 1000 sims/sec) / 3600 sec/hr = 2.2 hours, if each position corresponds to only one simulation (unclear)\n\nTotal: 29 days or 696 hours",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "22206.80954117433",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Variational (untied weights, MC) LSTM (Large)",
      "Organization": "University of Cambridge",
      "Publication date": "2015-12-16",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "66000000.0",
      "Parameters notes": "66M according to https://arxiv.org/pdf/1611.01462",
      "Training compute (FLOP)": "5886144000000000.0",
      "Training compute notes": "6 FLOP / parameter / token * 66000000 parameters * 929000 tokens * 16 epochs = 5.886144e+15 FLOP",
      "Training dataset": "Penn TreeBank (PTB)",
      "Training dataset size (gradients)": "929000",
      "Dataset size notes": "\"We had to use early stopping for the large model with [20]\u2019s variant as the model starts overfitting after 16 epochs.\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1512.05287",
      "Reference": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks",
      "Citations": "1838.0",
      "Authors": "Yarin Gal, Zoubin Ghahramani",
      "Abstract": "Recurrent neural networks (RNNs) stand at the forefront of many recent developments in deep learning. Yet a major difficulty with these models is their tendency to overfit, with dropout shown to fail when applied to recurrent layers. Recent results at the intersection of Bayesian modelling and deep learning offer a Bayesian interpretation of common deep learning techniques such as dropout. This grounding of dropout in approximate Bayesian inference suggests an extension of the theoretical results, offering insights into the use of dropout with RNN models. We apply this new variational inference based dropout technique in LSTM and GRU models, assessing it on language modelling and sentiment analysis tasks. The new approach outperforms existing techniques, and to the best of our knowledge improves on the single model state-of-the-art in language modelling with the Penn Treebank (73.4 test perplexity). This extends our arsenal of variational tools in deep learning.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\"The new approach outperforms existing techniques, and to the best of our knowledge improves on the single model state-of-the-art in language modelling with the Penn Treebank (73.4 test perplexity)\"",
      "Epochs": "16.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Advantage Learning",
      "Organization": "Google DeepMind",
      "Publication date": "2015-12-15",
      "Domain": "Games",
      "Task": "Atari",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "100000000",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "http://arxiv.org/abs/1512.04860v1",
      "Reference": "Increasing the Action Gap: New Operators for Reinforcement Learning",
      "Citations": "158.0",
      "Authors": "MG Bellemare, G Ostrovski, A Guez",
      "Abstract": "This paper introduces new optimality-preserving operators on Q-functions. We first describe an operator for tabular representations, the consistent Bellman operator, which incorporates a notion of local policy consistency. We show that this local consistency leads to an increase in the action gap at each state; increasing this gap, we argue, mitigates the undesirable effects of approximation and estimation errors on the induced greedy policies. This operator can also be applied to discretized continuous space and time problems, and we provide empirical results evidencing superior performance in this context. Extending the idea of a locally consistent operator, we then derive sufficient conditions for an operator to preserve optimality, leading to a family of operators which includes our consistent Bellman operator. As corollaries we provide a proof of optimality for Baird's advantage learning algorithm and derive other gap-increasing operators with interesting properties. We conclude with an empirical study on 60 Atari 2600 games illustrating the strong potential of these new operators.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We use our operators to obtain state-of-the-art empirical results on the Arcade Learning Environment (Bellemare et al. 2013).\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ResNet-152 (ImageNet)",
      "Organization": "Microsoft",
      "Publication date": "2015-12-10",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "60200000.0",
      "Parameters notes": "Taken from https://arxiv.org/abs/1605.07146",
      "Training compute (FLOP)": "1.041408e+19",
      "Training compute notes": "11.3 *10^9 mult-adds per forward pass (Table 1)\n2 FLOPS/ mult-add\n3 for forward & backward pass\n1.2 * 10^6 examples in dataset\n128 epochs\n\n-> 1.041408 \u00d7 10^19 FLOP\n\nAuthors of \"AI and Memory Wall\" (https://github.com/amirgholami/ai_and_memory_wall) estimated model's training compute as 11,000 PFLOP = 1.1*10^19 FLOP",
      "Training dataset": "ILSVRC 2012 subset of ImageNet",
      "Training dataset size (gradients)": "1280000",
      "Dataset size notes": "\"We evaluate our method on the ImageNet 2012 classification dataset [36] that consists of 1000 classes. The models are trained on the 1.28 million training images\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1512.03385",
      "Reference": "Deep Residual Learning for Image Recognition",
      "Citations": "215518.0",
      "Authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",
      "Abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "120.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "256.0",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ResNet-110 (CIFAR-10)",
      "Organization": "Microsoft",
      "Publication date": "2015-12-10",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "1700000.0",
      "Parameters notes": "Table 6",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "50000",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/1512.03385",
      "Reference": "Deep Residual Learning for Image Recognition",
      "Citations": "215518.0",
      "Authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ResNet-101 (ImageNet)",
      "Organization": "Microsoft",
      "Publication date": "2015-12-10",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "44500000.0",
      "Parameters notes": "Taken from https://arxiv.org/abs/1605.07146",
      "Training compute (FLOP)": "7.004e+18",
      "Training compute notes": "Forward FLOP: 15200000000\n120 epochs\n1280000*120*3*15200000000=7.00416e+18\n",
      "Training dataset": "ILSVRC 2012 subset of ImageNet",
      "Training dataset size (gradients)": "1280000",
      "Dataset size notes": "\"We evaluate our method on the ImageNet 2012 classification dataset [36] that consists of 1000 classes. The models are trained on the 1.28 million training images\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1512.03385",
      "Reference": "Deep Residual Learning for Image Recognition",
      "Citations": "215518.0",
      "Authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",
      "Abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "120.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "SSD",
      "Organization": "",
      "Publication date": "2015-12-08",
      "Domain": "Vision",
      "Task": "Object detection",
      "Parameters": "",
      "Parameters notes": "Can be calculated from the VGG-16 paper and Figure 2",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "COCO",
      "Training dataset size (gradients)": "2300000",
      "Dataset size notes": "Multiple datasets were used (PASCAL VOC, ILSVRC, COCO) but afaict COCO is the largest. Per this paper https://arxiv.org/abs/1703.06870v1, final paragraph before section 4.1, \"As in previous work [3, 21], we train using the union of 80k train images and a 35k subset of val images (trainval35k), and report ablations on the remaining 5k subset of val images (minival)\". So trainval35k is 80k + 35k = 115k",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/pdf/1512.02325",
      "Reference": "SSD: Single Shot MultiBox Detector",
      "Citations": "39468.0",
      "Authors": "Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg",
      "Abstract": "We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For 300\u00d7300 input, SSD achieves 72.1% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for 500\u00d7500 input, SSD achieves 75.1% mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at this https URL",
      "Organization categorization": "",
      "Country (of organization)": "",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "Also listed in Denis Panjuta's List of 100+ AI Algorithms",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "Note on training hardware below: unclear if the Titan X was for testing or training. \"We measure the speed with batch size 8 using Titan X and cuDNN v4 with Intel Xeon E5-2667v3@3.20GHz.\" I don't know if it's common to use the same hardware for both testing and training.\n\n",
      "Training hardware": "NVIDIA GeForce GTX TITAN X",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "VGG16",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Inception v3",
      "Organization": "Google,University College London (UCL)",
      "Publication date": "2015-12-02",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "23626728.0",
      "Parameters notes": "Table 3 from Xception paper",
      "Training compute (FLOP)": "1e+20",
      "Training compute notes": "Authors of \"AI and Memory Wall\" (https://github.com/amirgholami/ai_and_memory_wall) estimated model's training compute as 100,000 PFLOP = 1*10^20 FLOP",
      "Training dataset": "ILSVRC 2012 subset of ImageNet",
      "Training dataset size (gradients)": "1200000",
      "Dataset size notes": "The full dataset is a lot larger and has far more categories. When people say \"ImageNet\" they're usually referring to the subset of the full dataset with 1000 categories and 1.2million images, found here: https://image-net.org/challenges/LSVRC/2012/",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/1512.00567",
      "Reference": "Rethinking the inception architecture for computer vision.",
      "Citations": "29696.0",
      "Authors": "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna",
      "Abstract": "Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error on the validation set (3.6% error on the test set) and 17.3% top-1 error on the validation set.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "1218.1810104797394",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Netflix Recommender System",
      "Organization": "Netflix",
      "Publication date": "2015-12-01",
      "Domain": "Recommendation",
      "Task": "Recommender system",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://dl.acm.org/doi/pdf/10.1145/2843948",
      "Reference": "The Netflix Recommender System: Algorithms, Business Value, and Innovation",
      "Citations": "1092.0",
      "Authors": "CA Gomez-Uribe, N Hunt",
      "Abstract": "This article discusses the various algorithms that make up the Netflix recommender system, and describes its business purpose. We also describe the role of search and related algorithms, which for us turns into a\nrecommendations problem as well. We explain the motivations behind and review the approach that we use to improve the recommendation algorithms, combining A/B testing focused on improving member retention and medium term engagement, as well as offline experimentation using historical member engagement data. We discuss some of the issues in designing and interpreting A/B tests. Finally, we describe some current areas of focused innovation, which include making our recommender system global and language aware.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,Significant use,Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Multi-scale Dilated CNN",
      "Organization": "Princeton University,Intel Labs",
      "Publication date": "2015-11-23",
      "Domain": "Vision",
      "Task": "Image segmentation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1511.07122",
      "Reference": "Multi-Scale Context Aggregation by Dilated Convolutions",
      "Citations": "9057.0",
      "Authors": "Fisher Yu, Vladlen Koltun",
      "Abstract": "",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "3DDFA",
      "Organization": "Chinese Academy of Sciences,Michigan State University",
      "Publication date": "2015-11-23",
      "Domain": "Vision",
      "Task": "Face detection,3D reconstruction",
      "Parameters": "5355584.0",
      "Parameters notes": "Parameters based on Figure 2:\nCNN layers: 4*4*16*6+4*4*32*16=9728\nLocal CNN: 3*3*48*32*11*11+3*3*64*49*9*9=3958848\nFC: 9*9*64*256+234*256=1387008\nTotal: 9728+3958848+1387008=5355584\n",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "300W-LP",
      "Training dataset size (gradients)": "293901",
      "Dataset size notes": "\"We divide 300W-LP into 97,967 samples for training and 24,483 samples for testing, without identity overlapping. \"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1511.07212",
      "Reference": "Face Alignment in Full Pose Range: A 3D Total Solution\n",
      "Citations": "",
      "Authors": "Xiangyu Zhu, Zhen Lei, Xiaoming Liu, Hailin Shi, S. Li",
      "Abstract": "Face alignment, which fits a face model to an image and extracts the semantic meanings of facial pixels, has been an important topic in CV community. However, most algorithms are designed for faces in small to medium poses (below 45), lacking the ability to align faces in large poses up to 90. The challenges are three-fold: Firstly, the commonly used landmark-based face model assumes that all the landmarks are visible and is therefore not suitable for profile views. Secondly, the face appearance varies more dramatically across large poses, ranging from frontal view to profile view. Thirdly, labelling landmarks in large poses is extremely challenging since the invisible landmarks have to be guessed. In this paper, we propose a solution to the three problems in an new alignment framework, called 3D Dense Face Alignment (3DDFA), in which a dense 3D face model is fitted to the image via convolutional neutral network (CNN). We also propose a method to synthesize large-scale training samples in profile views to solve the third problem of data labelling. Experiments on the challenging AFLW database show that our approach achieves significant improvements over state-of-the-art methods.",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "China,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "https://paperswithcode.com/sota/3d-face-reconstruction-on-florence \n\n\"Experiments on the challenging AFLW database show that our approach achieves significant improvements over state-of-the-art methods.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT license:\nhttps://github.com/cleardusk/3DDFA (refers to the improved later version of the paper)",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "SAF R-CNN",
      "Organization": "Beijing Institute of Technology,Sun Yat-sen University,Panasonic R&D,National University of Singapore",
      "Publication date": "2015-10-28",
      "Domain": "Vision",
      "Task": "Object detection",
      "Parameters": "138000000.0",
      "Parameters notes": "Taken from VGG16 base model, ignoring minor architectural changes",
      "Training compute (FLOP)": "1.2311081250000001e+19",
      "Training compute notes": "Base model: 12291000000000002000\nBase model inference FLOP: 15300000000\nFinetune for 7 epochs with 62500 training examples\n15300000000*3*62500*7=20081250000000000=2e16\n\nTotal compute: 12291000000000002000+20081250000000000=12311081250000002000",
      "Training dataset": "",
      "Training dataset size (gradients)": "350000",
      "Dataset size notes": "\"There are totally 350,000 bounding boxes of about 2,300 unique pedestrians labeled in 250,000 frames\"\n\"We use dense sampling of the training data (every 4th frame\"",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/1510.08160",
      "Reference": "Scale-aware Fast R-CNN for Pedestrian Detection",
      "Citations": "",
      "Authors": "Jianan Li, Xiaodan Liang, ShengMei Shen, Tingfa Xu, Jiashi Feng, Shuicheng Yan",
      "Abstract": "In this work, we consider the problem of pedestrian detection in natural scenes. Intuitively, instances of pedestrians with different spatial scales may exhibit dramatically different features. Thus, large variance in instance scales, which results in undesirable large intra-category variance in features, may severely hurt the performance of modern object instance detection methods. We argue that this issue can be substantially alleviated by the divide-and-conquer philosophy. Taking pedestrian detection as an example, we illustrate how we can leverage this philosophy to develop a Scale-Aware Fast R-CNN (SAF R-CNN) framework. The model introduces multiple built-in sub-networks which detect pedestrians with scales from disjoint ranges. Outputs from all the sub-networks are then adaptively combined to generate the final detection results that are shown to be robust to large variance in instance scales, via a gate function defined over the sizes of object proposals. Extensive evaluations on several challenging pedestrian detection datasets well demonstrate the effectiveness of the proposed SAF R-CNN. Particularly, our method achieves state-of-the-art performance on Caltech, INRIA, and ETH, and obtains competitive results on KITTI. ",
      "Organization categorization": "Academia,Academia,Industry,Academia",
      "Country (of organization)": "China,China,Singapore,Singapore",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Pedestrian Detection on Caltech benchmark\n\n\"our method achieves state-of-the-art performance on Caltech, INRIA, and ETH, and obtains competitive results on KITTI. \"",
      "Epochs": "7.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA GeForce GTX TITAN X",
      "Hardware quantity": "1.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "290.9065505304121",
      "Base model": "VGG16",
      "Finetune compute (FLOP)": "2.008125e+16",
      "Finetune compute notes": "15300000000*3*62500*7=20081250000000000=2e16",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "1457.1166495707328",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "AlphaGo Fan",
      "Organization": "DeepMind",
      "Publication date": "2015-10-01",
      "Domain": "Games",
      "Task": "Go",
      "Parameters": "8209984.0",
      "Parameters notes": "The input to the policy network is a 19\u2009\u00d7\u200919\u2009\u00d7\u200948 image stack consisting of 48 feature planes. The first hidden layer zero pads the input into a 23\u2009\u00d7\u200923 image, then convolves k filters of kernel size 5\u2009\u00d7\u20095 with stride 1 with the input image and applies a rectifier nonlinearity. Each of the subsequent hidden layers 2 to 12 zero pads the respective previous hidden layer into a 21\u2009\u00d7\u200921 image, then convolves k filters of kernel size 3\u2009\u00d7\u20093 with stride 1, again followed by a rectifier nonlinearity. The final layer convolves 1 filter of kernel size 1\u2009\u00d7\u20091 with stride 1, with a different bias for each position, and applies a softmax function. The match version of AlphaGo used k\u2009=\u2009192 filters; Fig. 2b and Extended Data Table 3 additionally show the results of training with k\u2009=\u2009128, 256 and 384 filters.\n\nThe input to the value network is also a 19\u2009\u00d7\u200919\u2009\u00d7\u200948 image stack, with an additional binary feature plane describing the current colour to play. Hidden layers 2 to 11 are identical to the policy network, hidden layer 12 is an additional convolution layer, hidden layer 13 convolves 1 filter of kernel size 1\u2009\u00d7\u20091 with stride 1, and hidden layer 14 is a fully connected linear layer with 256 rectifier units. The output layer is a fully connected linear layer with a single tanh unit.",
      "Training compute (FLOP)": "3.8e+20",
      "Training compute notes": "Assume 0.3 utilisation rate, 1e13 GPU FLOP/s [single precision]. Trained in three stages using 50 GPUs over 3 weeks + 1 day + 1 week\n\nTraining compute = (50 GPUs)(29 days)(86400s/day)(0.3 utilisation rate)(1e13 FLOP/s) = 3.8e20 FLOPs\n",
      "Training dataset": "",
      "Training dataset size (gradients)": "12697600000",
      "Dataset size notes": "Supervised learning + self-play",
      "Confidence": "Likely",
      "Link": "https://www.nature.com/articles/nature16961",
      "Reference": "Mastering the game of Go with deep neural networks and tree search",
      "Citations": "18175.0",
      "Authors": "David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, Demis Hassabis",
      "Abstract": "A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo\u2019s own move selections and also the winner of AlphaGo\u2019s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100\u20130 against the previously published, champion-defeating AlphaGo.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "4828.322276803044",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Deep Deterministic Policy Gradients",
      "Organization": "Google DeepMind",
      "Publication date": "2015-09-09",
      "Domain": "Robotics",
      "Task": "Robotic manipulation,Self-driving car",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1509.02971",
      "Reference": "Continuous control with deep reinforcement learning",
      "Citations": "14582.0",
      "Authors": "TP Lillicrap, JJ Hunt, A Pritzel, N Heess, T Erez",
      "Abstract": "We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America,United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "BPE",
      "Organization": "University of Edinburgh",
      "Publication date": "2015-08-31",
      "Domain": "Language",
      "Task": "Translation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "WMT'15",
      "Training dataset size (gradients)": "50000000",
      "Dataset size notes": "[WORDS]\n\"We perform experiments on data from the shared translation task of WMT 2015. For English\u2192German, our training set consists of 4.2 million sentence pairs, or approximately 100 million tokens. For English\u2192Russian, the training set consists of 2.6 million sentence pairs, or approximately 50 million tokens\"\n\n100M tokens, around half will be in English, 0.75 words per token\n\n",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/1508.07909",
      "Reference": "Neural Machine Translation of Rare Words with Subword Units",
      "Citations": "7232.0",
      "Authors": "R Sennrich, B Haddow, A Birch",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DCNN",
      "Organization": "University of Maryland,Rutgers University",
      "Publication date": "2015-08-07",
      "Domain": "Vision",
      "Task": "Face verification",
      "Parameters": "5006000.0",
      "Parameters notes": "Table 1",
      "Training compute (FLOP)": "4.8098691e+17",
      "Training compute notes": "Total compute: 1535436288 forward pass flops *3* 1000000 iterations * 128 batch size = 589607534592000000 FLOP = 5.89607534592e17 FLOP\n\nForward pass FLOPs based on CNN details in Table 1:\nConv11: 2*3*3*32*100*100*1=5760000\nConv12: 2*3*3*64*100*100*32=368640000\nConv21: 2*3*3*64*50*50*64=184320000\nConv22: 2*3*3*64*50*50*128=368640000\nConv32: 2*3*3*128*25*25*96=138240000\nConv33: 2*3*3*96*25*25*192=207360000\nConv41: 2*3*3*192*13*13*128=74760192\nConv42: 2*3*3*128*13*13*256=99680256\nConv51: 2*3*3*256*7*7*160=36126720\nConv52: 2*3*3*160*7*7*320=45158400\nFc6: 2*320*10548=6750720\nTotal forward pass FLOPs: 5760000+368640000+184320000+368640000+138240000+207360000+74760192+99680256+36126720+45158400+6750720=1535436288\n\n\n1682000000000 FLOP / GPU / sec * 1 GPU * 9 days * 24 hours / day * 3600 sec / hour * 0.3 [assumed utilization] = 3.9237696e+17 FLOP\n\nsqrt(5.89607534592e17 * 3.9237696e+17) = 4.8098691e+17",
      "Training dataset": "CASIA Face Dataset",
      "Training dataset size (gradients)": "490356",
      "Dataset size notes": "\"The CASIA-WebFace dataset contains 494,414 face images of 10,575 subjects downloaded from the IMDB website. After removing the 27 overlapping subjects with the IJB-A dataset, there are 10548 subjects 1 and 490,356 face images.\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1508.01722",
      "Reference": "Unconstrained Face Verification using Deep CNN Features",
      "Citations": "",
      "Authors": "Jun-Cheng Chen, Vishal M. Patel, Rama Chellappa",
      "Abstract": "In this paper, we present an algorithm for unconstrained face verification based on deep convolutional features and evaluate it on the newly released IARPA Janus Benchmark A (IJB-A) dataset. The IJB-A dataset includes real-world unconstrained faces from 500 subjects with full pose and illumination variations which are much harder than the traditional Labeled Face in the Wild (LFW) and Youtube Face (YTF) datasets. The deep convolutional neural network (DCNN) is trained using the CASIA-WebFace dataset. Extensive experiments on the IJB-A dataset are provided.",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "https://paperswithcode.com/sota/face-verification-on-ijb-a ",
      "Epochs": "26.0",
      "Training time (hours)": "216.0",
      "Training time notes": "\"The DCNN model is trained for about 9 days using NVidia Tesla K40.\"",
      "Training hardware": "NVIDIA Tesla K40c",
      "Hardware quantity": "1.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "285.60949180561556",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Deep CNN + COTS",
      "Organization": "IEEE",
      "Publication date": "2015-07-26",
      "Domain": "Vision",
      "Task": "Face recognition",
      "Parameters": "5006000.0",
      "Parameters notes": "Taken from Table 1 of https://arxiv.org/abs/1508.01722 which uses the same architecture",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "494414",
      "Dataset size notes": "CASIA [6] dataset provides a large collection of labeled (based on subject names) training set for deep learning networks. It contains 494,414 images of 10,575 subjects",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/Face-Search-at-Scale%3A-80-Million-Gallery-Wang-Otto/be38934db99182c0e3100be58cd0299151b30625",
      "Reference": "Face Search at Scale: 80 Million Gallery",
      "Citations": "",
      "Authors": "Dayong Wang, C. Otto, Anil K. Jain",
      "Abstract": "Due to the prevalence of social media websites, one challenge facing computer vision researchers is to devise methods to process and search for persons of interest among the billions of shared photos on these websites. Facebook revealed in a 2013 white paper that its users have uploaded more than 250 billion photos, and are uploading 350 million new photos each day. Due to this humongous amount of data, large-scale face search for mining web images is both important and challenging. Despite significant progress in face recognition, searching a large collection of unconstrained face images has not been adequately addressed. To address this challenge, we propose a face search system which combines a fast search procedure, coupled with a state-of-the-art commercial off the shelf (COTS) matcher, in a cascaded framework. Given a probe face, we first filter the large gallery of photos to find the top-k most similar faces using deep features generated from a convolutional neural network. The k candidates are re-ranked by combining similarities from deep features and the COTS matcher. We evaluate the proposed face search system on a gallery containing 80 million web-downloaded face images. Experimental results demonstrate that the deep features are competitive with state-of-the-art methods on unconstrained face recognition benchmarks (LFW and IJB-A). Further, the proposed face search system offers an excellent trade-off between accuracy and scalability on datasets consisting of millions of images. Additionally, in an experiment involving searching for face images of the Tsarnaev brothers, convicted of the Boston Marathon bombing, the proposed face search system could find the younger brother's (Dzhokhar Tsarnaev) photo at rank 1 in 1 second on a 5M gallery and at rank 8 in 7 seconds on an 80M gallery.",
      "Organization categorization": "Industry",
      "Country (of organization)": "Multinational",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "https://paperswithcode.com/sota/face-verification-on-ijb-a \n\n\"Experimental results demonstrate that the deep features are competitive with state-of-the-art methods on unconstrained face recognition benchmarks (LFW and IJB-A). \"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA Tesla K40c",
      "Hardware quantity": "1.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "285.6858261304193",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "CompACT-Deep",
      "Organization": "University of California San Diego",
      "Publication date": "2015-07-19",
      "Domain": "Vision",
      "Task": "Object detection",
      "Parameters": "",
      "Parameters notes": "Uses combinations of finetuned VGG and AlexNet models",
      "Training compute (FLOP)": "",
      "Training compute notes": "Finetunes of VGG and AlexNet",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "From the paper: For Caltech, we used the training set size of [21]\n\n[21] is https://arxiv.org/pdf/1406.1134. From that paper:\n\nFor the following experiments, we rely solely on the training set of the Caltech Pedestrian Dataset [10]. Of the 71 minute long training videos (\u223c128k images), we use every fourth video as validation data and the rest for training.\n\n128k * 3/4 = 96k",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/Learning-Complexity-Aware-Cascades-for-Deep-Cai-Saberian/c3bd3b9782dc504ee4f2b8a12bd9c562a5c0d7ad",
      "Reference": "Learning Complexity-Aware Cascades for Deep Pedestrian Detection",
      "Citations": "",
      "Authors": "Zhaowei Cai, M. Saberian, N. Vasconcelos",
      "Abstract": "The design of complexity-aware cascaded detectors, combining features of very different complexities, is considered. A new cascade design procedure is introduced, by formulating cascade learning as the Lagrangian optimization of a risk that accounts for both accuracy and complexity. A boosting algorithm, denoted as complexity aware cascade training (CompACT), is then derived to solve this optimization. CompACT cascades are shown to seek an optimal trade-off between accuracy and complexity by pushing features of higher complexity to the later cascade stages, where only a few difficult candidate patches remain to be classified. This enables the use of features of vastly different complexities in a single detector. In result, the feature pool can be expanded to features previously impractical for cascade design, such as the responses of a deep convolutional neural network (CNN). This is demonstrated through the design of a pedestrian detector with a pool of features whose complexities span orders of magnitude. The resulting cascade generalizes the combination of a CNN with an object proposal mechanism: rather than a pre-processing stage, CompACT cascades seamlessly integrate CNNs in their stages. This enables state of the art performance on the Caltech and KITTI datasets, at fairly fast speeds.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "https://paperswithcode.com/sota/pedestrian-detection-on-caltech\n\n\"This enables state of the art performance on the Caltech and KITTI datasets, at fairly fast speeds.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA Tesla K40m",
      "Hardware quantity": "1.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "285.7303639078396",
      "Base model": "AlexNet,VGG16",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "BatchNorm",
      "Organization": "Google",
      "Publication date": "2015-06-15",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "13600000.0",
      "Parameters notes": "\"The network contains 13.6 \u00b7 106 parameters\"",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "ImageNet",
      "Training dataset size (gradients)": "12441600000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1502.03167",
      "Reference": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "Citations": "45577.0",
      "Authors": "Sergey Ioffe, Christian Szegedy",
      "Abstract": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "72.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "GoogLeNet / InceptionV1",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "YOLO",
      "Organization": "University of Washington,Allen Institute for AI,Facebook AI Research",
      "Publication date": "2015-06-08",
      "Domain": "Vision",
      "Task": "Object detection",
      "Parameters": "271684800.0",
      "Parameters notes": "Calculation based on figure 3 of the paper:\n7 * 7 * 3 * 64 + 3 * 3 * 64 * 192 + 1 * 1 * 192 * 128 + 3 * 3 * 128 * 256 + 1 * 1 * 256 * 256 + 3 * 3 * 256 * 512 + 4 * (1 * 1 * 512 * 256 + 3 * 3 * 256 * 512) + 1 * 1 * 512 * 512 + 3 * 3 * 512 * 1024 + 2 * (1 * 1 * 1024 * 512 + 3 * 3 * 512 * 1024) + 4 * (3 * 3 * 1024 * 1024) + 7 * 7 * 1024 * 4096 + 4096 * 7 * 7 * 30",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/1506.02640",
      "Reference": "You Only Look Once: Unified, Real-Time Object Detection",
      "Citations": "32413.0",
      "Authors": "Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi",
      "Abstract": "",
      "Organization categorization": "Academia,Research collective,Industry",
      "Country (of organization)": "United States of America,United States of America,United States of America,France",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "CFSS",
      "Organization": "SenseTime,Chinese University of Hong Kong (CUHK),Shenzhen Institute of Advanced Technology",
      "Publication date": "2015-06-07",
      "Domain": "Vision",
      "Task": "Face detection",
      "Parameters": "17408.0",
      "Parameters notes": "The paper trains two regression models, each using hardcoded SIFT / BRIEF features. \n68 keypoints (given in the paper) and 128 dimensional features per keypoint (assumption based on SIFT features)\n2*68*128=17408",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "300W-LP",
      "Training dataset size (gradients)": "141660",
      "Dataset size notes": "[images]",
      "Confidence": "Likely",
      "Link": "https://www.semanticscholar.org/paper/Face-alignment-by-coarse-to-fine-shape-searching-Zhu-Li/dc661f478c5a245a919c4b3faae8ea4f598b6e64",
      "Reference": "Face alignment by coarse-to-fine shape searching",
      "Citations": "",
      "Authors": "Shizhan Zhu, Cheng Li, Chen Change Loy, Xiaoou Tang",
      "Abstract": "We present a novel face alignment framework based on coarse-to-fine shape searching. Unlike the conventional cascaded regression approaches that start with an initial shape and refine the shape in a cascaded manner, our approach begins with a coarse search over a shape space that contains diverse shapes, and employs the coarse solution to constrain subsequent finer search of shapes. The unique stage-by-stage progressive and adaptive search i) prevents the final solution from being trapped in local optima due to poor initialisation, a common problem encountered by cascaded regression approaches; and ii) improves the robustness in coping with large pose variations. The framework demonstrates real-time performance and state-of-the-art results on various benchmarks including the challenging 300-W dataset.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "Hong Kong,Hong Kong,China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "https://paperswithcode.com/sota/face-alignment-on-aflw-19 \n\n\" The framework demonstrates real-time performance and state-of-the-art results on various benchmarks including the challenging 300-W dataset.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "1.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Faster R-CNN",
      "Organization": "Microsoft Research",
      "Publication date": "2015-06-04",
      "Domain": "Vision",
      "Task": "Object detection",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "102400000",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1506.01497",
      "Reference": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
      "Citations": "68927.0",
      "Authors": "S Ren, K He, R Girshick, J Sun",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "Open source",
      "Inference code accessibility": "Open source",
      "Accessibility notes": "MIT license for repo:\nhttps://github.com/ShaoqingRen/faster_rcnn \n\ncontains weights and training scripts",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "U-Net",
      "Organization": "University of Freiburg",
      "Publication date": "2015-05-18",
      "Domain": "Vision",
      "Task": "Image segmentation",
      "Parameters": "37676160.0",
      "Parameters notes": "Figure 1: 2*(1*3*3*64+64*3*3*64+64*3*3*128+128*3*3*128+128*3*3*256+256*3*3*256+256*3*3*512+512*3*3*512+512*3*3*1024+1024*3*3*1024)=37676160",
      "Training compute (FLOP)": "5.0832252e+16",
      "Training compute notes": "10*60*60*4706690000000*0.3=50832252000000000\n\u201ctraining time of only 10 hours on a NVidia Titan GPU (6 GB).\u201d\nUsing single precision FLOP: 4706.69 GFLOPS (https://www.gpuzoo.com/GPU-NVIDIA/GeForce_GTX_TITAN.html) ",
      "Training dataset": "",
      "Training dataset size (gradients)": "7864320",
      "Dataset size notes": "The training data is a set of 30 images (512x512 pixels",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/U-Net%3A-Convolutional-Networks-for-Biomedical-Image-Ronneberger-Fischer/6364fdaa0a0eccd823a779fcdd489173f938e91a",
      "Reference": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
      "Citations": "",
      "Authors": "O. Ronneberger, P. Fischer, T. Brox",
      "Abstract": "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .",
      "Organization categorization": "Academia",
      "Country (of organization)": "Germany",
      "Notability criteria": "SOTA improvement,Highly cited",
      "Notability criteria notes": "\" Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. \"",
      "Epochs": "",
      "Training time (hours)": "10.0",
      "Training time notes": "\"training time of only 10 hours on a NVidia Titan GPU (6 GB)\"",
      "Training hardware": "",
      "Hardware quantity": "1.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Fast R-CNN",
      "Organization": "Microsoft Research",
      "Publication date": "2015-04-30",
      "Domain": "Vision",
      "Task": "Object detection",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "25600000",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1504.08083",
      "Reference": "Fast R-CNN",
      "Citations": "27297.0",
      "Authors": "R Girshick",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "TC-DNN-BLSTM-DNN",
      "Organization": "Carnegie Mellon University (CMU)",
      "Publication date": "2015-04-06",
      "Domain": "Speech",
      "Task": "Speech recognition (ASR),Speech-to-text",
      "Parameters": "18413568.0",
      "Parameters notes": "3*40 inputs - 2L DNN (2048) - BLSTM (128*2) - 2L DNN (2048) - 3431 outputs\n3*40*2048 + 2048*2048 + 2*4*(2048+128)*128 + 256*2048 + 2048*2048 + 2048*3431=18413568\n",
      "Training compute (FLOP)": "1.9410191999999997e+17",
      "Training compute notes": "\"51 hours in wall clock time with a NVIDIA Tesla K20 GPU\"\nK20 FLOPs: 3524000000000\nCompute: 0.3*51*60*60*3524000000000=194101919999999970= 1.94e17\n",
      "Training dataset": "WSJ",
      "Training dataset size (gradients)": "29160000",
      "Dataset size notes": "\"We use si284 with approximately 81 hours of speech as the training set,\"",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/Deep-Recurrent-Neural-Networks-for-Acoustic-Chan-Lane/ae18f67c10a5d4da91f128ec7a6cf7c784122cd5",
      "Reference": "Deep Recurrent Neural Networks for Acoustic Modelling",
      "Citations": "",
      "Authors": "William Chan, Ian Lane",
      "Abstract": "We present a novel deep Recurrent Neural Network (RNN) model for acoustic modelling in Automatic Speech Recognition (ASR). We term our contribution as a TC-DNN-BLSTM-DNN model, the model combines a Deep Neural Network (DNN) with Time Convolution (TC), followed by a Bidirectional Long Short-Term Memory (BLSTM), and a final DNN. The first DNN acts as a feature processor to our model, the BLSTM then generates a context from the sequence acoustic signal, and the final DNN takes the context and models the posterior probabilities of the acoustic states. We achieve a 3.47 WER on the Wall Street Journal (WSJ) eval92 task or more than 8% relative improvement over the baseline DNN models.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "https://paperswithcode.com/sota/speech-recognition-on-wsj-eval92",
      "Epochs": "17.0",
      "Training time (hours)": "51.0",
      "Training time notes": "",
      "Training hardware": "NVIDIA Tesla K20m",
      "Hardware quantity": "1.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "263.01387537165505",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "genCNN + dyn eval",
      "Organization": "Chinese Academy of Sciences,Huawei Noah's Ark Lab,Dublin City University",
      "Publication date": "2015-03-17",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "8000000.0",
      "Parameters notes": "8M according to https://arxiv.org/pdf/1508.06615",
      "Training compute (FLOP)": "3.4153451e+16",
      "Training compute notes": "5046000000000 FLOP / sec/ GPU * 1 GPU * 48 hours [\"Likely\" confidence since 2 days of training refer to another dataset] * 3600 sec / hour * 0.3 [assumed utilization] = 2.6158464e+17 FLOP\n\nAssuming (!) 100 epochs (-> \"Speculative\" confidence\"):\n\n6 FLOP / parameter / token * 8000000 parameters * 929000 tokens * 100 epochs = 4.4592e+15 FLOP\n\nsqrt(2.6158464e+17*4.4592e+15 ) = 3.4153451e+16 FLOP\n\n________\nin the algorithmic progress report paper the estimation was 7.3 \u00d7 10^16 FLOP (hardware-based estimation assuming another Tesla K40 chip)\n",
      "Training dataset": "Penn TreeBank (PTB)",
      "Training dataset size (gradients)": "929000",
      "Dataset size notes": "",
      "Confidence": "Speculative",
      "Link": "https://aclanthology.org/P15-1151/",
      "Reference": "genCNN: A Convolutional Architecture for Word Sequence Prediction",
      "Citations": "33.0",
      "Authors": "Mingxuan Wang, Zhengdong Lu, Hang Li, Wenbin Jiang, Qun Liu",
      "Abstract": "We propose a convolutional neural network, named genCNN, for word sequence prediction. Different from previous work on neural networkbased language modeling and generation (e.g., RNN or LSTM), we choose not to greedily summarize the history of words as a fixed length vector. Instead, we use a convolutional neural network to predict the next word with the history of words of variable length. Also different from the existing feedforward networks for language modeling, our model can effectively fuse the local correlation and global correlation in the word sequence, with a convolution-gating strategy specifically designed for the task. We argue that our model can give adequate representation of the history, and therefore can naturally exploit both the short and long range dependencies. Our model is fast, easy to train, and readily parallelized. Our extensive experiments on text generation and n-best re-ranking in machine translation show that genCNN outperforms the state-ofthe-arts with big margins.",
      "Organization categorization": "Academia,Industry,Academia",
      "Country (of organization)": "China,China,Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"genCNN outperforms the state-ofthe-arts with big margins.\"",
      "Epochs": "",
      "Training time (hours)": "48.0",
      "Training time notes": "\" The optimization is done mainly on a Tesla K40 GPU, which takes about 2 days for the training on a dataset containing 1M sentences.\"\nthis training doesn't refer to the PTB dataset but to wiki dataset so it is likely an upper bound",
      "Training hardware": "NVIDIA Tesla K40s",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DQN-2015",
      "Organization": "Google",
      "Publication date": "2015-02-25",
      "Domain": "Games",
      "Task": "Atari",
      "Parameters": "1693362.0",
      "Parameters notes": "\"The input to the neural network consists of an 84x84x4 image produced by the preprocess-ing mapw. The first hidden layer convolves 32 filters of 8x8 with stride 4 with theinput image and applies a rectifier nonlinearity. The second hidden layer con-volves 64 filters of 4x4 with stride 2, again followed by a rectifier nonlinearity.This is followedby a thirdconvolutional layer thatconvolves 64 filtersof 3x3 withstride 1 followed by a rectifier. The final hidden layer is fully-connected and con-sists of 512 rectifier units. The output layer is a fully-connected linear layer with asingle output for each valid action. The number of valid actions varied between 4 and 18 on the games we considered.\"\n\nExample num params here: https://colab.research.google.com/drive/1Ty6SFYWd7EcKoxJohucL2OdiLR_3oXnI?usp=sharing",
      "Training compute (FLOP)": "",
      "Training compute notes": "This should be calculatable, just needs careful reasoning about compute per frame.",
      "Training dataset": "",
      "Training dataset size (gradients)": "12500000",
      "Dataset size notes": "Methods: \"we trained for a total of 50 million frames\"",
      "Confidence": "",
      "Link": "https://www.nature.com/articles/nature14236",
      "Reference": "Human-level control through deep reinforcement learning",
      "Citations": "25403.0",
      "Authors": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, Demis Hassabis ",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "TRPO",
      "Organization": "University of California (UC) Berkeley",
      "Publication date": "2015-02-19",
      "Domain": "Games",
      "Task": "Atari",
      "Parameters": "33500.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/pdf/1502.05477",
      "Reference": "Trust Region Policy Optimization",
      "Citations": "8305.0",
      "Authors": "John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel",
      "Abstract": "We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "Also listed in Denis Panjuta's List of 100+ AI Algorithms",
      "Epochs": "",
      "Training time (hours)": "30.0",
      "Training time notes": "\"The 500 iterations of our algorithm took about 30 hours (with slight variation between games) on a 16-core computer.\"",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "MSRA (C, PReLU)",
      "Organization": "Microsoft Research",
      "Publication date": "2015-02-06",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "87048800.0",
      "Parameters notes": "I used the architecture in table 3\nI ignored biases, and assumed a SPP bin size of 256\n\n3*7*7*96+96*3*3*384+384*3*3*384*5+384*3*3*768+768*3*3*768*5+768*3*3*896+896*3*3*896*5+896*(7*7+3*3+2*2+1)*4096+4096*4096+4096*1000=330581792\n\n\n\n",
      "Training compute (FLOP)": "2.397403008e+19",
      "Training compute notes": "\"training C on eight K40 GPUs, takes about 3-4 weeks\"\n0.33 util rate\n(From Imagenet paper-data, Besiroglu et al., forthcoming) ",
      "Training dataset": "",
      "Training dataset size (gradients)": "1280000",
      "Dataset size notes": "\"We perform the experiments on the 1000-class ImageNet 2012 dataset\", paper; ImageNet 2012 train set size from https://huggingface.co/datasets/imagenet-1k",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1502.01852",
      "Reference": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification",
      "Citations": "20078.0",
      "Authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",
      "Abstract": "Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass human-level performance (5.1%, Russakovsky et al.) on this visual recognition challenge.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "588.0",
      "Training time notes": "",
      "Training hardware": "NVIDIA Tesla K40t",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "1394.115708804708",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "True",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "VGG-Face",
      "Organization": "University of Oxford",
      "Publication date": "2015-01-01",
      "Domain": "Vision",
      "Task": "Face recognition",
      "Parameters": "138000000.0",
      "Parameters notes": "Their largest model uses the VGG16 architecture",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "2600000",
      "Dataset size notes": "Table 2 in https://www.bmva-archive.org.uk/bmvc/2015/papers/paper041/abstract041.pdf",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/Deep-Face-Recognition-Parkhi-Vedaldi/162ea969d1929ed180cc6de9f0bf116993ff6e06",
      "Reference": "Deep Face Recognition",
      "Citations": "",
      "Authors": "Omkar M. Parkhi, A. Vedaldi, Andrew Zisserman",
      "Abstract": "The goal of this paper is face recognition \u2013 from either a single photograph or from a set of faces tracked in a video. Recent progress in this area has been due to two factors: (i) end to end learning for the task using a convolutional neural network (CNN), and (ii) the availability of very large scale training datasets. We make two contributions: first, we show how a very large scale dataset (2.6M images, over 2.6K people) can be assembled by a combination of automation and human in the loop, and discuss the trade off between data purity and time; second, we traverse through the complexities of deep network training and face recognition to present methods and procedures to achieve comparable state of the art results on the standard LFW and YTF face benchmarks.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement,Highly cited",
      "Notability criteria notes": "https://paperswithcode.com/sota/face-verification-on-youtube-faces-db  \n\n\"achieve comparable state of the art results on the standard LFW and YTF face benchmarks.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA GeForce GTX Titan Black",
      "Hardware quantity": "4.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "2131.9956756796214",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "5455.4972816420095",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ADAM (CIFAR-10)",
      "Organization": "University of Amsterdam,OpenAI,University of Toronto",
      "Publication date": "2014-12-22",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "2370000.0",
      "Parameters notes": "CIFAR-10 with c64-c64-c128-1000 architecture\n\n\"Our CNN architecture has three alternating stages of 5x5 convolution filters and 3x3 max pooling with stride of 2 that are followed by a fully connected layer of 1000 rectified linear hidden units (ReLU\u2019s). \"\n\n\"Training cost over 45 epochs. CIFAR-10 with c64-c64-c128-1000 architecture.\"\n\nCIFAR-10's input dimension is 32x32x3\n\nParameter:\n3*64*5*5+64*64*5*5+64*128*5*5+128*4*4*1000+1000*10=2370000\n\n\n\n",
      "Training compute (FLOP)": "624979800000000.0",
      "Training compute notes": "From https://openai.com/blog/ai-and-compute/ Appendix\n\nless than 0.0007 pfs-days (86400*10^15*0.0007)\n\nManual estimate: \n3 (forward-backward adjustment) * 92589600 flop per forward pass * 50000 examples * 45 epochs = 624979800000000 (6.25e14)\n\n",
      "Training dataset": "CIFAR-10",
      "Training dataset size (gradients)": "50000",
      "Dataset size notes": "Assumed they used the standard CIFAR-10 training split",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1412.6980",
      "Reference": "Adam: A Method for Stochastic Optimization",
      "Citations": "160778.0",
      "Authors": "Diederik P. Kingma, Jimmy Ba",
      "Abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
      "Organization categorization": "Academia,Industry,Academia",
      "Country (of organization)": "Netherlands,United States of America,Canada",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Fractional Max-Pooling",
      "Organization": "University of Warwick",
      "Publication date": "2014-12-18",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "27000000.0",
      "Parameters notes": "27M weights in largest CIFAR-100 model",
      "Training compute (FLOP)": "1e+17",
      "Training compute notes": "For the 12M param model, training required \"18 hours on a GeForce GTX 780\". So would be somewhat larger for 27M.\n\n4 TFLOPS * 18 * 3600 * 0.4 = 1e17",
      "Training dataset": "CIFAR-100",
      "Training dataset size (gradients)": "901200",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/1412.6071v4",
      "Reference": "Fractional Max-Pooling",
      "Citations": "672.0",
      "Authors": "Benjamin Graham",
      "Abstract": "Convolutional networks almost always incorporate some form of spatial pooling, and very often it is alpha times alpha max-pooling with alpha=2. Max-pooling act on the hidden layers of the network, reducing their size by an integer multiplicative factor alpha. The amazing by-product of discarding 75% of your data is that you build into the network a degree of invariance with respect to translations and elastic distortions. However, if you simply alternate convolutional layers with max-pooling layers, performance is limited due to the rapid reduction in spatial size, and the disjoint nature of the pooling regions. We have formulated a fractional version of max-pooling where alpha is allowed to take non-integer values. Our version of max-pooling is stochastic as there are lots of different ways of constructing suitable pooling regions. We find that our form of fractional max-pooling reduces overfitting on a variety of datasets: for instance, we improve on the state-of-the art for CIFAR-100 without even using dropout.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"for instance, we improve on the state-of-the art for CIFAR-100 without even using dropout.\"",
      "Epochs": "250.0",
      "Training time (hours)": "18.0",
      "Training time notes": "",
      "Training hardware": "NVIDIA GeForce GTX 780",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "SNM-skip",
      "Organization": "Google",
      "Publication date": "2014-12-03",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "62000000000.0",
      "Parameters notes": "62B from Table 2",
      "Training compute (FLOP)": "2.97600000001e+20",
      "Training compute notes": "https://www.wolframalpha.com/input?i=0.8+billion+*+62+billion+*+6+FLOP",
      "Training dataset": "One Billion Word benchmark",
      "Training dataset size (gradients)": "800000000",
      "Dataset size notes": "1B from 'Our experimental setup used the One Billion Word Benchmark corpus' from section 4.1 - 'Total number of training tokens is about 0.8 billion'",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/1412.1454",
      "Reference": "Skip-gram Language Modeling Using Sparse Non-negative Matrix Probability Estimation",
      "Citations": "14.0",
      "Authors": "Noam Shazeer, Joris Pelemans, Ciprian Chelba",
      "Abstract": "We present a novel family of language model (LM) estimation techniques named Sparse Non-negative Matrix (SNM) estimation. A first set of experiments empirically evaluating it on the One Billion Word Benchmark shows that SNM n-gram LMs perform almost as well as the well-established Kneser-Ney (KN) models. When using skip-gram features the models are able to match the state-of-the-art recurrent neural network (RNN) LMs; combining the two modeling techniques yields the best known result on the benchmark. The computational advantages of SNM over both maximum entropy and RNN LM estimation are probably its main strength, promising an approach that has the same flexibility in combining arbitrary features effectively and yet should scale to very large amounts of data as gracefully as n-gram LMs do. ",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "'When using skip-gram features the models are able to match the state-of-the-art recurrent neural network (RNN) LMs; combining the two modeling techniques yields the best known result on the benchmark. ' - from abstract",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "TA-CNN",
      "Organization": "Chinese University of Hong Kong (CUHK)",
      "Publication date": "2014-11-29",
      "Domain": "Vision",
      "Task": "Object detection",
      "Parameters": "706048.0",
      "Parameters notes": "Architecture details in Figure 4\nConv1: 3*7*7*32=4704\nConv2: 32*5*5*48=38400Conv3: 48*3*3*64=27648\nConv4: 64*3*3*96=55296\nFc5: 5*2*96*500=480000\nFc6: 500*200=100000\nTotal: 4704+38400+27648+55296+480000+100000=706048\n",
      "Training compute (FLOP)": "1.0854e+16",
      "Training compute notes": "Training time: 3h\nAssumed FP32 GPU FLOPs in 2014: 3.35e+12Assumed utilization: 0.3\nTraining FLOP: 3*60*60*0.3*3.35e12=10854000000000000=1.08e16\n",
      "Training dataset": "",
      "Training dataset size (gradients)": "45000",
      "Dataset size notes": "Followed train test split detailed in Pedestrian Detection: An Evaluation of the State of the Art",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/1412.0069",
      "Reference": "Pedestrian Detection aided by Deep Learning Semantic Tasks",
      "Citations": "",
      "Authors": "Yonglong Tian, Ping Luo, Xiaogang Wang, Xiaoou Tang",
      "Abstract": "Deep learning methods have achieved great success in pedestrian detection, owing to its ability to learn features from raw pixels. However, they mainly capture middle-level representations, such as pose of pedestrian, but confuse positive with hard negative samples, which have large ambiguity, e.g. the shape and appearance of `tree trunk' or `wire pole' are similar to pedestrian in certain viewpoint. This ambiguity can be distinguished by high-level representation. To this end, this work jointly optimizes pedestrian detection with semantic tasks, including pedestrian attributes (e.g. `carrying backpack') and scene attributes (e.g. `road', `tree', and `horizontal'). Rather than expensively annotating scene attributes, we transfer attributes information from existing scene segmentation datasets to the pedestrian dataset, by proposing a novel deep model to learn high-level features from multiple tasks and multiple data sources. Since distinct tasks have distinct convergence rates and data from different datasets have different distributions, a multi-task objective function is carefully designed to coordinate tasks and reduce discrepancies among datasets. The importance coefficients of tasks and network parameters in this objective function can be iteratively estimated. Extensive evaluations show that the proposed approach outperforms the state-of-the-art on the challenging Caltech and ETH datasets, where it reduces the miss rates of previous deep models by 17 and 5.5 percent, respectively. ",
      "Organization categorization": "Academia",
      "Country (of organization)": "Hong Kong",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"he proposed approach outperforms the state-of-the-art on the challenging Caltech and ETH datasets, where it reduces the miss rates of previous deep models by 17 and 5.5 percent, respectively. \"",
      "Epochs": "",
      "Training time (hours)": "3.0",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "1.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Cascaded LNet-ANet",
      "Organization": "Chinese University of Hong Kong (CUHK)",
      "Publication date": "2014-11-28",
      "Domain": "Vision",
      "Task": "Face detection",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "ILSVRC 2012 subset of ImageNet,CelebA",
      "Training dataset size (gradients)": "9320000",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1411.7766",
      "Reference": "Deep Learning Face Attributes in the Wild",
      "Citations": "9136.0",
      "Authors": "Ziwei Liu, Ping Luo, Xiaogang Wang, Xiaoou Tang",
      "Abstract": "Predicting face attributes in the wild is challenging due to complex face variations. We propose a novel deep learning framework for attribute prediction in the wild. It cascades two CNNs, LNet and ANet, which are fine-tuned jointly with attribute tags, but pre-trained differently. LNet is pre-trained by massive general object categories for face localization, while ANet is pre-trained by massive face identities for attribute prediction. This framework not only outperforms the state-of-the-art with a large margin, but also reveals valuable facts on learning face representation.\n(1) It shows how the performances of face localization (LNet) and attribute prediction (ANet) can be improved by different pre-training strategies.\n(2) It reveals that although the filters of LNet are fine-tuned only with image-level attribute tags, their response maps over entire images have strong indication of face locations. This fact enables training LNet for face localization with only image-level annotations, but without face bounding boxes or landmarks, which are required by all attribute recognition works.\n(3) It also demonstrates that the high-level hidden neurons of ANet automatically discover semantic concepts after pre-training with massive face identities, and such concepts are significantly enriched after fine-tuning with attribute tags. Each attribute can be well explained with a sparse linear combination of these concepts.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Hong Kong",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Fully Convolutional Networks",
      "Organization": "University of California (UC) Berkeley",
      "Publication date": "2014-11-14",
      "Domain": "Vision",
      "Task": "Image segmentation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1411.4038",
      "Reference": "Fully Convolutional Networks for Semantic Segmentation",
      "Citations": "40541.0",
      "Authors": "J Long, E Shelhamer, T Darrell",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "SC-NLM",
      "Organization": "University of Toronto",
      "Publication date": "2014-11-10",
      "Domain": "Multimodal,Vision,Language",
      "Task": "Image captioning",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "COCO,Flickr30K Entities",
      "Training dataset size (gradients)": "5000000",
      "Dataset size notes": "Our LSTM encoder and SC-NLM decoder were trained by concatenating the Flickr30K dataset with the recently released Microsoft COCO dataset [46], which combined give us over 100,000 images and over 500,000 descriptions for training",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/Unifying-Visual-Semantic-Embeddings-with-Multimodal-Kiros-Salakhutdinov/2e36ea91a3c8fbff92be2989325531b4002e2afc",
      "Reference": "Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models",
      "Citations": "",
      "Authors": "Ryan Kiros, R. Salakhutdinov, R. Zemel",
      "Abstract": "Inspired by recent advances in multimodal learning and machine translation, we introduce an encoder-decoder pipeline that learns (a): a multimodal joint embedding space with images and text and (b): a novel language model for decoding distributed representations from our space. Our pipeline effectively unifies joint image-text embedding models with multimodal neural language models. We introduce the structure-content neural language model that disentangles the structure of a sentence to its content, conditioned on representations produced by the encoder. The encoder allows one to rank images and sentences while the decoder can generate novel descriptions from scratch. Using LSTM to encode sentences, we match the state-of-the-art performance on Flickr8K and Flickr30K without using object detections. We also set new best results when using the 19-layer Oxford convolutional network. Furthermore we show that with linear encoders, the learned embedding space captures multimodal regularities in terms of vector space arithmetic e.g. *image of a blue car* - \"blue\" + \"red\" is near images of red cars. Sample captions generated for 800 images are made available for comparison.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Canada",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "LRCN",
      "Organization": "UT Austin,University of Massachusetts Lowell,University of California (UC) Berkeley",
      "Publication date": "2014-11-07",
      "Domain": "Video",
      "Task": "Video description",
      "Parameters": "142552000.0",
      "Parameters notes": "1st model: CaffeNet fc6 feature extractor (4096-length vectors) -> LSTM with 1024 hidden units\n\n2nd model: CaffeNet fc6 feature extractor (4096-length vectors) -> 2 layer LSTM with 1000 hidden units\n\n3rd mode: Like the second, but has encoder and decoder LSTMs (both with 2 layers)\n\nAlexNet (close relative to CaffeNet) has 61M params.\n\nLSTM RNN number of parameters is given by L*(n*m + n^2 + n) where L:= Number of layers, n:= hidden units, m:= input vector length\n",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "TaCoS",
      "Training dataset size (gradients)": "400000",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/1411.4389",
      "Reference": "Long-term Recurrent Convolutional Networks for Visual Recognition and Description",
      "Citations": "5868.0",
      "Authors": "Jeff Donahue, Lisa Anne Hendricks, Marcus Rohrbach, Subhashini Venugopalan, Sergio Guadarrama, Kate Saenko, Trevor Darrell",
      "Abstract": "",
      "Organization categorization": "Academia,Academia,Academia",
      "Country (of organization)": "United States of America,United States of America,United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Spatially-Sparse CNN",
      "Organization": "University of Warwick",
      "Publication date": "2014-09-23",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "",
      "Parameters notes": "Parameter count not stated but is probably derivable from the paper.",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "CIFAR-10",
      "Training dataset size (gradients)": "901200",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1409.6070v1",
      "Reference": "Spatially-sparse convolutional neural networks",
      "Citations": "260.0",
      "Authors": "Benjamin Graham",
      "Abstract": "Convolutional neural networks (CNNs) perform well on problems such as handwriting recognition and image classification. However, the performance of the networks is often limited by budget and time constraints, particularly when trying to train deep networks.\nMotivated by the problem of online handwriting recognition, we developed a CNN for processing spatially-sparse inputs; a character drawn with a one-pixel wide pen on a high resolution grid looks like a sparse matrix. Taking advantage of the sparsity allowed us more efficiently to train and test large, deep CNNs. On the CASIA-OLHWDB1.1 dataset containing 3755 character classes we get a test error of 3.82%.\nAlthough pictures are not sparse, they can be thought of as sparse by adding padding. Applying a deep convolutional network using sparsity has resulted in a substantial reduction in test error on the CIFAR small picture datasets: 6.28% on CIFAR-10 and 24.30% for CIFAR-100.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "SOTA per https://paperswithcode.com/sota/image-classification-on-cifar-10",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Deeply-supervised nets",
      "Organization": "Microsoft Research",
      "Publication date": "2014-09-18",
      "Domain": "Vision",
      "Task": "Image classification,Digit recognition",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "MNIST,CIFAR-10,CIFAR-100,SVHN (Street View House Numbers)",
      "Training dataset size (gradients)": "598388",
      "Dataset size notes": "60000+50000+60000+600000",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/1409.5185",
      "Reference": "Deeply-Supervised Nets",
      "Citations": "2509.0",
      "Authors": "Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, Zhuowen Tu",
      "Abstract": "Our proposed deeply-supervised nets (DSN) method simultaneously minimizes classification error while making the learning process of hidden layers direct and transparent. We make an attempt to boost the classification performance by studying a new formulation in deep networks. Three aspects in convolutional neural networks (CNN) style architectures are being looked at: (1) transparency of the intermediate layers to the overall classification; (2) discriminativeness and robustness of learned features, especially in the early layers; (3) effectiveness in training due to the presence of the exploding and vanishing gradients. We introduce \"companion objective\" to the individual hidden layers, in addition to the overall objective at the output layer (a different strategy to layer-wise pre-training). We extend techniques from stochastic gradient methods to analyze our algorithm. The advantage of our method is evident and our experimental result on benchmark datasets shows significant performance gain over existing methods (e.g. all state-of-the-art results on MNIST, CIFAR-10, CIFAR-100, and SVHN).",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "our experimental result on benchmark datasets shows significant performance gain over existing methods (e.g. all state-of-the-art results on MNIST, CIFAR-10, CIFAR-100, and SVHN).",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GoogLeNet / InceptionV1",
      "Organization": "Google,University of Michigan,University of North Carolina",
      "Publication date": "2014-09-17",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "6797700.0",
      "Parameters notes": "Computed summing the parameters on table 1 of section 5",
      "Training compute (FLOP)": "1.51e+18",
      "Training compute notes": "AI and Compute  (https://openai.com/blog/ai-and-compute/) charts imply a value of 1.51e18 (value extracted using WebPlotDigitizer  https://automeris.io/WebPlotDigitizer/ ).\n\nBased on the paper, there are 1.5B multiply-adds per inference, and 1.2M images in the training set, but an unknown number of epochs. They decrease the learning rate by 4% every 8 epochs, so there are likely many. If the figure from AI and Compute is taken as true, there were likely 140 epochs",
      "Training dataset": "ILSVRC 2014 subset of ImageNet,ImageNet",
      "Training dataset size (gradients)": "571392000000",
      "Dataset size notes": "\"The ILSVRC 2014 classification challenge involves the\ntask of classifying the image into one of 1000 leaf-node categories in the Imagenet hierarchy. There are about 1.2 million images for training, 50,000 for validation and 100,000 images for testing\"\n...\n\"We participated in the challenge with no external data used for training.\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1409.4842",
      "Reference": "Going deeper with convolutions",
      "Citations": "46056.0",
      "Authors": "Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich",
      "Abstract": "We propose a deep convolutional neural network architecture codenamed \"Inception\", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.",
      "Organization categorization": "Industry,Academia,Academia",
      "Country (of organization)": "United States of America,United States of America,United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "827.0",
      "Training time (hours)": "",
      "Training time notes": "\"Although we used CPU based implementation only, a rough estimate suggests that the GoogLeNet network could be trained to convergence using few high-end GPUs within a week\"",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "SPN-4+KN5",
      "Organization": "Singapore University of Technology & Design,DSO National Laboratories",
      "Publication date": "2014-09-14",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "5000000.0",
      "Parameters notes": "Estimate from table 2 of https://arxiv.org/abs/1609.07843\n\nThe authors of the linked paper draw on estimates from table 3 of https://arxiv.org/pdf/1508.06615.pdf",
      "Training compute (FLOP)": "4.4e+16",
      "Training compute notes": "40h, 1 GPU, 1028e9 Peak FLOP/s, 30%\n\n1028000000000 FLOP/s/GPU * 1GPU * 40 hours * 3600 s/hour * 0.3 [assumed utilization] = 4.44096e+16 FLOP",
      "Training dataset": "Penn TreeBank (PTB)",
      "Training dataset size (gradients)": "929000",
      "Dataset size notes": "seems like the authors use a non-standard split for the dataset\n\n\"We performed our experiments on the commonly used Penn\nTreebank corpus [15], and adhered to the experimental setup\nused in previous work [6, 9]. We used sections 0-20, sections\n21-22, and sections 23-24 respectively as training, validation\nand test sets\"\n\napparently the most common split is \"In the most common split of this corpus, sections from 0 to 18 are used for training (38 219 sentences, 912 344 tokens), sections from 19 to 21 are used for validation (5 527 sentences, 131 768 tokens), and sections from 22 to 24 are used for testing (5 462 sentences, 129 654 tokens)\"\n\nunknown amount of epochs\nbatch size = 100, context length = 5? (because we look at N=4 previous words). training iterations unknown\n\n\"We used a learning rate of \u03b7= 0.1, a mini-batch size of 100,\nrandomly initialized the weights to a value between 0 and 1, and\nimposed an L2 penalty of 10\u22125 on all weights. With reference\nto Figure 2, We used K = 10000, feature vectors with D = 100\ndimensions, and N = 3 and N = 4 previous words.\"",
      "Confidence": "Likely",
      "Link": "https://www.comp.nus.edu.sg/~skok/papers/is14.pdf",
      "Reference": "Language modeling with sum-product networks",
      "Citations": "102.0",
      "Authors": "W. Cheng, Stanley Kok, Hoai Vu Pham, Hai Leong Chieu, K. M. A. Chai",
      "Abstract": "Sum product networks (SPNs) are a new class of deep probabilistic models. They can contain multiple hidden layers while keeping their inference and training times tractable. An SPN consists of interleaving layers of sum nodes and product nodes. A sum node can be interpreted as a hidden variable, and a product node can be viewed as a feature capturing rich interactions among an SPN\u2019s inputs. We show that the ability of SPN to use hidden layers to model complex dependencies among words, and its tractable inference and learning times, make it a suitable framework for a language model. Even though SPNs have been applied to a variety of vision problems [1, 2], we are the first to use it for language modeling. Our empirical comparisons with six previous language models indicate that our SPN has superior performance.",
      "Organization categorization": "Academia,Government",
      "Country (of organization)": "Singapore,Singapore",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Our empirical comparisons with\nsix previous language models indicate that our SPN has superior performance\"",
      "Epochs": "",
      "Training time (hours)": "40.0",
      "Training time notes": "\"\"e stopped training our SPN when its performance on the validation set\nstops improving at two consecutive evaluation points, or when it has run for 40 hours, whichever occurred first. (It turned out that both SPN-3 and SPN-4 ran for the maximum of 40 hours.)\nWe parallelized our SPN code2 to run on a GPU, and ran our experiments on a machine with a 2.4 GHz CPU and an NVIDIA Tesla C2075 GPU (448 CUDA cores, 5GB of device memory).\"",
      "Training hardware": "NVIDIA Tesla C2075",
      "Hardware quantity": "1.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "290.04546165358084",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "Open access (non-commercial)",
      "Accessibility notes": "code, no license specified: https://github.com/stakok/lmspn/tree/master/SPNLM \ntraining code: https://github.com/stakok/lmspn/blob/master/SPNLM/README.doc ",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Seq2Seq LSTM",
      "Organization": "Google",
      "Publication date": "2014-09-10",
      "Domain": "Language",
      "Task": "Translation",
      "Parameters": "1920000000.0",
      "Parameters notes": "The resulting LSTM has 384M parameters of which 64M are pure recurrent connections (32M for the \u201cencoder\u201d LSTM and 32M\nfor the \u201cdecoder\u201d LSTM).\nThe paper uses an ensemble of 5 LSTMs.",
      "Training compute (FLOP)": "5.6e+19",
      "Training compute notes": "384E+6 parameters * 2 FLOP/parameter * (348E+6 + 304E+6 points per epoch) * 7.5 epochs * 3 FLOP/point ~= 1.126656e+19 FLOP\nTimes 5 independent models in ensemble => 5.6E+19 FLOP\n\nIf we assume NVIDIA K40 (in use at the time): 10 days * 24 * 60 * 60 seconds/day * 8 GPUs * 33% * 5e12 FLOP/s * 5 models in ensemble ~= 5.7E+19 FLOP\n\nAuthors of \"AI and Memory Wall\" estimated model's training compute as 11,000 PFLOPS = 1.1*10^19 FLOPS\n(https://github.com/amirgholami/ai_and_memory_wall)",
      "Training dataset": "WMT14",
      "Training dataset size (gradients)": "870000000",
      "Dataset size notes": "[WORDS]\n\"We used the WMT\u201914 English to French dataset. We trained our models on a subset of 12M sentences consisting of 348M French words and 304M English words, which is a clean \u201cselected\u201d\nsubset from [29].\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1409.3215",
      "Reference": "Sequence to Sequence Learning with Neural Networks",
      "Citations": "21520.0",
      "Authors": "Ilya Sutskever, Oriol Vinyals, Quoc V. Le",
      "Abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "7.5",
      "Training time (hours)": "240.0",
      "Training time notes": "Training took about 10 days",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "VGG19",
      "Organization": "University of Oxford",
      "Publication date": "2014-09-04",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "144000000.0",
      "Parameters notes": "Source: Table 2\nhttps://arxiv.org/abs/1409.1556",
      "Training compute (FLOP)": "1.1e+19",
      "Training compute notes": "Authors of \"AI and Memory Wall\" (https://github.com/amirgholami/ai_and_memory_wall) estimated model's training compute as 11,000 PFLOP = 1.1*10^19 FLOP",
      "Training dataset": "ILSVRC 2012 subset of ImageNet",
      "Training dataset size (gradients)": "1300000",
      "Dataset size notes": "\"In this section, we present the image classification results achieved by the described\nConvNet architectures on the ILSVRC-2012 dataset (which was used for ILSVRC 2012\u20132014 challenges). The dataset includes images of 1000 classes, and is split into three sets: training (1.3M images), validation (50K images), and testing (100K images with held-out class labels).\"",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/1409.1556",
      "Reference": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "Citations": "107801.0",
      "Authors": "Karen Simonyan, Andrew Zisserman",
      "Abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "VGG16",
      "Organization": "University of Oxford",
      "Publication date": "2014-09-04",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "138000000.0",
      "Parameters notes": "Source: Table 2\nhttps://arxiv.org/abs/1409.1556",
      "Training compute (FLOP)": "1.2291e+19",
      "Training compute notes": "3 weeks * 4 Titan Black GPUs * 0.30 utilization\n\nSection 3.3: \"On a system equipped with four NVIDIA Titan Black GPUs, training a single net took 2\u20133 weeks depending on the architecture.\"\n\nTitan Black performance: 5.645 TFLOPS (assuming FP32)\n\nhttps://www.wolframalpha.com/input?i=5.645+TFLOPS+*+3+weeks+*+4+*+0.3\n\n\n",
      "Training dataset": "ILSVRC 2012 subset of ImageNet",
      "Training dataset size (gradients)": "1300000",
      "Dataset size notes": "\"In this section, we present the image classification results achieved by the described ConvNet architectures on the ILSVRC-2012 dataset (which was used for ILSVRC 2012\u20132014 challenges). The dataset includes images of 1000 classes, and is split into three sets: training (1.3M images), validation (50K images), and testing (100K images with held-out class labels).\"\n\nThis is confirmed by section 3.1 Training:\n\"The batch size was set to 256\"\n\"In total, the learning rate was decreased 3 times, and the learning was stopped after 370K iterations (74 epochs)\"\n256 * 370k/74 = 1.3M",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1409.1556",
      "Reference": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "Citations": "107801.0",
      "Authors": "Karen Simonyan; Andrew Zisserman",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "74.0",
      "Training time (hours)": "504.0",
      "Training time notes": "",
      "Training hardware": "NVIDIA GeForce GTX Titan Black",
      "Hardware quantity": "4.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "239.2376655010467",
      "Compute cost notes": "",
      "Training power draw (W)": "2137.653074766455",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "256.0",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "True",
      "Hardware acquisition cost": "5942.688938020667",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": "9359.364939195508"
    },
    {
      "Model": "RNNsearch-50*",
      "Organization": "Jacobs University Bremen,University of Montreal / Universit\u00e9 de Montr\u00e9al",
      "Publication date": "2014-09-01",
      "Domain": "Language",
      "Task": "Translation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "1.5552e+18",
      "Training compute notes": "From https://openai.com/blog/ai-and-compute/ Appendix.\n\n0.018 pfs-days\n(86400*10^15*0.018)\n\n252 hours in a Quadro K-6000 GPU (assumed utilization: 0.33)\n\n5196000000000 FLOP/s *252 hours * 3600 second/hour * 0.33 utilization = 1555200000000000000 FLOP",
      "Training dataset": "WMT'14 + selection",
      "Training dataset size (gradients)": "232000000",
      "Dataset size notes": "[WORDS]\n\"WMT \u201914 contains the following English-French parallel corpora: Europarl (61M words), news\ncommentary (5.5M), UN (421M) and two crawled corpora of 90M and 272.5M words respectively,\ntotaling 850M words. Following the procedure described in Cho et al. (2014a), we reduce the size of\nthe combined corpus to have 348M words using the data selection method by Axelrod et al. (2011).\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1409.0473",
      "Reference": "Neural Machine Translation by Jointly Learning to Align and Translate",
      "Citations": "28579.0",
      "Authors": "D Bahdanau, K Cho, Y Bengio",
      "Abstract": "",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "Germany,Canada",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA Quadro K6000",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "NPD",
      "Organization": "IEEE",
      "Publication date": "2014-08-06",
      "Domain": "Vision",
      "Task": "Face detection",
      "Parameters": "313856.0",
      "Parameters notes": "\"Our final detector contains 1,226 deep quadratic trees\"\n\"(depth of eight in this paper)\"\nParameters of the trained decision trees: 1226*(2^8)=313856\n",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "434600",
      "Dataset size notes": "\"Together with their mirrored images and perturbations in positions, we had 217,300 face images in total for training.\"",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/A-Fast-and-Accurate-Unconstrained-Face-Detector-Liao-Jain/fb67880d99ca29125866087dad2934ccc221378a",
      "Reference": "A Fast and Accurate Unconstrained Face Detector",
      "Citations": "",
      "Authors": "Shengcai Liao, Anil K. Jain, S. Li",
      "Abstract": "We propose a method to address challenges in unconstrained face detection, such as arbitrary pose variations and occlusions. First, a new image feature called Normalized Pixel Difference (NPD) is proposed. NPD feature is computed as the difference to sum ratio between two pixel values, inspired by the Weber Fraction in experimental psychology. The new feature is scale invariant, bounded, and is able to reconstruct the original image. Second, we propose a deep quadratic tree to learn the optimal subset of NPD features and their combinations, so that complex face manifolds can be partitioned by the learned rules. This way, only a single soft-cascade classifier is needed to handle unconstrained face detection. Furthermore, we show that the NPD features can be efficiently obtained from a look up table, and the detection template can be easily scaled, making the proposed face detector very fast. Experimental results on three public face datasets (FDDB, GENKI, and CMU-MIT) show that the proposed method achieves state-of-the-art performance in detecting unconstrained faces with arbitrary pose variations and occlusions in cluttered scenes.",
      "Organization categorization": "Industry",
      "Country (of organization)": "Multinational",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "https://paperswithcode.com/sota/face-detection-on-fddb \n\n\"Experimental results on three public face datasets (FDDB, GENKI, and CMU-MIT) show that the proposed method achieves state-of-the-art performance in detecting unconstrained faces with arbitrary pose variations and occlusions in cluttered scenes.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ACF-WIDER",
      "Organization": "Chinese Academy of Sciences",
      "Publication date": "2014-07-15",
      "Domain": "Vision",
      "Task": "Face detection",
      "Parameters": "6144.0",
      "Parameters notes": "The paper trains a decision tree ensemble.\n\u201cwe choose 2048 as the number of weak classifiers contained in the soft cascade. As each weak classifier is a depth-2 decision tree, it takes only two comparing operations to apply a weak classifier, which is quite fast.\u201d\nParameters: 2048*(2^2-1)=6144\n",
      "Training compute (FLOP)": "76380000000000.0",
      "Training compute notes": "Training compute: 10.2 minutes * 4 * 3.9* 10^9 cycles per second * 16 FLOP per cycle * 0.5 = 7.638*10^13 FLOPs\n\u201con a PC with Intel Core i7-3770 CPU and 16GB RAM\u201d\nIvy Bridge, 4 cores, 3.9GHz, 16 FP32 FLOP/cycle (https://en.wikipedia.org/wiki/Floating_point_operations_per_second ) \n\u201cand 10.2 mins for multi-scale version.\u201d",
      "Training dataset": "AFLW",
      "Training dataset size (gradients)": "144448",
      "Dataset size notes": "\u201cthere are in total 36, 112 positive samples and 108, 336 negative samples selected from AFLW\u201d\n",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1407.4023",
      "Reference": "Aggregate channel features for multi-view face detection",
      "Citations": "",
      "Authors": "Binh Yang, Junjie Yan, Zhen Lei, S. Li",
      "Abstract": "Face detection has drawn much attention in recent decades since the seminal work by Viola and Jones. While many subsequences have improved the work with more powerful learning algorithms, the feature representation used for face detection still can't meet the demand for effectively and efficiently handling faces with large appearance variance in the wild. To solve this bottleneck, we borrow the concept of channel features to the face detection domain, which extends the image channel to diverse types like gradient magnitude and oriented gradient histograms and therefore encodes rich information in a simple form. We adopt a novel variant called aggregate channel features, make a full exploration of feature design, and discover a multi-scale version of features with better performance. To deal with poses of faces in the wild, we propose a multi-view detection approach featuring score re-ranking and detection adjustment. Following the learning pipelines in Viola-Jones framework, the multi-view face detector using aggregate channel features shows competitive performance against state-of-the-art algorithms on AFW and FDDB test-sets, while runs at 42 FPS on VGA images.",
      "Organization categorization": "Academia",
      "Country (of organization)": "China",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "https://paperswithcode.com/sota/face-detection-on-wider-face-medium \n\n\"the multi-view face detector using aggregate channel features shows competitive performance against state-of-the-art algorithms on AFW and FDDB test-sets, while runs at 42 FPS on VGA images.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "1.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "SmooCT",
      "Organization": "University College London (UCL)",
      "Publication date": "2014-07-01",
      "Domain": "Games",
      "Task": "Poker",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "6.9e+16",
      "Training compute notes": "\"Each three-player agent was trained for about 12 billion episodes, requiring about 48 hours of training time [...] on a modern computer without using parallelization\"\n\nAssume an Intel i7 so 400e9 FLOP/s.\n6.9e16 = 400e9*60*60*48",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "\"Each three-player agentwas trained for about 12 billion episodes\"\n\nAn episode seems to be a round of betting.",
      "Confidence": "Likely",
      "Link": "https://www.semanticscholar.org/paper/Self-play-Monte-Carlo-tree-search-in-computer-poker-Heinrich-Silver/7b687599b4425aa959036071030e1212a3b359c7",
      "Reference": "Self-Play Monte-Carlo Tree Search in Computer Poker",
      "Citations": "16.0",
      "Authors": "Johannes Heinrich, David Silver",
      "Abstract": "Self-play reinforcement learning has proved to be successful in many perfect information two-player games.\nHowever, research carrying over its theoretical guarantees and practical success to games of imperfect information has been lacking. In this paper, we evaluate selfplay Monte-Carlo Tree Search (MCTS) in limit Texas Hold\u2019em and Kuhn poker. We introduce a variant of the established UCB algorithm and provide first empirical results demonstrating its ability to find approximate Nash equilibria.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "First RL system to achieve superhuman level at Poker Limit Texas Hold Em",
      "Epochs": "",
      "Training time (hours)": "48.0",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Multiresolution CNN",
      "Organization": "Google,Stanford University",
      "Publication date": "2014-06-23",
      "Domain": "Video",
      "Task": "Video classification",
      "Parameters": "126125568.0",
      "Parameters notes": "\"Using shorthand notation, the full [single frame] architecture is C(96, 11, 3)-N-P-C(256, 5, 1)-N-P-C(384, 3, 1)-C(384, 3, 1)-C(256, 3, 1)-P-FC(4096)-FC(4096), where C(d, f, s) indicates a convolutional layer with d filters of spatial size f \u00d7f, applied to the input with stride s\"\n\nTwo such single-frame architectures are concatenated as shown in figure 2\n\n\"Since the input is only of half the\nspatial size as the full-frame models, we take out the last\npooling layer to ensure that both streams still terminate in a\nlayer of size 7\u00d77\u00d7256. \"\n\nWe assume the input are T=10 frames with C=3 color channels each\n\n2*(256*(10*3*5*5+1) + 384*(256*3*3+1) + 384*(384*3*3+1) + 256*(384*3*3+1)) + (2*7*7*256 + 1)*4096 + (4096+1)*4096\n\n\n\n",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "50000000",
      "Dataset size notes": "\"We further estimate the size of our dataset of sampled frames to be on the order of 50 million examples and that our networks have each seen approximately 500 million examples throughout the training period in total.\"\n\nSo 5e+7 datapoints and 10 epochs.",
      "Confidence": "",
      "Link": "https://ieeexplore.ieee.org/document/6909619",
      "Reference": "Large-Scale Video Classification with Convolutional Neural Networks",
      "Citations": "6254.0",
      "Authors": "A Karpathy, G Toderici, S Shetty, T Leung",
      "Abstract": "",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DeepFace",
      "Organization": "Tel Aviv University,Facebook",
      "Publication date": "2014-06-23",
      "Domain": "Vision",
      "Task": "Face verification",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "4400000",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://ieeexplore.ieee.org/document/6909616",
      "Reference": "DeepFace: Closing the Gap to Human-Level Performance in Face Verification",
      "Citations": "6398.0",
      "Authors": "Y Taigman, M Yang, MA Ranzato",
      "Abstract": "",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "Israel,United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "RNN-WER",
      "Organization": "DeepMind,University of Toronto",
      "Publication date": "2014-06-22",
      "Domain": "Speech",
      "Task": "Speech recognition (ASR)",
      "Parameters": "26500000.0",
      "Parameters notes": "\"The network had five levels of bidirectional LSTM hidden layers, with 500 cells in each layer, giving a total of \u223c 26.5M weights.\"",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "WSJ",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "dataset is 81 hours\n\nAt 228 wpm (https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit)\nthat's 81*228*60 = 1,108,080\n\nanother source says WSJ contains 37k sentences, so this would be ~30 words per sentence which seems high but roughly right: https://www.arxiv-vanity.com/papers/1903.00216/",
      "Confidence": "Likely",
      "Link": "https://proceedings.mlr.press/v32/graves14.html",
      "Reference": "Towards End-To-End Speech Recognition with Recurrent Neural Networks",
      "Citations": "2805.0",
      "Authors": "Alex Graves, Navdeep Jaitly",
      "Abstract": "This paper presents a speech recognition system that directly transcribes audio data with text, without requiring an intermediate phonetic representation. The system is based on a combination of the deep bidirectional LSTM recurrent neural network architecture and the Connectionist Temporal Classification objective function. A modification to the objective function is introduced that trains the network to minimise the expectation of an arbitrary transcription loss function. This allows a direct optimisation of the word error rate, even in the absence of a lexicon or language model. The system achieves a word error rate of 27.3% on the Wall Street Journal corpus with no prior linguistic information, 21.9% with only a lexicon of allowed words, and 8.2% with a trigram language model. Combining the network with a baseline system further reduces the error rate to 6.7%.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland,Canada",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\"Finally, by combining the new model with a baseline, we\nhave achieved state-of-the-art accuracy on the Wall Street\nJournal corpus for speaker independent recognition.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Fragment embedding",
      "Organization": "Stanford University",
      "Publication date": "2014-06-21",
      "Domain": "Vision",
      "Task": "Entity embedding,Semantic segmentation",
      "Parameters": "144496000.0",
      "Parameters notes": "Model contains a word embedding. a matrix combining two word embeddings, and image embedding (built upon a pretrained RCNN image model.\nWord embedding: 400000 * 200 =80000000 (\"Here, We is a d \u00d7 400, 000 matrix that encodes a 1-of-k vector into a d-dimensional word vector representation (we use d = 200).\"\nEmbedding dimension: 1000 (\"The size of the embedded space is cross-validated, and we found that values of approximately 1000 generally work well.\"\nWord combination matrix: 400* 1000=400000\nImage embedding: 4096*1000=4096000 (\"We use the Caffe [41] implementation of the ImageNet Detection RCNN model [27] to detect objects in all images. On our machine with a Tesla K40 GPU, the RCNN processes one image in approximately 25 seconds. We discard the predictions for 200 ImageNet detection classes and only keep the 4096-D activations\")\nCNN: 60,000,000 \"The CNN architecture is identical to the one described in Girhsick et al. [26]. It contains approximately 60 million parameters\"\nTotal parameters: 4096000+80000000+400000+60000000=144,496,000",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Flickr30K Entities",
      "Training dataset size (gradients)": "15000000",
      "Dataset size notes": "Largest experiment uses 30000 training images, 30000 * 5 = 150,000 sentences",
      "Confidence": "Likely",
      "Link": "https://www.semanticscholar.org/paper/Deep-Fragment-Embeddings-for-Bidirectional-Image-Karpathy-Joulin/7f1b111f0bb703b0bd97aba505728a9b0d9b2a54",
      "Reference": "Deep Fragment Embeddings for Bidirectional Image Sentence Mapping",
      "Citations": "",
      "Authors": "A. Karpathy, Armand Joulin, Li Fei-Fei",
      "Abstract": "We introduce a model for bidirectional retrieval of images and sentences through a deep, multi-modal embedding of visual and natural language data. Unlike previous models that directly map images or sentences into a common embedding space, our model works on a finer level and embeds fragments of images (objects) and fragments of sentences (typed dependency tree relations) into a common space. We then introduce a structured max-margin objective that allows our model to explicitly associate these fragments across modalities. Extensive experimental evaluation shows that reasoning on both the global level of images and sentences and the finer level of their respective fragments improves performance on image-sentence retrieval tasks. Additionally, our model provides interpretable predictions for the image-sentence retrieval task since the inferred inter-modal alignment of fragments is explicit.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Extensive experimental evaluation shows that reasoning on both the global level of images and sentences and the finer level of their respective fragments significantly improves performance on image-sentence retrieval tasks.\"",
      "Epochs": "20.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "SPPNet",
      "Organization": "Microsoft,Xi\u2019an Jiaotong University,University of Science and Technology of China (USTC)",
      "Publication date": "2014-06-18",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "3.411072e+18",
      "Training compute notes": "\"All networks in this paper can be\ntrained on a single GeForce GTX Titan GPU (6 GB memory) within two to four weeks.\"\n4.7e12 FLOP/s * 4* 7*24*60*60 seconds * 0.3 utilisation",
      "Training dataset": "ImageNet-1k",
      "Training dataset size (gradients)": "1280000",
      "Dataset size notes": "Section 3.1: \"We train the networks on the 1000-category training\nset of ImageNet 2012.\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1406.4729",
      "Reference": "Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition",
      "Citations": "12164.0",
      "Authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",
      "Abstract": "",
      "Organization categorization": "Industry,Academia,Academia",
      "Country (of organization)": "United States of America,China,China",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "672.0",
      "Training time notes": "\"All networks in this paper can be trained on a single GeForce GTX Titan GPU (6 GB memory) within two to four weeks.\"",
      "Training hardware": "NVIDIA GeForce GTX TITAN",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "51.65209223027738",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "True",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GANs",
      "Organization": "University of Montreal / Universit\u00e9 de Montr\u00e9al",
      "Publication date": "2014-06-10",
      "Domain": "Image generation",
      "Task": "Image generation",
      "Parameters": "",
      "Parameters notes": "The paper outlines the G-D framework but doesn't provide information about the structures of their generator and discriminator.",
      "Training compute (FLOP)": "5.184e+17",
      "Training compute notes": "From https://openai.com/blog/ai-and-compute/ Appendix\n\n\"Less than 0.006 pfs-days\"\n(86400*10^15*0.006)\n\nSeems extremely speculative, unless someone at OpenAI privately corresponded with the authors. There is no information about compute or training in the GANs paper.",
      "Training dataset": "CIFAR-10",
      "Training dataset size (gradients)": "120000",
      "Dataset size notes": "\"We trained adversarial nets an a range of datasets including MNIST[23], the Toronto Face Database (TFD) [28], and CIFAR-10 [21].\"\n\nMNIST has 60k images \nhttps://en.wikipedia.org/wiki/MNIST_database\n\nTFD seems to have 2925 examples (?)\nhttps://www.cs.toronto.edu/~urtasun/courses/CSC411/hw3-411.pdf\n\nCIFAR-10 has 60k images\nhttps://www.cs.toronto.edu/~kriz/cifar.html\n\n",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/1406.2661",
      "Reference": "Generative Adversarial Networks",
      "Citations": "36870.0",
      "Authors": "Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio",
      "Abstract": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Canada",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Two-stream ConvNets for action recognition",
      "Organization": "University of Oxford",
      "Publication date": "2014-06-09",
      "Domain": "Video",
      "Task": "Video classification",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "1289500",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1406.2199",
      "Reference": "Two-Stream Convolutional Networks for Action Recognition in Videos",
      "Citations": "7945.0",
      "Authors": "Karen Simonyan, Andrew Zisserman",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GRUs",
      "Organization": "University of Montreal / Universit\u00e9 de Montr\u00e9al,Jacobs University,University of Maine",
      "Publication date": "2014-06-03",
      "Domain": "Language",
      "Task": "Language modeling,Translation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1406.1078",
      "Reference": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
      "Citations": "25165.0",
      "Authors": "Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio",
      "Abstract": "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
      "Organization categorization": "Academia,Academia,Academia",
      "Country (of organization)": "Canada,Germany,United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Dropout: SVHN",
      "Organization": "University of Toronto",
      "Publication date": "2014-06-01",
      "Domain": "Vision",
      "Task": "Image classification,Digit recognition",
      "Parameters": "47795232.0",
      "Parameters notes": "\"The best architecture that we found uses three convolutional layers each followed by a max-pooling layer. The convolutional layers have 96, 128 and 256 filters respectively. Each convolutional layer has a 5 \u00d7 5 receptive field applied with a stride of 1 pixel. Each max pooling layer pools 3 \u00d7 3 regions at strides of 2 pixels. The convolutional layers are followed by two fully connected hidden layers having 2048 units each.\"\nInputs: 32 x 32 x 3\nconv_1: 96 x 5 x 5 x 3 = 7,200\nconv_2: 128 x 5 x 5 x 96 = 307,200\nconv_3: 256 x 5 x 5 x 128 = 819,200\n(output shape after CNN 3 will be: 20 x 20 x 256)\n\nmax_pool has no learnable parameters but further reduces output shape to 9 x 9 x 256\n\nFFN_1: (9 x 9 x 256) x 2048 = 42,467,328\nFFN_2: 2048 x 2048 = 4,194,304\n\nSo total number of parameters appear to be 47,795,232",
      "Training compute (FLOP)": "",
      "Training compute notes": "Per image input:\n1st CNN: 2*32*32*5*5*3*96=14745600\n2nd CNN: 2*16*16*5*5*96*128=157286400\n3rd CNN: 2*8*8*5*5*128*256=104857600\n1st FC: 2*256*4*4*2048=16777216\n2nd FC: 2*2048*2048=8388608\nTotal forward FLOP: 14745600+157286400+104857600+16777216+8388608=302055424\n\n\nIf we assume 1 epoch, the 600k images would require 5e14 FLOPs. However in other datasets they trained for multiple epochs (e.g. MNIST has 60k train images and they trained for 1M weight updates, batch size unstated).",
      "Training dataset": "SVHN (Street View House Numbers)",
      "Training dataset size (gradients)": "600000",
      "Dataset size notes": "Appendix B.2: \"The SVHN data set consists of approximately 600,000 training images and 26,000 test images\"\n\ndimensionality  3072 (32 \u00d7 32 color)",
      "Confidence": "Confident",
      "Link": "https://jmlr.org/papers/v15/srivastava14a.html",
      "Reference": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
      "Citations": "41962.0",
      "Authors": "Nitish Shrivasta, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov",
      "Abstract": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different thinned networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Canada",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "SOTA on the Street View House Numbers dataset",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "Unreleased",
      "Accessibility notes": "http://www.cs.toronto.edu/~nitish/dropout see model files",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "AdaRNN",
      "Organization": "Beihang University",
      "Publication date": "2014-06-01",
      "Domain": "Language",
      "Task": "Sentiment classification",
      "Parameters": "13040.0",
      "Parameters notes": "D=25 \"For recursive neural models, the dimension of word vector is set to 25, and f = tanh is used as the nonlinearity function. We employ 10 composition matrices in AdaRNN.\"\nComposition matrices: \"W \u2208 R D\u00d72D is the composition matrix, and b is the bias vector.\"\nC=10 \"We employ 10 composition matrices in AdaRNN.\"\nCombination matrix: \"S \u2208 R C\u00d7(2D+|e|) is the matrix used to determine which composition function we use, vl , vr are the left and right child vectors, and e are external feature vector. In this work, e is a one-hot binary feature vector which indicates what the dependency type is.\"\n|e| > 4: (see Figure 2)\nWeights: 10 * 25 * 50 + 10 * (50+4) =13040 (ignoring embedding to the 25 dimension embedding space)",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "6248",
      "Dataset size notes": "\"Training data consists of 6,248 tweets,\"",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/Adaptive-Recursive-Neural-Network-for-Twitter-Dong-Wei/06e122f475a21d92dba137609c40f35690217475",
      "Reference": "Adaptive Recursive Neural Network for Target-dependent Twitter Sentiment Classification",
      "Citations": "",
      "Authors": "Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, M. Zhou, Ke Xu",
      "Abstract": "We propose Adaptive Recursive Neural Network (AdaRNN) for target-dependent Twitter sentiment classification. AdaRNN adaptively propagates the sentiments of words to target depending on the context and syntactic relationships between them. It consists of more than one composition functions, and we model the adaptive sentiment propagations as distributions over these composition functions. The experimental studies illustrate that AdaRNN improves the baseline methods. Furthermore, we introduce a manually annotated dataset for target-dependent Twitter sentiment analysis.",
      "Organization categorization": "Academia",
      "Country (of organization)": "China",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Paragraph Vector",
      "Organization": "Google",
      "Publication date": "2014-05-14",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "32000000.0",
      "Parameters notes": " 75000*400+5000*400=32000000\n\"We learn the word vectors and paragraph vectors using 75,000 training documents\"\n\"In PV-DM, the learned vector representations have 400 dimensions for both words and documents\"\nParagraph embedding of dimension number of paragraphs * embedding size\nWord embedding of dimension |V|*embedding size\nAssuming vocabulary of 5000 since results are compared directly to Maas et. al., 2011",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "IMDb",
      "Training dataset size (gradients)": "16500000",
      "Dataset size notes": "\"25,000 labeled training instances, 25,000 labeled test in-\nstances and 50,000 unlabeled training instances.\"",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/Distributed-Representations-of-Sentences-and-Le-Mikolov/f3de86aeb442216a8391befcacb49e58b478f512",
      "Reference": "Distributed Representations of Sentences and Documents",
      "Citations": "",
      "Authors": "Quoc V. Le, Tomas Mikolov",
      "Abstract": "Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, \"powerful,\" \"strong\" and \"Paris\" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperforms bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "HyperNEAT",
      "Organization": "University of Texas at Austin",
      "Publication date": "2014-03-05",
      "Domain": "Games",
      "Task": "Atari",
      "Parameters": "239712.0",
      "Parameters notes": "\"The ANN consists of three layers (Fig. 3): a substrate layer inwhich information from the game screen (raw pixels, objects, ornoise) is given as input to the network; a processing layer whichadds a nonlinear internal representation; and a nonlinear outputlayer from which actions are read and conveyed to the Atari em-ulator. Both the input and output layers are fully connected tothe processing layer. The substrate dimensionality of the inputand processinglayers is 810 in the case of the object repre-sentation and 1621 for the pixel and noise representations.3The output layer consists of a 33 substrate mirroring the ninepossible directions of the Atari 2600 joystick and a single noderepresenting thefire button\"",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "750000000",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://ieeexplore.ieee.org/abstract/document/6756960\n\nhttps://www.cs.utexas.edu/~mhauskn/papers/atari.pdf",
      "Reference": "A Neuroevolution Approach to General Atari Game Playing",
      "Citations": "195.0",
      "Authors": "M Hausknecht, J Lehman",
      "Abstract": "This paper addresses the challenge of learning to play many different video games with little domain-specific knowledge. Specifically, it introduces a neuroevolution approach to general Atari 2600 game playing. Four neuroevolution algorithms were paired with three different state representations and evaluated on a set of 61 Atari games. The neuroevolution agents represent different points along the spectrum of algorithmic sophistication - including weight evolution on topologically fixed neural networks (conventional neuroevolution), covariance matrix adaptation evolution strategy (CMA-ES), neuroevolution of augmenting topologies (NEAT), and indirect network encoding (HyperNEAT). State representations include an object representation of the game screen, the raw pixels of the game screen, and seeded noise (a comparative baseline). Results indicate that direct-encoding methods work best on compact state representations while indirect-encoding methods (i.e., HyperNEAT) allow scaling to higher dimensional representations (i.e., the raw game screen). Previous approaches based on temporal-difference (TD) learning had trouble dealing with the large state spaces and sparse reward gradients often found in Atari games. Neuroevolution ameliorates these problems and evolved policies achieve state-of-the-art results, even surpassing human high scores on three games. These results suggest that neuroevolution is a promising approach to general video game playing (GVGP).",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Neuroevolution ameliorates these problems and evolved policies achieve state-of-the-art results, even surpassing human high scores on three games\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GloVe (32B)",
      "Organization": "Stanford University",
      "Publication date": "2014-01-01",
      "Domain": "Language",
      "Task": "Semantic embedding",
      "Parameters": "120000000.0",
      "Parameters notes": "400k vocab * 300 vector dimensions",
      "Training compute (FLOP)": "",
      "Training compute notes": "\"The total run-time is split between populating X\nand training the model. The former depends on\nmany factors, including window size, vocabulary\nsize, and corpus size. Though we did not do so,\nthis step could easily be parallelized across multiple machines (see, e.g., Lebret and Collobert\n(2014) for some benchmarks). Using a single\nthread of a dual 2.1GHz Intel Xeon E5-2658 machine, populating X with a 10 word symmetric\ncontext window, a 400,000 word vocabulary, and\na 6 billion token corpus takes about 85 minutes.\nGiven X, the time it takes to train the model depends on the vector size and the number of iterations. For 300-dimensional vectors with the above settings (and using all 32 cores of the above machine), a single iteration takes 14 minutes. See Fig. 4 for a plot of the learning curve\"\n\n\"We run 50 iterations for vectors smaller than\n300 dimensions, and 100 iterations otherwise (see\nSection 4.6 for more details about the convergence\nrate).\"\n\nBut we are interested in the 42B token model",
      "Training dataset": "Common Crawl",
      "Training dataset size (gradients)": "315209597",
      "Dataset size notes": "\"We trained our model on five corpora of varying sizes: a 2010 Wikipedia dump with 1 billion tokens; a 2014 Wikipedia dump with 1.6 billion tokens; Gigaword 5 which has 4.3 billion tokens; the combination Gigaword5 + Wikipedia2014, which has 6 billion tokens; and on 42 billion tokens of web data, from Common Crawl\n\n[To demonstrate the scalability of the model, we also trained it on a much larger sixth corpus, containing 840 billion tokens of web data, but in this case we did not lowercase the vocabulary, so the results are not directly comparable.]\"",
      "Confidence": "",
      "Link": "https://nlp.stanford.edu/projects/glove/",
      "Reference": "GloVe: Global Vectors for Word Representation",
      "Citations": "30643.0",
      "Authors": "J Pennington, R Socher, CD Manning",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "Section 4.6 in original paper (https://nlp.stanford.edu/pubs/glove.pdf)\n\n85 min to populate coocurrence matrix\n+ 25 training iterations\n\nEach iteration takes 14 minutes on 32 cores ",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GloVe (6B)",
      "Organization": "Stanford University",
      "Publication date": "2014-01-01",
      "Domain": "Language",
      "Task": "Semantic embedding",
      "Parameters": "120000000.0",
      "Parameters notes": "400k vocab * 300 vector dimensions",
      "Training compute (FLOP)": "",
      "Training compute notes": "\"The total run-time is split between populating X\nand training the model. The former depends on\nmany factors, including window size, vocabulary\nsize, and corpus size. Though we did not do so,\nthis step could easily be parallelized across multiple machines (see, e.g., Lebret and Collobert\n(2014) for some benchmarks). Using a single\nthread of a dual 2.1GHz Intel Xeon E5-2658 machine, populating X with a 10 word symmetric\ncontext window, a 400,000 word vocabulary, and\na 6 billion token corpus takes about 85 minutes.\nGiven X, the time it takes to train the model depends on the vector size and the number of iterations. For 300-dimensional vectors with the above settings (and using all 32 cores of the above machine), a single iteration takes 14 minutes. See Fig. 4 for a plot of the learning curve\"\n\n\"We run 50 iterations for vectors smaller than\n300 dimensions, and 100 iterations otherwise (see\nSection 4.6 for more details about the convergence\nrate).\"\n\nDetails of dual 2.1GHz Intel Xeon E5-2658 machine:\nhttps://www.intel.com/content/www/us/en/products/sku/61428/intel-xeon-processor-e52658-20m-2-10-ghz-8-0-gts-intel-qpi/specifications.html",
      "Training dataset": "Gigaword5 + Wikipedia2014",
      "Training dataset size (gradients)": "66453980",
      "Dataset size notes": "\"We trained our model on five corpora of varying sizes: a 2010 Wikipedia dump with 1 billion tokens; a 2014 Wikipedia dump with 1.6 billion tokens; Gigaword 5 which has 4.3 billion tokens; the combination Gigaword5 + Wikipedia2014, which has 6 billion tokens; and on 42 billion tokens of web data, from Common Crawl\n\n[To demonstrate the scalability of the model, we also trained it on a much larger sixth corpus, containing 840 billion tokens of web data, but in this case we did not lowercase the vocabulary, so the results are not directly comparable.]\"",
      "Confidence": "",
      "Link": "https://nlp.stanford.edu/projects/glove/",
      "Reference": "GloVe: Global Vectors for Word Representation",
      "Citations": "30643.0",
      "Authors": "J Pennington, R Socher, CD Manning",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "Section 4.6 in original paper (https://nlp.stanford.edu/pubs/glove.pdf)\n\n85 min to populate coocurrence matrix\n+ 25 training iterations\n\nEach iteration takes 14 minutes on 32 cores ",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "OverFeat",
      "Organization": "New York University (NYU)",
      "Publication date": "2013-12-21",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "144000000.0",
      "Parameters notes": "144M (Table 4)",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "ImageNet-1k",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "\"We then extract 5 random crops (and their horizontal flips) of size 221x221 pixels and present these to the network in mini-batches of\nsize 128.\"",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1312.6229",
      "Reference": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",
      "Citations": "5148.0",
      "Authors": "Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, Yann LeCun",
      "Abstract": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "80.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA Tesla K20X",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Image generation",
      "Organization": "University of Amsterdam",
      "Publication date": "2013-12-20",
      "Domain": "Vision",
      "Task": "Image clustering",
      "Parameters": "784000.0",
      "Parameters notes": "\"We trained generative models (decoders) and corresponding encoders\n(a.k.a. recognition models) having 500 hidden units in case of MNIST\"\n\n784*500*2=784000 (Ignoring latent dimension)",
      "Training compute (FLOP)": "475200000000000.0",
      "Training compute notes": "From https://openai.com/blog/ai-and-compute/ Appendix\n\n\"less than 0.0000055 pfs-days\"\n(86400*10^15*0.0000055)\n\nFigure 2 shows evaluations with 10^8 training samples\n\n6*784000*100000000=470400000000000",
      "Training dataset": "MNIST",
      "Training dataset size (gradients)": "47040000",
      "Dataset size notes": "\"We trained generative models of images from the MNIST and Frey Face datasets\"\n\nMNIST has 60k images\nhttps://en.wikipedia.org/wiki/MNIST_database\n\nFrey Face has 2k images\nhttps://cs.nyu.edu/~roweis/data.html\n\n",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1312.6114",
      "Reference": "Auto-Encoding Variational Bayes",
      "Citations": "21760.0",
      "Authors": "DP Kingma, M Welling",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "Netherlands",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DQN",
      "Organization": "DeepMind",
      "Publication date": "2013-12-19",
      "Domain": "Games",
      "Task": "Atari",
      "Parameters": "836096.0",
      "Parameters notes": "\"The input to the neural network consists is an 84 \u00d7 84 \u00d7 4 image produced by \u03c6. The first hidden layer convolves 16 8 \u00d7 8 filters with stride 4 with the input image and applies a rectifier nonlinearity [10, 18]. The second hidden layer convolves 32 4 \u00d7 4 filters with stride 2, again followed by a rectifier nonlinearity. The final hidden layer is fully-connected and consists of 256 rectifier units. The output layer is a fully connected linear layer with a single output for each valid action. The number of valid actions varied between 4 and 18 on the games we considered.\"\n\nParameter: 4*16*8*8+16*32*4*4+10*10*32*256+18*256=836096",
      "Training compute (FLOP)": "2846883840000000.0",
      "Training compute notes": "Network is 84x84x4 input, 16, 8x8, stride 4, 32 4x4 stride 2, 256 fully connected\nFirst layer: 20*20*4*16*8*8 = 1638400\nSecond layer: 9*9*16*32*4*4 = 663552\nThird layer: 9*9*32*256 = 663552\nTotal ~ 2965504\n2965504 * 5M updates * 32 batch size * 2 multiply-add * 3 backward pass\n= 2965504*50000*100*32*6 = 2846883840000000\n\n",
      "Training dataset": "",
      "Training dataset size (gradients)": "160000000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1312.5602",
      "Reference": "Playing Atari with Deep Reinforcement Learning",
      "Citations": "13189.0",
      "Authors": "V Mnih, K Kavukcuoglu, D Silver, A Graves",
      "Abstract": "We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\"Our approach gave state-of-the-art results in six of the seven games it was tested on, with no adjustment of the architecture or hyperparameters.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Network in Network",
      "Organization": "National University of Singapore",
      "Publication date": "2013-12-16",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "630420",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1312.4400",
      "Reference": "Network In Network",
      "Citations": "6551.0",
      "Authors": "M Lin, Q Chen, S Yan",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "Singapore",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "RNN for 1B words",
      "Organization": "Google",
      "Publication date": "2013-12-11",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "20000000000.0",
      "Parameters notes": "20B from Table 1",
      "Training compute (FLOP)": "",
      "Training compute notes": "240 hours on 24 CPUs from Table 1. CPU model is not given, but there is mention of using SIMD instructions. 1 SIMD operation is around 4 FLOP. CPU can have around 3e9 operations per second. so around 12e9*24 * 240*3600 = 2.4e17 operations. This estimation doesn't include use of multiple threads. Including use of threads we would probably have around 10 times more operations  so around 2.4e18 FLOPs. This estimation is speculative.",
      "Training dataset": "One Billion Word benchmark",
      "Training dataset size (gradients)": "1000000000",
      "Dataset size notes": "from abstract: 'With almost one billion words of training data, '",
      "Confidence": "Speculative",
      "Link": "https://arxiv.org/abs/1312.3005",
      "Reference": "One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling",
      "Citations": "1205.0",
      "Authors": "Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, Tony Robinson",
      "Abstract": "We propose a new benchmark corpus to be used for measuring progress in statistical language modeling. With almost one billion words of training data, we hope this benchmark will be useful to quickly evaluate novel language modeling techniques, and to compare their contribution when combined with other advanced techniques. We show performance of several well-known types of language models, with the best results achieved with a recurrent neural network based language model. The baseline unpruned Kneser-Ney 5-gram model achieves perplexity 67.6; a combination of techniques leads to 35% reduction in perplexity, or 10% reduction in cross-entropy (bits), over that baseline.\nThe benchmark is available as a this http URL project; besides the scripts needed to rebuild the training/held-out data, it also makes available log-probability values for each word in each of ten held-out data sets, for each of the baseline n-gram models. ",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "from abstract: 'We show performance of several well-known types of language models, with the best results achieved with a recurrent neural network based language model. The baseline unpruned Kneser-Ney 5-gram model achieves perplexity 67.6; a combination of techniques leads to 35% reduction in perplexity, or 10% reduction in cross-entropy (bits), over that baseline. '",
      "Epochs": "",
      "Training time (hours)": "240.0",
      "Training time notes": "from Table 1,240 hours on 24 CPUs",
      "Training hardware": "",
      "Hardware quantity": "24.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "TransE",
      "Organization": "Universite de Technologie de Compi\u00e8gne \u2013 CNRS,Google",
      "Publication date": "2013-12-05",
      "Domain": "Language",
      "Task": "Entity embedding",
      "Parameters": "942000000.0",
      "Parameters notes": "Based on the TransE architecture, the authors give a formula for how the model size scales with the dimensionality of the dataset. The model scale is proportional to: k*(n_e+n_r) where k is the embeddings dimension, n_e is the number of entities, and n_r is the number of relationships.\n\nThey studied using the TransE model for two datasets: FB15k and FB1M. The FB15k model has 810000 parameters.\n\nFB15k has 14951 entities and 1345 relationships. FB1M has 1000000 entities and 23382 relationships. Therefore, the FB1M model will be bigger than the FB15k model by a factor of (23382e6)/(14951*1345) => N = 8.1e5 * (23382e6)/(14951*1345) = 942e6.",
      "Training compute (FLOP)": "1.340928e+18",
      "Training compute notes": "8 GPUs (they don't specify which, so I used the average for FP32 for 2017 from the write-up table)\n8 hours \n0.33 util rate",
      "Training dataset": "",
      "Training dataset size (gradients)": "17500000",
      "Dataset size notes": "\"it can be successfully trained on a large scale data set with 1M\nentities, 25k relationships and more than 17M training samples\"",
      "Confidence": "Speculative",
      "Link": "https://papers.nips.cc/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html",
      "Reference": "Translating Embeddings for Modeling Multi- relational Data",
      "Citations": "8347.0",
      "Authors": "Antoine Bordes, Nicolas Usunier, Alberto Garcia- Duran, Jason Weston, and Oksana Yakhnenko",
      "Abstract": "",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "France,United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "30.028097313317172",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DeViSE",
      "Organization": "Google",
      "Publication date": "2013-12-05",
      "Domain": "Vision",
      "Task": "Semantic embedding",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "5401200000",
      "Dataset size notes": "\"We trained a skip-gram text model on a corpus of 5.7 million documents (5.4 billion words) \"\nAdditionally, an image model component is trained on ImageNet (1.2M images)",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/DeViSE%3A-A-Deep-Visual-Semantic-Embedding-Model-Frome-Corrado/4aa4069693bee00d1b0759ca3df35e59284e9845",
      "Reference": "DeViSE: A Deep Visual-Semantic Embedding Model",
      "Citations": "",
      "Authors": "Andrea Frome, G. Corrado, Jonathon Shlens, Samy Bengio, J. Dean, Marc'Aurelio Ranzato, Tomas Mikolov",
      "Abstract": "Modern visual recognition systems are often limited in their ability to scale to large numbers of object categories. This limitation is in part due to the increasing difficulty of acquiring sufficient training data in the form of labeled images as the number of object categories grows. One remedy is to leverage data from other sources - such as text data - both to train visual models and to constrain their predictions. In this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as semantic information gleaned from unannotated text. We demonstrate that this model matches state-of-the-art performance on the 1000-class ImageNet object recognition challenge while making more semantically reasonable errors, and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training. Semantic knowledge improves such zero-shot predictions achieving hit rates of up to 18% across thousands of novel labels never seen by the visual model.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\" We demonstrate that this model matches state-of-the-art performance on the 1000-class ImageNet object recognition challenge while making more semantically reasonable errors, and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training.\"\n\nnot absolute SOTA",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Visualizing CNNs",
      "Organization": "New York University (NYU)",
      "Publication date": "2013-11-12",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "5.32e+17",
      "Training compute notes": "1 GPU * 12 days * 1.54 TFLOPS/GTX 580 * 0.33 utilization \n= 532 PF = 0.0062 pfs-days\n\nSource: https://openai.com/blog/ai-and-compute\n\n\"We stopped training after 70 epochs,\nwhich took around 12 days on a single GTX580 GPU\"",
      "Training dataset": "",
      "Training dataset size (gradients)": "7680000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1311.2901",
      "Reference": "Visualizing and Understanding Convolutional Networks",
      "Citations": "16599.0",
      "Authors": "MD Zeiler, R Fergus",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA GeForce GTX 580",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "12.994002368456531",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "True",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "R-CNN (T-net)",
      "Organization": "University of California (UC) Berkeley",
      "Publication date": "2013-11-11",
      "Domain": "Vision",
      "Task": "Object detection",
      "Parameters": "69003872.0",
      "Parameters notes": "Computed from architecture description in Caffee\n\nhttps://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/detection.ipynb",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/1311.2524",
      "Reference": "Rich feature hierarchies for accurate object detection and semantic segmentation",
      "Citations": "24626.0",
      "Authors": "Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Word2Vec (small)",
      "Organization": "Google",
      "Publication date": "2013-10-16",
      "Domain": "Language",
      "Task": "Semantic embedding",
      "Parameters": "207600000.0",
      "Parameters notes": "\"We discarded from the vocabulary all words that occurred less than 5 times in the training data, which resulted in a vocabulary of size 692K [...] Starting with the same news data as in the previous experiments, we first constructed the phrase based training corpus and then we trained several Skip-gram models using different hyperparameters. As before, we used vector dimensionality 300 and context size 5.\"",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "10000000000",
      "Dataset size notes": "\"For training the Skip-gram models, we have used a large dataset consisting of various news articles (an internal Google dataset with one billion words). We discarded from the vocabulary all words that occurred less than 5 times in the training data, which resulted in a vocabulary of size 692K\"",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/1310.4546",
      "Reference": "Distributed Representations of Words and Phrases and their Compositionality",
      "Citations": "34672.0",
      "Authors": "T Mikolov, I Sutskever, K Chen, GS Corrado",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Word2Vec (large)",
      "Organization": "Google",
      "Publication date": "2013-10-16",
      "Domain": "Language",
      "Task": "Semantic embedding",
      "Parameters": "692000000.0",
      "Parameters notes": "We discarded from the vocabulary all words that occurred less than 5 times in the training data, which resulted in a vocabulary of size 692K",
      "Training compute (FLOP)": "3.888e+16",
      "Training compute notes": "From https://openai.com/blog/ai-and-compute/ Appendix.\n\n\"less than 0.00045 pfs days\"\n(86400*10^15*0.00045)",
      "Training dataset": "",
      "Training dataset size (gradients)": "330000000000",
      "Dataset size notes": "\"we increased the amount of the training data by using a dataset with about 33 billion words\"",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/1310.4546",
      "Reference": "Distributed Representations of Words and Phrases and their Compositionality",
      "Citations": "34672.0",
      "Authors": "T Mikolov, I Sutskever, K Chen, GS Corrado",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "24.0",
      "Training time notes": "Table 5 appears to call the model \"Skip-Phrase\" and says it took 1 day",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "RNTN",
      "Organization": "Stanford University",
      "Publication date": "2013-10-01",
      "Domain": "Language",
      "Task": "Sentiment classification",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "1.422e+16",
      "Training compute notes": "\"The RNTN would usually achieve its best performance on the dev set after training for 3 - 5 hours.\"\n",
      "Training dataset": "",
      "Training dataset size (gradients)": "155063",
      "Dataset size notes": "\"The sentences in the treebank were split into a train (8544), dev (1101) and test splits (2210)\"\nTraining data: 215154*(8544/11855)=155063\n",
      "Confidence": "Likely",
      "Link": "https://www.semanticscholar.org/paper/Recursive-Deep-Models-for-Semantic-Compositionality-Socher-Perelygin/687bac2d3320083eb4530bf18bb8f8f721477600",
      "Reference": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
      "Citations": "",
      "Authors": "R. Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, A. Ng, Christopher Potts",
      "Abstract": "Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "5.0",
      "Training time notes": "The RNTN would usually achieve its best performance on the dev set after training for 3 - 5 hours.",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "RCTM",
      "Organization": "University of Oxford",
      "Publication date": "2013-10-01",
      "Domain": "Language",
      "Task": "Translation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "9331200000000000.0",
      "Training compute notes": "\"The training of an RCTM takes about 15 hours on 3 multicore CPUs\"\nGiven the publication year, a rough estimate for the CPU performance is 16 FP32 per cycle, 4 cores, clock speed 4GHz, utilization of 0.3.\n15*60*60*3*4*12*4000000000*0.3=9331200000000000=9.33e15",
      "Training dataset": "",
      "Training dataset size (gradients)": "4500000",
      "Dataset size notes": "\"The English sentences contain about 4.1M words\"",
      "Confidence": "Likely",
      "Link": "https://www.semanticscholar.org/paper/Recurrent-Continuous-Translation-Models-Kalchbrenner-Blunsom/944a1cfd79dbfb6fef460360a0765ba790f4027a",
      "Reference": "Recurrent Continuous Translation Models",
      "Citations": "",
      "Authors": "Nal Kalchbrenner, Phil Blunsom",
      "Abstract": "We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "15.0",
      "Training time notes": "The training of an RCTM takes about 15 hours on 3 multicore CPUs.",
      "Training hardware": "",
      "Hardware quantity": "3.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Hierarchical Scene Labeling (Stanford Background)",
      "Organization": "New York University (NYU)",
      "Publication date": "2013-08-01",
      "Domain": "Vision",
      "Task": "Semantic segmentation",
      "Parameters": "51609600.0",
      "Parameters notes": "Figure 1 details architecture, weights are shared between scales\n\u201cThe network is then applied to each 3-dimension input map Xs. This input is transformed into a 16-dimension feature map, using a bank of 16 filters, 10 connected to the Y channel, the 6 others connected to the U and V channels. The second layer transforms this 16-dimension feature map into a 64-dimension feature map, each map being produced by a combination of 8 randomly selected feature maps from the previous layer. Finally the 64-dimension feature map is transformed into a 256-dimension feature map, each map being produced by a combination of 32 randomly selected feature maps from the previous layer.\u201d\nL1: 1*7*7*10+2*7*7*6=1078\nL2: 8*64*7*7=25088\nFC head is split for the three networks (assumption)\nFC: 32*80*60*256+32*40*30*256+32*20*15*256=51609600\n\nThis is a very different estimate to the 0.5M trainable parameters mentioned in the paper(!) \"The convolutional network has roughly 0.5 million trainable parameters\"",
      "Training compute (FLOP)": "2.3774688e+17",
      "Training compute notes": "Forward FLOP\nSplit for the three sizes\nF1:\nL1: 2*1*7*7*10*320*240+2*2*7*7*6*320*240=165580800\nL2: 2*8*64*7*7*160*120=963379200\nFC: 2*32*80*60*256=78643200\nF2:\nL1: 2*1*7*7*10*160*120+2*2*7*7*6*160*120=41395200\nL2: 2*8*64*7*7*80*60=240844800\nFC: 2*32*40*30*256=19660800\nF3:\nL1: 2*1*7*7*10*80*60+2*2*7*7*6*80*60=10348800\nL2: 2*8*64*7*7*40*30=60211200\nFC: 2*32*20*15*256=4915200\nTotal: 165580800+963379200+78643200+41395200+240844800+19660800+10348800+60211200+4915200=1584979200\n\nTraining compute: 1584979200*3*50000000=237746880000000000=2.4e17\n",
      "Training dataset": "",
      "Training dataset size (gradients)": "71380800",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://ieeexplore.ieee.org/document/6338939",
      "Reference": "Learning Hierarchical Features for Scene Labeling",
      "Citations": "",
      "Authors": "Clement Farabet; Camille Couprie; Laurent Najman; Yann LeCun",
      "Abstract": "Scene labeling consists of labeling each pixel in an image with the category of the object it belongs to. We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features, and produces a powerful representation that captures texture, shape, and contextual information. We report results using multiple postprocessing methods to produce the final labeling. Among those, we propose a technique to automatically retrieve, from a pool of segmentation components, an optimal set of components that best explain the scene; these components are arbitrary, for example, they can be taken from a segmentation tree or from any family of oversegmentations. The system yields record accuracies on the SIFT Flow dataset (33 classes) and the Barcelona dataset (170 classes) and near-record accuracy on Stanford background dataset (eight classes), while being an order of magnitude faster than competing approaches, producing a 320\u00d7240 image labeling in less than a second, including feature extraction.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "1.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ReLU-Speech",
      "Organization": "Google,University of Toronto,New York University (NYU)",
      "Publication date": "2013-05-26",
      "Domain": "Speech",
      "Task": "Speech recognition (ASR)",
      "Parameters": "101706240.0",
      "Parameters notes": "\"The overall input dimensionality is 1040,\"\n\"All layers of our networks have 2560 hidden units \"\n\"used to generate 7969 context-dependent tied acoustic states\"\nLargest model: 12 hidden layers (Fig 4)\nParameters: 1040*2560+12*2560*2560+2560*7969=101706240\n",
      "Training compute (FLOP)": "1.2773376e+17",
      "Training compute notes": "\"across 4 machines using up to 4 CPUs each\"\nCPU model not specified, I assumed a Sandy Bridge with 16 FLOP/cycle and 3.3GhZ based on the publication year (4*16*3300000000=211200000000 FLOP/s per machine)\nCompute: 4*211200000000*168*60*60*0.3 = 1.53e17\n\nAlternatively, the training set is \"several hundred hours of speech\", with inputs consisting of 26 frames, each frame is 10ms apart.\nIf we assume 400h of training data, 400h/10ms/26= 5,538,461 inputs\n6 * 101706240 * 5,538,461 = 3.38e15 FLOPs per epoch. Number of epochs unstated.",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://www.semanticscholar.org/paper/On-rectified-linear-units-for-speech-processing-Zeiler-Ranzato/64da1980714cfc130632c5b92b9d98c2f6763de6",
      "Reference": "On rectified linear units for speech processing",
      "Citations": "",
      "Authors": "Matthew D. Zeiler, Marc'Aurelio Ranzato, R. Monga, Mark Z. Mao, K. Yang, Quoc V. Le, Patrick Nguyen, A. Senior, Vincent Vanhoucke, J. Dean, Geoffrey E. Hinton",
      "Abstract": "Deep neural networks have recently become the gold standard for acoustic modeling in speech recognition systems. The key computational unit of a deep network is a linear projection followed by a point-wise non-linearity, which is typically a logistic function. In this work, we show that we can improve generalization and make training of deep networks faster and simpler by substituting the logistic units with rectified linear units. These units are linear when their input is positive and zero otherwise. In a supervised setting, we can successfully train very deep nets from random initialization on a large vocabulary speech recognition task achieving lower word error rates than using a logistic network with the same topology. Similarly in an unsupervised setting, we show how we can learn sparse features that can be useful for discriminative tasks. All our experiments are executed in a distributed environment using several hundred machines and several hundred hours of speech data.",
      "Organization categorization": "Industry,Academia,Academia",
      "Country (of organization)": "United States of America,Canada,United States of America",
      "Notability criteria": "Training cost,SOTA improvement",
      "Notability criteria notes": "I don't see any standard benchmarks that they used for evaluations",
      "Epochs": "",
      "Training time (hours)": "168.0",
      "Training time notes": "\"The results we report are obtained by training for one week.\"",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Multilingual DNN",
      "Organization": "Google",
      "Publication date": "2013-05-26",
      "Domain": "Speech",
      "Task": "Speech recognition (ASR)",
      "Parameters": "206899200.0",
      "Parameters notes": "\"The input for the DNN is eleven contiguous frames of 40-dimensional log-filterbank features. The DNN consists of four hidden layers each with 2560 nodes\"\nNetwork structure: 3 multilingual shared layers, 1 language specific hidden layer + output layer (Figure 2)\nLanguage specific layer output sizes: 1600, 3300, 2900, 5700, 3500, 5500, 6200, 4700, 5100, 4900, 3700 (Table 1)\nShared: 11*40*2560+2560*2560+2560*2560=14233600\nLanguage heads: 11*2560*2560+2560*1600+2560*3300+2560*2900+2560*5700+2560*3500+2560*5500+2560*6200+2560*4700+2560*5100+2560*4900+2560*3700=192665600\nTotal: 14233600+192665600=206899200=2e8",
      "Training compute (FLOP)": "",
      "Training compute notes": "Could be estimated if we knew framerate of input filterbanks.",
      "Training dataset": "",
      "Training dataset size (gradients)": "3103200000",
      "Dataset size notes": "Trained on 80+100+220+270+920+1140+1450+1460+1490+1490=8620h of speech data (Table 1)\nConversion to words using an estimate of 150 wpm: 8620*60*150=77580000 words\n",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/Multilingual-acoustic-models-using-distributed-deep-Heigold-Vanhoucke/a41b826d23957d6ad4e9e794d20a583a9b567c5d",
      "Reference": "Multilingual acoustic models using distributed deep neural networks",
      "Citations": "",
      "Authors": "G. Heigold, Vincent Vanhoucke, A. Senior, Patrick Nguyen, Marc'Aurelio Ranzato, M. Devin, J. Dean",
      "Abstract": "Today's speech recognition technology is mature enough to be useful for many practical applications. In this context, it is of paramount importance to train accurate acoustic models for many languages within given resource constraints such as data, processing power, and time. Multilingual training has the potential to solve the data issue and close the performance gap between resource-rich and resource-scarce languages. Neural networks lend themselves naturally to parameter sharing across languages, and distributed implementations have made it feasible to train large networks. In this paper, we present experimental results for cross- and multi-lingual network training of eleven Romance languages on 10k hours of data in total. The average relative gains over the monolingual baselines are 4%/2% (data-scarce/data-rich languages) for cross- and 7%/2% for multi-lingual training. However, the additional gain from jointly training the languages on all data comes at an increased training time of roughly four weeks, compared to two weeks (monolingual) and one week (crosslingual).",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement,Training cost",
      "Notability criteria notes": "I don't see any standard benchmarks where they would claim SOTA results",
      "Epochs": "",
      "Training time (hours)": "672.0",
      "Training time notes": "\"increased training time of roughly four weeks\"\n4*7*24=672 hours of training",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Selective Search",
      "Organization": "University of Trento,University of Amsterdam",
      "Publication date": "2013-04-02",
      "Domain": "Vision",
      "Task": "Object detection,Image segmentation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://link.springer.com/article/10.1007/s11263-013-0620-5",
      "Reference": "Selective search for object recognition",
      "Citations": "5642.0",
      "Authors": "JRR Uijlings, KEA Van De Sande, T Gevers",
      "Abstract": "",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "Italy,Netherlands",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "PreTrans-3L-250H",
      "Organization": "University of Toronto",
      "Publication date": "2013-03-22",
      "Domain": "Speech",
      "Task": "Speech recognition (ASR)",
      "Parameters": "43000000.0",
      "Parameters notes": "Table 1",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/1303.5778",
      "Reference": "Speech Recognition with Deep Recurrent Neural Networks",
      "Citations": "8259.0",
      "Authors": "Alex Graves, Abdel-rahman Mohamed, Geoffrey Hinton",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "Canada",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DistBelief NNLM",
      "Organization": "Google",
      "Publication date": "2013-01-16",
      "Domain": "Language",
      "Task": "Semantic embedding",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "2.612736e+18",
      "Training compute notes": "Trained for 14 days on 180 CPU cores (Table 6)\nRoughly estimating the performance of CPUs in a HPC around 2013: 16 FP32 operations per cycle, 2.5GHz, 0.3 utilization\nTime: 14*24*60*60=1209600s\nFLOPs: 0.3*180*16*2500000000=2160000000000\nTraining compute: 1209600s * 2160000000000 = 2612736000000000000 = 2.61e18\nhttps://www.wolframalpha.com/input?i=16+FLOP+*+2.5+GHz+*+180+*+14+days+*+0.3",
      "Training dataset": "Google news",
      "Training dataset size (gradients)": "6000000000",
      "Dataset size notes": "Largest system is trained on 6B words (Table 6)",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/1301.3781",
      "Reference": "Efficient Estimation of Word Representations in Vector Space",
      "Citations": "41000.0",
      "Authors": "Tomas Mikolov, Kai Chen, G. Corrado, J. Dean",
      "Abstract": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "they seem to use their own evaluation protocol, I don't see any standard benchmarks",
      "Epochs": "",
      "Training time (hours)": "336.0",
      "Training time notes": "Trained for 14 days (Table 6)",
      "Training hardware": "",
      "Hardware quantity": "180.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "3255.1839637787602",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": "211961.15337731695"
    },
    {
      "Model": "DNN EM segmentation",
      "Organization": "IDSIA,SUPSI",
      "Publication date": "2012-12-03",
      "Domain": "Vision",
      "Task": "Image segmentation",
      "Parameters": "218896.0",
      "Parameters notes": "Table 1 shows architecture\n1st layer CNN: 48*4*4=768\n2nd layer CNN: 48*48*5*5=57600\n3rd layer CNN: 48*48*4*4=36864\n4th layer CNN: 48*48*4*4=36864\n1st layer FC: 48*3*3*200=86400\n2nd layer FC: 200*2=400\nTotal parameters: 768+57600+36864+36864+86400+400=218896",
      "Training compute (FLOP)": "4.78e+17",
      "Training compute notes": "\"This amounts to 3 million training examples in total, in which both classes are equally represented. [...] We take advantage of this property, and synthetically augment the training set at the beginning of each epoch by randomly mirroring each training instance, and/or rotating it by \u00b190\u25e6\"\nAssuming 3 x training examples per epoch due to data augmentation\n\nCounting estimate:\n1st layer CNN: 2*92*92*48*4*4=13000704\n2nd layer CNN: 2*42*42*48*48*5*5=203212800\n3rd layer CNN: 2*18*18*48*48*4*4=23887872\n4th layer CNN: 2*6*6*4*4*48*48=2654208\n1st layer FC: 2*48*3*3*200=172800\n2nd layer FC: 2*200*2=800\nTotal forward flop: 13000704+203212800+23887872+2654208+172800+800=242929184\nTraining compute: 242929184*3*9000000*30=196772639040000000=1.9e17\n\nGPU hour estimate\n340 minutes per epoch, 30 epochs\n4 GTX 580 with 1580000000000 FLOPs\nAssumed utilization of 0.3\n30*340*60*1580000000000*4*0.3=1160352000000000000=1.16e18\n\nGeometric mean: sqrt(196772639040000000*1160352000000000000)=4.78e17\n",
      "Training dataset": "ISBI 2012 EM Segmentation Challenge",
      "Training dataset size (gradients)": "3000000",
      "Dataset size notes": "[images]\n\"This amounts to 3 million training examples in total, in which both classes are equally represented.\"",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/Deep-Neural-Networks-Segment-Neuronal-Membranes-in-Ciresan-Giusti/09193e19b59fc8f05bee9d6efbfb1607ca5b6501",
      "Reference": "Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images",
      "Citations": "",
      "Authors": "D. Ciresan, A. Giusti, L. Gambardella, J. Schmidhuber",
      "Abstract": "We address a central problem of neuroanatomy, namely, the automatic segmentation of neuronal structures depicted in stacks of electron microscopy (EM) images. This is necessary to efficiently map 3D brain structure and connectivity. To segment biological neuron membranes, we use a special type of deep artificial neural network as a pixel classifier. The label of each pixel (membrane or non-membrane) is predicted from raw pixel values in a square window centered on it. The input layer maps each window pixel to a neuron. It is followed by a succession of convolutional and max-pooling layers which preserve 2D information and extract features with increasing levels of abstraction. The output layer produces a calibrated probability for each class. The classifier is trained by plain gradient descent on a 512 \u00d7 512 \u00d7 30 stack with known ground truth, and tested on a stack of the same size (ground truth unknown to the authors) by the organizers of the ISBI 2012 EM Segmentation Challenge. Even without problem-specific postprocessing, our approach outperforms competing techniques by a large margin in all three considered metrics, i.e. rand error, warping error and pixel error. For pixel error, our approach is the only one outperforming a second human observer.",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "Switzerland,Switzerland",
      "Notability criteria": "Historical significance,Highly cited",
      "Notability criteria notes": "",
      "Epochs": "30.0",
      "Training time (hours)": "17.0",
      "Training time notes": "Training time for one epoch varies from approximately 170 minutes for N1 (w = 65) to 340 minutes for N4 (w = 95). All nets are trained for 30 epochs, which leads to a total training time of several days.",
      "Training hardware": "NVIDIA GeForce GTX 580",
      "Hardware quantity": "4.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "3.9045853984413377",
      "Compute cost notes": "",
      "Training power draw (W)": "2116.2977934480996",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "1989.0118857119623",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": "4710.247852829266"
    },
    {
      "Model": "DistBelief Vision",
      "Organization": "Google",
      "Publication date": "2012-12-03",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "1700000000.0",
      "Parameters notes": "\"we used Downpour SGD to train the 1.7 billion parameter image model\"",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "ImageNet",
      "Training dataset size (gradients)": "16000000",
      "Dataset size notes": "For visual object recognition we trained a larger neural network with locally-connected receptive fields on the ImageNet data set of 16 million images",
      "Confidence": "Likely",
      "Link": "https://www.semanticscholar.org/paper/Large-Scale-Distributed-Deep-Networks-Dean-Corrado/3127190433230b3dc1abd0680bb58dced4bcd90e",
      "Reference": "Large Scale Distributed Deep Networks",
      "Citations": "",
      "Authors": "J. Dean, G. Corrado, R. Monga, Kai Chen, M. Devin, Quoc V. Le, Mark Z. Mao, Marc'Aurelio Ranzato, A. Senior, P. Tucker, Ke Yang, A. Ng",
      "Abstract": "Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement,Highly cited",
      "Notability criteria notes": "\" We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "DistBelief Speech",
      "Organization": "Google",
      "Publication date": "2012-12-03",
      "Domain": "Speech",
      "Task": "Speech recognition (ASR)",
      "Parameters": "47185920.0",
      "Parameters notes": "\"We used a deep network with five layers: four hidden layer with sigmoidal activations and 2560 nodes each, and a softmax output layer with 8192 nodes.\"\n\"The network was fully-connected layer-to-layer, for a total of approximately 42 million model parameters.\"\n2560*2560*4+2560*8192=47185920",
      "Training compute (FLOP)": "3.114e+17",
      "Training compute notes": "https://www.wolframalpha.com/input?i=6+FLOP+*+47185920+*+1.1+billion\nNumber of epochs unknown but most likely 1 and probably under 30.\nWe could narrow down the uncertainty further if we knew something about the hardware.",
      "Training dataset": "",
      "Training dataset size (gradients)": "1100000000",
      "Dataset size notes": "\"We trained on a data set of 1.1 billion weakly labeled examples\"",
      "Confidence": "Speculative",
      "Link": "https://www.semanticscholar.org/paper/Large-Scale-Distributed-Deep-Networks-Dean-Corrado/3127190433230b3dc1abd0680bb58dced4bcd90e",
      "Reference": "Large Scale Distributed Deep Networks",
      "Citations": "",
      "Authors": "J. Dean, G. Corrado, R. Monga, Kai Chen, M. Devin, Quoc V. Le, Mark Z. Mao, Marc'Aurelio Ranzato, A. Senior, P. Tucker, Ke Yang, A. Ng",
      "Abstract": "Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "120.0",
      "Training time notes": "Figure 4",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Bayesian automated hyperparameter tuning",
      "Organization": "University of Toronto,University of Sherbrooke,Harvard University",
      "Publication date": "2012-12-02",
      "Domain": "Other",
      "Task": "Mathematical simulation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "50000",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://arxiv.org/abs/1206.2944",
      "Reference": "Practical Bayesian optimization of machine learning algorithms",
      "Citations": "8745.0",
      "Authors": "J Snoek, H Larochelle, RP Adams",
      "Abstract": "",
      "Organization categorization": "Academia,Academia,Academia",
      "Country (of organization)": "Canada,Canada,United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "RNN+LDA+KN5+cache",
      "Organization": "Microsoft,Brno University of Technology",
      "Publication date": "2012-12-01",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "9000000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Penn TreeBank (PTB)",
      "Training dataset size (gradients)": "929000",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rnn_ctxt.pdf",
      "Reference": "Context dependent recurrent neural network language model",
      "Citations": "716.0",
      "Authors": "Tomas Mikolov, Geoffrey Zweig",
      "Abstract": "Recurrent neural network language models (RNNLMs) have recently demonstrated state-of-the-art performance across a variety of tasks. In this paper, we improve their performance by providing a contextual real-valued input vector in association with each word. This vector is used to convey contextual information about the sentence being modeled. By performing Latent Dirichlet Allocation using a block of preceding text, we achieve a topic-conditioned RNNLM. This approach has the key advantage of avoiding the data fragmentation associated with building multiple topic models on different data subsets. We report perplexity results on the Penn Treebank data, where we achieve a new state-of-the-art. We further apply the model to the Wall Street Journal speech recognition task, where we observe improvements in word-error-rate",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,Czechia",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We report perplexity results on the Penn Treebank data, where we achieve a new state-of-the-art\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "AlexNet",
      "Organization": "University of Toronto",
      "Publication date": "2012-09-30",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "60000000.0",
      "Parameters notes": "\"Our neural network architecture has 60 million parameters.\"",
      "Training compute (FLOP)": "4.7e+17",
      "Training compute notes": "1.2M images * 90 epochs * 0.75 GFLOP * (2 add-multiply) * (3 backward pass) \n= 470 PF = 0.0054 pfs-days\n\nSource: https://openai.com/blog/ai-and-compute/\n\nHardware method:\n2 GTX 580 3GB GPUs for \"between five and six days\". Assuming 5.5 days and 32-bit training:\n1.581 TFLOPS * 5.5 days * 2 = 1.5e18 FLOP\nComparing to the operation counting method, this implies around 31% MFU.\n\nAuthors of \"AI and Memory Wall\" (https://github.com/amirgholami/ai_and_memory_wall) estimated model's training compute as 460 PFLOP = 4.6*10^17 FLOP",
      "Training dataset": "ImageNet",
      "Training dataset size (gradients)": "2457600000",
      "Dataset size notes": "\"ImageNet is a dataset of over 15 million labeled high-resolution images belonging to roughly 22,000 categories. The images were collected from the web and labeled by human labelers using Amazon\u2019s Mechanical Turk crowd-sourcing tool. Starting in 2010, as part of the Pascal Visual Object Challenge, an annual competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has been held. ILSVRC uses a subset of ImageNet with roughly 1000 images in each of 1000 categories. In all, there are roughly 1.2 million training images, 50,000 validation images, and 150,000 testing images.\"",
      "Confidence": "Confident",
      "Link": "https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html",
      "Reference": "ImageNet Classification with Deep Convolutional Neural Networks",
      "Citations": "125497.0",
      "Authors": "Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton",
      "Abstract": "We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7\\% and 18.9\\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Canada",
      "Notability criteria": "Highly cited,Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "132.0",
      "Training time notes": "\"Our network takes between five and six days to train on two GTX 580 3GB GPUs.\"",
      "Training hardware": "NVIDIA GeForce GTX 580",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "0.3133",
      "Training compute cost (2023 USD)": "15.56975299843643",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "True",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "LSTM LM",
      "Organization": "RWTH Aachen University",
      "Publication date": "2012-09-09",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "102720000.0",
      "Parameters notes": "Multiple models were trained, the largest on transcribed French podcast data.\n\"We trained an LSTM LM using 300 hidden nodes and 27 M running words of indomain training data\"\n\"Corpus sizes in number of running words; the vocabulary size of the Treebank corpus is 10 K, for Quaero French it is 170 K\"\nEmbedding and unembedding: 2*170000*300=102000000\nLSTM: 4*600*300=720000\nTotal: 102000000+720000=102720000=1.03e8\n(Assuming the embedding dimension is the same as the LSTM layer)",
      "Training compute (FLOP)": "1.66e+16",
      "Training compute notes": "FLOP per input for LSTM layer is 4*2*(M+N)*M, for N inputs and M outputs.\n\nEmbedding FLOPs: 2 * 170000 * 300 = 102,000,000\nLSTM FLOPs: 4 * 2 * (300 + 300) * 300 = 1,440,000\nUnembedding FLOPs: 2 * 170000 * 300 = 102,000,000\nTotal: 205,440,000 FLOPs per word per forward pass\nFor 27M training input words and including backward passes: 27M * 3 * 205,440,000 = 1.66e16\n\nHowever, it sounds like they're doing something with a secondary acoustic model, so this may be an underestimate.",
      "Training dataset": "",
      "Training dataset size (gradients)": "27000000",
      "Dataset size notes": "\"We trained an LSTM LM using 300 hidden nodes and 27 M running words of indomain training data.\"",
      "Confidence": "Speculative",
      "Link": "https://www.semanticscholar.org/paper/LSTM-Neural-Networks-for-Language-Modeling-Sundermeyer-Schl%C3%BCter/f9a1b3850dfd837793743565a8af95973d395a4e",
      "Reference": "LSTM Neural Networks for Language Modeling",
      "Citations": "",
      "Authors": "M. Sundermeyer, R. Schl\u00fcter, H. Ney",
      "Abstract": "Neural networks have become increasingly popular for the task of language modeling. Whereas feed-forward networks only exploit a fixed context length to predict the next word of a sequence, conceptually, standard recurrent neural networks can take into account all of the predecessor words. On the other hand, it is well known that recurrent networks are difficult to train and therefore are unlikely to show the full potential of recurrent models. These problems are addressed by a the Long Short-Term Memory neural network architecture. In this work, we analyze this type of network on an English and a large French language modeling task. Experiments show improvements of about 8 % relative in perplexity over standard recurrent neural network LMs. In addition, we gain considerable improvements in WER on top of a state-of-the-art speech recognition system.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Germany",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Context-dependent RNN",
      "Organization": "Microsoft Research,Brno University of Technology",
      "Publication date": "2012-07-27",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "37000000",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://www.microsoft.com/en-us/research/wp-content/uploads/2012/07/rnn_ctxt_TR.sav_.pdf",
      "Reference": "Context Dependent Recurrent Neural Network Language Model",
      "Citations": "707.0",
      "Authors": "Tomas Mikolov, Geoffrey Zweig",
      "Abstract": "Recurrent neural network language models (RNNLMs) have recently demonstrated state-of-the-art performance across a variety of tasks. In this paper, we improve their performance by providing a contextual real-valued input vector in association with each word. This vector is used to convey contextual information about the sentence being modeled. By performing Latent Dirichlet Allocation using a block of preceding text, we achieve a topic-conditioned RNNLM. This approach has the key advantage of avoiding the data fragmentation associated with building multiple topic models on different data subsets. We report perplexity results on the Penn Treebank data, where we achieve a new state-of-the-art. We further apply the model to the Wall Street Journal speech recognition task, where we observe \nimprovements in word-error-rate.",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,Czechia",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "New SOTA perplexity on PTB",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Unsupervised High-level Feature Learner",
      "Organization": "Google",
      "Publication date": "2012-07-12",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "1000000000.0",
      "Parameters notes": "\"To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200x200 pixel images downloaded from the Internet)\"",
      "Training compute (FLOP)": "6e+17",
      "Training compute notes": "Assuming 1 epoch, 10 million images and 1 billion parameters, 6*N*D = 6*10^17 FLOP",
      "Training dataset": "",
      "Training dataset size (gradients)": "1200000000000",
      "Dataset size notes": "10 million 200x200 images extracted from Youtube videos",
      "Confidence": "Likely",
      "Link": "https://arxiv.org/abs/1112.6209",
      "Reference": "Building High-level Features Using Large Scale Unsupervised Learning",
      "Citations": "2909.0",
      "Authors": "Quoc V. Le, Marc'Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg S. Corrado, Jeff Dean, Andrew Y. Ng",
      "Abstract": "We consider the problem of building high-level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images using unlabeled images? To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200\u00d7200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting with these learned features, we trained our network to obtain 15.8% accuracy in recognizing 20,000 object categories from ImageNet, a leap of 70% relative improvement over the previous state-of-the-art.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\"we trained our network to obtain 15.8% accuracy in recognizing 20,000 object categories from ImageNet, a leap of 70% relative improvement over the previous state-of-the-art\"",
      "Epochs": "",
      "Training time (hours)": "72.0",
      "Training time notes": "\"We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. \"",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "16.029257461780738",
      "Compute cost notes": "Hardware not reported",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Dropout (TIMIT)",
      "Organization": "University of Toronto",
      "Publication date": "2012-06-03",
      "Domain": "Speech",
      "Task": "Speech recognition (ASR)",
      "Parameters": "48840185.0",
      "Parameters notes": "The input to the net is 21 adjacent frames with an advance of 10ms per frame. The neural net has 4 fully-connected hidden layers of 4000 units per layer and 185 \u201csoftmax\u201d output units that are subsequently merged into the 39 distinct classes used for the benchmark.",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "TIMIT",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "4162 utterances, guesstimated avg 10 words per utterance",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/1207.0580",
      "Reference": "Improving neural networks by preventing co-adaptation of feature detectors",
      "Citations": "7899.0",
      "Authors": "GE Hinton, N Srivastava, A Krizhevsky",
      "Abstract": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different thinned networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Canada",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA GeForce GTX 580",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "Unreleased",
      "Accessibility notes": "http://www.cs.toronto.edu/~nitish/dropout see model files",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Dropout (MNIST)",
      "Organization": "University of Toronto",
      "Publication date": "2012-06-03",
      "Domain": "Vision",
      "Task": "Character recognition (OCR)",
      "Parameters": "5594010.0",
      "Parameters notes": "We show results for 4 nets (784-800-800-10, 784-1200-1200-10, 784-2000-2000-10, 784-1200-1200-1200-10)\n\n784*2000+2000*2000+10*2000+6010=5594010",
      "Training compute (FLOP)": "6039370800000000.0",
      "Training compute notes": "Num mul-add / forward pass\n2 FLOPs / mult-add\n3 total mult-add / fp mult-add\n3000 epochs\n60000 training samples",
      "Training dataset": "MNIST",
      "Training dataset size (gradients)": "60000",
      "Dataset size notes": "The MNIST database contains 60,000 training images and 10,000 testing images (Wikipedia)",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1207.0580",
      "Reference": "Improving neural networks by preventing co-adaptation of feature detectors",
      "Citations": "7899.0",
      "Authors": "GE Hinton, N Srivastava, A Krizhevsky",
      "Abstract": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different thinned networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Canada",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA GeForce GTX 580",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "Unreleased",
      "Accessibility notes": "http://www.cs.toronto.edu/~nitish/dropout see model files",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Dropout (ImageNet)",
      "Organization": "University of Toronto",
      "Publication date": "2012-06-03",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "",
      "Parameters notes": "\"We achieved comparable performance of 48.6% error using a single neural network with five convolutional hidden layers interleaved with \u201cmax-pooling\u201d layer followed by two globally\nconnected layers and a final 1000-way softmax layer\"",
      "Training compute (FLOP)": "2.731968e+17",
      "Training compute notes": "\"a single NVIDIA GTX 580 GPU... Training on ImageNet takes\nroughly four days with dropout and two days without.\"\n1.581 TFLOP/s * 4 day * 86400 s/day * 0.5 utilization",
      "Training dataset": "ImageNet",
      "Training dataset size (gradients)": "2600000",
      "Dataset size notes": "In 2010, a subset of 1000 classes\nwith roughly 1000 examples per class was the basis of an object recognition competition...",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1207.0580",
      "Reference": "Improving neural networks by preventing co-adaptation of feature detectors",
      "Citations": "7899.0",
      "Authors": "GE Hinton, N Srivastava, A Krizhevsky",
      "Abstract": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different thinned networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Canada",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "96.0",
      "Training time notes": "4 days with dropout; 2 days without dropout",
      "Training hardware": "NVIDIA GeForce GTX 580",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "7.952555832759744",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "Unreleased",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "True",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Dropout (CIFAR)",
      "Organization": "University of Toronto",
      "Publication date": "2012-06-03",
      "Domain": "Vision",
      "Task": "Character recognition (OCR)",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "4268700000000000.0",
      "Training compute notes": "\"a single NVIDIA GTX 580 GPU. Training on CIFAR-10 takes roughly 90 minutes\" p17\n1.581 TFLOP/s * 90 min * 60 s/min * 0.5 utilization",
      "Training dataset": "CIFAR-10",
      "Training dataset size (gradients)": "60000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://arxiv.org/abs/1207.0580",
      "Reference": "Improving neural networks by preventing co-adaptation of feature detectors",
      "Citations": "7899.0",
      "Authors": "GE Hinton, N Srivastava, A Krizhevsky",
      "Abstract": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different thinned networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Canada",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "1.5",
      "Training time notes": "90 minutes",
      "Training hardware": "NVIDIA GeForce GTX 580",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Open (non-commercial)",
      "Inference code accessibility": "Unreleased",
      "Accessibility notes": "http://www.cs.toronto.edu/~nitish/dropout see model files",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "NLP from scratch",
      "Organization": "NEC Laboratories,Princeton University",
      "Publication date": "2011-11-08",
      "Domain": "Language",
      "Task": "Language Structure Modeling",
      "Parameters": "5000000.0",
      "Parameters notes": "\"The capacity of our network architectures lies mainly in the word lookup table, which contains 50 \u00d7 100,000 parameters to train. [...] most of the trainable parameters are located in the lookup tables.\"",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "852000000",
      "Dataset size notes": "\"Section 4 leverages large unlabeled data sets (\u223c 852 million words)\"",
      "Confidence": "",
      "Link": "https://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf",
      "Reference": "Natural Language Processing (Almost) from Scratch",
      "Citations": "7640.0",
      "Authors": "Ronan Collobert, J. Weston, L. Bottou, Michael Karlen, K. Kavukcuoglu, P. Kuksa",
      "Abstract": "",
      "Organization categorization": "Industry,Academia",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "72.0",
      "Training time notes": "\"Chunking and NER take about one hour to train, POS takes few hours, and SRL takes about three days.\"\nSRL is the longest task.",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "CNN committee (traffic sign)",
      "Organization": "IDSIA",
      "Publication date": "2011-10-03",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "1388800.0",
      "Parameters notes": "Parameters\nL1: 3*3*3*100=2700\nL3: 100*4*4*150=240000\nL5: 150*3*3*250=337500\nL7: 250*4*4*200=800000\nL8: 200*43=8600\nTotal: 2700+240000+337500+800000+8600=1388800\n",
      "Training compute (FLOP)": "991981425600000.0",
      "Training compute notes": "Training FLOP\nL1: 3*3*3*46*46*100=5713200\nL3: 100*4*4*20*20*150=96000000\nL5: 150*3*3*8*8*250=21600000\nL7: 250*4*4*200=800000\nL8: 200*43=8600\nTotal: 2*(5713200+96000000+21600000+800000+8600)=248243600\nTraining Compute: 248243600*3*26640*50=991981425600000=9.9e14",
      "Training dataset": "German Traffic Sign Recognition Benchmark (GTSRB)",
      "Training dataset size (gradients)": "53280",
      "Dataset size notes": "[images]\n\u201cThe training set consists of 26640 images\u201c\n\u201cwe resize all images to 48 \u00d7 48 pixels\u201d\n",
      "Confidence": "Likely",
      "Link": "https://www.semanticscholar.org/paper/A-committee-of-neural-networks-for-traffic-sign-Ciresan-Meier/dd7f8b53e6802787179a961e766760cbbe2d5011",
      "Reference": "A committee of neural networks for traffic sign classification",
      "Citations": "",
      "Authors": "D. Ciresan, U. Meier, Jonathan Masci, J. Schmidhuber",
      "Abstract": "We describe the approach that won the preliminary phase of the German traffic sign recognition benchmark with a better-than-human recognition rate of 98.98%.We obtain an even better recognition rate of 99.15% by further training the nets. Our fast, fully parameterizable GPU implementation of a Convolutional Neural Network does not require careful design of pre-wired feature extractors, which are rather learned in a supervised way. A CNN/MLP committee further boosts recognition performance.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Switzerland",
      "Notability criteria": "Historical significance,SOTA improvement",
      "Notability criteria notes": "\"We describe the approach that won the preliminary phase of the German traffic sign recognition benchmark with a better-than-human recognition rate of 98.98%.We obtain an even better recognition rate of 99.15% by further training the nets. \"",
      "Epochs": "50.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "NVIDIA GeForce GTX 580,NVIDIA GeForce GTX 480",
      "Hardware quantity": "4.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Adaptive Subgrad",
      "Organization": "Technion - Israel Institute of Technology,Google,University of California (UC) Berkeley",
      "Publication date": "2011-10-03",
      "Domain": "Language",
      "Task": "Text classification",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Reuters RCV1",
      "Training dataset size (gradients)": "800000",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://dl.acm.org/doi/10.5555/1953048.2021068",
      "Reference": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization",
      "Citations": "11018.0",
      "Authors": "J Duchi, E Hazan, Y Singer",
      "Abstract": "We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.",
      "Organization categorization": "Academia,Industry,Academia",
      "Country (of organization)": "Israel,United States of America,United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "CNN Committee (NIST)",
      "Organization": "IDSIA",
      "Publication date": "2011-09-18",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "128420.0",
      "Parameters notes": "1st CNN layer: 20*4*4=320\n2nd CNN layer: 20*40*9*9=64800\n1st FC layer: 40*3*3*150=54000\n2nd FC layer: 150*62=9300\nTotal : 320+64800+54000+9300=128420\n",
      "Training compute (FLOP)": "2.6e+16",
      "Training compute notes": "GPU hour estimate:\n30 epochs, 6 hours per network, 7 networks total\n7*6*60*60*(2*1580000000000+2*1345000000000)*0.3=2.65e17\n\nCounting estimate:\n800000 training examples, 30 epochs\n1st CNN layer: 2*20*26*26*4*4=432640\n2nd CNN layer: 2*20*6*6*9*9*40=4665600\n1st FC layer: 2*40*3*3*150=108000\n2nd FC layer: 2*150*62=18600\nTotal: 432640+4665600+108000+18600=5224840\n\n5224840*3*800000*30*7=2633319360000000=2.6e15\n\nGeometric mean: sqrt(2.65e17*2.6e15)=2.6e16\n",
      "Training dataset": "",
      "Training dataset size (gradients)": "3380475",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://www.semanticscholar.org/paper/Convolutional-Neural-Network-Committees-for-Ciresan-Meier/260a7615bbffec052d67dffde5bcf9b4687b50ee",
      "Reference": "Convolutional Neural Network Committees for Handwritten Character Classification",
      "Citations": "",
      "Authors": "D. Ciresan, U. Meier, L. Gambardella, J. Schmidhuber",
      "Abstract": "In 2010, after many years of stagnation, the MNIST handwriting recognition benchmark record dropped from 0.40% error rate to 0.35%. Here we report 0.27% for a committee of seven deep CNNs trained on graphics cards, narrowing the gap to human performance. We also apply the same architecture to NIST SD 19, a more challenging dataset including lower and upper case letters. A committee of seven CNNs obtains the best results published so far for both NIST digits and NIST letters. The robustness of our method is verified by analyzing 78125 different 7-net committees.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Switzerland",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "30.0",
      "Training time (hours)": "42.0",
      "Training time notes": "",
      "Training hardware": "NVIDIA GeForce GTX 580,NVIDIA GeForce GTX 480",
      "Hardware quantity": "4.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "CNN Committee (MNIST)",
      "Organization": "IDSIA",
      "Publication date": "2011-09-18",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "120620.0",
      "Parameters notes": "1st CNN layer: 20*4*4=320\n2nd CNN layer: 20*40*9*9=64800\n1st FC layer: 40*3*3*150=54000\n2nd FC layer: 150*10=1500\nTotal: 320+64800+54000+1500=120620",
      "Training compute (FLOP)": "5.2e+16",
      "Training compute notes": "GPU hour estimate:\n800 epochs, 14 hours per network, 7 networks total\n7*14*60*60*(2*1580000000000+2*1345000000000)*0.3=6.19e17\n\nCounting estimate:\n50000 examples, 800 epochs\n1st CNN layer: 2*20*26*26*4*4=432640\n2nd CNN layer: 2*20*6*6*9*9*40=4665600\n1st FC layer: 2*40*3*3*150=108000\n2nd FC layer: 2*150*10=3000\nTotal: 432640+4665600+108000+3000=5209240\n5209240*3*50000*800*7=4375761600000000=4.4e15\n\nGeometric mean: sqrt(4.4e15*6.19e17)=5.2e16",
      "Training dataset": "MNIST",
      "Training dataset size (gradients)": "420000",
      "Dataset size notes": "[images]\n\"Our CNNs are trained for around 800 epochs\"",
      "Confidence": "Likely",
      "Link": "https://www.semanticscholar.org/paper/Convolutional-Neural-Network-Committees-for-Ciresan-Meier/260a7615bbffec052d67dffde5bcf9b4687b50ee",
      "Reference": "Convolutional Neural Network Committees for Handwritten Character Classification",
      "Citations": "",
      "Authors": "D. Ciresan, U. Meier, L. Gambardella, J. Schmidhuber",
      "Abstract": "In 2010, after many years of stagnation, the MNIST handwriting recognition benchmark record dropped from 0.40% error rate to 0.35%. Here we report 0.27% for a committee of seven deep CNNs trained on graphics cards, narrowing the gap to human performance. We also apply the same architecture to NIST SD 19, a more challenging dataset including lower and upper case letters. A committee of seven CNNs obtains the best results published so far for both NIST digits and NIST letters. The robustness of our method is verified by analyzing 78125 different 7-net committees.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Switzerland",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "800.0",
      "Training time (hours)": "98.0",
      "Training time notes": "",
      "Training hardware": "NVIDIA GeForce GTX 580,NVIDIA GeForce GTX 480",
      "Hardware quantity": "4.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "High Performance CNN (NORB)",
      "Organization": "IDSIA,SUPSI",
      "Publication date": "2011-07-16",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "4878300.0",
      "Parameters notes": "L1: 6*300*6*6=64800\nL3: 500*600*4*4=4800000\nL5: (108/4)*500=13500\nTotal: 64800+4800000+13500=4878300",
      "Training compute (FLOP)": "2.57985e+16",
      "Training compute notes": "7 epochs, 35 min per epoch\n2 GTX 480 + 2 GTX 580\n580 FLOP: 1580000000000\n480 FLOP: 1345000000000\nFLOP: 7*35*60*(2*1580000000000+2*1345000000000)*0.3=25798500000000000=2.6e16\n",
      "Training dataset": "NORB",
      "Training dataset size (gradients)": "50000",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://people.idsia.ch/~juergen/ijcai2011.pdf",
      "Reference": "Flexible, High Performance Convolutional Neural Networks for Image Classification",
      "Citations": "",
      "Authors": "Dan C. Ciresan, Ueli Meier, Jonathan Masci, Luca M. Gambardella, J\u00fcrgen Schmidhuber",
      "Abstract": "We present a fast, fully parameterizable GPU implementation of Convolutional Neural Network variants. Our feature extractors are neither carefully designed nor pre-wired, but rather learned in a supervised way. Our deep hierarchical architectures achieve the best published results on benchmarks for object classification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with error rates of 2.53%, 19.51%, 0.35%, respectively. Deep nets trained by simple back-propagation perform better than more shallow ones. Learning is surprisingly rapid. NORB is completely trained within five epochs. Test error rates on MNIST drop to 2.42%, 0.97% and 0.48% after 1, 3 and 17 epochs, respectively.",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "Switzerland,Switzerland",
      "Notability criteria": "Historical significance,SOTA improvement",
      "Notability criteria notes": "",
      "Epochs": "7.0",
      "Training time (hours)": "4.0",
      "Training time notes": "",
      "Training hardware": "NVIDIA GeForce GTX 480,NVIDIA GeForce GTX 580",
      "Hardware quantity": "4.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Recursive Neural Network",
      "Organization": "Stanford University",
      "Publication date": "2011-06-28",
      "Domain": "Vision,Language",
      "Task": "Representation learning",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "WSJ",
      "Training dataset size (gradients)": "573285",
      "Dataset size notes": "Full WSJ dataset: 1000000 words (https://catalog.ldc.upenn.edu/LDC99T42) \nUsing 20 out of 24 splits for training: 1000000*(20/24)=833333.333",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/Parsing-Natural-Scenes-and-Natural-Language-with-Socher-Lin/9c0ddf74f87d154db88d79c640578c1610451eec",
      "Reference": "Parsing natural scenes and natural language with recursive neural networks",
      "Citations": "",
      "Authors": "R. Socher, Cliff Chiung-Yu Lin, A. Ng, Christopher D. Manning",
      "Abstract": "Recursive structure is commonly found in the inputs of different modalities such as natural scene images or natural language sentences. Discovering this recursive structure helps us to not only identify the units that an image or sentence contains but also how they interact to form a whole. We introduce a max-margin structure prediction architecture based on recursive neural networks that can successfully recover such structure both in complex scene images as well as sentences. The same algorithm can be used both to provide a competitive syntactic parser for natural language sentences from the Penn Treebank and to outperform alternative approaches for semantic scene segmentation, annotation and classification. For segmentation and annotation our algorithm obtains a new level of state-of-the-art performance on the Stanford background dataset (78.1%). The features from the image parse tree outperform Gist descriptors for scene classification by 4%.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Vector Space Model",
      "Organization": "Stanford University",
      "Publication date": "2011-06-19",
      "Domain": "Language",
      "Task": "Semantic embedding,Sentiment classification",
      "Parameters": "255000.0",
      "Parameters notes": "\"We build a fixed dictionary of the 5,000 most frequent tokens\"\n\"For all word vector models, we use 50-dimensional vectors\"\nParameters: 5000*50 + 5000=255000\n",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "IMDb",
      "Training dataset size (gradients)": "5650000",
      "Dataset size notes": "\"We train a variant of our model which uses 50,000 unlabeled reviews in addition to the labeled set of 25,000 reviews\"",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/Learning-Word-Vectors-for-Sentiment-Analysis-Maas-Daly/1c61f9ef06fe74505775a833ff849185757199e7\n\nhttps://ai.stanford.edu/~ang/papers/acl11-WordVectorsSentimentAnalysis.pdf",
      "Reference": "Learning Word Vectors for Sentiment Analysis",
      "Citations": "",
      "Authors": "Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, A. Ng, Christopher Potts",
      "Abstract": "Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term--document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment information as well as non-sentiment annotations. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "\"We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification\"\n\n\" Table 2\nshows classification performance on our subset of IMDB reviews. Our model showed superior performance to other approaches, and performed best when concatenated with bag of words representation. \"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Cross-Lingual POS Tagger",
      "Organization": "Carnegie Mellon University (CMU),Google Research",
      "Publication date": "2011-06-19",
      "Domain": "Language",
      "Task": "Part-of-speech tagging",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://aclanthology.org/P11-1061/",
      "Reference": "Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections",
      "Citations": "318.0",
      "Authors": "Dipanjan Das, Slav Petrov",
      "Abstract": "We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language. Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages. We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (BergKirkpatrick et al., 2010). Across eight European languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden Markov models induced with\nthe Expectation Maximization algorithm.",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Across eight European languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.\"\n\nevaluation datasets: CoNLL-X and CoNLL-2007",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Deep Autoencoders",
      "Organization": "University of Toronto",
      "Publication date": "2011-04-29",
      "Domain": "Vision",
      "Task": "Image representation",
      "Parameters": "139808256.0",
      "Parameters notes": "2*(3072*8192+8192*4096+4096*2048+2048*1024+1024*512+512*256+256*128+128*64+64*28)=139808256\n\"n each autoencoder, the hidden layers halve in size until they reach the desired size, except that we use 28 instead of 32\"",
      "Training compute (FLOP)": "3.672864e+16",
      "Training compute notes": "48*60*60*708500000000*0.3=36728640000000000=3.7e16\nGTX 285 with 708.5 GFLOP/s",
      "Training dataset": "",
      "Training dataset size (gradients)": "4915200000",
      "Dataset size notes": "\"We train on 1.6 million 32 \u00d7 32 color images\"",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/Using-very-deep-autoencoders-for-content-based-Krizhevsky-Hinton/88080d28536f36588740737f3b7a1f6c1a409654",
      "Reference": "Using very deep autoencoders for content-based image retrieval",
      "Citations": "",
      "Authors": "A. Krizhevsky, Geoffrey E. Hinton",
      "Abstract": "We show how to learn many layers of features on color images and we use these features to initialize deep autoencoders. We then use the autoencoders to map images to short binary codes. Using semantic hashing [6], 28-bit codes can be used to retrieve images that are similar to a query image in a time that is independent of the size of the database. This extremely fast retrieval makes it possible to search using multiple di erent transformations of the query image. 256-bit binary codes allow much more accurate matching and can be used to prune the set of images found using the 28-bit codes.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Canada",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "85.0",
      "Training time (hours)": "48.0",
      "Training time notes": "\"The entire training procedure for each autoencoder took about 2 days on an Nvidia GTX 285 GPU.\"",
      "Training hardware": "NVIDIA GeForce GTX 285",
      "Hardware quantity": "1.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "246.22598104391827",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "332.45248690713987",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Deep rectifier networks",
      "Organization": "University of Montreal / Universit\u00e9 de Montr\u00e9al",
      "Publication date": "2011-04-13",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "CIFAR-10,MNIST,NISTP,NORB",
      "Training dataset size (gradients)": "81920000",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://proceedings.mlr.press/v15/glorot11a.html",
      "Reference": "Deep sparse rectifier neural networks",
      "Citations": "8616.0",
      "Authors": "Xavier Glorot, Antoine Bordes, Yoshua Bengio",
      "Abstract": "While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Canada",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "YouTube Video Recommendation System",
      "Organization": "Google",
      "Publication date": "2010-09-26",
      "Domain": "Recommendation",
      "Task": "Recommender system",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "\"We currently handle millions of users\nand tens of billions of activity events with a total footprint\nof several terabytes of data\"\n\nIf 10M users each watch 1000 videos, that's 10B visualizations, which matches their \"activity events\" count.",
      "Confidence": "",
      "Link": "https://dl.acm.org/doi/10.1145/1864708.1864770",
      "Reference": "The YouTube Video Recommendation System",
      "Citations": "1160.0",
      "Authors": "J Davidson, B Liebald, J Liu, P Nandy",
      "Abstract": "We discuss the video recommendation system in use at YouTube, the world's most popular online video community. The system recommends personalized sets of videos to users based on their activity on the site. We discuss some of the unique challenges that the system faces and how we address them. In addition, we provide details on the experimentation and evaluation framework used to test and tune new algorithms. We also present some of the findings from these experiments.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,Significant use,Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "RNN LM",
      "Organization": "Johns Hopkins University",
      "Publication date": "2010-09-26",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "70265000.0",
      "Parameters notes": "This database entry refers to the 3xRNN rows in Table 2 (static and dynamic likely use the same model ensemble, but allow the model weights to update once when testing the dynamic version).\n\nI assume the 3xRNN represents interpolation between the three largest models shown explicitly (RNN 250/5, RNN 250/2, and RNN 400/10). This seems likely, since smaller models do considerably worse on their own.\n\nIn the following colab notebook, I estimate vocabulary sizes for the NYT Gigaword data at around 54.4k, 41.4k, and 27.6k for merge thresholds of 2, 5, and 10, respectively: https://colab.research.google.com/drive/1K5qH0EqXtFwTLESNtp4oelCM28GpGXt6#scrollTo=tedUkbgklNJ3\n\nSo the total number of parameters in each constituent model is:\n- RNN 250/2: (250 + 54.4k) * 250 + (250 * 54.4k) = 27,262,500\n- RNN 250/5: (250 + 41.4k) * 250 + (250 * 41.4k) = 20,762,500\n- RNN 400/10: (400 + 27.6k) * 400 + (400 * 27.6k) = 22,240,000\n\nIn total: 70,265,000 parameters",
      "Training compute (FLOP)": "5.396e+16",
      "Training compute notes": "\"Convergence is usually achieved after 10-20 epochs.\"\nTraining was done over a 6.4M subset of the NYT section of English Gigaword.\n\n6 * 70,265,000 * 20 * 6.4M = 5.396e16",
      "Training dataset": "WSJ",
      "Training dataset size (gradients)": "6400000",
      "Dataset size notes": "As it is very time consuming to train RNN LM on large data, we have used only up to 6.4M words for training RNN models.",
      "Confidence": "Speculative",
      "Link": "https://www.semanticscholar.org/paper/Recurrent-neural-network-based-language-model-Mikolov-Karafi%C3%A1t/9819b600a828a57e1cde047bbe710d3446b30da5",
      "Reference": "Recurrent neural network based language model",
      "Citations": "6038.0",
      "Authors": "Tomas Mikolov, M. Karafi\u00e1t, L. Burget, J. \u010cernock\u00fd, S. Khudanpur",
      "Abstract": "A new recurrent neural network based language model (RNN LM) with applications to speech recognition is presented. Results indicate that it is possible to obtain around 50% reduction of perplexity by using mixture of several RNN LMs, compared to a state of the art backoff language model. Speech recognition experiments show around 18% reduction of word error rate on the Wall Street Journal task when comparing models trained on the same amount of data, and around 5% on the much harder NIST RT05 task, even when the backoff model is trained on much more data than the RNN LM. We provide ample empirical evidence to suggest that connectionist language models are superior to standard n-gram techniques, except their high computational (training) complexity. Index Terms: language modeling, recurrent neural networks, speech recognition",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "1.0",
      "Training time (hours)": "504.0",
      "Training time notes": "\"it takes several weeks to train the most complex models.\"\nRough guess, 3 weeks = 504 hours\n\nAssuming these models trained at the same time on different machines.",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Pooling CNN (NORB)",
      "Organization": "University of Bonn",
      "Publication date": "2010-09-15",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "268664.0",
      "Parameters notes": "Figure 1\nC1: 2*16*5*5=800\nC3: 16*64*6*6=36864\nF5: 64*6*6*100=230400\nF6: 100*6=600\nTotal: 800+36864+230400+600=268664",
      "Training compute (FLOP)": "1456277227200000.0",
      "Training compute notes": "Forward FLOP\nC1: 2*2*16*5*5*92*92=13542400\nC3: 2*16*16*6*6*18*18=5971968\nF5: 2*64*6*6*100=460800\nF6: 2*100*6=1200\nTotal: 13542400+5971968+460800+1200=19976368\n\n1000 epochs, 24300 training examples\n\n19976368*3*24300*1000=1456277227200000=1.5e15\n",
      "Training dataset": "NORB",
      "Training dataset size (gradients)": "24300",
      "Dataset size notes": "Each pattern consists of a binocular pair of 96 \u00d7 96 grayscale images, with a total of 24,300 training patterns and the same amount of testing patterns.",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/Evaluation-of-Pooling-Operations-in-Convolutional-Scherer-M%C3%BCller/5d21006fa32ff69f6b0a646f26ce0db84f2f4d33",
      "Reference": "Evaluation of Pooling Operations in Convolutional Architectures for Object Recognition",
      "Citations": "",
      "Authors": "Dominik Scherer, Andreas C. M\u00fcller, Sven Behnke",
      "Abstract": "A common practice to gain invariant features in object recognition models is to aggregate multiple low-level features over a small neighborhood. However, the differences between those models makes a comparison of the properties of different aggregation functions hard. Our aim is to gain insight into different functions by directly comparing them on a fixed architecture for several common object recognition tasks. Empirical results show that a maximum pooling operation significantly outperforms subsampling operations. Despite their shift-invariant properties, overlapping pooling windows are no significant improvement over nonoverlapping pooling windows. By applying this knowledge, we achieve state-of-the-art error rates of 4.57% on the NORB normalized-uniform dataset and 5.6% on the NORB jittered-cluttered dataset.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Germany",
      "Notability criteria": "Historical significance,Highly cited,SOTA improvement",
      "Notability criteria notes": "\"we achieve state-of-the-art error rates of 4.57% on the NORB normalized-uniform dataset and 5.6% on the NORB jittered-cluttered dataset.\"",
      "Epochs": "1000.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Pooling CNN (Caltech 101)",
      "Organization": "University of Bonn",
      "Publication date": "2010-09-15",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "294912.0",
      "Parameters notes": "C1: 3*16*16*16=12288\nC3: 16*128*6*6=73728\nF5: 128*4*4*102=208896\nTotal: 12288+73728+208896=294912\n",
      "Training compute (FLOP)": "1221124128768000.0",
      "Training compute notes": "Forward FLOP\nC1: 2*3*16*16*16*125*125=384000000\nC3: 2*16*128*6*6*20*20=58982400\nF5: 2*128*4*4*102=417792\nTotal: 384000000+58982400+417792=443400192\n\n300 epochs\n30*102=3060 training examples\n\n443400192*3*3060*300=1221124128768000=1.2e15\n",
      "Training dataset": "",
      "Training dataset size (gradients)": "3060",
      "Dataset size notes": "30 * 102 = 3060",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/Evaluation-of-Pooling-Operations-in-Convolutional-Scherer-M%C3%BCller/5d21006fa32ff69f6b0a646f26ce0db84f2f4d33",
      "Reference": "Evaluation of Pooling Operations in Convolutional Architectures for Object Recognition",
      "Citations": "",
      "Authors": "Dominik Scherer, Andreas C. M\u00fcller, Sven Behnke",
      "Abstract": "A common practice to gain invariant features in object recognition models is to aggregate multiple low-level features over a small neighborhood. However, the differences between those models makes a comparison of the properties of different aggregation functions hard. Our aim is to gain insight into different functions by directly comparing them on a fixed architecture for several common object recognition tasks. Empirical results show that a maximum pooling operation significantly outperforms subsampling operations. Despite their shift-invariant properties, overlapping pooling windows are no significant improvement over nonoverlapping pooling windows. By applying this knowledge, we achieve state-of-the-art error rates of 4.57% on the NORB normalized-uniform dataset and 5.6% on the NORB jittered-cluttered dataset.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Germany",
      "Notability criteria": "Historical significance,Highly cited,SOTA improvement",
      "Notability criteria notes": "\"we achieve state-of-the-art error rates of 4.57% on the NORB normalized-uniform\ndataset and 5.6% on the NORB jittered-cluttered dataset.\"",
      "Epochs": "300.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ReLU (NORB)",
      "Organization": "University of Toronto",
      "Publication date": "2010-06-15",
      "Domain": "Vision",
      "Task": "Object recognition",
      "Parameters": "16210006.0",
      "Parameters notes": "\"The stereo-pair images are subsampled from their original resolution of 108 \u00d7 108 \u00d7 2 to 32 \u00d7 32 \u00d7 2 to speed up experiments [...]  the architecture\nwith the best results have 4000 units in the first layer\nand 2000 in the second [...] there are 58,320 test\ncases (9,720 cases per class) \"\n\nSo the architecture has (32*32*2+1)x4000 + (4000+1)*2000 + (2000+1)*58,320/9,720 parameters",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "291600",
      "Dataset size notes": "\"There are 291,600 training cases (48,600 cases per class) and 58,320 test cases (9,720 cases per class).\"",
      "Confidence": "",
      "Link": "https://dl.acm.org/doi/10.5555/3104322.3104425",
      "Reference": "Rectified linear units improve restricted boltzmann machines",
      "Citations": "18270.0",
      "Authors": "Nair, V., Hinton, G. E.",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "Canada",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ReLU (LFW)",
      "Organization": "University of Toronto",
      "Publication date": "2010-06-15",
      "Domain": "Vision",
      "Task": "Face recognition",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "233280",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://dl.acm.org/doi/10.5555/3104322.3104425",
      "Reference": "Rectified linear units improve restricted boltzmann machines",
      "Citations": "18270.0",
      "Authors": "Nair, V., Hinton, G. E.",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "Canada",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "iCCCP",
      "Organization": "Massachusetts Institute of Technology (MIT)",
      "Publication date": "2010-06-13",
      "Domain": "Vision",
      "Task": "Object detection",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "1080000000000000.0",
      "Training compute notes": "\u201cAll experiments are performed on a standard computer with a 3Ghz CPU\u201d\n\u201cIt takes 25 hours (about 25 iCCCP iterations) to train an object class with two mixture templates.\u201d\n\nIndividual models for some of the classes.\n\nAssuming a Core 2 with 8 FLOP/cycle was used with a utilization of 0.5:\n3000000000*8*25*60*60*0.5=1080000000000000",
      "Training dataset": "PASCAL VOC 2007",
      "Training dataset size (gradients)": "10000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://ieeexplore.ieee.org/document/5540096",
      "Reference": "Latent hierarchical structural learning for object detection",
      "Citations": "",
      "Authors": "Long Zhu; Yuanhao Chen; Alan Yuille; William Freeman",
      "Abstract": "We present a latent hierarchical structural learning method for object detection. An object is represented by a mixture of hierarchical tree models where the nodes represent object parts. The nodes can move spatially to allow both local and global shape deformations. The models can be trained discriminatively using latent structural SVM learning, where the latent variables are the node positions and the mixture component. But current learning methods are slow, due to the large number of parameters and latent variables, and have been restricted to hierarchies with two layers. In this paper we describe an incremental concave-convex procedure (iCCCP) which allows us to learn both two and three layer models efficiently. We show that iCCCP leads to a simple training algorithm which avoids complex multi-stage layer-wise training, careful part selection, and achieves good performance without requiring elaborate initialization. We perform object detection using our learnt models and obtain performance comparable with state-of-the-art methods when evaluated on challenging public PASCAL datasets. We demonstrate the advantages of three layer hierarchies - outperforming Felzenszwalb et al.'s two layer models on all 20 classes.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance,SOTA improvement",
      "Notability criteria notes": "\"We perform object detection using our learnt models and obtain performance comparable with state-of-the-art methods when evaluated on challenging public PASCAL datasets. \"",
      "Epochs": "",
      "Training time (hours)": "25.0",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "1.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Feedforward NN",
      "Organization": "University of Montreal / Universit\u00e9 de Montr\u00e9al",
      "Publication date": "2010-05-13",
      "Domain": "Vision",
      "Task": "Digit recognition",
      "Parameters": "7082000.0",
      "Parameters notes": "pg250 of the paper, section 2.3: \n\"We optimized feedforward neural networks with one to\nfive hidden layers, with one thousand hidden units per\nlayer\"\n\nInput is a flattened 32x32 image, which corresponds to an input vector of length 3072\n\nOutput is a number from 0-9, so 10 neurons\n\nNo. of params: 3072*1000 + 4*1000*1000 + 1000*10 = 7,082,000\n",
      "Training compute (FLOP)": "350000000000000.0",
      "Training compute notes": "Roughly two times the number of parameters for ops per forward pass. \n\nSo 2*7082000 params*3.5*140 epochs * 50k training images = 3.5e14",
      "Training dataset": "MNIST",
      "Training dataset size (gradients)": "90000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://proceedings.mlr.press/v9/glorot10a.html",
      "Reference": "Understanding the difficulty of training deep feedforward neural networks",
      "Citations": "18606.0",
      "Authors": "X Glorot, Y Bengio",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "Canada",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Stacked Denoising Autoencoders",
      "Organization": "University of Montreal / Universit\u00e9 de Montr\u00e9al,University of Toronto",
      "Publication date": "2010-01-03",
      "Domain": "Other",
      "Task": "Image classification",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "339250000",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://www.jmlr.org/papers/v11/vincent10a.html",
      "Reference": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion",
      "Citations": "7411.0",
      "Authors": "P Vincent, H Larochelle, I Lajoie, Y Bengio",
      "Abstract": "",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "Canada,Canada",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Super-vector coding",
      "Organization": "University of Illinois Urbana-Champaign (UIUC),NEC Laboratories,Rutgers University",
      "Publication date": "2010-01-01",
      "Domain": "Vision",
      "Task": "Image classification,Object recognition",
      "Parameters": "1025.0",
      "Parameters notes": "Somewhat low confidence, but it seems like the number of learnable parameters is the size of the codebook, plus parameters in the SVM used for classification. Since it is a linear SVM, there will be one parameter per input feature, plus a single bias term.\n\nSo in total, 512 learnable codebook values, plus 513 SVM parameters = 1025 parameters",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "PASCAL VOC 2007,PASCAL VOC 2009",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "\"PASCAL VOC 2007 consists of 9,963 images which are divided intothree subsets: training data (2501 images), validation data (2510 images), and\ntest data (4952 images). PASCAL VOC 2009 consists of 14,743 images and correspondingly are divided into three subsets: training data(3473 images), validation data(3581 images), and testing data (7689 images).\"\n\nPASCAL VOC 2009 is the larger experiment; images used in training is 3473 + 3581 = 7,054\n\nFor each image, the inputs for the codebook training are generated as follows: \"128-dimensional SIFT vectors are extracted over a grid with spacing of 4 pixels on three patch scales (16x16,25x25 and 31x31). The dimension of descriptors is reduced to 80 by applying principal component analysis (PCA). The codebooks C are trained on one million randomly sampled descriptors\"\n\nLoss function for learning the SV coding seems to look at the L2 loss on SIFT vectors, so there should be one gradient per descriptor, i.e. 1M inputs.",
      "Confidence": "Speculative",
      "Link": "http://tongzhang-ml.org/papers/eccv10_supervect.pdf",
      "Reference": "Image Classification using Super-Vector Coding of Local Image Descriptors",
      "Citations": "696.0",
      "Authors": "Xi Zhou, Kai Yu, Tong Zhang, and Thomas S. Huang",
      "Abstract": "This paper introduces a new framework for image classification \u0433sing local visual descriptors. The pipeline first performs a nonlinear feature transformation on descriptors, then aggregates the results together to form image-level representations, and finally applies a classification model. For all the three steps we suggest novel solutions which make our approach appealing in theory, more scalable in computation, and transparent in classification. Our experiments demonstrate that the proposed classification method achieves state-of-the-art accuracy on the well-known PASCAL benchmarks.",
      "Organization categorization": "Academia,Industry,Academia",
      "Country (of organization)": "United States of America,United States of America,United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Our experiments demonstrate that the proposed classification method achieves state-of-the-art accuracy on the well-known PASCAL benchmarks.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "LCNP NORB",
      "Organization": "",
      "Publication date": "2009-11-22",
      "Domain": "Vision",
      "Task": "Object recognition",
      "Parameters": "16818176.0",
      "Parameters notes": "Locally connected: 128*128*4*4*4*10 + 64*64*8*4*4*4 + 32*32*16*4*4*8 + 16*16*32*4*4*16=16777216\nClassification head: 16*16*32*5=40960\nTotal: 16777216+40960=16818176\n\"five regular layers with the dimensions 256\u00d7256, 128\u00d7128, . . ., 16\u00d716.\"\n\"size of the receptive field to be 4 \u00d7 4 neurons\"",
      "Training compute (FLOP)": "2452090060800000.0",
      "Training compute notes": "2*16818176*3*24300 *1000=2452090060800000",
      "Training dataset": "NORB",
      "Training dataset size (gradients)": "24300",
      "Dataset size notes": "(2) The NORB normalized-uniform dataset [2], which consists of stereoscopic grayscale images of 50 toys, belonging to 5 categories. Each of the 48,600 images (24,300 for training and 24,300 for testing) shows one toy under different lighting conditions, elevations and azimuths.",
      "Confidence": "Confident",
      "Link": "https://ieeexplore.ieee.org/document/5357786",
      "Reference": "Large-scale object recognition with CUDA-accelerated hierarchical neural networks\n",
      "Citations": "",
      "Authors": "Rafael Uetz, Sven Behnke",
      "Abstract": "Robust recognition of arbitrary object classes in natural visual scenes is an aspiring goal with numerous practical applications, for instance, in the area of autonomous robotics and autonomous vehicles. One obstacle on the way towards human-like recognition performance is the limitation of computational power, restricting the size of the training and testing dataset as well as the complexity of the object recognition system. In this work, we present a hierarchical, locally-connected neural network model that is well-suited for large-scale, high-performance object recognition. By using the NVIDIA CUDA framework, we create a massively parallel implementation of the model which is executed on a state-of-the-art graphics card. This implementation is up to 82 times faster than a single-core CPU version of the system. This significant gain in computational performance allows us to evaluate the model on a very large, realistic, and challenging set of natural images which we extracted from the LabelMe dataset. To compare our model to other approaches, we also evaluate the recognition performance using the well-known MNIST and NORB datasets, achieving a testing error rate of 0.76% and 2.87%, respectively.",
      "Organization categorization": "",
      "Country (of organization)": "",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "1000.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "LCNP MNIST",
      "Organization": "",
      "Publication date": "2009-11-22",
      "Domain": "Vision",
      "Task": "Object recognition",
      "Parameters": "11616256.0",
      "Parameters notes": "Locally connected: 128*128*4*4*4*5 + 64*64*8*4*4*4 + 32*32*16*4*4*8 + 16*16*32*4*4*16=11534336\nClassification head: 16*16*32*10=81920\nTotal: 11534336+81920=11616256\n\"five regular layers with the dimensions 256\u00d7256, 128\u00d7128, . . ., 16\u00d716.\"\n\"size of the receptive field to be 4 \u00d7 4 neurons\"",
      "Training compute (FLOP)": "4181852160000000.0",
      "Training compute notes": "2*11616256*3*60000*1000=4181852160000000",
      "Training dataset": "MNIST",
      "Training dataset size (gradients)": "50000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://ieeexplore.ieee.org/document/5357786",
      "Reference": "Large-scale object recognition with CUDA-accelerated hierarchical neural networks\n",
      "Citations": "",
      "Authors": "Rafael Uetz, Sven Behnke",
      "Abstract": "Robust recognition of arbitrary object classes in natural visual scenes is an aspiring goal with numerous practical applications, for instance, in the area of autonomous robotics and autonomous vehicles. One obstacle on the way towards human-like recognition performance is the limitation of computational power, restricting the size of the training and testing dataset as well as the complexity of the object recognition system. In this work, we present a hierarchical, locally-connected neural network model that is well-suited for large-scale, high-performance object recognition. By using the NVIDIA CUDA framework, we create a massively parallel implementation of the model which is executed on a state-of-the-art graphics card. This implementation is up to 82 times faster than a single-core CPU version of the system. This significant gain in computational performance allows us to evaluate the model on a very large, realistic, and challenging set of natural images which we extracted from the LabelMe dataset. To compare our model to other approaches, we also evaluate the recognition performance using the well-known MNIST and NORB datasets, achieving a testing error rate of 0.76% and 2.87%, respectively.",
      "Organization categorization": "",
      "Country (of organization)": "",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "1000.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "LCNP LabelMe",
      "Organization": "University of Bonn",
      "Publication date": "2009-11-22",
      "Domain": "Vision",
      "Task": "Object recognition",
      "Parameters": "13729792.0",
      "Parameters notes": "Locally connected: 128*128*4*4*4*7 + 64*64*8*4*4*4 + 32*32*16*4*4*8 + 16*16*32*4*4*16=13631488\nClassification head: 16*16*32*12=98304\nTotal: 13631488+98304=13729792\n\"five regular layers with the dimensions 256\u00d7256, 128\u00d7128, . . ., 16\u00d716.\"\n\"size of the receptive field to be 4 \u00d7 4 neurons\"",
      "Training compute (FLOP)": "3295150080000000.0",
      "Training compute notes": "2*13729792*3*40000*1000=3295150080000000",
      "Training dataset": "",
      "Training dataset size (gradients)": "40000",
      "Dataset size notes": "Table 1",
      "Confidence": "Confident",
      "Link": "https://ieeexplore.ieee.org/document/5357786",
      "Reference": "Large-scale object recognition with CUDA-accelerated hierarchical neural networks\n",
      "Citations": "",
      "Authors": "Rafael Uetz, Sven Behnke",
      "Abstract": "Robust recognition of arbitrary object classes in natural visual scenes is an aspiring goal with numerous practical applications, for instance, in the area of autonomous robotics and autonomous vehicles. One obstacle on the way towards human-like recognition performance is the limitation of computational power, restricting the size of the training and testing dataset as well as the complexity of the object recognition system. In this work, we present a hierarchical, locally-connected neural network model that is well-suited for large-scale, high-performance object recognition. By using the NVIDIA CUDA framework, we create a massively parallel implementation of the model which is executed on a state-of-the-art graphics card. This implementation is up to 82 times faster than a single-core CPU version of the system. This significant gain in computational performance allows us to evaluate the model on a very large, realistic, and challenging set of natural images which we extracted from the LabelMe dataset. To compare our model to other approaches, we also evaluate the recognition performance using the well-known MNIST and NORB datasets, achieving a testing error rate of 0.76% and 2.87%, respectively.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Germany",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "1000.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "BellKor 2007",
      "Organization": "AT&T",
      "Publication date": "2009-09-21",
      "Domain": "Recommendation",
      "Task": "Movie ratings",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Netflix Prize",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "The training data set consists of 100,480,507\nratings",
      "Confidence": "",
      "Link": "https://www.semanticscholar.org/paper/The-BellKor-solution-to-the-Netflix-Prize-Bell-Koren/f4ebb542c752a0dc423f94fd121e2edb8f6275ba",
      "Reference": "The BellKor solution to the Netflix Prize",
      "Citations": "241.0",
      "Authors": "RM Bell, Y Koren, C Volinsky",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Won Netflix prize",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Two Stage Feature Extraction (MNIST)",
      "Organization": "New York University (NYU)",
      "Publication date": "2009-09-01",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "258800.0",
      "Parameters notes": "CNN: 32*5*5+32*64*5*5=52000\nFC: 1024*200+200*10=206800\nTotal: 52000+206800=258800",
      "Training compute (FLOP)": "20754000000000.0",
      "Training compute notes": "Assuming no padding and stride 1\n\u201con the 50,000 non-validation training samples until the best error rate on the validation set was reached (this took 30 epochs)\u201c\nFirst CNN layer: 2*32*24*24*5*5=921600\nSecond CNN layer: 2*64*16*5*5*8*8=3276800\nFirst FC layer: 2*4*4*64*200=409600\nSecond FC layer: 2*200*10=4000\nTotal forward FLOP: 921600+3276800+409600+4000=4612000\nTotral training compute: 4612000*3*30*50000=20754000000000\n\nThey train another model in this paper that I think is at least an order of magnitude larger than this one; however, they do not provide the number of epochs it was trained for, so I'm only able to calculate the FLOPs for the smaller model (which still claims SOTA performance on its own dataset). \n\n",
      "Training dataset": "MNIST",
      "Training dataset size (gradients)": "50000",
      "Dataset size notes": "\"experiments were run on the MNIST dataset,\nwhich contains 60,000 gray-scale 28x28 pixel digit images\nfor training and 10,000 images for testing\"",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/What-is-the-best-multi-stage-architecture-for-Jarrett-Kavukcuoglu/1f88427d7aa8225e47f946ac41a0667d7b69ac52",
      "Reference": "What is the best multi-stage architecture for object recognition?",
      "Citations": "",
      "Authors": "Kevin Jarrett, K. Kavukcuoglu, Marc'Aurelio Ranzato, Yann LeCun",
      "Abstract": "In many recent object recognition systems, feature extraction stages are generally composed of a filter bank, a non-linear transformation, and some sort of feature pooling layer. Most systems use only one stage of feature extraction in which the filters are hard-wired, or two stages where the filters in one or both stages are learned in supervised or unsupervised mode. This paper addresses three questions: 1. How does the non-linearities that follow the filter banks influence the recognition accuracy? 2. does learning the filter banks in an unsupervised or supervised manner improve the performance over random filters or hardwired filters? 3. Is there any advantage to using an architecture with two stages of feature extraction, rather than one? We show that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks. We show that two stages of feature extraction yield better accuracy than one. Most surprisingly, we show that a two-stage system with random filters can yield almost 63% recognition rate on Caltech-101, provided that the proper non-linearities and pooling layers are used. Finally, we show that with supervised refinement, the system achieves state-of-the-art performance on NORB dataset (5.6%) and unsupervised pre-training followed by supervised refinement produces good accuracy on Caltech-101 (\u226b 65%), and the lowest known error rate on the undistorted, unprocessed MNIST dataset (0.53%).",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "They claim SOTA performance on the MNIST and NORB datasets. Is also highly cited",
      "Epochs": "33.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "MatrixFac for Recommenders",
      "Organization": "Yahoo Research,AT&T",
      "Publication date": "2009-08-07",
      "Domain": "Recommendation",
      "Task": "Recommender system",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Netflix Prize",
      "Training dataset size (gradients)": "100480507",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://ieeexplore.ieee.org/document/5197422",
      "Reference": "Matrix factorization techniques for recommender systems",
      "Citations": "9234.0",
      "Authors": "Yehuda Koren, Robert Bell, and Chris Volinsky",
      "Abstract": "",
      "Organization categorization": "Industry,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Pragmatic Theory solution (Netflix 2009)",
      "Organization": "Pragmatic Theory Inc.",
      "Publication date": "2009-08-01",
      "Domain": "Recommendation",
      "Task": "Movie ratings",
      "Parameters": "",
      "Parameters notes": "This is an ensemble of many smaller models. Ideally, the number of parameters of all the sub-models should be added up and recorded here.",
      "Training compute (FLOP)": "",
      "Training compute notes": "This is an ensemble of many smaller models. Ideally, the training compute of all the sub-models should be added up and recorded here.",
      "Training dataset": "Netflix Prize",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "\"Netflix provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies.\"",
      "Confidence": "Confident",
      "Link": "https://www.asc.ohio-state.edu/statistics/statgen/joul_aut2009/PragmaticTheory.pdf",
      "Reference": "The Pragmatic Theory solution to the Netflix Grand Prize",
      "Citations": "111.0",
      "Authors": "M Piotte, M Chabbert",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "Canada",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Netflix grand prize winner (along with two other teams)",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "BigChaos OptiBlend",
      "Organization": "AT&T",
      "Publication date": "2009-08-01",
      "Domain": "Recommendation",
      "Task": "Movie ratings",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Netflix Prize",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "\"Netflix provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies.\"",
      "Confidence": "",
      "Link": "https://www.asc.ohio-state.edu/statistics/statgen/joul_aut2009/BigChaos.pdf",
      "Reference": "The BigChaos Solution to the Netflix Grand Prize",
      "Citations": "237.0",
      "Authors": "A T\u00f6scher, M Jahrer, RM Bell",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "won Netflix prize\n",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "BellKor 2009",
      "Organization": "AT&T",
      "Publication date": "2009-08-01",
      "Domain": "Recommendation",
      "Task": "Movie ratings",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Netflix Prize",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "\"Netflix provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies.\"",
      "Confidence": "",
      "Link": "https://www2.seas.gwu.edu/~simhaweb/champalg/cf/papers/KorenBellKor2009.pdf",
      "Reference": "The BellKor Solution to the Netflix Grand Prize",
      "Citations": "513.0",
      "Authors": "Y Koren",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "Introduced new algorithms; won Netflix Grand Prize",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "BellKor 2008",
      "Organization": "AT&T",
      "Publication date": "2009-08-01",
      "Domain": "Recommendation",
      "Task": "Movie ratings",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Netflix Prize",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "\"Netflix provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies.\"",
      "Confidence": "",
      "Link": "https://cseweb.ucsd.edu/classes/fa17/cse291-b/reading/ProgressPrize2008_BellKor.pdf",
      "Reference": "The BellKor 2008 Solution to the Netflix Prize",
      "Citations": "162.0",
      "Authors": "RM Bell, Y Koren, C Volinsky",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "Won Netflix prize",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GPU DBNs",
      "Organization": "Stanford University",
      "Publication date": "2009-06-15",
      "Domain": "Other",
      "Task": "Miscellaneous image analysis",
      "Parameters": "100000000.0",
      "Parameters notes": "\"For example, we are able to reduce the time required to learn a four-layer DBN with 100 million free parameters from several weeks to around a single day.\"",
      "Training compute (FLOP)": "1000000000000000.0",
      "Training compute notes": "https://www.getguesstimate.com/models/19602\n\n6435 GPU seconds for 1M examples\nSingle GTX 280 with 622.1 GFLOPS\nAll results are reported for 1M examples, unclear if they ran larger training experiments.",
      "Training dataset": "",
      "Training dataset size (gradients)": "121344000000",
      "Dataset size notes": "Table 2 shows the running time for processing 1 million\nexamples for RBMs of varying size",
      "Confidence": "Confident",
      "Link": "https://dl.acm.org/doi/abs/10.1145/1553374.1553486?utm_campaign=The+Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-95Z7-X4Dl-RJK6gYKvjyDIrYaGhBeqWoc0ldqyPEKni0Ip6UE7452hr-ygY52z00LBpYgM",
      "Reference": "Large-scale Deep Unsupervised Learning using Graphics Processors",
      "Citations": "1032.0",
      "Authors": "R Raina, A Madhavan, AY Ng",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "RBM Image Classifier",
      "Organization": "University of Toronto",
      "Publication date": "2009-04-08",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "80000000.0",
      "Parameters notes": "Best performing model (see Figure 3.1) had 10,000 hidden units in one hidden layer and 8000 visible units",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "CIFAR-10",
      "Training dataset size (gradients)": "6144050000",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf",
      "Reference": "Learning Multiple Layers of Features from Tiny Images",
      "Citations": "39635.0",
      "Authors": "Alex Krizhevsky",
      "Abstract": "Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It\nis, in principle, an excellent dataset for unsupervised training of deep generative models, but previous\nresearchers who have tried this have found it di\u001ecult to learn a good set of \u001clters from the images.\nWe show how to train a multi-layer generative model that learns to extract meaningful features which\nresemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute\nthe work among multiple machines connected on a network, we show how training such a model can be\ndone in reasonable time.\nA second problematic aspect of the tiny images dataset is that there are no reliable class labels\nwhich makes it hard to use for object recognition experiments. We created two sets of reliable labels.\nThe CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of\neach of 100 non-overlapping classes. Using these labels, we show that object recognition is signi\u001ccantly\nimproved by pre-training a layer of features on a large set of unlabeled tiny images.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Canada",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "BP-DBN",
      "Organization": "University of Toronto",
      "Publication date": "2009-01-01",
      "Domain": "Speech",
      "Task": "Speech recognition (ASR)",
      "Parameters": "18030592.0",
      "Parameters notes": "429*2048+2048*2048*4+2048*183=18030592\n\nTable 2: largest model has 5 layers of 2048 into 183 softmax, 429 input",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "TIMIT",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://www.cs.utoronto.ca/~gdahl/papers/dbnPhoneRec.pdf",
      "Reference": "Deep Belief Networks for phone recognition",
      "Citations": "",
      "Authors": "Abdel-rahman Mohamed, George Dahl, and Geoffrey Hinton",
      "Abstract": "Hidden Markov Models (HMMs) have been the state-of-the-art techniques for acoustic modeling despite their unrealistic independence assumptions and the very limited representational capacity of their hidden states. There are many proposals in the research community for deeper models that are capable of modeling the many types of variability present in the speech generation process. Deep Belief Networks (DBNs) have recently proved to be very effective for a variety of machine learning problems and this paper applies DBNs to acoustic modeling. On the standard TIMIT corpus, DBNs consistently outperform other techniques and the best DBN achieves a phone error rate (PER) of 23.0% on the TIMIT core test set.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Canada",
      "Notability criteria": "Historical significance,SOTA improvement",
      "Notability criteria notes": "\"On the standard TIMIT corpus, DBNs consistently outperform other techniques and the best DBN achieves a phone error rate (PER) of 23.0% on the TIMIT core test set.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "\"Training DBNs of the sizes used in this paper is quite computationally expensive. Training was accelerated by exploiting a graphics processor. A single pass over the entire training set during\npretraining took about 5 minutes. An epoch of fine-tuning with  backpropagation took around 13 minutes. The discriminative gradient computation for hybrid training was substantially more expensive. Each epoch of hybrid fine-tuning took around an hour. These time estimates represent the largest architecture running on one of the GPUs in a NVIDIA Tesla S1070 system, using the library\nin [27].\"",
      "Training hardware": "NVIDIA Tesla S1070",
      "Hardware quantity": "1.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "984",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "0.0",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GNN",
      "Organization": "University of Siena",
      "Publication date": "2008-12-09",
      "Domain": "Other",
      "Task": "Binary classification",
      "Parameters": "30.0",
      "Parameters notes": "5*5+5=30\n(3 layer network with 5 hidden neurons)",
      "Training compute (FLOP)": "1614600000.0",
      "Training compute notes": "2*30 parameters *5000 epochs *207 training examples*26 nodes per example=1614600000",
      "Training dataset": "",
      "Training dataset size (gradients)": "207",
      "Dataset size notes": "Mutagenesis task with 230 total examples, of which 207 (90%) were used for training. ",
      "Confidence": "Confident",
      "Link": "https://ieeexplore.ieee.org/document/4700287",
      "Reference": "The Graph Neural Network Model",
      "Citations": "",
      "Authors": "Franco Scarselli; Marco Gori; Ah Chung Tsoi; Markus Hagenbuchner; Gabriele Monfardini",
      "Abstract": "Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function tau(G,n) isin IR m that maps a graph G and one of its nodes n into an m -dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Italy",
      "Notability criteria": "Historical significance,Highly cited",
      "Notability criteria notes": "",
      "Epochs": "5000.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "1.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Open weights (unrestricted)",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "HLBL",
      "Organization": "University of Toronto",
      "Publication date": "2008-12-08",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "1846400.0",
      "Parameters notes": "\"Except for where stated otherwise, the models used for the experiments used 100 dimensional feature vectors and a context size of 5.\"\n\"The vocabulary size for this dataset is 17964.\"\nEmbedding: 17964 * 100 = 1796400\nContext matrices: 5 * 100 * 100 = 50000\nUnembedding: 0 (tied embedding \u201cwhile the matrix of weights from the hidden layer to the output layer is simply the feature vector matrix R\u201d)\nTotal: 1796400 + 50000 = 1846400\n",
      "Training compute (FLOP)": "",
      "Training compute notes": "6ND: 6 * 1846400 * 14000000 = 155,097,600,000,000 FLOP per epoch.\nNot stated how many epochs for training.",
      "Training dataset": "",
      "Training dataset size (gradients)": "14000000",
      "Dataset size notes": "\"The dataset consists of a 14 million word training set\"",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/A-Scalable-Hierarchical-Distributed-Language-Model-Mnih-Hinton/a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb",
      "Reference": "A Scalable Hierarchical Distributed Language Model",
      "Citations": "",
      "Authors": "A. Mnih, Geoffrey E. Hinton",
      "Abstract": "Neural probabilistic language models (NPLMs) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words, which was two orders of magnitude faster than the non-hierarchical model it was based on. However, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical neural models as well as the best n-gram models.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Canada",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "HLBL with largest tree (T7) takes 32 minutes per epoch. Unstated how many epochs they trained for.",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "BigChaos 2008",
      "Organization": "AT&T",
      "Publication date": "2008-11-25",
      "Domain": "Recommendation",
      "Task": "Movie ratings",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Netflix Prize",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "\"Netflix provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies.\"",
      "Confidence": "",
      "Link": "https://www.asc.ohio-state.edu/statistics/statgen/joul_aut2009/BigChaos.pdf",
      "Reference": "The BigChaos Solution to the Netflix Prize 2008",
      "Citations": "35.0",
      "Authors": "A T\u00f6scher, M Jahrer",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "Winners of the 2008 Netflix Price",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "FP32",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Sparse digit recognition SVM",
      "Organization": "University of Lubeck",
      "Publication date": "2008-11-19",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://pubmed.ncbi.nlm.nih.gov/19000969/",
      "Reference": "Simple method for high-performance digit recognition based on sparse coding",
      "Citations": "124.0",
      "Authors": "Kai Labusch, Erhadt Barth, Thomas Martinetz",
      "Abstract": "In this brief paper, we propose a method of feature extraction for digit recognition that is inspired by vision research: a sparse-coding strategy and a local maximum operation. We show that our method, despite its simplicity, yields state-of-the-art classification results on a highly competitive digit-recognition benchmark. We first employ the unsupervised Sparsenet algorithm to learn a basis for representing patches of handwritten digit images. We then use this basis to extract local coefficients. In a second step, we apply a local maximum operation to implement local shift invariance. Finally, we train a support vector machine (SVM) on the resulting feature vectors and obtain state-of-the-art classification performance in the digit recognition task defined by the MNIST benchmark. We compare the different classification performances obtained with sparse coding, Gabor wavelets, and principal component analysis (PCA). We conclude that the learning of a sparse representation of local image patches combined with a local maximum operation for feature extraction can significantly improve recognition performance.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Germany",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"Finally, we train a support vector machine (SVM) on the resulting feature vectors and obtain state-of-the-art classification performance in the digit recognition task defined by the MNIST benchmark\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Denoising Autoencoders",
      "Organization": "University of Montreal / Universit\u00e9 de Montr\u00e9al",
      "Publication date": "2008-07-05",
      "Domain": "Other",
      "Task": "Image classification",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "7840000",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://dl.acm.org/doi/10.1145/1390156.1390294",
      "Reference": "Extracting and Composing Robust Features with Denoising Autoencoders",
      "Citations": "7894.0",
      "Authors": "Pascal Vincent, Hugo Larechelle, Yoshua Bengio, Pierre- Antoine Manzagol",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "Canada",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Deep Multitask NLP Network",
      "Organization": "NEC Laboratories",
      "Publication date": "2008-07-05",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "1500000.0",
      "Parameters notes": "With a word vector size of 50 and a vocabulary size of 30,000, the embedding matrix has 1,500,000 parameters. There are also some small convolutional and dense layers with far fewer parameters.",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "PropBank,Penn TreeBank (PTB),Wikipedia",
      "Training dataset size (gradients)": "633000000",
      "Dataset size notes": "Section 7: \"631 million words\nfrom Wikipedia\"",
      "Confidence": "Speculative",
      "Link": "https://dl.acm.org/doi/10.1145/1390156.1390177",
      "Reference": "A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning\n",
      "Citations": "7095.0",
      "Authors": "Ronan Collobert, Jason Weston",
      "Abstract": "We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in stateof-the-art performance.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,SOTA improvement",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "168.0",
      "Training time notes": "1 week on 1 computer",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "BLSTM for handwriting (2)",
      "Organization": "University of Bern,IDSIA,Technical University of Munich",
      "Publication date": "2007-12-03",
      "Domain": "Vision",
      "Task": "Character recognition (OCR)",
      "Parameters": "100881.0",
      "Parameters notes": "For the raw input representation,\nthere were 4 input units and a total of 100,881 weights",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "3298424",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://proceedings.neurips.cc/paper/2007/hash/4b0250793549726d5c1ea3906726ebfe-Abstract.html",
      "Reference": "Unconstrained online handwriting recognition with recurrent neural networks",
      "Citations": "341.0",
      "Authors": "Alex Graves, Marcus Liwicki, Horst Bunke, J\u00fcrgen Schmidhuber, Santiago Fern\u00e1ndez",
      "Abstract": "On-line handwriting recognition is unusual among sequence labelling tasks in that the underlying generator of the observed data, i.e. the movement of the pen, is recorded directly. However, the raw data can be difficult to interpret because each letter is spread over many pen locations. As a consequence, sophisticated pre-processing is required to obtain inputs suitable for conventional sequence labelling algorithms, such as HMMs. In this paper we describe a system capable of directly transcribing raw on-line handwriting data. The system consists of a recurrent neural network trained for sequence labelling, combined with a probabilistic language model. In experiments on an unconstrained on-line database, we record excellent results using either raw or pre-processed data, well outperforming a benchmark HMM in both cases.",
      "Organization categorization": "Academia,Academia,Academia",
      "Country (of organization)": "Switzerland,Switzerland,Germany",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"In experiments on an unconstrained\nonline database, we record excellent results using either raw or preprocessed data, well outperforming a state-of-the-art HMM based system in both cases.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Enhanced Neighborhood-Based Filtering",
      "Organization": "AT&T",
      "Publication date": "2007-10-28",
      "Domain": "Recommendation",
      "Task": "Recommender system",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "100000000",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://ieeexplore.ieee.org/abstract/document/4470228",
      "Reference": "Scalable Collaborative Filtering with Jointly Derived Neighborhood Interpolation Weights",
      "Citations": "687.0",
      "Authors": "RM Bell, Y Koren",
      "Abstract": "Recommender systems based on collaborative filtering predict user preferences for products or services by learning past user-item relationships. A predominant approach to collaborative filtering is neighborhood based (\"k-nearest neighbors\"), where a user-item preference rating is interpolated from ratings of similar items and/or users. We enhance the neighborhood-based approach leading to substantial improvement of prediction accuracy, without a meaningful increase in running time. First, we remove certain so-called \"global effects\" from the data to make the ratings more comparable, thereby improving interpolation accuracy. Second, we show how to simultaneously derive interpolation weights for all nearest neighbors, unlike previous approaches where each weight is computed separately. By globally solving a suitable optimization problem, this simultaneous interpolation accounts for the many interactions between neighbors leading to improved accuracy. Our method is very fast in practice, generating a prediction in about 0.2 milliseconds. Importantly, it does not require training many parameters or a lengthy preprocessing, making it very practical for large scale applications. Finally, we show how to apply these methods to the perceivably much slower user-oriented approach. To this end, we suggest a novel scheme for low dimensional embedding of the users. We evaluate these methods on the netflix dataset, where they deliver significantly better results than the commercial netflix cinematch recommender system.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"We evaluate these methods on the Netflix dataset, where they deliver significantly better results than the commercial Netflix Cinematch recommender system.\"\n\nthey don't claim absolute SOTA on any of the benchmarks",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "BLSTM for handwriting (1)",
      "Organization": "University of Bern,IDSIA,Technical University of Munich",
      "Publication date": "2007-09-23",
      "Domain": "Vision",
      "Task": "Character recognition (OCR),Image classification",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "405478",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://people.idsia.ch//~juergen/icdar_2007.pdf",
      "Reference": "A Novel Approach to On-Line Handwriting Recognition Based on Bidirectional Long Short-Term Memory Networks",
      "Citations": "287.0",
      "Authors": "M Liwicki, A Graves, S Fern\u00e0ndez",
      "Abstract": "In this paper we introduce a new connectionist approach to on-line handwriting recognition and address in particular the problem of recognizing handwritten whiteboard notes. The approach uses a bidirectional recurrent neural network with the long short-term memory architecture. We use a recently introduced objective function, known as Connectionist Temporal Classification (CTC), that directly trains the network to label unsegmented sequence data. Our new system achieves a word recognition rate of 74.0 %, compared with 65.4 % using a previously developed HMMbased recognition system.\n",
      "Organization categorization": "Academia,Academia,Academia",
      "Country (of organization)": "Switzerland,Switzerland,Germany",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "SB-LM",
      "Organization": "Google",
      "Publication date": "2007-06-22",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "300000000000.0",
      "Parameters notes": "Table 2",
      "Training compute (FLOP)": "1.4494464e+18",
      "Training compute notes": "Assuming a Nehalem based processor with 8 FLOP/cycle (https://www.agner.org/optimize/microarchitecture.pdf#page=105.06) , 2 cores and 2.33 GHz clock speed: 8*2*2330000000=37280000000 FLOP/s\nTrained for 1 day on 1500 machines (Table 2)\nCompute: 1500*37280000000*1*24*60*60*0.3=1449446400000000000=1.4e18\n",
      "Training dataset": "",
      "Training dataset size (gradients)": "1800000000000",
      "Dataset size notes": "Table 2",
      "Confidence": "Likely",
      "Link": "https://www.semanticscholar.org/paper/Large-Language-Models-in-Machine-Translation-Brants-Popat/ba786c46373892554b98df42df7af6f5da343c9d",
      "Reference": "Large Language Models in Machine Translation",
      "Citations": "",
      "Authors": "T. Brants, Ashok Popat, P. Xu, F. Och, J. Dean",
      "Abstract": "Systems, methods, and computer program products for machine translation are provided. In some implementations a system is provided. The system includes a language model including a collection of n-grams from a corpus, each n-gram having a corresponding relative frequency in the corpus and an order n corresponding to a number of tokens in the n-gram, each n-gram corresponding to a backoff n-gram having an order of n-1 and a collection of backoff scores, each backoff score associated with an n-gram, the backoff score determined as a function of a backoff factor and a relative frequency of a corresponding backoff n-gram in the corpus.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Training cost,Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "24.0",
      "Training time notes": "Table 2",
      "Training hardware": "",
      "Hardware quantity": "1500.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "KN-LM",
      "Organization": "Google",
      "Publication date": "2007-06-22",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "21000000000.0",
      "Parameters notes": "Table 2",
      "Training compute (FLOP)": "7.7303808e+17",
      "Training compute notes": "Trained for 2 days on 400 machines (Table 2)\nAssuming a Nehalem based processor with 8 FLOP/cycle (https://www.agner.org/optimize/microarchitecture.pdf#page=105.06) , 2 cores and 2.33 GHz clock speed: 8*2*2330000000=37280000000 FLOP/s\nCompute: 400*37280000000*2*24*60*60*0.3=773038080000000000=7.7e17",
      "Training dataset": "",
      "Training dataset size (gradients)": "31000000000",
      "Dataset size notes": "Table 2",
      "Confidence": "Likely",
      "Link": "https://www.semanticscholar.org/paper/Large-Language-Models-in-Machine-Translation-Brants-Popat/ba786c46373892554b98df42df7af6f5da343c9d",
      "Reference": "Large Language Models in Machine Translation",
      "Citations": "",
      "Authors": "T. Brants, Ashok Popat, P. Xu, F. Och, J. Dean",
      "Abstract": "Systems, methods, and computer program products for machine translation are provided. In some implementations a system is provided. The system includes a language model including a collection of n-grams from a corpus, each n-gram having a corresponding relative frequency in the corpus and an order n corresponding to a number of tokens in the n-gram, each n-gram corresponding to a backoff n-gram having an order of n-1 and a collection of backoff scores, each backoff score associated with an n-gram, the backoff score determined as a function of a backoff factor and a relative frequency of a corresponding backoff n-gram in the corpus.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Training cost,Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "48.0",
      "Training time notes": "Table 2",
      "Training hardware": "",
      "Hardware quantity": "400.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "\u03bb-WASP",
      "Organization": "UT Austin",
      "Publication date": "2007-06-01",
      "Domain": "Language",
      "Task": "Language Structure Modeling",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "\"Table 1 summarizes the results at the end of the learning curves (792 training examples for \u03bbWASP, WASP and SCISSOR, 600 for Z&C)\"",
      "Confidence": "",
      "Link": "https://www.aclweb.org/anthology/P07-1121/",
      "Reference": "Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus",
      "Citations": "383.0",
      "Authors": "YW Wong, R Mooney",
      "Abstract": "This paper presents the first empirical results to our knowledge on learning synchronous grammars that generate logical forms. Using statistical machine translation techniques, a semantic parser based on a synchronous context-free grammar augmented with \u03bboperators is learned given a set of training sentences and their correct logical forms.\nThe resulting parser is shown to be the bestperforming system so far in a database query domain.\n",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"The resulting parser is shown to be the bestperforming system so far in a database query domain\"\n\n\"The result is a robust semantic parser for predicate logic, and it is the best-performing system so far in the GEOQUERY domain.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Greedy layer-wise DNN training",
      "Organization": "University of Montreal / Universit\u00e9 de Montr\u00e9al",
      "Publication date": "2006-12-04",
      "Domain": "Other",
      "Task": "Image classification,Regression",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "107100000",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://dl.acm.org/doi/10.5555/2976456.2976476",
      "Reference": "Greedy layer-wise training of deep networks",
      "Citations": "5605.0",
      "Authors": "Y Bengio, P Lamblin, D Popovici",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "Canada",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Local Binary Patterns for facial recognition",
      "Organization": "University of Oulu,IEEE",
      "Publication date": "2006-12-01",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "",
      "Parameters notes": "Shallowly investigated, couldn't find much.\n",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "736",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.456.1094&rep=rep1&type=pdf",
      "Reference": "Face Description with Local Binary Patterns: Application to Face Recognition",
      "Citations": "5915.0",
      "Authors": "Timo Ahonen, Abdenour Hadid, and Matti Pietikainen",
      "Abstract": "",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "Finland,Multinational",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Dimensionality Reduction",
      "Organization": "University of Toronto",
      "Publication date": "2006-07-18",
      "Domain": "Vision",
      "Task": "Face recognition",
      "Parameters": "3800000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "47040000",
      "Dataset size notes": "After fine-tuning on all 60,000 training images, the autoencoder was tested on 10,000 new images and produced much better reconstructions than did PCA\n(Fig. 2B)",
      "Confidence": "",
      "Link": "https://www.cs.toronto.edu/~hinton/science.pdf",
      "Reference": "Reducing the dimensionality of data with neural networks.",
      "Citations": "15697.0",
      "Authors": "GE Hinton, RR Salakhutdinov",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "Canada",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Deep Belief Nets",
      "Organization": "University of Toronto,National University of Singapore",
      "Publication date": "2006-07-18",
      "Domain": "Vision",
      "Task": "Character recognition (OCR)",
      "Parameters": "1600000.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "30 epochs of single-layer \"greedy training\" on each layer, then 300 epochs of training with the \"up-down algorithm\" (backpropagation) on the whole network.\n\n\"The greedy training took a few hours per layer in Matlab on a 3GHz Xeon processor\"\n\"The network that performed best on the validation set was then tested and had an error rate of 1.39%. This network was then trained on all 60,000 training images8 until its error-rate on the full training set was as low as its final error-rate had been on the initial training set of 44,000 images. This took a further 59 epochs making the total learning time about a week.\"\n\nThe most training compute of any other model developed in 2006 or earlier was 6e15 FLOP.\nThe DBN was trained for one week on a 3GHz Xeon processor. ChatGPT estimates the processor can do one 128-bit FMA per cycle, so 8 FLOP/cycle * 3e9 cycle/s = 24e9 FLOP/s -> 3e16 FLOP/week. I'm guessing utilization would have been 30-80% (MFU is higher on CPUs), so 2e16 is a plausible upper bound.",
      "Training dataset": "MNIST",
      "Training dataset size (gradients)": "47100000",
      "Dataset size notes": "\"The network that performed best on the validation set was\nthen tested and had an error rate of 1.39%. This network was\nthen trained on all 60,000 training images8 until its error-rate\non the full training set was as low as its final error-rate had\nbeen on the initial training set of 44,000 images.\"",
      "Confidence": "",
      "Link": "https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf",
      "Reference": "A fast learning algorithm for deep belief nets",
      "Citations": "16071.0",
      "Authors": "GE Hinton, S Osindero, YW Teh",
      "Abstract": "",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "Canada,Singapore",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "330.0",
      "Training time (hours)": "168.0",
      "Training time notes": "\"a further 59 epochs [of training] making the total learning time about a week\"",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "SVM-CNN",
      "Organization": "New York University (NYU)",
      "Publication date": "2006-06-17",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "90857.0",
      "Parameters notes": "",
      "Training compute (FLOP)": "745200000000000.0",
      "Training compute notes": "Training time: 5880+330=6210 minutes (Table 1)\n\u201cThe timing is normalized to hypothetical 1GHz single CPU.\u201d\nAssuming utilization of 0.5 and AMD Athlon 64 Processor with 4 FLOP/cycle\nCompute: 6210*60*4*1000000000*0.5=745200000000000 = 7.4e14\n",
      "Training dataset": "NORB",
      "Training dataset size (gradients)": "583200",
      "Dataset size notes": "To generate the training set, each image was perturbed with 10 different configurations of the above parameters, which makes up 291,600 image pairs of size 108\u00d7108. The testing set has 2 drawings of perturbations, and have ",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/Large-scale-Learning-with-SVM-and-Convolutional-for-Huang-LeCun/cf03fdf52dd6e4249cbbdbd0bffbbbe5ca389feb",
      "Reference": "Large-scale Learning with SVM and Convolutional for Generic Object Categorization",
      "Citations": "",
      "Authors": "Fu Jie Huang, Yann LeCun",
      "Abstract": "The detection and recognition of generic object categories with invariance to viewpoint, illumination, and clutter requires the combination of a feature extractor and a classifier. We show that architectures such as convolutional networks are good at learning invariant features, but not always optimal for classification, while Support Vector Machines are good at producing decision surfaces from wellbehaved feature vectors, but cannot learn complicated invariances. We present a hybrid system where a convolutional network is trained to detect and recognize generic objects, and a Gaussian-kernel SVM is trained from the features learned by the convolutional network. Results are given on a large generic object recognition task with six categories (human figures, four-legged animals, airplanes, trucks, cars, and \"none of the above\"), with multiple instances of each object category under various poses, illuminations, and backgrounds. On the test set, which contains different object instances than the training set, an SVM alone yields a 43.3% error rate, a convolutional net alone yields 7.2% and an SVM on top of features produced by the convolutional net yields 5.9%.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "Only normalized training times are reported",
      "Training hardware": "",
      "Hardware quantity": "1.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Spatial Pyramid Matching",
      "Organization": "INRIA,University of Illinois Urbana-Champaign (UIUC),Ecole Normale Sup\u00e8rieure",
      "Publication date": "2006-06-17",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "3030",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://inc.ucsd.edu/mplab/users/marni/Igert/Lazebnik_06.pdf",
      "Reference": "Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories",
      "Citations": "9807.0",
      "Authors": "S Lazebnik, C Schmid, J Ponce",
      "Abstract": "",
      "Organization categorization": "Academia,Academia,Academia",
      "Country (of organization)": "France,United States of America,France",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Crazy Stone",
      "Organization": "INRIA",
      "Publication date": "2006-05-29",
      "Domain": "Games",
      "Task": "Go",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://inria.hal.science/inria-00116992v1",
      "Reference": "Efficient selectivity and backup operators in Monte-Carlo tree search",
      "Citations": "",
      "Authors": "R\u00e9mi Coulom",
      "Abstract": "A Monte-Carlo evaluation consists in estimating a position by averaging the outcome of several random continuations. The method can serve as an evaluation function at the leaves of a min-max tree. This paper presents a new framework to combine tree search with Monte-Carlo evaluation, that does not separate between a min-max phase and a Monte-Carlo phase. Instead of backing-up the min-max value close to the root, and the average value at some depth, a more general backup operator is defined that progressively changes from averaging to minmax as the number of simulations grows. This approach provides a finegrained control of the tree growth, at the level of individual simulations, and allows efficient selectivity. The resulting algorithm was implemented in a 9 \u00d7 9 Go-playing program, Crazy Stone, that won the 10th KGS computer-Go tournament.",
      "Organization categorization": "Academia",
      "Country (of organization)": "France",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "First ML model to use MCTS, plausibly it was SOTA for computer Go\n\n\"The resulting algorithm was implemented in a 9 \u00d7 9 Go-playing program, Crazy Stone, that won the 10th KGS computer-Go tournament.\"",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "FAST",
      "Organization": "University of Cambridge",
      "Publication date": "2006-05-07",
      "Domain": "Video",
      "Task": "Corner detection",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://link.springer.com/chapter/10.1007/11744023_34",
      "Reference": "Machine Learning for High-Speed Corner Detection",
      "Citations": "5419.0",
      "Authors": "Edward Rosten and Tom Drummond",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "TFE SVM",
      "Organization": "Centre de Recherche en Automatique de Nancy (CRAN),CENPARMI",
      "Publication date": "2006-02-02",
      "Domain": "Vision",
      "Task": "Digit recognition",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "600000",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://hal.archives-ouvertes.fr/hal-00018426/en",
      "Reference": "A trainable feature extractor for handwritten digit recognition",
      "Citations": "365.0",
      "Authors": "Fabian Lauer, Ching Y Suen, Gerard Bloch",
      "Abstract": "This article focuses on a particular task among pattern recognition, the handwritten digit recognition. More precisely, the problems of feature extraction and classification are explored. A trainable feature extractor based on the LeNet5 convolutional neural network architecture is introduced to solve the first problem in a black box scheme without prior knowledge on the data. The classification task is performed by Support Vector Machines to enhance the generalization ability of LeNet5. In order to increase the recognition rate, new training samples are generated by affine transformations and elastic distortions. Experiments are performed on the well known MNIST database to validate the method and the results show that the system can outperfom both SVMs and LeNet5 while providing performances comparable to the best performance on this database. Moreover, an analysis of the errors is conducted to discuss possible means of enhancement and their limitations.",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "France,Canada",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "best at affine-transformed digits in table 4",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "RankNet",
      "Organization": "Microsoft Research,Microsoft",
      "Publication date": "2005-08-07",
      "Domain": "Search",
      "Task": "Search",
      "Parameters": "5711.0",
      "Parameters notes": "Model is \"a two layer net with 10 hidden units\"\nInput is of size 569 \"In all, we use 569 features\"\nParameters:\n569*10 + 10 for hidden layer\n10*1 + 1 for output layer",
      "Training compute (FLOP)": "3482081588304.0",
      "Training compute notes": "FLOPs per forward pass: 2*parameters = 11422\nFLOPs per pair (data point): two forward passes and one backward pass (\"A forward prop is performed for the first sample; each node\u2019s activation and gradient value are stored; a forward prop is then performed for the second sample, and the activations and gradients are again stored. The gradient of the cost is then *formula*\") = 2*11422 + 2*11422 = 45688\nTotal FLOPs = (FLOPs per pair = 45688)*(pairs = 3,464,289)*(epochs = 22) = 3.48E12",
      "Training dataset": "",
      "Training dataset size (gradients)": "3464289",
      "Dataset size notes": "\"This resulted in our training on 384,314 query/document feature vectors, and on 3,464,289 pairs.\"",
      "Confidence": "Confident",
      "Link": "https://dl.acm.org/doi/abs/10.1145/1102351.1102363",
      "Reference": "Learning to rank using gradient descent",
      "Citations": "",
      "Authors": "Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, Greg HullenderAuthors Info & Claims",
      "Abstract": "We investigate using gradient descent methods for learning ranking functions; we propose a simple probabilistic cost function, and we introduce RankNet, an implementation of these ideas using a neural network to model the underlying ranking function. We present test results on toy data and on data from a commercial internet search engine.",
      "Organization categorization": "Industry,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "22.0",
      "Training time (hours)": "5.85",
      "Training time notes": "Table 6",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Histograms of Oriented Gradients",
      "Organization": "INRIA",
      "Publication date": "2005-06-25",
      "Domain": "Vision",
      "Task": "Object detection",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "14658",
      "Dataset size notes": " we produced a new and significantly more\nchallenging data set, \u2018INRIA\u2019, containing 1805 64\u00d7128 im-\nages",
      "Confidence": "",
      "Link": "https://ieeexplore.ieee.org/document/1467360",
      "Reference": "Histograms of oriented gradients for human detection",
      "Citations": "36578.0",
      "Authors": "N Dalal, B Triggs",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "France",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Hierarchical LM",
      "Organization": "",
      "Publication date": "2005-01-06",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "115848000000000.0",
      "Training compute notes": "\"The computations were performed on Athlon processors with a 1.2 GHz clock\"\nFP32 per cycle: 4 (\"The bottom line is that the Athlon is capable of delivering as many as four 32-bit, single-precision floating-point results per clock cycle\", https://www.pctechguide.com/amd-technology/amd-athlon) \nTraining time per epoch: 1609s (table 1)\nEpochs: 30 \"Training is performed over about 20 to 30 epochs according to validation set perplexity (early stopping).\"\nAssumed utilization: 0.5 \nCompute estimate: 0.5*1200000000*4*30*1609=115848000000000=1.16e14",
      "Training dataset": "Brown corpus",
      "Training dataset size (gradients)": "900000",
      "Dataset size notes": "\"The corpus has 1,105,515 occurrences of words, split into 3 sets: 900,000 for training, 100,000 for validation (model selection), and 105,515 for testing\"",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/Hierarchical-Probabilistic-Neural-Network-Language-Morin-Bengio/c19fbefdeead6a4154a22a9c8551a18b1530033a",
      "Reference": "Hierarchical Probabilistic Neural Network Language Model",
      "Citations": "",
      "Authors": "Frederic Morin, Yoshua Bengio",
      "Abstract": "In recent years, variants of a neural network architecture for statistical language modeling have been proposed and successfully applied, e.g. in the language modeling component of speech recognizers. The main advantage of these architectures is that they learn an embedding for words (or other symbols) in a continuous space that helps to smooth the language model and provide good generalization even when the number of training examples is insufficient. However, these models are extremely slow in comparison to the more commonly used n-gram models, both for training and recognition. As an alternative to an importance sampling method proposed to speed-up training, we introduce a hierarchical decomposition of the conditional probabilities that yields a speed-up of about 200 both during training and recognition. The hierarchical decomposition is a binary hierarchical clustering constrained by the prior knowledge extracted from the WordNet semantic hierarchy.",
      "Organization categorization": "",
      "Country (of organization)": "",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "30.0",
      "Training time (hours)": "13.4",
      "Training time notes": "Training time per epoch: 1609s (table 1)\nTotal training time 30*1609/60/60=13.408h\n",
      "Training hardware": "",
      "Hardware quantity": "1.0",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "LMICA",
      "Organization": "",
      "Publication date": "2004-12-01",
      "Domain": "Vision",
      "Task": "Object detection",
      "Parameters": "4096000.0",
      "Parameters notes": "64*64*1000=4096000\n\"100000 samples of natural scenes of 64 \u00d7 64 pixels were given as X\"\n\"LMICA was carried out in 1000 layers\"\n",
      "Training compute (FLOP)": "2782080000000000.0",
      "Training compute notes": "69*60*60*8*2800000000*0.5=2782080000000000=2.78e15\n\"it consumed about 69 hours with Intel 2.8GHz CPU\"\n- Assuming they used an Intel Pentium 4 processor with 8 FLOP/cycle (https://en.wikipedia.org/wiki/FLOPS)",
      "Training dataset": "",
      "Training dataset size (gradients)": "100000",
      "Dataset size notes": "\"100000 samples of natural scenes of 64 \u00d7 64 pixels were given as X\"",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/Linear-Multilayer-Independent-Component-Analysis-Matsuda-Yamaguchi/7061b01572fbff2e223ce3abb59f397895b1ebf1",
      "Reference": "Linear Multilayer Independent Component Analysis for Large Natural Scenes\n",
      "Citations": "",
      "Authors": "Yoshitatsu Matsuda, K. Yamaguchi",
      "Abstract": "In this paper, linear multilayer ICA (LMICA) is proposed for extracting independent components from quite high-dimensional observed signals such as large-size natural scenes. There are two phases in each layer of LMICA. One is the mapping phase, where a one-dimensional mapping is formed by a stochastic gradient algorithm which makes more highly-correlated (non-independent) signals be nearer incrementally. Another is the local-ICA phase, where each neighbor (namely, highly-correlated) pair of signals in the mapping is separated by the MaxKurt algorithm. Because LMICA separates only the highly-correlated pairs instead of all ones, it can extract independent components quite efficiently from appropriate observed signals. In addition, it is proved that LMICA always converges. Some numerical experiments verify that LMICA is quite efficient and effective in large-size natural image processing.",
      "Organization categorization": "",
      "Country (of organization)": "",
      "Notability criteria": "Training cost,Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Invariant CNN",
      "Organization": "New York University (NYU)",
      "Publication date": "2004-06-27",
      "Domain": "Vision",
      "Task": "Object recognition",
      "Parameters": "90575.0",
      "Parameters notes": "\"The network has a total of 90,575 trainable parameters.\"",
      "Training compute (FLOP)": "974230000000.0",
      "Training compute notes": "\"A full propagation through the network requires 3,896,920 multiply-adds.\" - it's not entirely clear whether this refers to a forward pass or forward + backward pass (I assumed the latter)\n\"We used a stochastic version of the Levenberg-Marquardt algorithm with diagonal approximation of the Hessian [7], for approximately 250,000 online updates.\"\n3896920*250000=974230000000=9.7e11",
      "Training dataset": "",
      "Training dataset size (gradients)": "24300",
      "Dataset size notes": "\"normalized-uniform set: 5 classes, centered, unperturbed objects on uniform backgrounds. 24,300 training samples, 24,300 testing samples.\"",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/Learning-methods-for-generic-object-recognition-to-LeCun-Huang/f354310098e09c1e1dc88758fca36767fd9d084d",
      "Reference": "Learning methods for generic object recognition with invariance to pose and lighting",
      "Citations": "",
      "Authors": "Yann LeCun, Fu Jie Huang, L. Bottou",
      "Abstract": "We assess the applicability of several popular learning methods for the problem of recognizing generic visual categories with invariance to pose, lighting, and surrounding clutter. A large dataset comprising stereo image pairs of 50 uniform-colored toys under 36 azimuths, 9 elevations, and 6 lighting conditions was collected (for a total of 194,400 individual images). The objects were 10 instances of 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. Five instances of each category were used for training, and the other five for testing. Low-resolution grayscale images of the objects with various amounts of variability and surrounding clutter were used for training and testing. Nearest neighbor methods, support vector machines, and convolutional networks, operating on raw pixels or on PCA-derived features were tested. Test error rates for unseen object instances placed on uniform backgrounds were around 13% for SVM and 7% for convolutional nets. On a segmentation/recognition task with highly cluttered images, SVM proved impractical, while convolutional nets yielded 16/7% error. A real-time version of the system was implemented that can detect and classify objects in natural scenes at around 10 frames per second.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,Historical significance",
      "Notability criteria notes": "",
      "Epochs": "10.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "NPLM (Brown)",
      "Organization": "University of Montreal / Universit\u00e9 de Montr\u00e9al",
      "Publication date": "2003-03-15",
      "Domain": "Language",
      "Task": "Text autocompletion",
      "Parameters": "4124233.0",
      "Parameters notes": "\"The number of free parameters is |V|(1 + nm + h) + h(1 + (n \u2212 1)m) [...] For example, consider the following architecture used in the experiments on the AP (Associated Press) news data: the vocabulary size is |V| = 17,964, the number of hidden units is h = 60, the order of the model is n = 6, the number of word features is m = 100\"\n\nBrown corpus: n=5, h=100, m=30, |V|=16383\n16383*(1+5*30+100)+100*(1+(5-1)*30)=4124233",
      "Training compute (FLOP)": "132076260000000.0",
      "Training compute notes": "\"For example, consider the following architecture used in the experiments on the AP (Associated Press) news data: the vocabulary size is |V| = 17,964, the number of hidden units is h = 60, the order of the model is n = 6, the number of word features is m = 100. The total number of numerical operations to process a single training example is approximately |V|(1+nm+h)+h(1+nm)+nm\"\n\nBrown corpus: n=5, h=100, m=30, |V|=16383, dataset size = 800000, epochs=20\nForward FLOP: 16383*(1+5*30+100)+100*(1+5*30)+5*30=4127383\nAdjusting for backward pass with 1:1 ratio, as the by far largest layer (embedding) doesn't require gradients w.r.t. inputs.\nTotal FLOP: 2*4127383*800000*20=1.3207626e+14",
      "Training dataset": "Brown corpus",
      "Training dataset size (gradients)": "13994528",
      "Dataset size notes": "\"Comparative experiments were performed on the Brown corpus which is a stream of 1,181,041 words, from a large variety of English texts and books. The first 800,000 words were used for training, the following 200,000 for validation (model selection, weight decay, early stopping) and the remaining 181,041 for testing. The number of different words is 47,578 (including punctuation, distinguishing between upper and lower case, and including the syntactical marks used to separate texts and paragraphs). Rare words with frequency \u2264 3 were merged into a single symbol, reducing the vocabulary size to |V| = 16,383.\"",
      "Confidence": "Confident",
      "Link": "https://dl.acm.org/doi/10.5555/944919.944966",
      "Reference": "A Neural Probabilistic Language Model",
      "Citations": "7637.0",
      "Authors": "Yoshua Bengio, R\u00e9jean Ducharme, Pascal Vincent, Christian Jauvin",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "Canada",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "NPLM (AP News)",
      "Organization": "University of Montreal / Universit\u00e9 de Montr\u00e9al",
      "Publication date": "2003-03-15",
      "Domain": "Language",
      "Task": "Text autocompletion",
      "Parameters": "11904264.0",
      "Parameters notes": "\"The number of free parameters is |V|(1 + nm + h) + h(1 + (n \u2212 1)m) [...] For example, consider the following architecture used in the experiments on the AP (Associated Press) news data: the vocabulary size is |V| = 17,964, the number of hidden units is h = 60, the order of the model is n = 6, the number of word features is m = 100\"\n\nAP News: n=6, h=60, m=100, |V|=17964\n17964*(1+6*100+60)+60*(1+(6-1)*100)=11904264",
      "Training compute (FLOP)": "1666869200000000.0",
      "Training compute notes": "\"For example, consider the following architecture used in the experiments on the AP (Associated Press) news data: the vocabulary size is |V| = 17,964, the number of hidden units is h = 60, the order of the model is n = 6, the number of word features is m = 100. The total number of numerical operations to process a single training example is approximately |V|(1+nm+h)+h(1+nm)+nm\"\n\nAP News: n=6, h=60, m=100, |V|=17964, dataset=13994528, epochs=5\nForward FLOP: 17964*(1+6*100+60)+60*(1+6*100)+6*100=11910864\nAdjusting for backward pass with 1:1 ratio, as the by far largest layer (embedding) doesn't require gradients w.r.t. inputs.\nTotal FLOP: 2*11910864*13994528*5=1.6668692e+15",
      "Training dataset": "Brown corpus",
      "Training dataset size (gradients)": "13994528",
      "Dataset size notes": "\"Comparative experiments were performed on the Brown corpus which is a stream of 1,181,041 words, from a large variety of English texts and books. The first 800,000 words were used for training, the following 200,000 for validation (model selection, weight decay, early stopping) and the remaining 181,041 for testing. The number of different words is 47,578 (including punctuation, distinguishing between upper and lower case, and including the syntactical marks used to separate texts and paragraphs). Rare words with frequency \u2264 3 were merged into a single symbol, reducing the vocabulary size to |V| = 16,383.\"",
      "Confidence": "Confident",
      "Link": "https://dl.acm.org/doi/10.5555/944919.944966",
      "Reference": "A Neural Probabilistic Language Model",
      "Citations": "7637.0",
      "Authors": "Yoshua Bengio, R\u00e9jean Ducharme, Pascal Vincent, Christian Jauvin",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "Canada",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "LDA",
      "Organization": "Stanford University",
      "Publication date": "2003-02-02",
      "Domain": "Language",
      "Task": "Document classification",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "Multiple experiments with different tasks and datasets",
      "Confidence": "Unknown",
      "Link": "https://jmlr.org/papers/volume3/blei03a/blei03a.pdf",
      "Reference": "Latent Dirichlet Allocation",
      "Citations": "38724.0",
      "Authors": "David M. Blei, Andrew Y. Ng, Michael I. Jordan",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Statistical Shape Constellations",
      "Organization": "California Institute of Technology",
      "Publication date": "2003-01-01",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://link.springer.com/content/pdf/10.1007/3-540-45054-8_2.pdf",
      "Reference": "Unsupervised Learning of Models for Recognition",
      "Citations": "949.0",
      "Authors": "M. Weber, M. Welling, and P. Perona",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Thumbs Up?",
      "Organization": "Cornell University,IBM",
      "Publication date": "2002-05-28",
      "Domain": "Language",
      "Task": "Sentiment classification",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "IMDb",
      "Training dataset size (gradients)": "1400",
      "Dataset size notes": "yielding a corpus of 752 negative and\n1301 positive reviews",
      "Confidence": "",
      "Link": "https://arxiv.org/abs/cs/0205070",
      "Reference": "Thumbs up? Sentiment Classification using Machine Learning Techniques",
      "Citations": "10656.0",
      "Authors": "Bo Pang, Lillian Lee, Shivakumar Vaithyanathan",
      "Abstract": "",
      "Organization categorization": "Academia,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Decision tree (classification)",
      "Organization": "Mitsubishi Electric Research Labs,Compaq CRL",
      "Publication date": "2001-12-08",
      "Domain": "Vision",
      "Task": "Face recognition",
      "Parameters": "12000.0",
      "Parameters notes": "Iteratively learns decision tree features, where each feature has 2 parameters (threshold and parity). They learn 6000 features total: \"The complete face detection cascade has 38 stages with over\n6000 features\"\n\nAdditional weights (see Table 1) are a temporary variable of the optimization and not part of the final model. ",
      "Training compute (FLOP)": "63000000000000.0",
      "Training compute notes": "\nThe training compute can be tediously worked out from the pseudocode. I think for dataset size D, number of filters T, the training compute is roughly 180k * D * 3 * T = 6.3e13 FLOPs\n\nThe training evaluates each of 180k candidate features at each step, repeated for 6000 steps (as 1 feature is selected per round). \nThe operations for one feature evaluation are unclear, but should be low (they only compare specific integer positions in the image). Estimated at 10op. \n180000*6000*10*14460=1.56168e+14",
      "Training dataset": "",
      "Training dataset size (gradients)": "753616",
      "Dataset size notes": "Section 5: 4916 hand labeled faces  + 9544 non-face images = 14460",
      "Confidence": "Likely",
      "Link": "https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf",
      "Reference": "Rapid object detection using a boosted cascade of simple features",
      "Citations": "23449.0",
      "Authors": "P. Viola, M. Jones",
      "Abstract": "",
      "Organization categorization": "Industry,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Gradient Boosting Machine",
      "Organization": "Stanford University",
      "Publication date": "2001-10-01",
      "Domain": "Mathematics",
      "Task": "Pattern classification,Binary classification,Regression",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "5000",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boostingmachine/10.1214/aos/1013203451.full",
      "Reference": "Greedy function approximation: A gradient boosting machine",
      "Citations": "17891.0",
      "Authors": "Jerome H. Friedman",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Immediate trihead",
      "Organization": "Brown University",
      "Publication date": "2001-07-06",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://dl.acm.org/doi/10.3115/1073012.1073029",
      "Reference": "Immediate-Head Parsing for Language Models",
      "Citations": "422.0",
      "Authors": "Eugene Charniak",
      "Abstract": "We present two language models based upon an \"immediate-head\" parser --- our name for a parser that conditions all events below a constituent c upon the head of c. While all of the most accurate statistical parsers are of the immediate-head variety, no previous grammatical language model uses this technology. The perplexity for both of these models significantly improve upon the trigram model base-line as well as the best previous grammar-based language model. For the better of our two models these improvements are 24% and 14% respectively. We also suggest that improvement of the underlying parser should significantly improve the model's perplexity and that even in the near term there is a lot of potential for improvement in immediate-head language models.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement",
      "Notability criteria notes": "\"The perplexity for both of these models significantly improve upon the trigram model base-line as well as the best previous grammar based language model\"\n\nI don't see any standard benchmark that they claim SOTA on",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "PoE MNIST",
      "Organization": "University College London (UCL)",
      "Publication date": "2000-11-28",
      "Domain": "Vision",
      "Task": "Digit recognition",
      "Parameters": "3925310.0",
      "Parameters notes": "10 models, one for each digit. Largest models: 500 epochs, 500 hidden units (Table 2)\n\"The largest network was the best, even though each digit model contains 392,500 parameters trained on only 4,400 images\"\n\"the classification network had 30 inputs and therefore 300 weights and 10 output biases.\"\n\nTotal: 392500*10 + 310 = 3,925,310",
      "Training compute (FLOP)": "51810000000000.0",
      "Training compute notes": "Each model was trained on 4400 examples: \"The largest network was the best, even though each digit model contains 392,500 parameters trained on only 4,400 images.\"\n\nTable 2, largest network trained 500 epochs.\n10 * 6 * 392500 * 4400 * 500 = 51,810,000,000,000",
      "Training dataset": "MNIST",
      "Training dataset size (gradients)": "54000",
      "Dataset size notes": "Total training data size is 60000 but the subnetworks were trained on smaller subsets.",
      "Confidence": "Confident",
      "Link": "https://proceedings.neurips.cc/paper_files/paper/2000/file/1f1baa5b8edac74eb4eaa329f14a0361-Paper.pdf",
      "Reference": "Recognizing Hand-written Digits Using Hierarchical Products of Experts",
      "Citations": "",
      "Authors": "Guy Mayraz, Geoffrey E. Hinton",
      "Abstract": "The product of experts learning procedure [1] can discover a set of stochastic binary features that constitute a non-linear generative model of handwritten images of digits. The quality of generative models learned in this way can be assessed by learning a separate model for each class of digit and then comparing the unnormalized probabilities of test images under the 10 different class-specific models. To improve discriminative performance, it is helpful to learn a hierarchy of separate models for each digit class. Each model in the hierarchy has one layer of hidden units and the nth level model is trained on data that consists of the activities of the hidden units in the already trained (n - 1)th level model. After training, each level produces a separate, unnormalized log probabilty score. With a three-level hierarchy for each of the 10 digit classes, a test image produces 30 scores which can be used as inputs to a supervised, logistic classification network that is trained on separate data. On the MNIST database, our system is comparable with current state-of-the-art discriminative methods, demonstrating that the product of experts learning procedure can produce effective generative models of high-dimensional data.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "500.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Neural LM",
      "Organization": "University of Montreal / Universit\u00e9 de Montr\u00e9al",
      "Publication date": "2000-11-28",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "6906980.0",
      "Parameters notes": "(30959*100) + (8*100*120) + (120*30959) = 6,906,980\n\"This is obtained with a network with the direct architecture, 100 randomly initialized words features, 120 hidden units, and n = 8 words of context.\"\n\"The Hansard corpus (Canadian parliament proceedings, French version) is a stream of about 34 million words, of which 32 millions (set A) was used for training, 1.1 million (set B) was used for validation, and 1.2 million (set C) was used for out-of-sample tests. The original data has 106, 936 different words, and those with frequency <= 10 were merged into a single token, yielding IVI = 30,959 different words.\"\n",
      "Training compute (FLOP)": "6339000000000000.0",
      "Training compute notes": "The authors use a trick to avoid having to calculate the final layer for all possible words in the vocabulary. They precompute a \"short list\" of the most common word following any 2 precursor words with a smoothed trigram model, and then only calculate the softmax over words on the short list. This means only a negligible fraction of the unembedding parameters get used, so the effective number of parameters appears to be (30959*100) + (8*100*120) = 3,191,900\n\n\"Apparent convergence of the stochastic gradient descent procedure was obtained after around 10 epochs for Hansard\"\n\n6ND:\n6*3191900*33100000*10=6.339e15\n",
      "Training dataset": "Hansard Corpus",
      "Training dataset size (gradients)": "32000000",
      "Dataset size notes": "The Hansard corpus (Canadian parliament proceedings, French version) is a stream of about 34 million words, of which 32 millions (set A) was used for training, 1.1 million (set B) was used for validation, and 1.2 million (set C) was used for out-of-sample tests.",
      "Confidence": "Confident",
      "Link": "https://papers.nips.cc/paper_files/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf",
      "Reference": "A Neural Probabilistic Language Model",
      "Citations": "7637.0",
      "Authors": "Yoshua Bengio, R\u00e9jean Ducharme, Pascal Vincent, Christian Janvin",
      "Abstract": "A goal of statistical language modeling is to learn the joint probability function of sequences of words. This is intrinsically difficult because of the curse of dimensionality: we propose to fight it with its own weapons. In the proposed approach one learns simultaneously (1) a distributed representation for each word (i.e. a similarity between words) along with (2) the probability function for word sequences, expressed with these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar to words forming an already seen sentence. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach very significantly improves on a state-of-the-art trigram model.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Canada",
      "Notability criteria": "Training cost,Historical significance,Highly cited",
      "Notability criteria notes": "",
      "Epochs": "10.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "IBM Model 4",
      "Organization": "University of Southern California,IBM,University of Pennsylvania",
      "Publication date": "1999-07-02",
      "Domain": "Language",
      "Task": "Translation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "800000",
      "Dataset size notes": "[WORDS]\nSee FIgure 6",
      "Confidence": "",
      "Link": "http://www-i6.informatik.rwth-aachen.de/publications/download/266/al-onaizan--1999.pdf",
      "Reference": "Statistical machine translation",
      "Citations": "1921.0",
      "Authors": "Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, Dan Melamed, Franz-Josef Och, David Purdy, Noah A. Smith, and David Yarowsky",
      "Abstract": "",
      "Organization categorization": "Academia,Industry,Academia",
      "Country (of organization)": "United States of America,United States of America,United States of America",
      "Notability criteria": "Highly cited,Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "RECONTRA-uncategorized",
      "Organization": "",
      "Publication date": "1999-06-02",
      "Domain": "Language",
      "Task": "Translation",
      "Parameters": "112000.0",
      "Parameters notes": "8*61*160+160*160+160*52=112000\n\"a network with 61 input units, 52 outputs, 160 hidden traits and 8 (4+I+3) delayed inputs\"\nTable 1\n",
      "Training compute (FLOP)": "3864000000000.0",
      "Training compute notes": " 2*112000*3*11.5*5000*100=3864000000000=3.9e12\n\"was trained for 100 epochs using the 5,000 pairs\"\n\"The length of the non-categorized Spanish sentences ranged from 3 to 20 and the length of the non-categorized English sentences, from 3 to 17\"",
      "Training dataset": "",
      "Training dataset size (gradients)": "57500",
      "Dataset size notes": "5000*11.5=57500\nThe length of the non-categorized Spanish sentences ranged from 3 to 20 and the length of the non-categorized English sentences, from 3 to 17",
      "Confidence": "Likely",
      "Link": "https://www.semanticscholar.org/paper/Text-to-Text-Machine-Translation-Using-the-RECONTRA-Casta%C3%B1o-Casacuberta/47ee046b416d9258952cb8f4b0e2b6e65f334fad",
      "Reference": "Text-to-text machine translation using the RECONTRA connectionist model",
      "Citations": "",
      "Authors": "M. A. Casta\u00f1o, F. Casacuberta",
      "Abstract": "Encouragingly accurate translations have recently been obtained using a connectionist translator called RECONTRA (Recurrent Connectionist Translator). In contrast to traditional Knowledge-Based systems, this model is built from training data resulting in an Example-Based approach. It directly carries out the translation between the source and target language and employs a simple (recurrent) connectionist topology and a simple training scheme. This paper extends previous work exploring the capabilities of this RECONTRA model to perform text-to-text translations in limited-domain tasks.",
      "Organization categorization": "",
      "Country (of organization)": "",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "",
      "Epochs": "100.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "RECONTRA-categorized",
      "Organization": "",
      "Publication date": "1999-06-02",
      "Domain": "Language",
      "Task": "Translation",
      "Parameters": "66780.0",
      "Parameters notes": "6*50*140+140*140+140*37=66780\nTable 1\n\n",
      "Training compute (FLOP)": "8013600000000.0",
      "Training compute notes": "2*66780*3*500*5000*8=8013600000000=8e12\n\"The number of words of the categorized sentences ranged from 3 to 13 for the Spanish ones and from 3 to 12 for the English ones.\"\n\"was trained up to 500 epochs using the 5,000 categorized pairs\"",
      "Training dataset": "",
      "Training dataset size (gradients)": "40000",
      "Dataset size notes": "5000*8=40000 words\nThe number of words of the categorized sentences ranged from 3 to 13 for the Spanish ones and from 3 to 12 for the English ones.\n",
      "Confidence": "Likely",
      "Link": "https://www.semanticscholar.org/paper/Text-to-Text-Machine-Translation-Using-the-RECONTRA-Casta%C3%B1o-Casacuberta/47ee046b416d9258952cb8f4b0e2b6e65f334fad",
      "Reference": "Text-to-text machine translation using the RECONTRA connectionist model",
      "Citations": "",
      "Authors": "M. A. Casta\u00f1o, F. Casacuberta",
      "Abstract": "Encouragingly accurate translations have recently been obtained using a connectionist translator called RECONTRA (Recurrent Connectionist Translator). In contrast to traditional Knowledge-Based systems, this model is built from training data resulting in an Example-Based approach. It directly carries out the translation between the source and target language and employs a simple (recurrent) connectionist topology and a simple training scheme. This paper extends previous work exploring the capabilities of this RECONTRA model to perform text-to-text translations in limited-domain tasks.",
      "Organization categorization": "",
      "Country (of organization)": "",
      "Notability criteria": "Training cost",
      "Notability criteria notes": "",
      "Epochs": "500.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "LSTM with forget gates",
      "Organization": "IDSIA",
      "Publication date": "1999-01-02",
      "Domain": "Language",
      "Task": "Language Structure Modeling",
      "Parameters": "276.0",
      "Parameters notes": "See Table 1",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "140870000",
      "Dataset size notes": "Training was stopped after at most 30000\ntraining streams, each of which was ended\nwhen the first prediction error or the\n100000th successive input symbol occurred\n\nNOTE this is a weird task. Not sure how to measure dataset size (#seqs? #symbols?)",
      "Confidence": "",
      "Link": "https://ieeexplore.ieee.org/document/818041",
      "Reference": "Learning to forget: Continual prediction with LSTM",
      "Citations": "6283.0",
      "Authors": "F. A. Gers, J. Schmidhuber, and F. Cummins",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "Switzerland",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "LeNet-5",
      "Organization": "AT&T",
      "Publication date": "1998-11-01",
      "Domain": "Vision",
      "Task": "Character recognition (OCR)",
      "Parameters": "60000.0",
      "Parameters notes": "\"[LeNet5] contains 390408 connections, but only 60000 trainable free parameters because of the weight sharing\"",
      "Training compute (FLOP)": "2810937600000.0",
      "Training compute notes": "\"[LeNet5] contains 390408 connections\" = multiply-adds\nMNIST - 60,000 data points\n20 epochs\n390408*60000*6*20=2.810938e+12",
      "Training dataset": "MNIST",
      "Training dataset size (gradients)": "60000",
      "Dataset size notes": "The MNIST database contains 60,000 training images and 10,000 testing images (Wikipedia)",
      "Confidence": "Confident",
      "Link": "http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf",
      "Reference": "Gradient-based Learning Applied to Document Recognition",
      "Citations": "57900.0",
      "Authors": "Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, Patrick Haffner",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance,Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "LSTM",
      "Organization": "Technical University of Munich",
      "Publication date": "1997-11-15",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "10504.0",
      "Parameters notes": "Table 2\n\nhttp://www.bioinf.jku.at/publications/older/2604.pdf",
      "Training compute (FLOP)": "31512000000000.0",
      "Training compute notes": "\"Due to limited computation time, training is stopped after 5 million sequence presentations\"\n\nEach sequence has p=100 elements in the long-delay setting.\n\nCOMPUTE = PRESENTATIONS * PRESENTATION LENGTH * UPDATE COMPUTE PER TOKEN\n\n5000000*100*6*10,504.0=",
      "Training dataset": "",
      "Training dataset size (gradients)": "853000",
      "Dataset size notes": "Table 8. The rightmost column lists numbers of training sequences required to achieve the stopping\ncriterion.\n\nThis applies to experiment 5 (multiplication)\n\nSequences have random lengths, on the order of 100-1000 (table 7 )",
      "Confidence": "Confident",
      "Link": "https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltext",
      "Reference": "Long short-term memory",
      "Citations": "98595.0",
      "Authors": "Sepp Hochreiter ; Jurgen Schmidhuber",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "Germany",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Bidirectional RNN",
      "Organization": "Advanced Telecommunications Research Institute",
      "Publication date": "1997-11-01",
      "Domain": "Speech",
      "Task": "Speech recognition (ASR)",
      "Parameters": "13000.0",
      "Parameters notes": "Page 7: \"The structures of all networks are adjusted so that\neach of them has about the same number of free parameters\n(approximately 13 000 here\"",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "TIMIT",
      "Training dataset size (gradients)": "142910",
      "Dataset size notes": "\"the training data set consisting of 3696 sentences\nfrom 462 speakers\"\n\nAssuming avg sentence length of 20 words\n\n3696 * 20 total words",
      "Confidence": "",
      "Link": "https://ieeexplore.ieee.org/document/650093",
      "Reference": "Bidirectional recurrent neural networks",
      "Citations": "7990.0",
      "Authors": "M. Schuster, KK Paliwal",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "Japan",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Deep Blue",
      "Organization": "IBM",
      "Publication date": "1997-05-01",
      "Domain": "Games",
      "Task": "Chess",
      "Parameters": "8000.0",
      "Parameters notes": "\"The new chess chip had a completely redesigned evaluation function, going from around 6400 features to over 8000\"",
      "Training compute (FLOP)": "",
      "Training compute notes": "The 8000 features were tuned using a mix of human judgment and automated tools using data on chess matches. Unclear how much total \"compute\" went into this.",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Speculative",
      "Link": "https://www.sciencedirect.com/science/article/pii/S0004370201001291",
      "Reference": "Deep Blue",
      "Citations": "1992.0",
      "Authors": "Murray Campbell, A. Joseph Hoane Jr., Feng-hsiung Hsu",
      "Abstract": "Deep Blue is the chess machine that defeated then-reigning World Chess Champion Garry Kasparov in a six-game match in 1997. There were a number of factors that contributed to this success, including:\n\u2022a single-chip chess search engine,\n\n\u2022a massively parallel system with multiple levels of parallelism,\n\n\u2022a strong emphasis on search extensions,\n\n\u2022a complex evaluation function, and\n\n\u2022effective use of a Grandmaster game database.\n\n\nThis paper describes the Deep Blue system, and gives some of the rationale that went into the design decisions behind Deep Blue.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance,Highly cited",
      "Notability criteria notes": "Defeated Kasparov in 1997, which was a famous AI milestone.",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "SOM-CNN",
      "Organization": "",
      "Publication date": "1997-01-31",
      "Domain": "Vision",
      "Task": "Face recognition",
      "Parameters": "32015.0",
      "Parameters notes": "SOM: 125\nL1: 20*3*3*3=540\nL2: 25*3*3*20*0.3=1350\nL3: 25*5*6*40=30000\nTotal: 125+540+1350+30000=32,015\n(see Table 1)",
      "Training compute (FLOP)": "31431600000.0",
      "Training compute notes": "Forward pass flop based on architecture (see Table 1): \nFirst CNN layer: 23*28 -> 3*3*20\n2*21*26*3*3*20=196560\nSecond CNN layer: 20*11*13 -> 3*3*25*0.3\n2*9*11*3*3*20*25*0.3=267300\nFC layer: 25*5*6 -> 40\n2*25*5*6*40=60000\nTotal forward FLOP: 196560+267300+60000=523860\nTotal training compute: 523860*3*20000=31431600000\nTraining steps: \"The network was trained with backpropagation [13] for a total of 20 000 updates.\"",
      "Training dataset": "",
      "Training dataset size (gradients)": "129000",
      "Dataset size notes": "\"a total of 200 training images and 200 test images.\" but note that images are sub-sampled to smaller local windows for training.",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/Face-recognition%3A-a-convolutional-neural-network-Lawrence-Giles/86890c82b589e24007c56e1f40c5f928a0e04183",
      "Reference": "Face recognition: a convolutional neural-network approach",
      "Citations": "",
      "Authors": "S. Lawrence, C. Lee Giles, A. Tsoi, A. Back",
      "Abstract": "We present a hybrid neural-network for human face recognition which compares favourably with other methods. The system combines local image sampling, a self-organizing map (SOM) neural network, and a convolutional neural network. The SOM provides a quantization of the image samples into a topological space where inputs that are nearby in the original space are also nearby in the output space, thereby providing dimensionality reduction and invariance to minor changes in the image sample, and the convolutional neural network provides partial invariance to translation, rotation, scale, and deformation. The convolutional network extracts successively larger features in a hierarchical set of layers. We present results using the Karhunen-Loeve transform in place of the SOM, and a multilayer perceptron (MLP) in place of the convolutional network for comparison. We use a database of 400 images of 40 individuals which contains quite a high degree of variability in expression, pose, and facial details. We analyze the computational complexity and discuss how new classes could be added to the trained recognizer.",
      "Organization categorization": "",
      "Country (of organization)": "",
      "Notability criteria": "Historical significance,Highly cited",
      "Notability criteria notes": "",
      "Epochs": "100.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "AdaBoost.M2 Digit Recognition",
      "Organization": "AT&T",
      "Publication date": "1996-07-03",
      "Domain": "Vision",
      "Task": "Digit recognition",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "9709",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://cseweb.ucsd.edu/~yfreund/papers/boostingexperiments.pdf",
      "Reference": "Experiments with a New Boosting Algorithm",
      "Citations": "12977.0",
      "Authors": "Yoav Freund, Robert E. Schapire",
      "Abstract": "In an earlier paper, we introduced a new \u201cboosting\u201d algorithm called AdaBoost which, theoretically, can be used to significantly reduce the error of any learning algorithm that con- sistently generates classifiers whose performance is a little better than random guessing. We also introduced the related notion of a \u201cpseudo-loss\u201d which is a method for forcing a learning algorithm of multi-label concepts to concentrate on the labels that are hardest to discriminate. In this paper, we describe experiments we carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems.\nWe performed two sets of experiments. The first set compared boosting to Breiman\u2019s \u201cbagging\u201d method when used to aggregate various classifiers (including decision trees and single attribute- value tests). We compared the performance of the two methods on a collection of machine-learning benchmarks. In the second set of experiments, we studied in more detail the performance of boosting using a nearest-neighbor classifier on an OCR problem.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "Also listed in Denis Panjuta's List of 100+ AI Algorithms",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "System 11",
      "Organization": "Carnegie Mellon University (CMU)",
      "Publication date": "1996-06-18",
      "Domain": "Vision",
      "Task": "Face detection",
      "Parameters": "6452.0",
      "Parameters notes": "System 11 is a combination of Network 1 and Network 2\n\nNetwork 1 has 2095 connections and network 2 has 4357 connections (see table 1)",
      "Training compute (FLOP)": "25859616000.0",
      "Training compute notes": "Since there is no parameter sharing, the forward compute is roughly twice that of the number of parameters. We use a 2:1 forward-backward ratio as this is a shallow network, with most connections in the first layer.\n\nNumber of passes (Section 2.1):\n* \"Nearly 1,050 face examples were gathered from face databases [...]\"\n* \"Fifteen face examples are generated for the training set from each original image\"\n\nTraining loop:\n1. \"initial set of nonface images by generating 1,000 random images\"\n2. Train (presumably on whole set)\n3. Run + collect false positives\n4. \"Select up to 250 of these subimages [...] and add them into the training set [...] Go to step 2\"\n\n\"A typical training run selects approximately 8,000 nonface images \"\n\nSelecting 8,000 nonface images implies 8000/250 = 32 loops.\n\nAssuming compute is 3 * N * D, we have\n* Loop 1: D = 15*1050 + 1000\n* Loop 2: D = 15*1050 + 1000 + 250\n* So on.\n\nHence D overall is 32*(15*1050 + 1000) + 250*32/2*(32+1) = 668,000.\n\nHence compute = 3 * 12904 * 668e3 = 25859616000",
      "Training dataset": "",
      "Training dataset size (gradients)": "23750",
      "Dataset size notes": "\"A typical training\nrun selects approximately 8000 non-face images from the\n146,212,178 subimages that are available at all locations\nand scales in the training scenery images.\"\n\n\"Nearly 1050 face examples were gathered from face databases at CMU and Harvard [...] In the training set,15 face examples are generated from each\noriginal image [...]\"\n\n\"Create an initial set of non-face images by generating\n1000 images with random pixel intensities\"",
      "Confidence": "Confident",
      "Link": "https://ieeexplore.ieee.org/document/655647",
      "Reference": "Neural Network-Based Face Detection",
      "Citations": "6011.0",
      "Authors": "HA Rowley, S Baluja, T Kanade",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "MUSIC perceptron",
      "Organization": "",
      "Publication date": "1996-06-03",
      "Domain": "Vision",
      "Task": "Image completion",
      "Parameters": "13607.0",
      "Parameters notes": "230*55+56*15+16*6+7*3=13607 (Figure 2)",
      "Training compute (FLOP)": "881733600000.0",
      "Training compute notes": "2*13607*3*10800000=881733600000=8.8e11\nTraining steps: 400*27000=10800000\n\"After 400 epochs the error of the network\"",
      "Training dataset": "",
      "Training dataset size (gradients)": "81000",
      "Dataset size notes": "\u201cThe training experiments were carried out on a database of 30,000 photos. Therefor the database was split into ten sets. Nine of them were used for the training and one for the testing.\u201d",
      "Confidence": "Confident",
      "Link": "https://ieeexplore.ieee.org/document/549237",
      "Reference": "A neural network for grey level and color correction used in photofinishing",
      "Citations": "",
      "Authors": "M. Kocheisen; U.A. Muller; G. Troster",
      "Abstract": "The application of a multilayer perceptron for color and gray level correction in the field of photofinishing is presented. It is shown, that a neural network can improve the overall performance of a state of the art photo printer. The improved correction ability will reduce the number of unsalable pictures and thus lowers the production costs for the photo laboratory. The training experiments were carried out on a database of 30,000 photos using the MUSIC parallel supercomputer. The MUSIC system made it possible, for the first time, to process this large database in a reasonable time.",
      "Organization categorization": "",
      "Country (of organization)": "",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "400.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "LISSOM",
      "Organization": "University of Texas at Austin",
      "Publication date": "1995-11-27",
      "Domain": "Vision",
      "Task": "Digit recognition",
      "Parameters": "432800.0",
      "Parameters notes": "Total connections 32*32*20*20+20*20*48+20*20*10=432800\nInput: 32*32, Lissom: 20*20, Output: 10 (Figure 1a), up to 48 lateral connections per Lissom neuron (Figure 1b)\n",
      "Training compute (FLOP)": "195544800000.0",
      "Training compute notes": "Lissom connections: 32*32*20*20+20*20*48=428800\nLissom compute: 2*428800*3*38*2000=195532800000=1.96e11\nPerceptron connections: 20*20*10=4000\nPerceptron compute: 2*4000*3*500*1700=20400000000=2e10\nTotal compute: 195532800000+12000000=195544800000=1.96e11\n\"LISSOM was trained with 2000 patterns\"\n\"The initial self-organizing map was formed in 8 epochs over the training set, gradually reducing the neighborhood radius from 20 to 8. The lateral connections were then added to the system, and over another 30 epochs,\"\n\"Of these, 1700 were used to train the perceptron layer, \"\n\"After the SOM and LISSOM maps were organized, a complete set of activation patterns on the two maps were collected. These patterns then formed the training input for the perceptron layer. Two separate versions were each trained for 500 epochs,\"\n\n",
      "Training dataset": "",
      "Training dataset size (gradients)": "2000",
      "Dataset size notes": "\"LISSOM was trained with 2000 patterns\"",
      "Confidence": "Likely",
      "Link": "https://www.semanticscholar.org/paper/Laterally-Interconnected-Self-Organizing-Maps-in-Choe-Sirosh/785f5facc76538ceba6f6b9e2d7b641d322e9854",
      "Reference": "Laterally Interconnected Self-Organizing Maps in Hand-Written Digit Recognition",
      "Citations": "",
      "Authors": "Yoonsuck Choe, Joseph Sirosh, R. Miikkulainen",
      "Abstract": "An application of laterally interconnected self-organizing maps (LISSOM) to handwritten digit recognition is presented. The lateral connections learn the correlations of activity between units on the map. The resulting excitatory connections focus the activity into local patches and the inhibitory connections decorrelate redundant activity on the map. The map thus forms internal representations that are easy to recognize with e.g. a perceptron network. The recognition rate on a subset of NIST database 3 is 4.0% higher with LISSOM than with a regular Self-Organizing Map (SOM) as the front end, and 15.8% higher than recognition of raw input bitmaps directly. These results form a promising starting point for building pattern recognition systems with a LISSOM map as a front end.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "SOTA improvement,Historical significance",
      "Notability criteria notes": "\"The recognition rate on a subset of NIST database 3 is 4.0% higher with LISSOM than with a regular Self-Organizing Map (SOM) as the front end, and 15.8% higher than recognition of raw input bitmaps directly. \"",
      "Epochs": "38.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Support Vector Machines",
      "Organization": "AT&T,Bell Laboratories",
      "Publication date": "1995-09-01",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "100000000.0",
      "Parameters notes": "Section 6.2.2: \"...polynomials\nof degree 4 (that have more than 10^8 free parameters)...\"\nThey used 4-degree polynomials for MNIST",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "MNIST",
      "Training dataset size (gradients)": "60000",
      "Dataset size notes": "Section 6.2: \"The large database consists of 60,000 training and 10,000 test patterns\"",
      "Confidence": "",
      "Link": "https://link.springer.com/article/10.1007/BF00994018",
      "Reference": "Support-Vector Networks",
      "Citations": "48968.0",
      "Authors": "C Cortes, V Vapnik",
      "Abstract": "",
      "Organization categorization": "Industry,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Predictive Coding NN",
      "Organization": "Technical University of Munich",
      "Publication date": "1994-12-02",
      "Domain": "Language",
      "Task": "Language modeling",
      "Parameters": "206910.0",
      "Parameters notes": "5*80*430+430+430*80+80=206910\n\"P has nk input units and k output units. n is called the \"time-window size\"\n\"Note that the time-window was quite small (n = 5).\"\n\"alphabet consisted of k = 80 possible characters\"\n\"P had 430 hidden units\"",
      "Training compute (FLOP)": "18621900000000.0",
      "Training compute notes": "2*206910*3*15000000=18621900000000=1.86e13\n\"The training phase consisted of 25 sweeps through the training set\"",
      "Training dataset": "",
      "Training dataset size (gradients)": "600000",
      "Dataset size notes": "Training dataset: 15000*40=600000\n\"The training set for the predictor was given by a set of 40 articles from the newspaper Miinchner M erkur, each containing between 10000 and 20000 characters.\"",
      "Confidence": "Confident",
      "Link": "https://proceedings.neurips.cc/paper/1994/file/5705e1164a8394aace6018e27d20d237-Paper.pdf",
      "Reference": "Predictive Coding with Neural Nets: Application to Text Compression",
      "Citations": "",
      "Authors": "J. Schmidhuber, Stefan Heil",
      "Abstract": "To compress text files, a neural predictor network P is used to approximate the conditional probability distribution of possible \"next characters\", given n previous characters. P's outputs are fed into standard coding algorithms that generate short codes for characters with high predicted probability and long codes for highly unpredictable characters. Tested on short German newspaper articles, our method outperforms widely used Lempel-Ziv algorithms (used in UNIX functions such as \"compress\" and \"gzip\").",
      "Organization categorization": "Academia",
      "Country (of organization)": "Germany",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "25.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "NeuroChess",
      "Organization": "",
      "Publication date": "1994-12-02",
      "Domain": "Games",
      "Task": "Chess",
      "Parameters": "72251.0",
      "Parameters notes": "\"Prior to learning an evaluation function, the model M (175 input, 165 hidden, and 175 output units)\" = 58,090 parameters\n\"NeuroChess then learns an evaluation network V (175 input units, 0 to 80 hidden units, and one output units).\" = 14,161 parameters\nTotal: 58,090 + 14,161 = 72,251",
      "Training compute (FLOP)": "858730812676.0",
      "Training compute notes": "Lower bound: 0.3*2*24*60*60*1400000=72576000000=7.26e10\nUpper bound: 0.3*14*24*60*60*1400000*20=10160640000000=1.02e13\nGeometric mean: 858730812676=8.59e11 (speculative)\n\"Thus far, experiments lasted for 2 days to 2 weeks on I to 20 SUN Sparc Stations. \"\nSparcStation has 1.4 MFLOPS (https://ieeexplore.ieee.org/document/63671)\n",
      "Training dataset": "",
      "Training dataset size (gradients)": "9600000",
      "Dataset size notes": "\"is trained using a database of 120,000 expert games.\"",
      "Confidence": "Speculative",
      "Link": "https://www.semanticscholar.org/paper/Learning-to-Play-the-Game-of-Chess-Thrun/4bc7a6dcb9e0e6c7a26800532e2a00f5572eea47",
      "Reference": "Learning to Play the Game of Chess",
      "Citations": "",
      "Authors": "S. Thrun",
      "Abstract": "This paper presents NeuroChess, a program which learns to play chess from the final outcome of games. NeuroChess learns chess board evaluation functions, represented by artificial neural networks. It integrates inductive neural network learning, temporal differencing, and a variant of explanation-based learning. Performance results illustrate some of the strengths and weaknesses of this approach.",
      "Organization categorization": "",
      "Country (of organization)": "",
      "Notability criteria": "Historical significance,Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Mixture of linear models",
      "Organization": "",
      "Publication date": "1994-12-02",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "384000.0",
      "Parameters notes": "\u201cIn the example we describe, 7000 training images are sufficient to fit 384,000 parameters\u201c",
      "Training compute (FLOP)": "453600000000.0",
      "Training compute notes": "0.3*12*60*60*35000000=453600000000=4.54e11\nAssuming a utilization of 0.3 and interpreting \"overnight\" as 12 hours.\n\u201cthe training procedure is fast enough to do the fitting overnight on an R4400-based machine. \u201c\nR4400 has 35MFLOPS (\u201cCompare this to the 200MHz R4400 which is rated at about 35MFLOPS\u201d, http://www.sgidepot.co.uk/perf.html)",
      "Training dataset": "",
      "Training dataset size (gradients)": "1792000",
      "Dataset size notes": "\"7000 training images are sufficient\"",
      "Confidence": "Likely",
      "Link": "https://www.semanticscholar.org/paper/Recognizing-Handwritten-Digits-Using-Mixtures-of-Hinton-Revow/9dea20c1e5bbb1f543ff08113ffde5380c679f1f",
      "Reference": "Recognizing Handwritten Digits Using Mixtures of Linear Models",
      "Citations": "",
      "Authors": "Geoffrey E. Hinton, M. Revow, P. Dayan",
      "Abstract": "We construct a mixture of locally linear generative models of a collection of pixel-based images of digits, and use them for recognition. Different models of a given digit are used to capture different styles of writing, and new images are classified by evaluating their log-likelihoods under each model. We use an EM-based algorithm in which the M-step is computationally straightforward principal components analysis (PCA). Incorporating tangent-plane information [12] about expected local deformations only requires adding tangent vectors into the sample covariance matrices for the PCA, and it demonstrably improves performance.",
      "Organization categorization": "",
      "Country (of organization)": "",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "12.0",
      "Training time notes": "\"the training procedure is fast enough to do the fitting overnight on an R4400-based machine.\"",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "JPMAX",
      "Organization": "",
      "Publication date": "1994-12-02",
      "Domain": "Vision",
      "Task": "Image representation",
      "Parameters": "4446.0",
      "Parameters notes": "Inputs are 12x12 pixels (Figure 3). They first train the architecture in Figure 2 a), then freeze it and train the additional layer in Figure 3 b).\n\nFigure 2 a): 2*(12*12*15) + 2*15 = 4,350\nFigure 2 b): 2*(12*12*15) + 2*15 + 2*(15*3) + 2*3 = 4,446",
      "Training compute (FLOP)": "80828280.0",
      "Training compute notes": "Training 2a):  \"The learning took about 3000 iterations of steepest descent\" Assuming each iteration refers to a single image.\n6 * 4446 * 3000 = 80,028,000\n\nTraining 2b): \"While keeping the first layer of weights frozen, this network was trained using exactly the same cost function as the first layer for about 30 iterations using a gradient-based learning method.\"\n\n6 * 4446 * 30 = 800,280\n\nTotal: 80,828,280",
      "Training dataset": "",
      "Training dataset size (gradients)": "1500",
      "Dataset size notes": "\u201cFigure 3: 10 of the 1500 training patterns\u201d",
      "Confidence": "Speculative",
      "Link": "https://proceedings.neurips.cc/paper_files/paper/1994/hash/4b0250793549726d5c1ea3906726ebfe-Abstract.html",
      "Reference": "JPMAX: Learning to Recognize Moving Objects as a Model-fitting Problem",
      "Citations": "",
      "Authors": "Suzanna Becker",
      "Abstract": "Unsupervised learning procedures have been successful at low-level feature extraction and preprocessing of raw sensor data. So far, however, they have had limited success in learning higher-order representations, e.g., of objects in visual images. A promising ap(cid:173) proach is to maximize some measure of agreement between the outputs of two groups of units which receive inputs physically sep(cid:173) arated in space, time or modality, as in (Becker and Hinton, 1992; Becker, 1993; de Sa, 1993). Using the same approach, a much sim(cid:173) pler learning procedure is proposed here which discovers features in a single-layer network consisting of several populations of units, and can be applied to multi-layer networks trained one layer at a time. When trained with this algorithm on image sequences of moving geometric objects a two-layer network can learn to perform accurate position-invariant object classification.",
      "Organization categorization": "",
      "Country (of organization)": "",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GroupLens",
      "Organization": "Massachusetts Institute of Technology (MIT)",
      "Publication date": "1994-10-22",
      "Domain": "Recommendation",
      "Task": "Recommender system",
      "Parameters": "",
      "Parameters notes": "For each pair of users, the system computes the correlation between their scores in the articles they have rated.\n\nThen to make the prediction of a score for a given article and user the system computes a weighted average taking into account the correlations with each other user, the average rating of each user and the average rating of the article.\n\nSo the system in total has n+m+n*n ~= n*n parameters, where n is the number of users and m is the number of articles.\n\nTo address scaling issues, the system is partioned into clusters of users. It's very unclear what is the number of users per cluster, though the Daily ratings traffic table provided suggests that is around 10k users ",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "For each pair of users, the system computes the correlation between their scores in the articles they have rated.\n\nThen to make the prediction of a score for a given article and user the system computes a weighted average taking into account the correlations with each other user, the average rating of each user and the average rating of the article.\n\nSo the system in total has n+m+n*n ~= n*n parameters, where n is the number of users and m is the number of articles.\n\nTo address scaling issues, the system is partioned into clusters of users. It's very unclear what is the number of users per cluster, though the Daily ratings traffic table provided suggests that is around 10k users ",
      "Confidence": "",
      "Link": "https://dl.acm.org/doi/10.1145/192844.192905",
      "Reference": "GroupLens: an Open Architecture for Collaborative Filtering of Netnews",
      "Citations": "7733.0",
      "Authors": "Paul Resnick, Neophytos Iacovou, Mitesh Suchak, Peter Bergstrom, John Riedl",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Ceramic-MLP",
      "Organization": "Sapienza Universit\u00e0 di Roma",
      "Publication date": "1994-01-07",
      "Domain": "Materials science",
      "Task": "Pattern classification",
      "Parameters": "1888.0",
      "Parameters notes": "Parameters: 100*16 + 16*16 + 16*2 = 1888\nArchitecture: \"The topology of the classifier was X-Y-Y-2, where X is the number of input components, Y is the number of neurons in each hidden layer and the number of neurons in the output layer is two, which is the number of classes. The two hidden layers were considered to have the same number of nodes for simplification purposes. \"\nInput size: \"Each pattern consists of a 10 x 10 pixel sub-image.\"\nHidden size: \"Experiments have been made on networks with 6, 9, 12 and 16 hidden nodes. \"",
      "Training compute (FLOP)": "4531200000.0",
      "Training compute notes": "Compute estimate: 2*1888*3*400000=4531200000=4.53e9\nTraining steps: \"In Fig. 6 we report the classification results obtained on the testing set in the 12 and 16 component compressed data after 400000 training iterations\"",
      "Training dataset": "",
      "Training dataset size (gradients)": "80",
      "Dataset size notes": "After the pre-processing phase, a training set of 80 patterns and a testing set of 64 patterns were available.",
      "Confidence": "Likely",
      "Link": "https://www.sciencedirect.com/science/article/abs/pii/S0921883108605506",
      "Reference": "Ceramic powder characterization by multilayer perceptron (MLP) data compression and classification",
      "Citations": "",
      "Authors": "G. Bonifazi, P. Burrascano",
      "Abstract": "A neural network approach for pattern classification has been explored in the present paper as part of the recent resurgence of interest in this area. Our research has focused on how a multilayer feedforward structure performs in the particular problem of particle characterization. The proposed procedure, after suitable data preprocessing, consists of two distinct phases: in the former, a feedforward neural network is used to obtain an image data compression. In the latter, a neural classifier is trained on the compressed data. All the tests have been conducted on a sample constituted by two different typologies of ceramic particles, each characterized by a different microstructure. The sample image of different particles acquired and directly digitalized by scanning electron microscopy has been processed in order to achieve the best conditions to obtain the boundary profile of each particle. The boundary is thus assumed to be representative of the morphological characteristics of the ceramic products. Using the neural approach, a classification accuracy as high as 100% on a training set of 80 sub-images was achieved. These networks correctly classified up to 96.9% of 64 testing patterns not contained in the training set.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Italy",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "5000.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ANN Eye Tracker",
      "Organization": "",
      "Publication date": "1993-11-29",
      "Domain": "Vision",
      "Task": "Miscellaneous image analysis",
      "Parameters": "5620.0",
      "Parameters notes": "15*15*20+20+50*10*2+100=5620\nHidden layer is split, 15*15 image input, 2*50 output neurons (see Figure 2)\nHidden size up to 20 neurons (\"This architecture was used with varying numbers of hidden units in the single, divided, hidden layer; experiments with 10, 16 and 20 hidden units were performed. \")",
      "Training compute (FLOP)": "17534400000.0",
      "Training compute notes": "2*5620*3*520000=17534400000\nTraining examples: 2000*260=520000\n\"As mentioned before, 2000 image/position pairs were gathered for training\"\n\"All of the networks described in this paper are trained with the same parameters for 260 epochs\"",
      "Training dataset": "",
      "Training dataset size (gradients)": "4000",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/Non-Intrusive-Gaze-Tracking-Using-Artificial-Neural-Baluja-Pomerleau/574c0cf98825bf09b0ab7bbfe9ba89cd6090745e",
      "Reference": "Non-Intrusive Gaze Tracking Using Artificial Neural Networks",
      "Citations": "",
      "Authors": "S. Baluja, D. Pomerleau",
      "Abstract": "We have developed an artificial neural network based gaze tracking system which can be customized to individual users. A three layer feed forward network, trained with standard error back propagation, is used to determine the position of a user''s gaze from the appearance of the user''s eye. Unlike other gaze trackers, which normally require the user to wear cumbersome headgear, or to use a chin rest to ensure head immobility, our system is entirely non-intrusive. Currently, the best intrusive gaze tracking systems are accurate to approximately 0.75 degrees. In our experiments, we have been able to achieve an accuracy of 1.5 degrees, while allowing head mobility. In its current implementation, our system works at 15 hz. In this paper we present an empirical analysis of the performance of a large number of artificial neural network architectures for this task. Suggestions for further explorations for neurally based gaze trackers are presented, and are related to other similar artificial neural network applications such as autonomous road following.",
      "Organization categorization": "",
      "Country (of organization)": "",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "260.0",
      "Training time (hours)": "0.6",
      "Training time notes": "\"Training the 8x2 hidden layer network using the 15x40 input retina, with 2000 images, takes approximately 30-40 minutes on a Sun SPARC 10 machine. \"",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Siamese-TDNN",
      "Organization": "Bell Laboratories",
      "Publication date": "1993-08-01",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "744.0",
      "Parameters notes": "\"The input is 8 by 200 units, the first convolutional layer is 6 by 192 units with each unit's receptive field covering 8 by 9 units of the input. The first averaging layer is 6 by 64 units, the second convolution layer is 4 by 57 with 6 by 8 receptive fields and the second averaging layer is 4 by 19\"\n\"Two separate sub-networks based on Time Delay Neural Networks (Lang and Hinton, 1988, Guyon et al. 1990) act on each input pattern to extract features,\"\n\"All weights could be learnt, but the two sub-networks were constrained to have identical weights.\"\nL1: H=1, W=200, C=8, K=9, D=6\nL2: H=1, W=64, C=6, K=8, D=4\nParameters:  7*9*8+5*8*6=744",
      "Training compute (FLOP)": "12869570138112.0",
      "Training compute notes": "8073216*3*7701*69=12869570138112=1.29e13\nForward pass flop: 2*(2*200*200*8*6+2*64*64*6*4)=8073216\n\"We used up to 7,701 signature pairs\"\nEpochs: 69 (Table 1)\n",
      "Training dataset": "",
      "Training dataset size (gradients)": "7701",
      "Dataset size notes": "\"We used up to 7,701 signature pairs\"",
      "Confidence": "Likely",
      "Link": "https://www.semanticscholar.org/paper/Signature-Verification-Using-A-%22Siamese%22-Time-Delay-Bromley-Bentz/997dc5d9a058753f034422afe7bd0cc0b8ad808b",
      "Reference": "Signature Verification using a \"Siamese\" Time Delay Neural Network",
      "Citations": "",
      "Authors": "J. Bromley, James W. Bentz, L. Bottou, Isabelle M Guyon, Yann LeCun, C. Moore, Eduard S\u00e4ckinger, Roopak Shah",
      "Abstract": "This paper describes an algorithm for verification of signatures written on a pen-input tablet. The algorithm is based on a novel, artificial neural network, called a \"Siamese\" neural network. This network consists of two identical sub-networks joined at their outputs. During training the two sub-networks extract features from two signatures, while the joining neuron measures the distance between the two feature vectors. Verification consists of comparing an extracted feature vector with a stored feature vector for the signer. Signatures closer to this stored representation than a chosen threshold are accepted, all other signatures are rejected as forgeries.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance,Highly cited",
      "Notability criteria notes": "",
      "Epochs": "69.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "IBM-5",
      "Organization": "IBM",
      "Publication date": "1993-06-15",
      "Domain": "Language",
      "Task": "Translation",
      "Parameters": "1658364.0",
      "Parameters notes": "The model is initiallized with 2.44E+09 translation probabilities, which are progressively culled until 1,658,364 remain. There are other parameters in the models (eg the fertility probabilities that relate each word in the input to the number of words it will align to) but the parameter count is dominated by the translation probabilities.",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "Proceedings of the Canadian parliament",
      "Training dataset size (gradients)": "28850104",
      "Dataset size notes": "\"They used the algorithm to extract a large number of translations from several years of the proceedings of the Canadian parliament. From these translations, we have chosen as our training data those for which both the English sentence and the French sentence are 30 or fewer words in length. This is a collection of 1,778,620 translations.\"",
      "Confidence": "",
      "Link": "https://dl.acm.org/doi/10.5555/972470.972474",
      "Reference": "The Mathematics of Statistical Machine Translation: Parameter Estimation",
      "Citations": "5752.0",
      "Authors": "Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, Robert L. Mercer",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Boosting",
      "Organization": "Bell Laboratories",
      "Publication date": "1992-11-30",
      "Domain": "Vision",
      "Task": "Digit recognition",
      "Parameters": "2578.0",
      "Parameters notes": "\u201cThe network has 4645 neurons, 2578 different weights, and 98442 connections.\u201c",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "29127",
      "Dataset size notes": "\u201cdivided into 9709 training examples and 2007 validation samples.\u201d",
      "Confidence": "Likely",
      "Link": "https://www.semanticscholar.org/paper/Improving-Performance-in-Neural-Networks-Using-a-Drucker-Schapire/77b5185dafb9e5b884a677a32713e54c253a4e0b",
      "Reference": "Improving Performance in Neural Networks Using a Boosting Algorithm",
      "Citations": "",
      "Authors": "H. Drucker, R. Schapire, Patrice Y. Simard",
      "Abstract": "A boosting algorithm converts a learning machine with error rate less than 50% to one with an arbitrarily low error rate. However, the algorithm discussed here depends on having a large supply of independent training samples. We show how to circumvent this problem and generate an ensemble of learning machines whose performance in optical character recognition problems is dramatically improved over that of a single network. We report the effect of boosting on four databases (all handwritten) consisting of 12,000 digits from segmented ZIP codes from the United State Postal Service (USPS) and the following from the National Institute of Standards and Testing (NIST): 220,000 digits, 45,000 upper case alphas, and 45,000 lower case alphas. We use two performance measures: the raw error rate (no rejects) and the reject rate required to achieve a 1% error rate on the patterns not rejected. Boosting improved performance in some cases by a factor of three.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Cancer drug mechanism prediction",
      "Organization": "National Cancer Institute",
      "Publication date": "1992-10-16",
      "Domain": "Medicine",
      "Task": "Drug discovery",
      "Parameters": "594.0",
      "Parameters notes": "\u201cThe network shown has 60 input PEs, one for each cell line, and 6 output PEs\u201c\n\u201cNeural networks with three to nine hidden layer PEs used\u201d\n9*60 + 6*9 = 594",
      "Training compute (FLOP)": "53460000.0",
      "Training compute notes": "2*594*3*15000=53460000=5.35e7\n\u201cThe extent of training was 15,000 presentations\u201c",
      "Training dataset": "",
      "Training dataset size (gradients)": "141",
      "Dataset size notes": "",
      "Confidence": "Likely",
      "Link": "https://pubmed.ncbi.nlm.nih.gov/1411538/",
      "Reference": "Neural computing in cancer drug development: predicting mechanism of action",
      "Citations": "",
      "Authors": "John N. Weinstein, Kurt W. Kohn, Michael R. Grever, Vellarkad N.\nViswanadhan, Lawrence V. Rubinstein, Anne P. Monks, Dominic A. Scudiero, Lester Welch, Antonis D. Koutsoukos, August J. Chiausa, Kenneth D. Paull",
      "Abstract": "Described here are neural networks capable of predicting a drug's mechanism of action from its pattern of activity against a panel of 60 malignant cell lines in the National Cancer Institute's drug screening program. Given six possible classes of mechanism, the network misses the correct category for only 12 out of 141 agents (8.5 percent), whereas linear discriminant analysis, a standard statistical technique, misses 20 out of 141 (14.2 percent). The success of the neural net indicates several things. (i) The cell line response patterns are rich in information about mechanism. (ii) Appropriately designed neural networks can make effective use of that information. (iii) Trained networks can be used to classify prospectively the more than 10,000 agents per year tested by the screening program. Related networks, in combination with classical statistical tools, will help in a variety of ways to move new anticancer agents through the pipeline from in vitro studies to clinical application.",
      "Organization categorization": "Government",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Golem",
      "Organization": "Alan Turing Institute",
      "Publication date": "1992-10-01",
      "Domain": "Biology",
      "Task": "Protein folding prediction",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "1612",
      "Dataset size notes": "Table 1",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/Protein-secondary-structure-prediction-using-Muggleton-King/9f744e48091a24b569435d070920e60db45f4fdc",
      "Reference": "Protein secondary structure prediction using logic-based machine learning.",
      "Citations": "",
      "Authors": "S. Muggleton, Ross D. King, M. J. Sternberg",
      "Abstract": "Many attempts have been made to solve the problem of predicting protein secondary structure from the primary sequence but the best performance results are still disappointing. In this paper, the use of a machine learning algorithm which allows relational descriptions is shown to lead to improved performance. The Inductive Logic Programming computer program, Golem, was applied to learning secondary structure prediction rules for alpha/alpha domain type proteins. The input to the program consisted of 12 non-homologous proteins (1612 residues) of known structure, together with a background knowledge describing the chemical and physical properties of the residues. Golem learned a small set of rules that predict which residues are part of the alpha-helices--based on their positional relationships and chemical and physical properties. The rules were tested on four independent non-homologous proteins (416 residues) giving an accuracy of 81% (+/- 2%). This is an improvement, on identical data, over the previously reported result of 73% by King and Sternberg (1990, J. Mol. Biol., 216, 441-457) using the machine learning program PROMIS, and of 72% using the standard Garnier-Osguthorpe-Robson method. The best previously reported result in the literature for the alpha/alpha domain type is 76%, achieved using a neural net approach. Machine learning also has the advantage over neural network and statistical methods in producing more understandable results.",
      "Organization categorization": "Government",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Historical significance,SOTA improvement",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "TD-Gammon",
      "Organization": "IBM",
      "Publication date": "1992-05-01",
      "Domain": "Games",
      "Task": "Backgammon",
      "Parameters": "25000.0",
      "Parameters notes": "\"The best performance was obtained with a network containing 80 hidden units and over 25,000 weights.\"",
      "Training compute (FLOP)": "18232157622832.703",
      "Training compute notes": "Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.\n\nOpenAI estimate: 1.8e13\n\nHardware estimate (likely overestimates due to simulation effort)\n\"on an IBM RS/6000 workstation, the smallest network was trained in several hours, while the largest net required two weeks of simulation time.\"\nIBM RS/6000 achieves 1.5 GFLOPS on Linpack (https://link.springer.com/rwe/10.1007/978-0-387-09766-4_232) \n14*24*60*60*0.5*1500000000=9.072e+14\n\nOperation counting estimate: \nForward FLOP: 50000\nEach legal move had to be evaluated separately, assuming an average of 10 move options (+2 for backward passes of the chosen move):\n50000*12*300000*21=3.78e+12\n\nKeeping the OpenAI estimate as the median estimate. ",
      "Training dataset": "",
      "Training dataset size (gradients)": "6300000",
      "Dataset size notes": "\"This network was trained\nfor over 300,000 training games\"\n\nEach backgammon game has an avg of around 21 movements\nhttps://www.bkgm.com/rgb/rgb.cgi?view+712",
      "Confidence": "Speculative",
      "Link": "https://papers.nips.cc/paper/1991/file/68ce199ec2c5517597ce0a4d89620f55-Paper.pdf",
      "Reference": "Practical Issues in Temporal Difference Learning",
      "Citations": "1344.0",
      "Authors": "G Tesauro",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Weight Decay",
      "Organization": "",
      "Publication date": "1991-12-02",
      "Domain": "Speech",
      "Task": "Speech synthesis",
      "Parameters": "8386.0",
      "Parameters notes": " 7*26*40+40+40*26+26=8386\n\"The network had 7 x 26 input units, 40 hidden units and 26 output units\"",
      "Training compute (FLOP)": "75474000000.0",
      "Training compute notes": "2*8386*3*1500000=75474000000=7.55e10\n\"It was trained on 400 to 5000 random words from the data base of around 20.000 words,\"\n\"The top full line corresponds to the generalization error after 300 epochs\"\n",
      "Training dataset": "",
      "Training dataset size (gradients)": "25000",
      "Dataset size notes": "\"It was trained on 400 to 5000 random words from the data base of around 20.000 words,\"",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/A-Simple-Weight-Decay-Can-Improve-Generalization-Krogh-Hertz/48e1de7d085808004d5f0493d486669a3d2930b5",
      "Reference": "A Simple Weight Decay Can Improve Generalization",
      "Citations": "",
      "Authors": "A. Krogh, J. Hertz",
      "Abstract": "It has been observed in numerical simulations that a weight decay can improve generalization in a feed-forward neural network. This paper explains why. It is proven that a weight decay has two effects in a linear network. First, it suppresses any irrelevant components of the weight vector by choosing the smallest vector that solves the learning problem. Second, if the size is chosen right, a weight decay can suppress some of the effects of static noise on the targets, which improves generalization quite a lot. It is then shown how to extend these results to networks with hidden layers and non-linear units. Finally the theory is confirmed by some numerical simulations using the data from NetTalk.",
      "Organization categorization": "",
      "Country (of organization)": "",
      "Notability criteria": "Highly cited,Historical significance",
      "Notability criteria notes": "",
      "Epochs": "300.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "RAAM",
      "Organization": "",
      "Publication date": "1990-11-01",
      "Domain": "Other",
      "Task": "Representation learning",
      "Parameters": "1536.0",
      "Parameters notes": "Largest model:\n\"A 48-16-48 RAAM learned to construct representations \"\nParameters 48*16*2 = 1536",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "29",
      "Dataset size notes": "29 sentence fragments (Figure 10)",
      "Confidence": "Confident",
      "Link": "https://www.sciencedirect.com/science/article/abs/pii/000437029090005K?via%3Dihub",
      "Reference": "Recursive Distributed Representations",
      "Citations": "",
      "Authors": "Jordan B. Pollack",
      "Abstract": "A longstanding difficulty for connectionist modeling has been how to represent variable-sized recursive data structures, such as trees and lists, in fixed-width patterns. This paper presents a connectionist architecture which automatically develops compact distributed representations for such compositional structures, as well as efficient accessing mechanisms for them. Patterns which stand for the internal nodes of fixed-valence trees are devised through the recursive use of backpropagation on three-layer auto-associative encoder networks. The resulting representations are novel, in that they combine apparently immiscible aspects of features, pointers, and symbol structures. They form a bridge between the data structures necessary for high-level cognitive tasks and the associative, pattern recognition machinery provided by neural networks.",
      "Organization categorization": "",
      "Country (of organization)": "",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "SexNet compression",
      "Organization": "",
      "Publication date": "1990-10-01",
      "Domain": "Vision",
      "Task": "Image representation",
      "Parameters": "72940.0",
      "Parameters notes": "900*40*2+40+900=72940\n\u201cImages sampled at 30x30 were compressed using a 900x40x900 fully connected back-propagation network\u201d",
      "Training compute (FLOP)": "78775200000.0",
      "Training compute notes": "2*72940*3*90*2000=78775200000\n\u201cThe compression network trained for 2000 runs on each of 90 faces\u201d",
      "Training dataset": "",
      "Training dataset size (gradients)": "81000",
      "Dataset size notes": "\u201cThe compression network trained for 2000 runs on each of 90 faces\u201d",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/SEXNET%3A-A-Neural-Network-Identifies-Sex-From-Human-Golomb-Lawrence/cbf90aa78fea0c8a1028705d92bc4bc7808ddeeb",
      "Reference": "SEXNET: A Neural Network Identifies Sex From Human Faces",
      "Citations": "",
      "Authors": "B. Golomb, D. T. Lawrence, T. Sejnowski",
      "Abstract": "Sex identification in animals has biological importance. Humans are good at making this determination visually, but machines have not matched this ability. A neural network was trained to discriminate sex in human faces, and performed as well as humans on a set of 90 exemplars. Images sampled at 30\u00d730 were compressed using a 900\u00d740\u00d7900 fully-connected back-propagation network; activities of hidden units served as input to a back-propagation \"SexNet\" trained to produce values of 1 for male and 0 for female faces. The network's average error rate of 8.1% compared favorably to humans, who averaged 11.6%. Some SexNet errors mimicked those of humans.",
      "Organization categorization": "",
      "Country (of organization)": "",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "2000.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "SexNet classification",
      "Organization": "",
      "Publication date": "1990-10-01",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "1640.0",
      "Parameters notes": "Largest classification model: 40*40 + 40=1640 (Figure 2)",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "80",
      "Dataset size notes": "\u201cEach training on a different 80 faces, leaving a distinct set of 10 untrained faces for testing\u201d",
      "Confidence": "Likely",
      "Link": "https://www.semanticscholar.org/paper/SEXNET%3A-A-Neural-Network-Identifies-Sex-From-Human-Golomb-Lawrence/cbf90aa78fea0c8a1028705d92bc4bc7808ddeeb",
      "Reference": "SEXNET: A Neural Network Identifies Sex From Human Faces",
      "Citations": "",
      "Authors": "B. Golomb, D. T. Lawrence, T. Sejnowski",
      "Abstract": "Sex identification in animals has biological importance. Humans are good at making this determination visually, but machines have not matched this ability. A neural network was trained to discriminate sex in human faces, and performed as well as humans on a set of 90 exemplars. Images sampled at 30\u00d730 were compressed using a 900\u00d740\u00d7900 fully-connected back-propagation network; activities of hidden units served as input to a back-propagation \"SexNet\" trained to produce values of 1 for male and 0 for female faces. The network's average error rate of 8.1% compared favorably to humans, who averaged 11.6%. Some SexNet errors mimicked those of humans.",
      "Organization categorization": "",
      "Country (of organization)": "",
      "Notability criteria": "Historical significance,Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ISR network",
      "Organization": "Stanford University",
      "Publication date": "1990-10-01",
      "Domain": "Vision",
      "Task": "Character recognition (OCR)",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "600000",
      "Dataset size notes": "\u201cWe used a training and test set of about 9,000 and 1,800 characters respectively. \u201c",
      "Confidence": "Confident",
      "Link": "https://papers.nips.cc/paper_files/paper/1990/hash/e46de7e1bcaaced9a54f1e9d0d2f800d-Abstract.html",
      "Reference": "Integrated Segmentation and Recognition of Hand-Printed Numerals",
      "Citations": "",
      "Authors": "James Keeler, David Rumelhart, Wee Leow",
      "Abstract": "Neural network algorithms have proven useful for recognition of individ(cid:173) ual, segmented characters. However, their recognition accuracy has been limited by the accuracy of the underlying segmentation algorithm. Con(cid:173) ventional, rule-based segmentation algorithms encounter difficulty if the characters are touching, broken, or noisy. The problem in these situations is that often one cannot properly segment a character until it is recog(cid:173) nized yet one cannot properly recognize a character until it is segmented. We present here a neural network algorithm that simultaneously segments and recognizes in an integrated system. This algorithm has several novel features: it uses a supervised learning algorithm (backpropagation), but is able to take position-independent information as targets and self-organize the activities of the units in a competitive fashion to infer the positional information. We demonstrate this ability with overlapping hand-printed numerals.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Bankruptcy-NN",
      "Organization": "",
      "Publication date": "1990-06-17",
      "Domain": "Other",
      "Task": "Binary classification",
      "Parameters": "36.0",
      "Parameters notes": "\"The input layer consisted of the five nodes, one for each of the ratios. The hidden layer consisted of 5 node.;. The output layer consisted of only one neuron\"\n30 weights + 6 biases = 36",
      "Training compute (FLOP)": "3059337600.0",
      "Training compute notes": " 2*36*3*74*191400=3,059,337,600=3.06e9\n\"Convergence was reached after 191,400 iterations\"",
      "Training dataset": "",
      "Training dataset size (gradients)": "74",
      "Dataset size notes": "\"The first (training) subsample of 74 firms data\"\n5 inputs per firm\n74*5 = 370",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/A-neural-network-model-for-bankruptcy-prediction-Odom-Sharda/ead9fa02902850a7418fb5ba720f3d9d8ab2f88b",
      "Reference": "A neural network model for bankruptcy prediction",
      "Citations": "",
      "Authors": "M. Odom, R. Sharda",
      "Abstract": "A neural network model is developed for prediction of bankruptcy, and it is tested using financial data from various companies. The same set of data is analyzed using a more traditional method of bankruptcy prediction, multivariate discriminant analysis. A comparison of the predictive abilities of both the neural network and the discriminant analysis method is presented. The results show that neural networks might be applicable to this problem",
      "Organization categorization": "",
      "Country (of organization)": "",
      "Notability criteria": "Historical significance,Highly cited",
      "Notability criteria notes": "",
      "Epochs": "191400.0",
      "Training time (hours)": "24.0",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "NETtalk reimplementation",
      "Organization": "Oregon State University",
      "Publication date": "1990-06-01",
      "Domain": "Speech",
      "Task": "Text-to-speech (TTS)",
      "Parameters": "27480.0",
      "Parameters notes": "203*120+120*26=27480\n\u201c203 input units, 120 hidden units, and 26 output units\u201d",
      "Training compute (FLOP)": "35811936000.0",
      "Training compute notes": "Updated FLOP estimate: 2*27480*3*1000*7.24*30=35811936000=3.6e10",
      "Training dataset": "",
      "Training dataset size (gradients)": "7242",
      "Dataset size notes": "\u201cThis training set was further subdivided to extract smaller training sets of 1000, 800, 400, 200, 100, and 50 words\u201c",
      "Confidence": "Confident",
      "Link": "https://www.sciencedirect.com/science/article/abs/pii/B9781558601413500079",
      "Reference": "A Comparative Study of ID3 and Backpropagation for English Text-to-speech Mapping",
      "Citations": "",
      "Authors": "Thomas G. Dietterich, Hermann Hild, Ghulum Bakiri",
      "Abstract": "The performance of the error backpropagation (BP) and ID3 learning algorithms was compared on the task of mapping English text to phonemes and stresses. Under the distributed output code developed by Sejnowski and Rosenberg, it is shown that BP consistently out-performs ID3 on this task by several percentage points. Three hypotheses explaining this difference were explored: (a) ID3 is overfitting the training data, (b) BP is able to share hidden units across several output units and hence can learn the output units better, and (c) BP captures statistical information that ID3 does not. We conclude that only hypothesis (c) is correct. By augmenting ID3 with a simple statistical learning procedure, the performance of BP can be approached but not matched. More complex statistical procedures can improve the performance of both BP and ID3 substantially. A study of the residual errors suggests that there is still substantial room for improvement in learning methods for text-to-speech mapping.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance,Training cost",
      "Notability criteria notes": "",
      "Epochs": "30.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Zip CNN",
      "Organization": "AT&T,Bell Laboratories",
      "Publication date": "1989-12-01",
      "Domain": "Vision",
      "Task": "Character recognition (OCR)",
      "Parameters": "9760.0",
      "Parameters notes": "\"In summary, the network has 1256 units, 64,660 connections, and 9760 independent parameters\"",
      "Training compute (FLOP)": "1496338054440.0",
      "Training compute notes": "Its a deep CNN so we assume a backward-forward ratio of 2:1\n 2*64660*3*23*167693=1496338054440\n\"The network was trained for 23\npasses through the training set (167,693 pattern presentations).\"",
      "Training dataset": "Buffalo zips",
      "Training dataset size (gradients)": "7291",
      "Dataset size notes": "The digits were written\nby many different people, using a great variety of sizes, writing styles,\nand instruments, with widely varying amounts of care; 7291 examples\nare used for training the network and 2007 are used for testing the generalization performance",
      "Confidence": "Confident",
      "Link": "https://ieeexplore.ieee.org/document/6795724",
      "Reference": "Backpropagation applied to handwritten zip code recognition",
      "Citations": "11725.0",
      "Authors": "Y. LeCun B. Boser J. S. Denker D. Henderson R. E. Howard W. Hubbard L. D. Jackel",
      "Abstract": "",
      "Organization categorization": "Industry,Industry",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Speaker-independent vowel classification",
      "Organization": "University of Washington",
      "Publication date": "1989-11-27",
      "Domain": "Speech",
      "Task": "Speech recognition (ASR)",
      "Parameters": "3040.0",
      "Parameters notes": "\u201cThe MLP consisted of 64 inputs (the DFf coefficients. each nonnalized between zero and one), a single hidden layer of 40 units, and 12 output units;\u201d",
      "Training compute (FLOP)": "7485696000.0",
      "Training compute notes": "2*3040*3*410400=7485696000=7.49e9\n\u201cThe network was trained on 100 iterations through the 4104 training vectors.\u201d",
      "Training dataset": "",
      "Training dataset size (gradients)": "4104",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/Performance-Comparisons-Between-Backpropagation-and-Atlas-Cole/e42d2b89fcb4a1a3dfa63408f424f76975ed1e1b",
      "Reference": "Performance Comparisons Between Backpropagation Networks and Classification Trees on Three Real-World Applications",
      "Citations": "",
      "Authors": "L. Atlas, R. Cole, J. Connor, M. El-Sharkawi, R. Marks, Y. Muthusamy, E. Barnard",
      "Abstract": "Multi-layer perceptrons and trained classification trees are two very different techniques which have recently become popular. Given enough data and time, both methods are capable of performing arbitrary non-linear classification. We first consider the important differences between multi-layer perceptrons and classification trees and conclude that there is not enough theoretical basis for the clear-cut superiority of one technique over the other. For this reason, we performed a number of empirical tests on three real-world problems in power system load forecasting, power system security prediction, and speaker-independent vowel identification. In all cases, even for piecewise-linear trees, the multi-layer perceptron performed as well as or better than the trained classification trees.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "100.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Handwritten digit recognition network",
      "Organization": "AT&T",
      "Publication date": "1989-11-27",
      "Domain": "Vision",
      "Task": "Digit recognition",
      "Parameters": "2578.0",
      "Parameters notes": "\"In summary, the network has 4635 units, 98442 connections, and 2578 independent parameters.\u201c",
      "Training compute (FLOP)": "181440000000.0",
      "Training compute notes": "1.4e6 * 3 * 24 * 60* 60 * 0.5 = 181440000000 = 1.81e11\n\"A complete training session (30 passes through the training set plus test) takes about 3 days on a SUN SPARCstation 1\"\nSparcstation 1 has an estimated compute of 1.4 MFLOPS (source: https://ieeexplore.ieee.org/document/63671 )",
      "Training dataset": "",
      "Training dataset size (gradients)": "9840",
      "Dataset size notes": "\"After 30 training passes the error rate on training set (7291 handwritten plus 2549 printed digits)\"",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/Handwritten-Digit-Recognition-with-a-Network-LeCun-Boser/86ab4cae682fbd49c5a5bedb630e5a40fa7529f6",
      "Reference": "Handwritten Digit Recognition with a Back-Propagation Network",
      "Citations": "",
      "Authors": "Yann LeCun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard, L. Jackel",
      "Abstract": "We present an application of back-propagation networks to handwritten digit recognition. Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task. The input of the network consists of normalized images of isolated digits. The method has 1% error rate and about a 9% reject rate on zipcode digits provided by the U.S. Postal Service.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "30.0",
      "Training time (hours)": "72.0",
      "Training time notes": "\"A complete training session (30 passes through the training set plus test) takes about 3 days on a SUN SPARCstation 1\"",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Truck backer-upper",
      "Organization": "Stanford University",
      "Publication date": "1989-06-18",
      "Domain": "Driving",
      "Task": "Self-driving car",
      "Parameters": "805.0",
      "Parameters notes": "6*25+25+8*45+45*6=805 (see Figure 6)",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://ieeexplore.ieee.org/document/118723",
      "Reference": "The truck backer-upper: an example of self-learning in neural networks\n",
      "Citations": "",
      "Authors": "Derrick Nguyen, Bernard Widrow",
      "Abstract": "Neural networks can be used to solve highly nonlinear control problems. A two-layer neural network containing 26 adaptive neural elements has learned to back up a computer-simulated trailer truck to a loading dock, even when initially jackknifed. It is not yet known how to design a controller to perform this steering task. Nevertheless, the neural net was able to learn of its own accord to do this, regardless of initial conditions. Experience gained with the truck backer-upper should be applicable to a wide variety of nonlinear control problems.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Invariant image recognition",
      "Organization": "Complutense University of Madrid",
      "Publication date": "1989-06-18",
      "Domain": "Vision",
      "Task": "Representation learning",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "27000000000.0",
      "Training compute notes": "0.5*6*60*60*2.5e6 = 27000000000 = 2.7e10\nTrained for 6h on a SUN-4 (section 4)\nAssumed utilization of 0.5\nSUN-4 is estimated at 2.5e6 FLOP/s (Nordhaus, 2007)",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://ieeexplore.ieee.org/document/118669",
      "Reference": "Invariant image recognition using a multi-network neural model",
      "Citations": "",
      "Authors": "V. Cruz, G. Crist\u00f3bal, T. Michaux, S. Barquin",
      "Abstract": "A new model which permits visual patterns to be invariant to affine transforms (translations, rotations, and dimensions) is presented. A training multilayer fully connected network of ADALINE neurons is proposed as a preprocessing step for invariant image extraction. A second neural network has been trained by the popular backpropagation algorithm for recovering the real image without distortions. First, the sample invariants are obtained by the preprocessing network. In the second step, the general invariant that includes all the sample invariants is computed. Afterward, the reordered sample invariants are input to a multilayer neural network trained by the backpropagation algorithm. The original image, without distortions, is obtained in the output of this system. Several test images have been computed, and evaluation of the results shows that in the case of images with intrinsic perceptual similarity, the learning procedure leads to a global invariant extraction that requires less computational effort in comparison with an arbitrary training selection. After the training process, this system is able to extract the generalized invariant image from an arbitrary picture recovering the input image without distortions.<<ETX>>",
      "Organization categorization": "Academia",
      "Country (of organization)": "Spain",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "6.0",
      "Training time notes": "Section 4",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Q-learning",
      "Organization": "University of London",
      "Publication date": "1989-01-01",
      "Domain": "Robotics,Games",
      "Task": "Route finding,System control",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "200000",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "http://www.cs.rhul.ac.uk/~chrisw/thesis.html",
      "Reference": "Learning from delayed rewards",
      "Citations": "8025.0",
      "Authors": "Christopher Watkins",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "MLP baggage detector",
      "Organization": "Science Applications International Corporation / SAIC",
      "Publication date": "1989-01-01",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "",
      "Parameters notes": "3 layer network, input layer (<20), hidden layer, output layer (3)",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "20000",
      "Dataset size notes": "\"The database contains about 20,000 bags without simulants and a like number with; although, because of changes made in the systems as they were developed, not all measurements are on the same basis.\"",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/Detection-of-explosives-in-checked-airline-baggage-Shea-Lin/71da4057401f459bc079696a029aee45a0a89728",
      "Reference": "Detection of explosives in checked airline baggage using an artificial neural system",
      "Citations": "",
      "Authors": "Patrick M Shea, Vincent Lin",
      "Abstract": "An artificial neural system (ANS) has been applied to the problem of discriminating between suitcases with and without explosives. The input to the ANS was data gathered during the field tests of a prototype explosive detection system. The performance of the ANS is contrasted with the standard statistical technique (discriminant analysis) used, and is shown to exceed the performance of the standard technique over a substantial range. The system that generated the data, the nature of the data, the basics of discriminant analysis, and the technique used in developing the network are described.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "One of the first real-world use cases of neural networks",
      "Epochs": "2000.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "MLN-ASR",
      "Organization": "McGill University",
      "Publication date": "1988-08-01",
      "Domain": "Speech",
      "Task": "Speech recognition (ASR)",
      "Parameters": "10000.0",
      "Parameters notes": "\u201cFor an MLN of about 10,000 links, the time was 115 CPU msecs for the recognition of a spoken letter and 317 msecs for the learning of a spoken letter on the SUN 4/280. A 20% reduction was obtained on the VAX 8650\u201d",
      "Training compute (FLOP)": "296425000.0",
      "Training compute notes": "\u201cFor an MLN of about 10,000 links, the time was 115 CPU msecs for the recognition of a spoken letter and 317 msecs for the learning of a spoken letter on the SUN 4/280. A 20% reduction was obtained on the VAX 8650\u201d, \u201cLearning and recognition were performed on a VAX 8650.\u201d\nDataset: 70*10*2=1400 (Train) 10*10*2=200 (Test)\n\u201cThe ten words of the El set were pronounced twice by 80 speakers (40 males and 40 females)\u201d\n\u201cThe data from 70 speakers were used as a training set while the data from the remaining 10 speakers (6 males and 4 females) were used for the test\u201d\nVAX 8650 FLOPS  = 1.67E+06 (Nordhaus)\nTraining time: 317ms * 1400  * 0.8 = 355040ms = 355s\nEstimate: 0.5 * 1.67e6 * 355 = 296425000 = 2.96e8",
      "Training dataset": "",
      "Training dataset size (gradients)": "12600",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://aaai.org/papers/00734-aaai88-130-data-driven-execution-of-multi-layered-networks-for-automatic-speech-recognition/",
      "Reference": "Data-Driven Execution of Multi-Layered Networks for Automatic Speech Recognition",
      "Citations": "",
      "Authors": "Renato De Mori, Yoshua Bengio, R\u00e9gis Cardin",
      "Abstract": "A set of Multi-Layered Networks (MLN) for Automatic Speech Recognition (ASR) is proposed. Such a set allows the integration of information extracted with variable resolution in the time and frequency domains and to keep the number of links between nodes of the networks small in order to allow significant generalization during learning with a reasonable training set size. Subsets of networks can be executed depending on preconditions based on descriptions of the time evolution of signal energies allowing spectral properties that are significant in different acoustic situations to be learned.\nPreliminary experiments on speaker-independent recognition of the letters of the E-set are reported. Voices from 70 speakers were used for learning. Voices of 10 new speakers were used for test. An overall error rate of 9.5% was obtained in the test showing that results better than those previously reported can be achieved.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Canada",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "0.1",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Latent semantic analysis",
      "Organization": "University of Chicago,Bell Laboratories,University of Western Ontario",
      "Publication date": "1988-04-05",
      "Domain": "Language",
      "Task": "Semantic embedding,Document representation,Language Structure Modeling,Search",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "MED (MEDLINE / Cranfield MED)",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://asistdl.onlinelibrary.wiley.com/doi/10.1002/%28SICI%291097-4571%28199009%2941%3A6%3C391%3A%3AAID-ASI1%3E3.0.CO%3B2-9\n\nhttps://www.cs.csustan.edu/~mmartin/LDS/Deerwester-et-al.pdf",
      "Reference": "Indexing by latent semantic analysis",
      "Citations": "",
      "Authors": "Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, Richard Harshman",
      "Abstract": "A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (\u201csemantic structure\u201d) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. Initial tests find this completely automatic method for retrieval to be promising. \u00a9 1990 John Wiley & Sons, Inc.",
      "Organization categorization": "Academia,Industry,Academia",
      "Country (of organization)": "United States of America,United States of America,Canada",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "Might be the first model that used word embeddings",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Translation-invariant MLP",
      "Organization": "Carnegie Mellon University (CMU)",
      "Publication date": "1987-06-15",
      "Domain": "",
      "Task": "Object recognition",
      "Parameters": "816.0",
      "Parameters notes": "Network: 12-60-6-16\nWeights: 6*60+60*6+6*16=816\nLayer 2 was only sparsely connected to input layer",
      "Training compute (FLOP)": "18032947200.0",
      "Training compute notes": "FLOPs: 2*816*3*160*23020=18032947200=1.8e10",
      "Training dataset": "",
      "Training dataset size (gradients)": "160",
      "Dataset size notes": "",
      "Confidence": "Confident",
      "Link": "https://www.cs.toronto.edu/~hinton/absps/parle.pdf",
      "Reference": "Learning Translation Invariant Recognition in a Massively Parallel Network",
      "Citations": "",
      "Authors": "Geoffrey E. Hinton",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "23020.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "NetTalk (transcription)",
      "Organization": "Princeton University",
      "Publication date": "1987-06-06",
      "Domain": "Speech",
      "Task": "Speech synthesis",
      "Parameters": "18629.0",
      "Parameters notes": "\"The connections in the network are specified by a total of 18629\nweight parameters (including a variable threshold for each unit)\"",
      "Training compute (FLOP)": "28328002560.0",
      "Training compute notes": "18629 params * 2 FLOP/param * (3 for forward + backward pass) * 55 epochs * 1024 words/epoch * 4.5 letters/word",
      "Training dataset": "",
      "Training dataset size (gradients)": "5120",
      "Dataset size notes": "We used the first two pages of transcriptions, which contained 1024 words from a child in firstgrade",
      "Confidence": "Confident",
      "Link": "http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=03A3D3EDF0BAF35405ABCF083411B55E?doi=10.1.1.154.7012&rep=rep1&type=pdf",
      "Reference": "Parallel Networks that Learn to Pronounce English Text",
      "Citations": "2558.0",
      "Authors": "TJ Sejnowski, CR Rosenberg",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,Historical significance",
      "Notability criteria notes": "",
      "Epochs": "55.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "NetTalk (dictionary)",
      "Organization": "Princeton University",
      "Publication date": "1987-06-06",
      "Domain": "Speech",
      "Task": "Speech synthesis",
      "Parameters": "18629.0",
      "Parameters notes": "\"The connections in the network are specified by a total of 18629 weight parameters (including a variable threshold for each unit)\"",
      "Training compute (FLOP)": "27664065000.0",
      "Training compute notes": "18629 params * 2 FLOP/param * (3 for forward + backward pass) * 55 epochs * 1000 words/epoch * 4.5 letters/word (estimated number of letters)",
      "Training dataset": "",
      "Training dataset size (gradients)": "5000",
      "Dataset size notes": "\"A subset of the 1000 most commonly occurring words was selected from this dictionary based on frequency counts in the Brown corpus\"",
      "Confidence": "Confident",
      "Link": "http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=03A3D3EDF0BAF35405ABCF083411B55E?doi=10.1.1.154.7012&rep=rep1&type=pdf",
      "Reference": "Parallel Networks that Learn to Pronounce English Text",
      "Citations": "2558.0",
      "Authors": "TJ Sejnowski, CR Rosenberg",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,Historical significance",
      "Notability criteria notes": "",
      "Epochs": "55.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Optimized Multi-Scale Edge Detection",
      "Organization": "Massachusetts Institute of Technology (MIT)",
      "Publication date": "1986-11-01",
      "Domain": "Vision",
      "Task": "Object detection",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4767851",
      "Reference": "A Computational Approach To Edge Detection",
      "Citations": "37931.0",
      "Authors": "John Canny",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "MLP with back-propagation",
      "Organization": "University of California San Diego,Carnegie Mellon University (CMU)",
      "Publication date": "1986-10-01",
      "Domain": "Mathematics",
      "Task": "Triplet completion",
      "Parameters": "720.0",
      "Parameters notes": "Architecture in Figure 3: \n\n24+12 input -> 6 + 6 hidden -> 12 hidden -> 6 hidden -> 24 output\n\nParameters: 6*24+6*12+12*12+6*12+24*12=720",
      "Training compute (FLOP)": "673920000.0",
      "Training compute notes": "We assume that the number of mult-adds per pass is equal to the number of parameters -> 2*720=1440 FLOP per forward pass.\n\n\"We trained the network for 1500 sweeps\"\nThere are 104 relationship triplets (\"[...] of the 104 possible triplets\")\n\nFLOP: 1500*104*3*1440=673920000\n\n",
      "Training dataset": "",
      "Training dataset size (gradients)": "104",
      "Dataset size notes": "There are 104 relationship triplets (\"[...] of the 104 possible triplets\")",
      "Confidence": "Confident",
      "Link": "https://www.semanticscholar.org/paper/Learning-representations-by-back-propagating-errors-Rumelhart-Hinton/052b1d8ce63b07fec3de9dbb583772d860b7c769",
      "Reference": "Learning representations by back-propagating errors",
      "Citations": "29621.0",
      "Authors": "Rumelhart, David E.; Hinton, Geoffrey E.; Williams, Ronald J.",
      "Abstract": "",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Distributed representation NN",
      "Organization": "Carnegie Mellon University (CMU)",
      "Publication date": "1986-08-15",
      "Domain": "Other",
      "Task": "Representation learning",
      "Parameters": "432.0",
      "Parameters notes": "Parameters: 24*6 + 12*6 + 12*12 + 12*6 =432\n\"Figure 5: The activity levels in a five-layer network after it has learned. The bottom layer has 24 input units on the left for representing person 1 and 12 units on the right for representing the relationship. The white squares inside these two groups show the activity levels of the units. There is one active unit in the first group (representing Colin) and one in the second group (representing has-aunt). Each of the two groups of input units is totally connected to its own group of 6 units in the second layer. These two groups of 6 must learn to encode the input terms as distributed patterns of activity. The second layer is totally connected to the central layer of 12 units, and this layer is connected to the penultimate layer of 6 units.\"\n",
      "Training compute (FLOP)": "388800000.0",
      "Training compute notes": " 2*432*3*1500*100=388800000=3.9e8\n\"After 1500 sweeps through all 100 training examples the weights were very stable \"",
      "Training dataset": "",
      "Training dataset size (gradients)": "100",
      "Dataset size notes": "\"After 1500 sweeps through all 100 training examples the weights were very stable \"",
      "Confidence": "Confident",
      "Link": "https://www.cs.toronto.edu/~hinton/absps/families.pdf",
      "Reference": "Learning distributed representations of concepts.",
      "Citations": "",
      "Authors": "Geoffrey E. Hinton",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance,Highly cited",
      "Notability criteria notes": "",
      "Epochs": "1500.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Error Propagation",
      "Organization": "University of California San Diego,Carnegie Mellon University (CMU)",
      "Publication date": "1986-01-03",
      "Domain": "Other",
      "Task": "Text classification,Image classification",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "64",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://stanford.edu/~jlmcc/papers/PDP/Volume%201/Chap8_PDP86.pdf",
      "Reference": "Learning internal representations by error propagation",
      "Citations": "27322.0",
      "Authors": "D. E. Rumelhart, G. E. Hinton, and R. J. Williams",
      "Abstract": "",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Hierarchical Cognitron",
      "Organization": "NHK Broadcasting Science Research Laboratories",
      "Publication date": "1984-04-01",
      "Domain": "Other",
      "Task": "Pattern recognition",
      "Parameters": "9315.0",
      "Parameters notes": "Parameters 5*5*9*3*3 + 3*3*9*3*3*9 + 9*3*3*9 = 9315\n\"The numbers of excitatory cells in these four layers were: 7x7 in U0, 5x5 in  U1, 3x3 in U2, and 9 in U3\"\n\"Each feature-extracting cell in layer U1 receives excitatory modifiable afferent connections from 3x3 cells in layer U0\"\n\"On the other hand, each feature, each extracting cell in layers U2 and U3 receives excitatory modifiable connections from all 9 cells in each of the 3 x 3 hypercolumns in the layer preceding it. Therefore, it receives 3 x 3 x 9 afferent excitatory modifiable connections altogether\"\n",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "5",
      "Dataset size notes": "\"Five training patterns used for the self-organization are shown in Fig. 4\"",
      "Confidence": "Speculative",
      "Link": "https://link.springer.com/article/10.1007/BF00337157",
      "Reference": "A hierarchical neural network model for associative memory",
      "Citations": "",
      "Authors": "K. Fukushima",
      "Abstract": "A hierarchical neural network model with feedback interconnections, which has the function of associative memory and the ability to recognize patterns, is proposed. The model consists of a hierarchical multi-layered network to which efferent connections are added, so as to make positive feedback loops in pairs with afferent connections. The cell-layer at the initial stage of the network is the input layer which receives the stimulus input and at the same time works as an output layer for associative recall. The deepest layer is the output layer for pattern-recognition. Pattern-recognition is performed hierarchically by integrating information by converging afferent paths in the network. For the purpose of associative recall, the integrated information is again distributed to lower-order cells by diverging efferent paths. These two operations progress simultaneously in the network. If a fragment of a training pattern is presented to the network which has completed its self-organization, the entire pattern will gradually be recalled in the initial layer. If a stimulus consisting of a number of training patterns superposed is presented, one pattern gradually becomes predominant in the recalled output after competition between the patterns, and the others disappear. At about the same time when the recalled pattern reaches a steady state in he initial layer, in the deepest layer of the network, a response is elicited from the cell corresponding to the category of the finally-recalled pattern. Once a steady state has been reached, the response of the network is automatically extinguished by inhibitory signals from a steadiness-detecting cell. If the same stimulus is still presented after inhibition, a response for another pattern, formerly suppressed, will now appear, because the cells of the network have adaptation characteristics which makes the same response unlikely to recur. Since inhibition occurs repeatedly, the superposed input patterns are recalled one by one in turn.",
      "Organization categorization": "Industry",
      "Country (of organization)": "Japan",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ASE+ACE",
      "Organization": "University of Massachusetts Amherst",
      "Publication date": "1983-09-01",
      "Domain": "Robotics",
      "Task": "Pole balancing",
      "Parameters": "324.0",
      "Parameters notes": "The system consists of two parts: ACE and ASE, each with 162 weights (=324 parameters). Found in Figures 2 and 3.",
      "Training compute (FLOP)": "324000000.0",
      "Training compute notes": "324 * 2 * 500000 = 324000000 = 3.24e8. The calculation assumes \"compute per forward pass\" = \"number of parameters\" = \"compute per backward pass\". Their model only has a single layer and is trained with simple update rules instead of gradient descent. Training details are described in Section IX.\n\nNote that this is the compute for a single run; they appear to have repeated training 10 times for the ASE+ACE system.",
      "Training dataset": "",
      "Training dataset size (gradients)": "500000",
      "Dataset size notes": "\"Runs consisted of 100 trials unless the run's duration exceeded 500 000 time steps (approximately 2.8 h of simulated real time)\" \n\"Almost all runs of the ASE/ACE system [...], were terminated after 500 000\" (Section IX)",
      "Confidence": "Likely",
      "Link": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6313077",
      "Reference": "Neuronlike adaptive elements that can solve difficult learning control problems",
      "Citations": "4296.0",
      "Authors": "Andrew G. Barto, Richard S. Sutton, and Charles W. Anderson",
      "Abstract": "It is shown how a system consisting of two neuronlike adaptive elements can solve a difficult learning control problem. The task is to balance a pole that is hinged to a movable cart by applying forces to the cart's base. It is argued that the learning problems faced by adaptive elements that are components of adaptive networks are at least as difficult as this version of the pole-balancing problem. The learning system consists of a single associative search element (ASE) and a single adaptive critic element (ACE). In the course of learning to balance the pole, the ASE constructs associations between input and output by searching under the influence of reinforcement feedback, and the ACE constructs a more informative evaluation function than reinforcement feedback alone can provide. The differences between this approach and other attempts to solve problems using neurolike elements are discussed, as is the relation of this work to classical and instrumental conditioning in animal learning studies and its possible implications for research in the neurosciences.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "2.8",
      "Training time notes": "\"Runs consisted of 100 trials unless the run's duration exceeded 500 000 time steps (approximately 2.8 h of simulated real time)\" \n\"Almost all runs of the ASE/ACE system [...], were terminated after 500 000\" (Section IX)",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Hopfield network",
      "Organization": "California Institute of Technology",
      "Publication date": "1982-04-01",
      "Domain": "Other",
      "Task": "Sequence memorization",
      "Parameters": "9900.0",
      "Parameters notes": "My understanding is that the biggest Hopfield networks they studied had N=100 units. \n\nEach unit has 99 synapses Tij from each other unit, for a total of 100*99 parameters",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://www.pnas.org/doi/10.1073/pnas.79.8.2554",
      "Reference": "Neural networks and physical systems with emergent collective computational abilities",
      "Citations": "23315.0",
      "Authors": "JJ Hopfield",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Kohonen network",
      "Organization": "Helsinki University of Technology",
      "Publication date": "1981-07-25",
      "Domain": "Mathematics",
      "Task": "Dimensionality reduction",
      "Parameters": "4096.0",
      "Parameters notes": "The input vectors are 3D.\nI could not find the grid size, but from the images it looks 8x8.\nSo the network was 8x8x3 parameters.",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "4000",
      "Dataset size notes": "??? Seemingly no info",
      "Confidence": "",
      "Link": "https://link.springer.com/article/10.1007/BF00337288",
      "Reference": "Self-organized formation of topologically correct feature maps",
      "Citations": "11841.0",
      "Authors": "T Kohonen",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "Finland",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Neocognitron",
      "Organization": "NHK Broadcasting Science Research Laboratories",
      "Publication date": "1980-04-01",
      "Domain": "Vision",
      "Task": "Character recognition (OCR)",
      "Parameters": "1140576.0",
      "Parameters notes": "\"The synaptic connections from S-layers to C-layers\nare fixed and unmodifiable. [...]\nThe numbers of excitatory cells in these seven layers are: 16x16 in U0, 16x16x24 in Us1, 10x10x 24 in Uc1, 8x8x24 in Us2, 6x 6x 24 in Uc2, 2x2x24 in Us3, and 24 in Uc3 \n[...]\n the number of input synapses to each S-cell is 5 x 5 in layer Us1 and 5x5x24 in layers Us2 and Us3\n[...]\nThe number of excitatory input synapses to each C-cell is 5x5 in layers Uc1 and Uc2, and is 2x2 in\nlayer Uc3\n\"\n\nThe number of synapses into each S-layer is:\n\nS1: (16*16*24)*(5*5) \nS2: (8*8*24)*(5*5*24)\nS3: (2*2*24)*(5*5*24)\n\nWe assume one parameter a per synapse into each cell in a S-layer, and one parameter b per each cell in a S-layer.",
      "Training compute (FLOP)": "273738240.0",
      "Training compute notes": "\"It does not necessarily mean that all of these input synapses are\nalways fully reinforced. In usual situations, only some of these input synapses are reinforced, and the rest of them remains in small values [...] Each of the five stimulus patterns has been presented 20 times to the network. By that time, self organization of the network has almost been completed.\"\n\nWe multiply by 2 to account for multadds in the forward pass. \nThere is no real backward pass, weights are only updated sparsely. Estimating 20% additional weight update compute compared to the forward pass:\n2*1,140,576.0*1.2*100=273738240\n",
      "Training dataset": "",
      "Training dataset size (gradients)": "5",
      "Dataset size notes": "\"In order to self-organize the network, we have presented five stimulus patterns \"0\", \"1\", \"2\", \"3\", and \"4\", which are shown in Fig. 6\"",
      "Confidence": "Confident",
      "Link": "https://link.springer.com/article/10.1007/BF00344251",
      "Reference": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position",
      "Citations": "5782.0",
      "Authors": "K Fukushima, S Miyake",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "Japan",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Transfer Learning",
      "Organization": "University of Zagreb",
      "Publication date": "1976-07-01",
      "Domain": "Vision",
      "Task": "Digit recognition,Character recognition (OCR)",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "IBM29",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://www.informatica.si/index.php/informatica/article/view/2828",
      "Reference": "The influence of pattern similarity and transfer learning on the base perceptron training.",
      "Citations": "",
      "Authors": "Stevo Bozinovski, Ante Fulgos",
      "Abstract": "This paper describes a work on transfer learning in neural networks carried out in 1970s and early 1980s, which produced its first publication in 1976. In the contemporary research on transfer learning there is a belief that pioneering work on transfer learning took place in early 1990s, and this paper updates that knowledge, pointing out that the transfer learning research started more than a decade earlier. This paper reviews that 1970s research and addresses important issues relevant for the current transfer learning research. It gives a mathematical model and geometric interpretation of transfer learning, and  a measure of transfer learning indicating positive, negative, and no transfer learning. It presents experimental investigation in the mentioned types of transfer learning. And it gives an application of transfer learning in pattern recognition using datasets of images. ",
      "Organization categorization": "Academia",
      "Country (of organization)": "Croatia",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "The first paper on transfer learning",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "Unreleased",
      "Training code accessibility": "Unreleased",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Cognitron",
      "Organization": "Biological Cybernetics",
      "Publication date": "1975-09-01",
      "Domain": "Other",
      "Task": "Miscellaneous image analysis,Image classification",
      "Parameters": "21600.0",
      "Parameters notes": "4 layers, 288 neurons per layer, weights connect each neuron to only 25 neurons in the previous layer.\nOnly 3 layers with learnable weights, the first layer is just an input representation (see Fig 5)\n3*288*25 parameters",
      "Training compute (FLOP)": "5184000.0",
      "Training compute notes": "No real backward pass as weights are sparsely updated. Assuming 20% additional compute for the weight update.\nTotal compute estimate: 100*2*3*288*25*1.2 = 5184000\n",
      "Training dataset": "",
      "Training dataset size (gradients)": "5",
      "Dataset size notes": "5 examples presented for (at least) 20 cycles = 100 training steps",
      "Confidence": "Confident",
      "Link": "https://link.springer.com/article/10.1007%2FBF00342633",
      "Reference": "Cognitron: a self-organizing multilayered neural network",
      "Citations": "791.0",
      "Authors": "Kunihiko Fukushima",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "Japan",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "Precursor of the Neocognitron",
      "Epochs": "20.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Piecewise linear model",
      "Organization": "University of Kansas",
      "Publication date": "1973-11-01",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "357.0",
      "Parameters notes": "16 input features + bias = 17 input features\n7*6/2 = 21 Hyperplanes\n17*21 = 357 parameters\n\"For the multicategory problem involving NR categories, a total of NR(NR - 1)/2 hyperplanes are used to partition the pattern space.\"\n\"The input variables to the classifier consisted of the mean variance of the four textural features (f1,f2,f3, andfg obtained from the distance 1 gray-tone spatial-dependence matrices) and eight spectral features (comprised of the mean variance of the image gray-tone values) in each of the four spectral bands\"\n",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "314",
      "Dataset size notes": "\"Number of training samples = 314;\"",
      "Confidence": "Confident",
      "Link": "https://ieeexplore.ieee.org/document/4309314",
      "Reference": "Textural Features for Image Classification",
      "Citations": "",
      "Authors": "R. Haralick, K. Shanmugam, I. Dinstein",
      "Abstract": "Texture is one of the important characteristics used in identifying objects or regions of interest in an image, whether the image be a photomicrograph, an aerial photograph, or a satellite image. This paper describes some easily computable textural features based on gray-tone spatial dependancies, and illustrates their application in category-identification tasks of three different kinds of image data: photomicrographs of five kinds of sandstones, 1:20 000 panchromatic aerial photographs of eight land-use categories, and Earth Resources Technology Satellite (ERTS) multispecial imagery containing seven land-use categories. We use two kinds of decision rules: one for which the decision regions are convex polyhedra (a piecewise linear decision rule), and one for which the decision regions are rectangular parallelpipeds (a min-max decision rule). In each experiment the data set was divided into two parts, a training set and a test set. Test set identification accuracy is 89 percent for the photomicrographs, 82 percent for the aerial photographic imagery, and 83 percent for the satellite imagery. These results indicate that the easily computable textural features probably have a general applicability for a wide variety of image-classification applications.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance,Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Decision tree adaline",
      "Organization": "Tokyo Medical and Dental University",
      "Publication date": "1969-05-01",
      "Domain": "Medicine",
      "Task": "Medical diagnosis",
      "Parameters": "2450.0",
      "Parameters notes": "5 adaline were trained on binary decisions (p. 1)\nEach adaline had up to 490 input weights (\u201cmeshes\u201d)\nTotal parameters = 5*490=2450\n",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "40 positive and negative training examples (p. 2)",
      "Confidence": "Confident",
      "Link": "https://pubmed.ncbi.nlm.nih.gov/5820353/",
      "Reference": "A use of Adaline as an automatic method for interpretation of the electrocardiogram and the vectorcardiogram",
      "Citations": "",
      "Authors": "T Sano, S Tsuchiya, F Suzuki",
      "Abstract": "A learning machine\" adaline neuron\" was employed for automatic diagnosis of the vectorcardiogram and the electrocardiogram. The frontal circle and the horizontal circle were divided into 480 meshes. The features were expressed by a binary digit, whether the vector loops passed through each mesh or not. In a part of the trials, 5 sets of binary digits were applied in addition to QRS duration and direction of inscription of QRS loops and T loops. In this trial a total of 490 meshes were used. Vectorcardiograms were taken by FRANK'S method in 235 cases. Several methods of adaline usage were tried. The best result was obtained so far by successive dichotomies based on the principle of the logical decision tree. First the normal patterns and the abnormal patterns were divided. The correct ratio was 96.5% when the 490 meshes were employed, cases of an output value within\u00b110 units being regarded as undecided. Next the abnormal cases were divided into two groups depending on whether the QRS duration was more than 0.12 seconds or less. The group of cases with a QRS duration of less than 0.12 seconds was divided into right ventricular hypertrophy and others. The correct ratio was 98.6%. The remaining cases were divided into left ventricular hypertrophy and myocardial infarction, the correct ratio being 88.8%. The group of cases with a QRS duration of more than 0.12 seconds was easily divided into complete left and right bundle branch block in all cases. Here the number of meshes could be decreased to 59 meshes without changing the accuracy appreciably. This attempt showed that the application of the adaline for automatic diagnosis of the vectorcardiogram and the electrocardiogram is promising.",
      "Organization categorization": "Academia",
      "Country (of organization)": "Japan",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "GLEE",
      "Organization": "University of Edinburgh",
      "Publication date": "1968-07-01",
      "Domain": "Games",
      "Task": "Tic Tac Toe",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "6000",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.474.2430",
      "Reference": "Boxes: An Experiment in Adaptive Control",
      "Citations": "590.0",
      "Authors": "Michie and Chambers",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Boxes (pole)",
      "Organization": "University of Edinburgh",
      "Publication date": "1968-07-01",
      "Domain": "Games",
      "Task": "Pole balancing",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.474.2430",
      "Reference": "Boxes: An Experiment in Adaptive Control",
      "Citations": "590.0",
      "Authors": "Michie and Chambers",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "United Kingdom of Great Britain and Northern Ireland",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "LTE speaker verification system",
      "Organization": "IBM",
      "Publication date": "1966-11-01",
      "Domain": "Speech",
      "Task": "Speech recognition (ASR)",
      "Parameters": "2061.0",
      "Parameters notes": "2 connected systems, 1st level LTE and 2nd level LTE.\n1st Level: 1810 parameters (\"Thus, every 20 msec after the beginning of the utterance, the 15 filter amplitudes were each represented by a 12-bit code, resulting in a 180-bit time sample of the spectrum for that interval. Each time sample was fed to the first-level LTE's, which reduced it to a 10-bit code\")\n2nd Level: 251 parameters (\"This resulted in a 250-bit input pattern to the second level for the first half-second of each utterance. Each 250-bit pattern was then classified by the LTE into one of two classes\")",
      "Training compute (FLOP)": "105917060.0",
      "Training compute notes": "1st and 2nd level system are trained separately, multiple versions of both are trained, I chose the largest clearly described training runs.\n\n1st level LTE compute: 2*1810*28700=103894000=1.04e8\n1st level steps: 28700 (\"Only 287 samples were selected to train the 10 LTE's. The same algorithm was used as that used with the 100-class gain. Two LTE's\nconverged before 100 training passes.\")\n\n2nd level LTE compute: 2*251*4030=2023060=2e6\n2nd level steps: 4030 (31 epochs, 130 training examples, see Table 3)\n\nTotal compute: 103894000+2023060=105917060=1.06e8 (assuming no backward pass since they didn't use backpropagation)\n",
      "Training dataset": "",
      "Training dataset size (gradients)": "417",
      "Dataset size notes": "Split between both systems, 287 for 1st level, 130 for 2nd level.",
      "Confidence": "Likely",
      "Link": "https://pubs.aip.org/asa/jasa/article-abstract/40/5/966/754180/Experimental-Studies-in-Speaker-Verification-Using?redirectedFrom=fulltext",
      "Reference": "Experimental Studies in Speaker Verification, Using an Adaptive System",
      "Citations": "",
      "Authors": "K. P. Li; J. E. Dammann; W. D. Chapman",
      "Abstract": "This paper describes an investigation of the capability of a two\u2010level adaptive linear threshold element (LTE) system to perform speaker discriminations. The study also includes an investigation of discriminating a speaker from an unknown population. The problem has been confined to the verification of an utterance as that of an expected informant. The environment of the experiments is discussed, and the experimental system is described. At the first level LTE, four different kinds of training have been developed for effective transformation and data reduction. At the second\u2010level LTE, different training conditions and different decision processes are investigated and evaluated. Over 90% accuracy is obtained in separating a known speaker from impostors.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "131.0",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Heuristic Reinforcement Learning",
      "Organization": "Purdue University",
      "Publication date": "1965-10-01",
      "Domain": "Robotics",
      "Task": "System control",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "1080000.0",
      "Training compute notes": "Figure 10 shows their largest system is trained for 3h and was trained on an analog IBM 1620 that was simulated on a digital IBM 1710.\nNordhaus, 2007 lists the IBM 1620 at 200 multiplications per second and doesn\u2019t contain the 1710\nFlops estimate: 0.5 * 3 * 60 * 60 * 200 = 1080000 = 1.08e6\nAssumed utilization of 0.5\n",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Speculative",
      "Link": "https://ieeexplore.ieee.org/document/1098193",
      "Reference": "A heuristic approach to reinforcement learning control systems",
      "Citations": "",
      "Authors": "M. Waltz, K. Fu",
      "Abstract": "This paper describes a learning control system using a reinforcement technique. The controller is capable of controlling a plant that may be nonlinear and nonstationary. The only a priori information required by the controller is the order of the plant. The approach is to design a controller which partitions the control measurement space into sets called control situations and then learns the best control choice for each control situation. The control measurements are those indicating the state of the plant and environment. The learning is accomplished by reinforcement of the probability of choosing a particular control choice for a given control situation. The system was stimulated on an IBM 1710-GEDA hybrid computer facility. Experimental results obtained from the simulation are presented.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance,Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "3.0",
      "Training time notes": "Figure 10",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Print Recognition Logic",
      "Organization": "IBM",
      "Publication date": "1963-01-01",
      "Domain": "Vision",
      "Task": "Character recognition (OCR)",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "22500000.0",
      "Training compute notes": "0.5*2.5*60*60*5000 = 22500000 = 2.25e7\nAssumed utilization of 0.5\nTrained for 2-3h on an IBM 7090 (from Introduction)\nEstimated IBM 7090 at 5000 FLOP/s based on multiplications per second (Nordhaus, 2007)\nNote: the Nordhaus estimate is very different from Wikipedia's estimate of 100000 FLOP/s, which cites a PowerPoint as source.",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Speculative",
      "Link": "https://ieeexplore.ieee.org/document/5392331",
      "Reference": "Computer-Automated Design of Multifont Print Recognition Logic",
      "Citations": "",
      "Authors": "L. Kamentsky, Chao-Ning Liu",
      "Abstract": "A computer program has been written to design character recognition logic based on the processing of data samples. This program consists of two subroutines: (1) to search for logic circuits having certain constraints on hardware design, and (2) to evaluate these logics in terms of their discriminating ability over samples of the character set they are expected to recognize. An executive routine is used to apply these subroutines to select a complete logic with a given performance and complexity. This logic consists of 39 to 96 and gates connected to a shift register and a table look-up or resistance network comparison system.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "2.5",
      "Training time notes": "2-3h (from Introduction)",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "MADALINE I",
      "Organization": "Stanford University",
      "Publication date": "1962-07-01",
      "Domain": "Other",
      "Task": "Text classification,Image classification",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "256",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://www.proquest.com/openview/7898314db50a218b58052ac91e3bde1e/1?",
      "Reference": "An adaptive logic system with generalizing properties",
      "Citations": "75.0",
      "Authors": "William Combs Ridgway",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Linear Decision Functions",
      "Organization": "Bell Laboratories",
      "Publication date": "1962-06-01",
      "Domain": "Mathematics",
      "Task": "Binary classification",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "1559250.0",
      "Training compute notes": "0.5*45*35*1980 = 1559250 = 1.56e6\nTrained using IBM punched cards, computation took 45 * 35s for all 10 digits (Section Estimating the Linear Decision Function).\nMultiplications per second estimate based on publication year: 1.98e3 (regression on Nordhaus data).\nAssumed utilization of 0.5",
      "Training dataset": "",
      "Training dataset size (gradients)": "500",
      "Dataset size notes": "\"Fifty different people were asked, resulting in a sample size of 50 for each of the ten pattern classes. \"",
      "Confidence": "Speculative",
      "Link": "https://ieeexplore.ieee.org/document/4066882?denied=",
      "Reference": "Linear Decision Functions, with Application to Pattern Recognition",
      "Citations": "",
      "Authors": "W. Highleyman",
      "Abstract": "Many pattern recognition machines may be considered to consist of two principal parts, a receptor and a categorizer. The receptor makes certain measurements on the unknown pattern to be recognized; the categorizer determines from these measurements the particular allowable pattern class to which the unknown pattern belongs. This paper is concerned with the study of a particular class of categorizers, the linear decision function. The optimum linear decision function is the best linear approximation to the optimum decision function in the following sense: 1) \"Optimum\" is taken to mean minimum loss (which includes minimum error systems). 2) \"Linear\" is taken to mean that each pair of pattern classes is separated by one and only one hyperplane in the measurement space. This class of categorizers is of practical interest for two reasons: 1) It can be empirically designed without making any assumptions whatsoever about either the distribution of the receptor measurements or the a priori probabilities of occurrence of the pattern classes, providing an appropriate pattern source is available. 2) Its implementation is quite simple and inexpensive. Various properties of linear decision functions are discussed. One such property is that a linear decision function is guaranteed to perform at least as well as a minimum distance categorizer. Procedures are then developed for the estimation (or design) of the optimum linear decision function based upon an appropriate sampling from the pattern classes to be categorized.",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance,Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "0.4375",
      "Training time notes": "\"Forty-five hyperplanes are required in the complete linear decision function\"\n\"About 35 seconds, on the average, was required to determine a hyperplane, given an initial position.\"",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "ADALINE",
      "Organization": "Stanford University",
      "Publication date": "1960-06-30",
      "Domain": "Vision",
      "Task": "Pattern recognition",
      "Parameters": "17.0",
      "Parameters notes": "\"The machine's total experience is stored in the values of the weights a0,...,a16\"",
      "Training compute (FLOP)": "6600.0",
      "Training compute notes": "\"The method of searching that has proven most useful is the method of steepest descent\"\n\nApparently each pattern was only shown once to the system.\n\nSo the training compute is (forward pass compute) * (3 for backprop) * dataset size\n\nThis is a single layer (and single neuron) which does not require gradients w.r.t. inputs - 1:1 forward-backward ratio\n\n",
      "Training dataset": "",
      "Training dataset size (gradients)": "100",
      "Dataset size notes": "\"The best system, arrived at by slow precise adaptation on the full body of 100 noisy patterns, was able to classify these patterns as desired except for twelve errors.\"\n\nhttps://isl.stanford.edu/~widrow/papers/c1960adaptiveswitching.pdf",
      "Confidence": "Confident",
      "Link": "https://isl.stanford.edu/~widrow/papers/c1960adaptiveswitching.pdf",
      "Reference": "Adaptive switching circuits",
      "Citations": "6329.0",
      "Authors": "Widrow and Hoff",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Perceptron (1960)",
      "Organization": "Cornell Aeronautical Laboratory",
      "Publication date": "1960-03-30",
      "Domain": "Vision",
      "Task": "Image classification",
      "Parameters": "1000.0",
      "Parameters notes": "\" The first program was designed to handle\nup to 1000 A units, and a 72 by 72 sensory mosaic. It\nwas found that this large sensory system presented\nstimuli with a fineness of grain considerably better than\nthe limits of discrimination of a thousand-unit percep-\ntron, and at the same time, required an excessive\namount of time for stimulus transformations, since each\nilluminated point in the stimulus must be transformed\nindividually into its image point.\"",
      "Training compute (FLOP)": "720000000.0",
      "Training compute notes": "4000 * 12000 * 15\nfrom the text \"This program uses the IBM 704 computer to simulate per-ceptual learning, recognition, and spontaneous classification of visual stimuli in the perceptron,\"\nfrom https://en.wikipedia.org/wiki/IBM_704 The 704 can execute up to 12,000 floating-point additions per second.\n\" For the first system, the computing time averaged about 15 seconds per stimulus cycle, \"\nIn Fig 10 we see up to 4000 stimuli",
      "Training dataset": "",
      "Training dataset size (gradients)": "100",
      "Dataset size notes": "from the text \"The two main simulation programs total about 5000 words each.\"",
      "Confidence": "Speculative",
      "Link": "https://www.semanticscholar.org/paper/Perceptron-Simulation-Experiments-Rosenblatt/ae76ce1ba27ac29addce4aab93b927e9bc7f7c67",
      "Reference": "Perceptron Simulation Experiments",
      "Citations": "394.0",
      "Authors": "Frank Rosenblatt",
      "Abstract": "An experimental simulation program, which has been in progress at the Cornell Aeronautical Laboratory since 1957, is described. This program uses the IBM 704 computer to simulate perceptual learning, recognition, and spontaneous classification of visual stimuli in the perceptron, a theoretical brain model which has been described elsewhere. The paper includes a brief review of the organization of simple perceptrons, and theoretically predicted performance curves are compared with those obtained from the simulation programs, in several types of experiments, designed to study \"forced\" and \"spontaneous\" learning of pattern discriminations.",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Pattern recognition and reading by machine",
      "Organization": "Sandia Corporation",
      "Publication date": "1959-12-01",
      "Domain": "Vision",
      "Task": "Character recognition (OCR)",
      "Parameters": "2625.0",
      "Parameters notes": "A two bit state is recorded for each of the 75 cell pairs and each of the 25+10 characters recognized.",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "180",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://dl.acm.org/doi/10.1145/1460299.1460326",
      "Reference": "Pattern recognition and reading by machine",
      "Citations": "587.0",
      "Authors": "W. W. Bledsoe, I. Browning",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Samuel Neural Checkers",
      "Organization": "IBM",
      "Publication date": "1959-07-01",
      "Domain": "Games",
      "Task": "Checkers",
      "Parameters": "16.0",
      "Parameters notes": "\"with 16 terms for generalization learning\"\n\n\"Mention has been made several times of the procedure\nfor replacing terms in the scoring polynomial. The program, as it is currently running, contains 38 different\nterms (in addition to the piece-advantage term), 16 of\nthese being included in the scoring polynomial at anyone\ntime and the remaining 22 being kept in reserve.\"",
      "Training compute (FLOP)": "428400000.0",
      "Training compute notes": "\"it can learn to do this in a remarkably short period of time 8 or 10 hours of machine-playing time)\"\n\n\"The availability of a larger and faster machine (the IBM 704), coupled with many detailed changes in the programming procedure, leads to a fairly interesting game being played, even without any learning.\"\n\n\"The Type 704 is the first large-scale, commercially available computer to employ fully automatic floating point arithmetic commands. [...]. Floating point addition or subtraction operations require 84 microseconds.\"\n\nsource: https://www.ibm.com/ibm/history/exhibits/mainframe/mainframe_PP704.html\n\n\"An idea of the learning ability of this procedure can be gained by analyzing an initial test series of 28 games\"\n\n\"Each game averaged 68 moves (34 to a side), of which approximately 20 caused changes to be made in the scoring polynomial.\"\n\n10*3600*1000000/84=428571428",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "Based on number of board positions\n\nAt the present time the memory tape contains something over 53,000 board positions (averaging 3.8 word search) which have been selected from a much larger\nnumber of positions by means of the culling techniques\ndescribed. While this is still far from the number which\nwould tax the listing and searching procedures used in\nthe program, rough estimates, based on the frequency\nwith which the saved boards are utilized during normal\nplay (these figures being tabulated automatically), indicate that a library tape containing at least 20 times the\npresent number of board positions would be needed to\nimprove the midgame play significantly. At the present\nrate of acquisition of new positions this would require\nan inordinate amount of play and, consequently, of\nmachine time.",
      "Confidence": "Likely",
      "Link": "https://ieeexplore.ieee.org/abstract/document/5392560",
      "Reference": "Some studies in machine learning using the game of checkers",
      "Citations": "5063.0",
      "Authors": "Arthur L. Samuel",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Pandemonium (morse)",
      "Organization": "Massachusetts Institute of Technology (MIT)",
      "Publication date": "1959-02-01",
      "Domain": "Language",
      "Task": "Morse translation",
      "Parameters": "",
      "Parameters notes": "The paper mentions 11 function types. Unclear how many times they are called (number of \"demons\" in their Pandemonium implementation).",
      "Training compute (FLOP)": "600000000.0",
      "Training compute notes": "The paper mentions using an IBM 704, which can execute up to 12,000 floating-point additions per second (https://wikiless.org/wiki/IBM_704). My best guess as to how long it ran for ranges between 1h to 2 days, which when plugged into guesstimate (https://www.getguesstimate.com/models/19625), i.e., taking the log mean, gives a mean estimate of 600M",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "??? Might need to make a guesstimate here.",
      "Confidence": "Speculative",
      "Link": "https://aitopics.org/doc/classics:504E1BAC/",
      "Reference": "Pandemonium: A Paradigm for Learning",
      "Citations": "1453.0",
      "Authors": "OG Selfridge",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Highly cited,Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Perceptron Mark I",
      "Organization": "Cornell Aeronautical Laboratory,Cornell University",
      "Publication date": "1957-01-01",
      "Domain": "Other",
      "Task": "Binary classification",
      "Parameters": "1000.0",
      "Parameters notes": "\"Figure 4.8 Illustration of the Mark 1 perceptron hardware. The photograph on the left shows how the inputs were obtained using a simple camera system in which an input scene, in this case a printed character, was illuminated by powerful lights, and an image focussed onto a 20 \u00d7 20 array of cadmium sulphide photocells, giving a primitive 400 pixel image. The perceptron also had a patch board, shown in the middle photograph, which allowed different configurations of input features to be tried. Often these were wired up at random to demonstrate the ability of the perceptron to learn without the need for precise wiring, in contrast to a modern digital computer. The photograph on the right shows one of the racks of adaptive weights. Each weight was implemented using a rotary variable resistor, also called a potentiometer, driven by an electric motor thereby allowing the value of the weight to be adjusted automatically by the learning algorithm.\"\n\nsource: Bishop, Christopher M. (2006). Pattern Recognition and Machine Learning\n\nThe Perceptron had a 400-pixel visual input and 1000 neurons in the hidden layer. https://twitter.com/DiegoKuonen/status/1130352233223262208",
      "Training compute (FLOP)": "694894.9377361819",
      "Training compute notes": "Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.\n\nAdditional experiment described in https://babel.hathitrust.org/cgi/pt?id=coo.31924004657973&seq=70 \n- 400 input (20x20) - 512 hidden with 40 fixed connections each (not learned) - 1 output (learned)\nParameters: 512*1=512\nForward flop: 41*512=20992\nForward + \u201cbackward flop\u201d: 43*512=22016 (only last layer was adjusted)\n100*22016=2201600\n",
      "Training dataset": "",
      "Training dataset size (gradients)": "100",
      "Dataset size notes": "Appendix II describes an experiment with 6 stimulus patterns\n\nhttps://babel.hathitrust.org/cgi/pt?id=coo.31924004657973&seq=47 describes simulation experiments with \"X\" and \"E\" patterns using 100 total training stimuli",
      "Confidence": "Likely",
      "Link": "https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf",
      "Reference": "The Perceptron\u2014a perceiving and recognizing automaton",
      "Citations": "1610.0",
      "Authors": "F Rosenblatt",
      "Abstract": "",
      "Organization categorization": "Academia,Academia",
      "Country (of organization)": "United States of America,United States of America",
      "Notability criteria": "Historical significance,Highly cited",
      "Notability criteria notes": "First modern neural network ",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Sequence-based pattern recognition",
      "Organization": "Massachusetts Institute of Technology (MIT)",
      "Publication date": "1955-03-01",
      "Domain": "Vision",
      "Task": "Character recognition (OCR)",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://dl.acm.org/doi/10.1145/1455292.1455310",
      "Reference": "Pattern recognition and modern computers",
      "Citations": "290.0",
      "Authors": "O. G. Selfridge",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Self Organizing System",
      "Organization": "Massachusetts Institute of Technology (MIT)",
      "Publication date": "1955-03-01",
      "Domain": "Other",
      "Task": "Pattern recognition",
      "Parameters": "225.0",
      "Parameters notes": "Figure 4 contains the learnt weight matrix",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "2",
      "Dataset size notes": "\" The modifier was then\ndisabled so that no further changes in the net could\noccur and all 256 possible input patterns were then presented in turn.\"\n\n\"For these purposes, 16-element nets (8 input and 8\noutput) were used because it was desired to exhaust all\npossible input patterns, and we were limited to about\n2^8 inputs by available time. \"",
      "Confidence": "",
      "Link": "https://dl.acm.org/doi/10.1145/1455292.1455309",
      "Reference": "Generalization of pattern recognition in a self-organizing system",
      "Citations": "93.0",
      "Authors": "W. A. Clark and B. G. Farley",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Genetic algorithm",
      "Organization": "Institute for Advanced Study",
      "Publication date": "1954-07-02",
      "Domain": "Mathematics,Biology",
      "Task": "Numerical simulation",
      "Parameters": "",
      "Parameters notes": "",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "Unknown",
      "Link": "https://link.springer.com/article/10.1007/BF01556771",
      "Reference": "Numerical testing of evolution theories",
      "Citations": "266.0",
      "Authors": "NA Barricelli",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "Possibly first computer simulation of a genetic evolution algorithm",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "SNARC",
      "Organization": "Harvard University",
      "Publication date": "1952-01-08",
      "Domain": "Robotics",
      "Task": "Maze solving",
      "Parameters": "40.0",
      "Parameters notes": "The link below seems to suggest the SNARC had 40 cells, each with a dial that acts as a configurable weight.\n\nhttps://www.webofstories.com/play/marvin.minsky/137",
      "Training compute (FLOP)": "",
      "Training compute notes": "",
      "Training dataset": "",
      "Training dataset size (gradients)": "",
      "Dataset size notes": "",
      "Confidence": "",
      "Link": "https://en.wikipedia.org/wiki/Stochastic_neural_analog_reinforcement_calculator",
      "Reference": "A Neural-Analogue Calculator Based upon a Probability Model of Reinforcement",
      "Citations": "33.0",
      "Authors": "Marvin Minsky",
      "Abstract": "",
      "Organization categorization": "Academia",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    },
    {
      "Model": "Theseus",
      "Organization": "Bell Laboratories",
      "Publication date": "1950-07-02",
      "Domain": "Robotics",
      "Task": "Maze solving",
      "Parameters": "40.0",
      "Parameters notes": "The learned part is the maze configuration. There are 25 squares of the maze. The 16 squares to the left top corner have each one adjacent square down and one adjacent square up, for a total of 16*2 walls. We only need to count the 8 spare walls connecting the squares in the right side and the bottom side. In total there are 16*2+8 walls.",
      "Training compute (FLOP)": "40.0",
      "Training compute notes": "The \"training\" consists on the mouse running around and checking each wall (assuming each relay switch is one operation).",
      "Training dataset": "",
      "Training dataset size (gradients)": "40",
      "Dataset size notes": "Each wall Theseus bumps into is a datapoint",
      "Confidence": "Confident",
      "Link": "https://www.technologyreview.com/2018/12/19/138508/mighty-mouse/",
      "Reference": "Mighty Mouse",
      "Citations": "",
      "Authors": "Claude Shannon",
      "Abstract": "",
      "Organization categorization": "Industry",
      "Country (of organization)": "United States of America",
      "Notability criteria": "Historical significance",
      "Notability criteria notes": "",
      "Epochs": "",
      "Training time (hours)": "",
      "Training time notes": "",
      "Training hardware": "",
      "Hardware quantity": "",
      "Hardware utilization (MFU)": "",
      "Training compute cost (2023 USD)": "",
      "Compute cost notes": "",
      "Training power draw (W)": "",
      "Base model": "",
      "Finetune compute (FLOP)": "",
      "Finetune compute notes": "",
      "Batch size": "",
      "Batch size notes": "",
      "Model accessibility": "",
      "Training code accessibility": "",
      "Inference code accessibility": "",
      "Accessibility notes": "",
      "Numerical format": "",
      "Frontier model": "True",
      "Hardware acquisition cost": "",
      "Hardware utilization (HFU)": "",
      "Training compute cost (cloud)": "",
      "Training compute cost (upfront)": ""
    }
  ],
  "ML Hardware": [
    {
      "Hardware name": "Amazon Trainium3",
      "Manufacturer": "Amazon AWS",
      "Type": "GPU",
      "Notes": "Source for TDP: https://newsletter.semianalysis.com/p/aws-trainium3-deep-dive-a-potential\nto do: fill in memory specs",
      "Release date": "2025-12-02",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "671000000000000.0",
      "FP8 performance (FLOP/s)": "2517000000000000.0",
      "FP4 performance (FLOP/s)": "2517000000000000.0",
      "Memory (bytes)": "",
      "Memory bandwidth (byte/s)": "",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "700.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "183000000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "671000000000000.0",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "",
      "Transistors (millions)": "",
      "Link to datasheet": "https://awsdocs-neuron.readthedocs-hosted.com/en/latest/about-neuron/arch/neuron-hardware/trainium3.html ",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-12-05 21:21:23+00:00",
      "Max performance": "671000000000000.0",
      "Energy efficiency": "958571428571.4286",
      "Total processing performance (bit-OP/s)": "2.0136e+16",
      "Price-performance": "",
      "ML OP/s": "2517000000000000.0"
    },
    {
      "Hardware name": "Google TPU v7 Ironwood",
      "Manufacturer": "Google",
      "Type": "TPU",
      "Notes": "source for release date: https://www.cnbc.com/2025/11/06/google-unveils-ironwood-seventh-generation-tpu-competing-with-nvidia.html\nGoogle does not officially state TPUv7's TDP. In the blog post announcing TPUv7 (https://blog.google/products/google-cloud/ironwood-tpu-age-of-inference/ ), Google disclosed that TPUv7 is 29.3x more efficient than the TPUv2 in peak FLOP/s per TDP (measuring peak FLOP/s as either FP8 or bf16 FLOP/s when FP8 is not supported). \n\nTPUv2 does 46 TFLOP/s peak and has a TDP of 280 W. \n\nTPUv7 does 4614 TFLOP/s peak (FP8). So the TPUv7's TDP is (4614 TFLOP/s / 46 TFLOP/s) * 280 W / 29.3x efficiency = ~959 W. Note that this may be a slightly approximate figure due to rounding on Google's part. We round to 960 W.\n",
      "Release date": "2025-11-06",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "2307000000000000.0",
      "FP8 performance (FLOP/s)": "4614000000000000.0",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "192000000000.0",
      "Memory bandwidth (byte/s)": "7370000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "960.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "",
      "Transistors (millions)": "",
      "Link to datasheet": "https://blog.google/products/google-cloud/ironwood-tpu-age-of-inference/\nhttps://docs.cloud.google.com/tpu/docs/tpu7x",
      "Source for the price": "",
      "ML models": "Gemini 3 Pro",
      "Last modified": "2025-12-16 02:08:48+00:00",
      "Max performance": "2307000000000000.0",
      "Energy efficiency": "2403125000000.0",
      "Total processing performance (bit-OP/s)": "3.6912e+16",
      "Price-performance": "",
      "ML OP/s": "4614000000000000.0"
    },
    {
      "Hardware name": "Huawei Ascend 920",
      "Manufacturer": "Huawei",
      "Type": "NPU",
      "Notes": "",
      "Release date": "2025-10-01",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "900000000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "",
      "Memory bandwidth (byte/s)": "4000000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "SMIC",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "6.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://www.tomshardware.com/pc-components/gpus/huawei-introduces-the-ascend-920-ai-chip-to-fill-the-void-left-by-nvidias-h20",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-30 16:53:19+00:00",
      "Max performance": "900000000000000.0",
      "Energy efficiency": "",
      "Total processing performance (bit-OP/s)": "",
      "Price-performance": "",
      "ML OP/s": "900000000000000.0"
    },
    {
      "Hardware name": "NVIDIA GB300 (Blackwell Ultra)",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "Individual GPUs are sometimes called Blackwell Ultra, but systems are referred to as GB300. GB300 is only available in systems containing two or more GPUs. Performance figures are per individual GPU, not per GB300 superchip. Performance figures are listed without sparsity.\n\nhttps://resources.nvidia.com/en-us-blackwell-architecture/blackwell-ultra-datasheet",
      "Release date": "2025-08-22",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "2500000000000000.0",
      "FP8 performance (FLOP/s)": "5000000000000000.0",
      "FP4 performance (FLOP/s)": "1.5e+16",
      "Memory (bytes)": "287500000000.0",
      "Memory bandwidth (byte/s)": "8000000000000.0",
      "Intranode bandwidth (byte/s)": "1800000000000.0",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "1400.0",
      "FP64 (double precision) performance (FLOP/s)": "1300000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "80000000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "1250000000000000.0",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "165000000000000.0",
      "INT4 performance (OP/s)": "1.5e+16",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "4.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://resources.nvidia.com/en-us-dgx-systems/dgx-b300-datasheet\nhttps://www.nvidia.com/en-us/data-center/gb300-nvl72/",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-11-21 23:11:07+00:00",
      "Max performance": "1.5e+16",
      "Energy efficiency": "10714285714285.717",
      "Total processing performance (bit-OP/s)": "6.112e+16",
      "Price-performance": "",
      "ML OP/s": "5000000000000000.0"
    },
    {
      "Hardware name": "NVIDIA B300 (Blackwell Ultra)",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "Individual GPUs are sometimes called Blackwell Ultra, but systems are referred to as B300. Performance figures are listed without sparsity.",
      "Release date": "2025-08-22",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "2250000000000000.0",
      "FP8 performance (FLOP/s)": "4500000000000000.0",
      "FP4 performance (FLOP/s)": "1.4e+16",
      "Memory (bytes)": "270000000000.0",
      "Memory bandwidth (byte/s)": "7700000000000.0",
      "Intranode bandwidth (byte/s)": "1800000000000.0",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "1100.0",
      "FP64 (double precision) performance (FLOP/s)": "1200000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "75000000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "1100000000000000.0",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "157500000000000.0",
      "INT4 performance (OP/s)": "1.35e+16",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "640.0",
      "Process size (nm)": "4.0",
      "Transistors (millions)": "208000.0",
      "Link to datasheet": "https://developer.nvidia.com/blog/inside-nvidia-blackwell-ultra-the-chip-powering-the-ai-factory-era/\n\nhttps://resources.nvidia.com/en-us-blackwell-architecture/blackwell-ultra-datasheet",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-12-10 15:45:36+00:00",
      "Max performance": "1.35e+16",
      "Energy efficiency": "12272727272727.271",
      "Total processing performance (bit-OP/s)": "",
      "Price-performance": "",
      "ML OP/s": "4500000000000000.0"
    },
    {
      "Hardware name": "AMD Instinct MI355X",
      "Manufacturer": "AMD",
      "Type": "GPU",
      "Notes": "release announcement (source for release date): https://www.tomshardware.com/pc-components/gpus/amd-announces-mi350x-and-mi355x-ai-gpus-claims-up-to-4x-generational-gain-up-to-35x-faster-inference-performance ",
      "Release date": "2025-06-12",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "2516600000000000.0",
      "FP8 performance (FLOP/s)": "4600000000000000.0",
      "FP4 performance (FLOP/s)": "9200000000000000.0",
      "Memory (bytes)": "288000000000.0",
      "Memory bandwidth (byte/s)": "8000000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "1400.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "2340000000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "5033200000000000.0",
      "INT4 performance (OP/s)": "1.00663e+16",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "3.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/product-briefs/amd-instinct-mi355x-gpu-brochure.pdf",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-11-05 20:59:45+00:00",
      "Max performance": "1.00663e+16",
      "Energy efficiency": "7190214285714.286",
      "Total processing performance (bit-OP/s)": "3.744e+16",
      "Price-performance": "",
      "ML OP/s": "5033200000000000.0"
    },
    {
      "Hardware name": "AMD Instinct MI350X",
      "Manufacturer": "AMD",
      "Type": "GPU",
      "Notes": "release announcement (source for release date): https://www.tomshardware.com/pc-components/gpus/amd-announces-mi350x-and-mi355x-ai-gpus-claims-up-to-4x-generational-gain-up-to-35x-faster-inference-performance ",
      "Release date": "2025-06-12",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "2309600000000000.0",
      "FP8 performance (FLOP/s)": "4600000000000000.0",
      "FP4 performance (FLOP/s)": "9200000000000000.0",
      "Memory (bytes)": "288000000000.0",
      "Memory bandwidth (byte/s)": "8000000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "1000.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "2340000000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "4614000000000000.0",
      "INT4 performance (OP/s)": "9227500000000000.0",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "3.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/product-briefs/amd-instinct-mi350x-gpu-brochure.pdf ",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-11-05 20:57:17+00:00",
      "Max performance": "9227500000000000.0",
      "Energy efficiency": "9227500000000.0",
      "Total processing performance (bit-OP/s)": "3.744e+16",
      "Price-performance": "",
      "ML OP/s": "4614000000000000.0"
    },
    {
      "Hardware name": "NVIDIA GB200",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "GBB00 is only available in systems containing two or more GPUs. Performance figures are *per GPU*, not per GB200 superchip (which contains 2 GPUs). ",
      "Release date": "2025-02-15",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "2500000000000000.0",
      "FP8 performance (FLOP/s)": "5000000000000000.0",
      "FP4 performance (FLOP/s)": "1e+16",
      "Memory (bytes)": "186000000000.0",
      "Memory bandwidth (byte/s)": "8000000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "1200.0",
      "FP64 (double precision) performance (FLOP/s)": "45000000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "90000000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "1250000000000000.0",
      "FP16 (half precision) performance (FLOP/s)": "2500000000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "5000000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "",
      "Transistors (millions)": "",
      "Link to datasheet": "https://www.nvidia.com/en-us/data-center/gb200-nvl2/#specs\nhttps://drive.google.com/file/d/1Wc9tdHdy3UthQ6ttW98RgrdUx8Vva5tU/view",
      "Source for the price": "",
      "ML models": "Granite-4.0-H-Small,Granite-4.0-H-Micro,Granite-4.0-H-Tiny,MAI-Image-1,SWE-1.5,Llama 4 Scout + ScaleRL",
      "Last modified": "2025-11-13 14:51:26+00:00",
      "Max performance": "5000000000000000.0",
      "Energy efficiency": "4166666666666.6665",
      "Total processing performance (bit-OP/s)": "4e+16",
      "Price-performance": "",
      "ML OP/s": "5000000000000000.0"
    },
    {
      "Hardware name": "Amazon Trainium2",
      "Manufacturer": "Amazon AWS",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2024-12-03",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "667000000000000.0",
      "FP8 performance (FLOP/s)": "1299000000000000.0",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "96000000000.0",
      "Memory bandwidth (byte/s)": "2900000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "500.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "",
      "Transistors (millions)": "",
      "Link to datasheet": "https://semianalysis.com/2024/12/03/amazons-ai-self-sufficiency-trainium2-architecture-networking/\nhttps://awsdocs-neuron.readthedocs-hosted.com/en/latest/about-neuron/arch/neuron-hardware/trainium2.html",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-10-29 15:08:53+00:00",
      "Max performance": "667000000000000.0",
      "Energy efficiency": "1334000000000.0",
      "Total processing performance (bit-OP/s)": "1.0672e+16",
      "Price-performance": "",
      "ML OP/s": "1299000000000000.0"
    },
    {
      "Hardware name": "NVIDIA H200 SXM",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2024-11-18",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "989500000000000.0",
      "FP8 performance (FLOP/s)": "1979000000000000.0",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "141000000000.0",
      "Memory bandwidth (byte/s)": "4800000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "700.0",
      "FP64 (double precision) performance (FLOP/s)": "33450000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "66910000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "494500000000000.0",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "1979000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "5.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://nvdam.widen.net/s/nb5zzzsjdf/hpc-datasheet-sc23-h200-datasheet-3002446",
      "Source for the price": "",
      "ML models": "SEA-LION V3 Llama3.1 8B,SEA-LION V3 Llama3.1 70B,EXAONE 4.0 (32B),EXAONE 4.0 (1.2B),Gemma-SEA-LION-v4-27B-IT",
      "Last modified": "2025-12-29 05:59:41+00:00",
      "Max performance": "1979000000000000.0",
      "Energy efficiency": "2827142857142.857",
      "Total processing performance (bit-OP/s)": "1.5832e+16",
      "Price-performance": "",
      "ML OP/s": "1979000000000000.0"
    },
    {
      "Hardware name": "NVIDIA B100",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "Listed performance figures are without sparsity.",
      "Release date": "2024-11-15",
      "Release price (USD)": "35000.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "1750000000000000.0",
      "FP8 performance (FLOP/s)": "3500000000000000.0",
      "FP4 performance (FLOP/s)": "7000000000000000.0",
      "Memory (bytes)": "192000000000.0",
      "Memory bandwidth (byte/s)": "8000000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "700.0",
      "FP64 (double precision) performance (FLOP/s)": "30000000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "875000000000000.0",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "3500000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "",
      "Transistors (millions)": "",
      "Link to datasheet": "https://www.anandtech.com/show/21310/nvidia-blackwell-architecture-and-b200b100-accelerators-announced-going-bigger-with-smaller-data\n\nhttps://web.archive.org/web/20240809084215/https://www.hyperstack.cloud/nvidia-blackwell-b100",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-11-13 14:52:21+00:00",
      "Max performance": "3500000000000000.0",
      "Energy efficiency": "5000000000000.0",
      "Total processing performance (bit-OP/s)": "2.8e+16",
      "Price-performance": "100000000000.0",
      "ML OP/s": "3500000000000000.0"
    },
    {
      "Hardware name": "NVIDIA B200",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "Listed performance figures are without sparsity.",
      "Release date": "2024-11-15",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "2250000000000000.0",
      "FP8 performance (FLOP/s)": "4500000000000000.0",
      "FP4 performance (FLOP/s)": "9000000000000000.0",
      "Memory (bytes)": "180000000000.0",
      "Memory bandwidth (byte/s)": "7700000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "1000.0",
      "FP64 (double precision) performance (FLOP/s)": "31040000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "62080000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "1125000000000000.0",
      "FP16 (half precision) performance (FLOP/s)": "2250000000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "4500000000000000.0",
      "INT4 performance (OP/s)": "9000000000000000.0",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "",
      "Transistors (millions)": "",
      "Link to datasheet": "https://www.primeline-solutions.com/media/categories/server/nach-gpu/nvidia-hgx-h200/nvidia-blackwell-b200-datasheet.pdf\nhttps://drive.google.com/file/d/1Wc9tdHdy3UthQ6ttW98RgrdUx8Vva5tU/view",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-11-13 14:52:20+00:00",
      "Max performance": "9000000000000000.0",
      "Energy efficiency": "9000000000000.0",
      "Total processing performance (bit-OP/s)": "3.6e+16",
      "Price-performance": "",
      "ML OP/s": "4500000000000000.0"
    },
    {
      "Hardware name": "Huawei Ascend 910C",
      "Manufacturer": "Huawei",
      "Type": "GPU",
      "Notes": "Most reports, e.g. Trendforce indicate 800 TFLOP/s in FP16. SemiAnalysis claims 780 TFLOP/s.",
      "Release date": "2024-10-15",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "800000000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "128000000000.0",
      "Memory bandwidth (byte/s)": "3200000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "784000000000.0",
      "TDP (W)": "700.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "800000000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "SMIC,TSMC",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "7.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://www.trendforce.com/news/2025/03/13/news-huaweis-ascend-910c-takes-on-nvidia-as-chinas-ai-race-heats-up-more-alleged-details/\nhttps://www.huawei.com/en/news/2025/9/hc-xu-keynote-speech?\nhttps://newsletter.semianalysis.com/p/huawei-ai-cloudmatrix-384-chinas-answer-to-nvidia-gb200-nvl72",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-12-11 22:42:47+00:00",
      "Max performance": "800000000000000.0",
      "Energy efficiency": "1142857142857.1428",
      "Total processing performance (bit-OP/s)": "1.28e+16",
      "Price-performance": "",
      "ML OP/s": "800000000000000.0"
    },
    {
      "Hardware name": "AMD Instinct MI325X",
      "Manufacturer": "AMD",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2024-10-10",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "1307400000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "256000000000.0",
      "Memory bandwidth (byte/s)": "6000000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "1000.0",
      "FP64 (double precision) performance (FLOP/s)": "81700000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "163400000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "653700000000000.0",
      "FP16 (half precision) performance (FLOP/s)": "1300000000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "2600000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "2100.0",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "1000.0",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "5.0",
      "Transistors (millions)": "153000.0",
      "Link to datasheet": "https://www.amd.com/en/products/accelerators/instinct/mi300/mi325x.html",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-11-05 21:14:16+00:00",
      "Max performance": "2600000000000000.0",
      "Energy efficiency": "2600000000000.0",
      "Total processing performance (bit-OP/s)": "2.08e+16",
      "Price-performance": "",
      "ML OP/s": "2600000000000000.0"
    },
    {
      "Hardware name": "Intel Habana Gaudi3",
      "Manufacturer": "Intel",
      "Type": "",
      "Notes": "Intel says that the Gaudi3's BF16 FLOPS will be 4x that of the Gaudi2. We were not able to find the die area for this product.\nMore useful source is this whitepaper: https://www.intel.com/content/www/us/en/content-details/817486/intel-gaudi-3-ai-accelerator-white-paper.html\n",
      "Release date": "2024-09-24",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "1678000000000000.0",
      "FP8 performance (FLOP/s)": "1678000000000000.0",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "128000000000.0",
      "Memory bandwidth (byte/s)": "3700000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "900.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "14300000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "459000000000000.0",
      "FP16 (half precision) performance (FLOP/s)": "28700000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "64.0",
      "Process size (nm)": "5.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://perma.cc/83JH-F3M4",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:20:22+00:00",
      "Max performance": "1678000000000000.0",
      "Energy efficiency": "1864444444444.4443",
      "Total processing performance (bit-OP/s)": "2.6848e+16",
      "Price-performance": "",
      "ML OP/s": "1678000000000000.0"
    },
    {
      "Hardware name": "Maia 100 (M100)",
      "Manufacturer": "Microsoft",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2024-08-27",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "800000000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "",
      "Memory bandwidth (byte/s)": "",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "500.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "820.0",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "5.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://www.techpowerup.com/326105/microsoft-unveils-new-details-on-maia-100-its-first-custom-ai-chip\n\nhttps://techcommunity.microsoft.com/blog/azureinfrastructureblog/inside-maia-100-revolutionizing-ai-workloads-with-microsofts-custom-ai-accelerat/4229118",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:20:23+00:00",
      "Max performance": "800000000000000.0",
      "Energy efficiency": "1600000000000.0",
      "Total processing performance (bit-OP/s)": "1.28e+16",
      "Price-performance": "",
      "ML OP/s": "800000000000000.0"
    },
    {
      "Hardware name": "Google TPU v6e Trillium",
      "Manufacturer": "Google",
      "Type": "TPU",
      "Notes": "Google does not officially state TPUv6's TDP. In the blog post announcing TPUv7 (https://blog.google/products/google-cloud/ironwood-tpu-age-of-inference/ ), Google disclosed that TPUv6 is 14.6x more efficient than the TPUv2 in peak FLOP/s per TDP (measuring peak FLOP/s as either FP8 or bf16 FLOP/s when FP8 is not supported). \n\nTPUv2 does 46 TFLOP/s peak and has a TDP of 280 W. TPUv6 does 918 TFLOP/s peak (FP8). \nSo the TPUv6's TDP is (918 TFLOP/s / 46 TFLOP/s) * 280 W / 14.6x efficiency = ~383 W. Note that this may be a slightly approximate figure due to rounding on Google's part. We round to 380 W.",
      "Release date": "2024-05-14",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "918000000000000.0",
      "FP8 performance (FLOP/s)": "918000000000000.0",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "32000000000.0",
      "Memory bandwidth (byte/s)": "1640000000000.0",
      "Intranode bandwidth (byte/s)": "448000000000.0",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "380.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "1836000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "",
      "Transistors (millions)": "",
      "Link to datasheet": "https://cloud.google.com/tpu/docs/v6e",
      "Source for the price": "",
      "ML models": "Gemini 2.0 Flash,Gemini 2.0 Flash-Lite (Feb 2024),FGN,DataRater test model (1B),Gemini Robotics-ER 1.5,Gemini Robotics 1.5 ",
      "Last modified": "2025-12-16 02:09:49+00:00",
      "Max performance": "1836000000000000.0",
      "Energy efficiency": "4831578947368.421",
      "Total processing performance (bit-OP/s)": "1.4688e+16",
      "Price-performance": "",
      "ML OP/s": "1836000000000000.0"
    },
    {
      "Hardware name": "Meta MTIA v2",
      "Manufacturer": "Meta",
      "Type": "Other",
      "Notes": "",
      "Release date": "2024-04-10",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "177000000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "256000000.0",
      "Memory bandwidth (byte/s)": "1000000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "90.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "2760000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "354000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "421.0",
      "Base clock (MHz)": "1350.0",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "5.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://ai.meta.com/blog/next-generation-meta-training-inference-accelerator-AI-MTIA/",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:20:24+00:00",
      "Max performance": "354000000000000.0",
      "Energy efficiency": "3933333333333.3335",
      "Total processing performance (bit-OP/s)": "2832000000000000.0",
      "Price-performance": "",
      "ML OP/s": "354000000000000.0"
    },
    {
      "Hardware name": "NVIDIA H100 NVL",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2024-03-14",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "94000000000.0",
      "Memory bandwidth (byte/s)": "3938000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "400.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "835500000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "1080.0",
      "Boost clock (MHz)": "1785.0",
      "Memory clock (MHz)": "2619.0",
      "Memory bus (bit)": "6016.0",
      "Tensor cores": "",
      "Process size (nm)": "",
      "Transistors (millions)": "",
      "Link to datasheet": "https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/h100/PB-11773-001_v01.pdf\n\nhttps://www.pny.com/nvidia-h100-nvl",
      "Source for the price": "",
      "ML models": "Boltz-2,s1-32B,s1.1",
      "Last modified": "2025-10-29 21:55:41+00:00",
      "Max performance": "835500000000000.0",
      "Energy efficiency": "2088750000000.0",
      "Total processing performance (bit-OP/s)": "1.3368e+16",
      "Price-performance": "",
      "ML OP/s": "835500000000000.0"
    },
    {
      "Hardware name": "MTT S4000",
      "Manufacturer": "Moore Threads (Tencent)",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2023-12-19",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "100000000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "48000000000.0",
      "Memory bandwidth (byte/s)": "768000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "450.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "25000000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "50000000000000.0",
      "FP16 (half precision) performance (FLOP/s)": "49150000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "200000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "128.0",
      "Process size (nm)": "12.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://en.mthreads.com/product/S4000\nhttps://www.techpowerup.com/gpu-specs/mtt-s4000.c4261",
      "Source for the price": "",
      "ML models": "MooER",
      "Last modified": "2025-07-21 09:20:26+00:00",
      "Max performance": "200000000000000.0",
      "Energy efficiency": "444444444444.44446",
      "Total processing performance (bit-OP/s)": "1600000000000000.0",
      "Price-performance": "",
      "ML OP/s": "200000000000000.0"
    },
    {
      "Hardware name": "Google TPU v5p",
      "Manufacturer": "Google",
      "Type": "TPU",
      "Notes": "Google does not officially state TPUv5p's TDP. \nIn the blog post announcing TPUv7 (https://blog.google/products/google-cloud/ironwood-tpu-age-of-inference/), Google disclosed that TPUv5p is 5.2x more efficient than the TPUv2 in peak FLOP/s per TDP (measuring peak FLOP/s as either FP8 or bf16 FLOP/s when FP8 is not supported). \n\nTPUv2 does 46 TFLOP/s peak and has a TDP of 280 W. TPUv5p does 459 TFLOP/s peak. So the TPUv5p's TDP is (459 TFLOP/s / 46 TFLOP/s) * 280 W / 5.2x efficiency = ~537 W. Note that this may be a slightly approximate figure due to rounding on Google's part. We round to 540 W.",
      "Release date": "2023-12-06",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "459000000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "95000000000.0",
      "Memory bandwidth (byte/s)": "2765000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "540.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "918000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "2.0",
      "Process size (nm)": "",
      "Transistors (millions)": "",
      "Link to datasheet": "https://cloud.google.com/tpu/docs/v5p",
      "Source for the price": "",
      "ML models": "Genie,Gemma 2 27B,AFM-on-device,Gemma 3 27B,FGN",
      "Last modified": "2025-12-12 22:57:49+00:00",
      "Max performance": "918000000000000.0",
      "Energy efficiency": "1700000000000.0",
      "Total processing performance (bit-OP/s)": "7344000000000000.0",
      "Price-performance": "",
      "ML OP/s": "918000000000000.0"
    },
    {
      "Hardware name": "AMD Instinct MI300X",
      "Manufacturer": "AMD",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2023-12-06",
      "Release price (USD)": "15000.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "1307400000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "192000000000.0",
      "Memory bandwidth (byte/s)": "5300000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "750.0",
      "FP64 (double precision) performance (FLOP/s)": "81700000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "163400000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "653700000000000.0",
      "FP16 (half precision) performance (FLOP/s)": "1307400000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "2614900000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "1017.0",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "5.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/data-sheets/amd-instinct-mi300x-data-sheet.pdf\nhttps://www.techpowerup.com/gpu-specs/radeon-instinct-mi300x.c4179",
      "Source for the price": "https://www.tomshardware.com/tech-industry/artificial-intelligence/nvidias-h100-ai-gpus-cost-up-to-four-times-more-than-amds-competing-mi300x-amds-chips-cost-dollar10-to-dollar15k-apiece-nvidias-h100-has-peaked-beyond-dollar40000",
      "ML models": "LoongRL 14B",
      "Last modified": "2025-11-08 01:45:30+00:00",
      "Max performance": "2614900000000000.0",
      "Energy efficiency": "3486533333333.333",
      "Total processing performance (bit-OP/s)": "2.09192e+16",
      "Price-performance": "174326666666.66666",
      "ML OP/s": "2614900000000000.0"
    },
    {
      "Hardware name": "AMD Radeon Instinct MI308X",
      "Manufacturer": "AMD",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2023-12-06",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "192000000000.0",
      "Memory bandwidth (byte/s)": "10300000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "750.0",
      "FP64 (double precision) performance (FLOP/s)": "81720000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "81720000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "653700000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "1017.0",
      "Base clock (MHz)": "1000.0",
      "Boost clock (MHz)": "2100.0",
      "Memory clock (MHz)": "2525.0",
      "Memory bus (bit)": "8192.0",
      "Tensor cores": "1216.0",
      "Process size (nm)": "5.0",
      "Transistors (millions)": "153000.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/radeon-instinct-mi308x.c4295",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-27 17:28:45+00:00",
      "Max performance": "653700000000000.0",
      "Energy efficiency": "871600000000.0",
      "Total processing performance (bit-OP/s)": "",
      "Price-performance": "",
      "ML OP/s": "653700000000000.0"
    },
    {
      "Hardware name": "Sunway SW26010-Pro",
      "Manufacturer": "Sunway",
      "Type": "",
      "Notes": "",
      "Release date": "2023-11-26",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "96000000000.0",
      "Memory bandwidth (byte/s)": "307200000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "",
      "FP64 (double precision) performance (FLOP/s)": "13800000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "27600000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "55296000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "14.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://www.tomshardware.com/tech-industry/supercomputers/chinas-secretive-sunway-pro-cpu-quadruples-performance-over-its-predecessor-allowing-the-supercomputer-supercomputer-to-hit-exaflop-speeds",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:20:27+00:00",
      "Max performance": "55296000000000.0",
      "Energy efficiency": "",
      "Total processing performance (bit-OP/s)": "884736000000000.0",
      "Price-performance": "",
      "ML OP/s": "55296000000000.0"
    },
    {
      "Hardware name": "NVIDIA HGX H20",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2023-11-09",
      "Release price (USD)": "12000.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "148000000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "96000000000.0",
      "Memory bandwidth (byte/s)": "4000000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "400.0",
      "FP64 (double precision) performance (FLOP/s)": "1000000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "44000000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "74000000000000.0",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "296000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "",
      "Transistors (millions)": "",
      "Link to datasheet": "https://viperatech.com/shop/nvidia-hgx-h20/",
      "Source for the price": "https://www.tomshardware.com/tech-industry/artificial-intelligence/distributor-claims-that-nvidia-has-allegedly-stopped-taking-orders-on-hgx-h20-gpu-processors#:~:text=The%20product%20has%2096%20GB,tag%20of%20around%20%2410%2C000%20%2D%20%2413%2C000.\n\nhttps://www.techpowerup.com/318595/nvidia-readying-h20-ai-gpu-for-chinese-market",
      "ML models": "",
      "Last modified": "2025-07-21 09:20:28+00:00",
      "Max performance": "296000000000000.0",
      "Energy efficiency": "740000000000.0",
      "Total processing performance (bit-OP/s)": "2368000000000000.0",
      "Price-performance": "24666666666.666668",
      "ML OP/s": "296000000000000.0"
    },
    {
      "Hardware name": "NVIDIA L2 PCle",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2023-11-09",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "96500000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "24000000000.0",
      "Memory bandwidth (byte/s)": "300000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "24100000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "48300000000000.0",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "193000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "",
      "Transistors (millions)": "",
      "Link to datasheet": "https://viperatech.com/shop/nvidia-l2-pcie/#:~:text=NVIDIA's%20L2%20PCle%2C%20part%20of,handling%20of%20data%2Dintensive%20tasks.",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:20:28+00:00",
      "Max performance": "193000000000000.0",
      "Energy efficiency": "",
      "Total processing performance (bit-OP/s)": "1544000000000000.0",
      "Price-performance": "",
      "ML OP/s": "193000000000000.0"
    },
    {
      "Hardware name": "NVIDIA L20 PCle",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2023-09-16",
      "Release price (USD)": "4299.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "119500000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "48000000000.0",
      "Memory bandwidth (byte/s)": "864000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "275.0",
      "FP64 (double precision) performance (FLOP/s)": "927400000000.0",
      "FP32 (single precision) performance (FLOP/s)": "59350000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "59800000000000.0",
      "FP16 (half precision) performance (FLOP/s)": "59350000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "239000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "609.0",
      "Base clock (MHz)": "1440.0",
      "Boost clock (MHz)": "2520.0",
      "Memory clock (MHz)": "2250.0",
      "Memory bus (bit)": "384.0",
      "Tensor cores": "11776.0",
      "Process size (nm)": "5.0",
      "Transistors (millions)": "76300.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/l20.c4206\nhttps://wccftech.com/nvidia-china-compliant-h20-gpu-41-percent-fewer-cores-lower-performance-vs-top-hopper-h100/",
      "Source for the price": "https://viperatech.com/shop/nvidia-l20-pcie/",
      "ML models": "",
      "Last modified": "2025-07-21 09:20:29+00:00",
      "Max performance": "239000000000000.0",
      "Energy efficiency": "869090909090.909",
      "Total processing performance (bit-OP/s)": "1912000000000000.0",
      "Price-performance": "55594324261.45616",
      "ML OP/s": "239000000000000.0"
    },
    {
      "Hardware name": "Google TPU v5e",
      "Manufacturer": "Google",
      "Type": "TPU",
      "Notes": "No official TDP, SemiAnalysis estimates 300W TDP https://newsletter.semianalysis.com/p/tpuv7-google-takes-a-swing-at-the",
      "Release date": "2023-08-29",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "197000000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "16000000000.0",
      "Memory bandwidth (byte/s)": "819000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "393000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "325.0",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "1.0",
      "Process size (nm)": "",
      "Transistors (millions)": "",
      "Link to datasheet": "https://cloud.google.com/tpu/docs/v5e",
      "Source for the price": "",
      "ML models": "Gemma 7B,Gemini Nano-2,Gemini Nano-1,Gemma 2 2B,Gemma 1.1 7B Instruct,Imagen 3,PaliGemma,GameNGen,ALOHA Unleashed,Universal-1,DataGemma,Gemma 2B,Flexi-JEST++,JEST++,Gemma 3 1B,Gemma 3 4B,Gemini Robotics,GenCast,JEST-L++,PaliGemma 2 3B Mix 448,PaliGemma 2 3B Mix 224,Gemini Robotics-ER,Marin 8B,Aeneas,EmbeddingGemma,SigLIP 2,Gemini Robotics-ER 1.5,Gemini Robotics 1.5 ",
      "Last modified": "2025-12-12 23:06:38+00:00",
      "Max performance": "393000000000000.0",
      "Energy efficiency": "",
      "Total processing performance (bit-OP/s)": "3152000000000000.0",
      "Price-performance": "",
      "ML OP/s": "393000000000000.0"
    },
    {
      "Hardware name": "NVIDIA GH200",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2023-08-08",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "989500000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "",
      "Memory bandwidth (byte/s)": "4900000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "700.0",
      "FP64 (double precision) performance (FLOP/s)": "33450000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "66910000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "494500000000000.0",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "1979000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "5.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://resources.nvidia.com/en-us-data-center-overview-mc/en-us-data-center-overview/grace-hopper-superchip-datasheet-partner",
      "Source for the price": "",
      "ML models": "Bielik 7B,Bielik-11B-v2,Apertus 8B,Apertus 70B",
      "Last modified": "2025-09-12 17:54:16+00:00",
      "Max performance": "1979000000000000.0",
      "Energy efficiency": "2827142857142.857",
      "Total processing performance (bit-OP/s)": "1.5832e+16",
      "Price-performance": "",
      "ML OP/s": "1979000000000000.0"
    },
    {
      "Hardware name": "MetaX MXC500 (PCIe)",
      "Manufacturer": "MetaX",
      "Type": "GPGPU",
      "Notes": "",
      "Release date": "2023-06-13",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "64000000000.0",
      "Memory bandwidth (byte/s)": "18000000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "350.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "15000000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "120000000000000.0",
      "FP16 (half precision) performance (FLOP/s)": "240000000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "480000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "7.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://blog.csdn.net/qq_23934063/article/details/132473834\nhttps://data.eastmoney.com/report/zw_industry.jshtml?infocode=AP202306151590965492",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:20:31+00:00",
      "Max performance": "480000000000000.0",
      "Energy efficiency": "1371428571428.5715",
      "Total processing performance (bit-OP/s)": "3840000000000000.0",
      "Price-performance": "",
      "ML OP/s": "480000000000000.0"
    },
    {
      "Hardware name": "MetaX MXN100",
      "Manufacturer": "MetaX",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2023-06-07",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "",
      "Memory bandwidth (byte/s)": "",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "80000000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "160000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "",
      "Transistors (millions)": "",
      "Link to datasheet": "https://www.metax-tech.com/ndetail/12469.html",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:20:31+00:00",
      "Max performance": "160000000000000.0",
      "Energy efficiency": "",
      "Total processing performance (bit-OP/s)": "1280000000000000.0",
      "Price-performance": "",
      "ML OP/s": "160000000000000.0"
    },
    {
      "Hardware name": "Meta MTIA v1",
      "Manufacturer": "Meta",
      "Type": "Other",
      "Notes": "",
      "Release date": "2023-05-18",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "51200000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "128000000.0",
      "Memory bandwidth (byte/s)": "800000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "25.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "1600000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "51000000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "102400000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "373.0",
      "Base clock (MHz)": "800.0",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "7.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://ai.meta.com/blog/meta-training-inference-accelerator-AI-MTIA/\nhttps://ai.meta.com/blog/next-generation-meta-training-inference-accelerator-AI-MTIA/",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:20:32+00:00",
      "Max performance": "102400000000000.0",
      "Energy efficiency": "4096000000000.0",
      "Total processing performance (bit-OP/s)": "819200000000000.0",
      "Price-performance": "",
      "ML OP/s": "102400000000000.0"
    },
    {
      "Hardware name": "AWS Inferentia2",
      "Manufacturer": "Amazon AWS",
      "Type": "",
      "Notes": "",
      "Release date": "2023-04-13",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "190000000000000.0",
      "FP8 performance (FLOP/s)": "190000000000000.0",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "32000000000.0",
      "Memory bandwidth (byte/s)": "820000000000.0",
      "Intranode bandwidth (byte/s)": "1000000000000.0",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "47500000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "190000000000000.0",
      "FP16 (half precision) performance (FLOP/s)": "190000000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "380000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "",
      "Transistors (millions)": "",
      "Link to datasheet": "https://perma.cc/YED7-PTH6",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-11-13 14:48:27+00:00",
      "Max performance": "380000000000000.0",
      "Energy efficiency": "",
      "Total processing performance (bit-OP/s)": "3610000000000000.0",
      "Price-performance": "",
      "ML OP/s": "380000000000000.0"
    },
    {
      "Hardware name": "NVIDIA H800 SXM5",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2023-03-21",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "989000000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "80000000000.0",
      "Memory bandwidth (byte/s)": "3360000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "700.0",
      "FP64 (double precision) performance (FLOP/s)": "29650000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "59300000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "454500000000000.0",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "1979000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "814.0",
      "Base clock (MHz)": "1095.0",
      "Boost clock (MHz)": "1755.0",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "5120.0",
      "Tensor cores": "456.0",
      "Process size (nm)": "5.0",
      "Transistors (millions)": "80000.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/h800-sxm5.c3975",
      "Source for the price": "",
      "ML models": "DeepSeekMoE-16B,DeepSeek-V2 (MoE-236B),RWKV-5 (Eagle) 7B,DeepSeek-V3,LLaDA,Step-Video-T2V,MAP-Neo,MiniMax-Text-01,MiniMax-VL-01,MiniMax-M1-40k,MiniMax-M1-80k,Seed1.5-VL,ERNIE-4.5-300B-A47B,Xingrui AI (\u661f\u777fAI),NTele-R1-32B-V1,Step-Omni,Kimi K2,YuE,Skywork-OR1-32B,dots.llm1,Ring-flash-linear-2.0,Ring-mini-linear-2.0",
      "Last modified": "2025-10-28 19:12:08+00:00",
      "Max performance": "1979000000000000.0",
      "Energy efficiency": "2827142857142.857",
      "Total processing performance (bit-OP/s)": "1.5832e+16",
      "Price-performance": "",
      "ML OP/s": "1979000000000000.0"
    },
    {
      "Hardware name": "NVIDIA GH100",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2023-03-21",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "989500000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "",
      "Memory bandwidth (byte/s)": "3072000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "700.0",
      "FP64 (double precision) performance (FLOP/s)": "33450000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "66910000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "494500000000000.0",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "1979000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "",
      "Transistors (millions)": "",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/nvidia-gh100.g1011\n\nhttps://videocardz.net/gpu/nvidia-gh100",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:20:33+00:00",
      "Max performance": "1979000000000000.0",
      "Energy efficiency": "2827142857142.857",
      "Total processing performance (bit-OP/s)": "1.5832e+16",
      "Price-performance": "",
      "ML OP/s": "1979000000000000.0"
    },
    {
      "Hardware name": "NVIDIA L4",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "We divide performance by 2 to convert it from performance over sparse matrices to performance over dense matrices.",
      "Release date": "2023-03-21",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "121000000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "24000000000.0",
      "Memory bandwidth (byte/s)": "300100000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "72.0",
      "FP64 (double precision) performance (FLOP/s)": "473300000000.0",
      "FP32 (single precision) performance (FLOP/s)": "30290000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "60000000000000.0",
      "FP16 (half precision) performance (FLOP/s)": "30290000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "242000000000000.0",
      "INT4 performance (OP/s)": "484000000000000.0",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "294.0",
      "Base clock (MHz)": "795.0",
      "Boost clock (MHz)": "2040.0",
      "Memory clock (MHz)": "1563.0",
      "Memory bus (bit)": "192.0",
      "Tensor cores": "240.0",
      "Process size (nm)": "5.0",
      "Transistors (millions)": "35800.0",
      "Link to datasheet": "https://nvdam.widen.net/s/rvq98gbwsw/l4-datasheet-2595652\nhttps://www.techpowerup.com/gpu-specs/l4.c4091",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:20:33+00:00",
      "Max performance": "484000000000000.0",
      "Energy efficiency": "6722222222222.223",
      "Total processing performance (bit-OP/s)": "1936000000000000.0",
      "Price-performance": "",
      "ML OP/s": "242000000000000.0"
    },
    {
      "Hardware name": "NVIDIA GeForce RTX 4070",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2023-02-08",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "12000000000.0",
      "Memory bandwidth (byte/s)": "504200000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "285.0",
      "FP64 (double precision) performance (FLOP/s)": "455400000000.0",
      "FP32 (single precision) performance (FLOP/s)": "29150000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "29150000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "233000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "295.0",
      "Base clock (MHz)": "1920.0",
      "Boost clock (MHz)": "2475.0",
      "Memory clock (MHz)": "1313.0",
      "Memory bus (bit)": "192.0",
      "Tensor cores": "",
      "Process size (nm)": "",
      "Transistors (millions)": "35800.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/geforce-rtx-4070.c3924",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:20:35+00:00",
      "Max performance": "233000000000000.0",
      "Energy efficiency": "817543859649.1228",
      "Total processing performance (bit-OP/s)": "1864000000000000.0",
      "Price-performance": "",
      "ML OP/s": "233000000000000.0"
    },
    {
      "Hardware name": "Intel Data Center GPU Max 1100",
      "Manufacturer": "Intel",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2023-01-10",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "48000000000.0",
      "Memory bandwidth (byte/s)": "1230000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "300.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "22220000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "22220000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "Intel",
      "Die Size (mm^2)": "1280.0",
      "Base clock (MHz)": "1000.0",
      "Boost clock (MHz)": "1550.0",
      "Memory clock (MHz)": "600.0",
      "Memory bus (bit)": "8192.0",
      "Tensor cores": "448.0",
      "Process size (nm)": "10.0",
      "Transistors (millions)": "100000.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/data-center-gpu-max-1100.c4066",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:20:35+00:00",
      "Max performance": "22220000000000.0",
      "Energy efficiency": "74066666666.66667",
      "Total processing performance (bit-OP/s)": "711040000000000.0",
      "Price-performance": "",
      "ML OP/s": "22220000000000.0"
    },
    {
      "Hardware name": "Intel Data Center GPU Max 1350",
      "Manufacturer": "Intel",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2023-01-10",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "96000000000.0",
      "Memory bandwidth (byte/s)": "2460000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "450.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "44440000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "44440000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "Intel",
      "Die Size (mm^2)": "1280.0",
      "Base clock (MHz)": "750.0",
      "Boost clock (MHz)": "1550.0",
      "Memory clock (MHz)": "1200.0",
      "Memory bus (bit)": "8192.0",
      "Tensor cores": "896.0",
      "Process size (nm)": "10.0",
      "Transistors (millions)": "100000.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/data-center-gpu-max-1350.c4067",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:20:36+00:00",
      "Max performance": "44440000000000.0",
      "Energy efficiency": "98755555555.55556",
      "Total processing performance (bit-OP/s)": "1422080000000000.0",
      "Price-performance": "",
      "ML OP/s": "44440000000000.0"
    },
    {
      "Hardware name": "Intel Data Center GPU Max 1550",
      "Manufacturer": "Intel",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2023-01-10",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "128000000000.0",
      "Memory bandwidth (byte/s)": "3280000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "600.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "52430000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "52430000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "Intel",
      "Die Size (mm^2)": "1280.0",
      "Base clock (MHz)": "900.0",
      "Boost clock (MHz)": "1600.0",
      "Memory clock (MHz)": "1600.0",
      "Memory bus (bit)": "8192.0",
      "Tensor cores": "1024.0",
      "Process size (nm)": "10.0",
      "Transistors (millions)": "100000.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/data-center-gpu-max-1350.c4067",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:20:36+00:00",
      "Max performance": "52430000000000.0",
      "Energy efficiency": "87383333333.33333",
      "Total processing performance (bit-OP/s)": "1677760000000000.0",
      "Price-performance": "",
      "ML OP/s": "52430000000000.0"
    },
    {
      "Hardware name": "AMD Radeon Instinct MI300",
      "Manufacturer": "AMD",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2023-01-04",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "128000000000.0",
      "Memory bandwidth (byte/s)": "6550000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "600.0",
      "FP64 (double precision) performance (FLOP/s)": "47870000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "47870000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "383000000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "1017.0",
      "Base clock (MHz)": "1000.0",
      "Boost clock (MHz)": "1700.0",
      "Memory clock (MHz)": "1600.0",
      "Memory bus (bit)": "8192.0",
      "Tensor cores": "880.0",
      "Process size (nm)": "5.0",
      "Transistors (millions)": "153000.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/radeon-instinct-mi300.c4019",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-27 17:27:01+00:00",
      "Max performance": "383000000000000.0",
      "Energy efficiency": "638333333333.3334",
      "Total processing performance (bit-OP/s)": "6128000000000000.0",
      "Price-performance": "",
      "ML OP/s": "383000000000000.0"
    },
    {
      "Hardware name": "AMD Instinct MI300A",
      "Manufacturer": "AMD",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2023-01-04",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "980600000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "128000000000.0",
      "Memory bandwidth (byte/s)": "5300000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "760.0",
      "FP64 (double precision) performance (FLOP/s)": "61300000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "122600000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "490300000000000.0",
      "FP16 (half precision) performance (FLOP/s)": "980600000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "1961200000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "5.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://www.amd.com/content/dam/amd/en/documents/instinct-tech-docs/data-sheets/amd-instinct-mi300a-data-sheet.pdf",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:20:37+00:00",
      "Max performance": "1961200000000000.0",
      "Energy efficiency": "2580526315789.4736",
      "Total processing performance (bit-OP/s)": "1.56896e+16",
      "Price-performance": "",
      "ML OP/s": "1961200000000000.0"
    },
    {
      "Hardware name": "Baidu Kunlun RG800",
      "Manufacturer": "Kunlunxin Baidu",
      "Type": "XPU-R",
      "Notes": "https://archive.ph/XFJOZ",
      "Release date": "2023-01-01",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "32000000000.0",
      "Memory bandwidth (byte/s)": "512000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "130.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "32000000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "128000000000000.0",
      "INT16 performance (OP/s)": "128000000000000.0",
      "INT8 performance (OP/s)": "256000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "",
      "Transistors (millions)": "",
      "Link to datasheet": "",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:20:39+00:00",
      "Max performance": "256000000000000.0",
      "Energy efficiency": "1969230769230.7693",
      "Total processing performance (bit-OP/s)": "2048000000000000.0",
      "Price-performance": "",
      "ML OP/s": "256000000000000.0"
    },
    {
      "Hardware name": "MetaX MXC500 (OAM)",
      "Manufacturer": "MetaX",
      "Type": "GPGPU",
      "Notes": "",
      "Release date": "2023-01-01",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "64000000000.0",
      "Memory bandwidth (byte/s)": "18000000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "450.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "18000000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "140000000000000.0",
      "FP16 (half precision) performance (FLOP/s)": "280000000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "560000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "7.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://blog.csdn.net/qq_23934063/article/details/132473834\nhttps://data.eastmoney.com/report/zw_industry.jshtml?infocode=AP202306151590965492",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:20:39+00:00",
      "Max performance": "560000000000000.0",
      "Energy efficiency": "1244444444444.4443",
      "Total processing performance (bit-OP/s)": "4480000000000000.0",
      "Price-performance": "",
      "ML OP/s": "560000000000000.0"
    },
    {
      "Hardware name": "Tianshu Zhixin Zhikai 100",
      "Manufacturer": "Iluvatar CoreX",
      "Type": "GPU",
      "Notes": "https://zhuanlan.zhihu.com/p/593594163",
      "Release date": "2022-12-20",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "",
      "Memory bandwidth (byte/s)": "",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "24000000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "96000000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "384000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "",
      "Transistors (millions)": "",
      "Link to datasheet": "",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:20:40+00:00",
      "Max performance": "384000000000000.0",
      "Energy efficiency": "",
      "Total processing performance (bit-OP/s)": "3072000000000000.0",
      "Price-performance": "",
      "ML OP/s": "384000000000000.0"
    },
    {
      "Hardware name": "NVIDIA RTX 6000 Ada Generation",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "NVIDIA does not market this product for use in a datacenter, but the company describes it as a data center product in technical documentation (https://perma.cc/XUH2-YUMD), and the company's data center drivers are compatible with it (https://perma.cc/UXN9-637L). We divide performance by 2 to convert it from performance over sparse matrices to performance over dense matrices.",
      "Release date": "2022-12-03",
      "Release price (USD)": "6799.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "729000000000000.0",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "",
      "Memory bandwidth (byte/s)": "",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "",
      "FP64 (double precision) performance (FLOP/s)": "1423000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "91060000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "91060000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "608.0",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "5.0",
      "Transistors (millions)": "76300.0",
      "Link to datasheet": "https://perma.cc/63JP-L3WM\nhttps://www.techpowerup.com/gpu-specs/rtx-6000-ada-generation.c3933",
      "Source for the price": "",
      "ML models": "BADGER,PARM",
      "Last modified": "2025-07-21 09:20:40+00:00",
      "Max performance": "91060000000000.0",
      "Energy efficiency": "",
      "Total processing performance (bit-OP/s)": "5832000000000000.0",
      "Price-performance": "13393146050.889835",
      "ML OP/s": "729000000000000.0"
    },
    {
      "Hardware name": "NVIDIA A800 PCIe 40 GB",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2022-11-08",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "311800000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "",
      "Memory bandwidth (byte/s)": "1560000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "250.0",
      "FP64 (double precision) performance (FLOP/s)": "9746000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "19490000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "155900000000000.0",
      "FP16 (half precision) performance (FLOP/s)": "77970000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "826.0",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "1410.0",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "5120.0",
      "Tensor cores": "432.0",
      "Process size (nm)": "7.0",
      "Transistors (millions)": "54200.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/a800-pcie-40-gb.c3964\nhttps://www.techpowerup.com/gpu-specs/a800-pcie-80-gb.c3965\nhttps://www.techpowerup.com/gpu-specs/a800-sxm4-80-gb.c3966",
      "Source for the price": "",
      "ML models": "FLM-101B,Skywork-13B,DreamLLM,BGE-M3 Embedding,OmniGen,ProSST,Baichuan 2-7B,YaYi 2.0,Baichuan1-7B,DiffPepBuilder,DiLoCoX (Qwen1.5-107B on WT-103)",
      "Last modified": "2025-07-21 09:20:46+00:00",
      "Max performance": "311800000000000.0",
      "Energy efficiency": "1247200000000.0",
      "Total processing performance (bit-OP/s)": "4988800000000000.0",
      "Price-performance": "",
      "ML OP/s": "311800000000000.0"
    },
    {
      "Hardware name": "AMD Radeon RX 7900 XTX",
      "Manufacturer": "AMD",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2022-11-03",
      "Release price (USD)": "999.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "24000000000.0",
      "Memory bandwidth (byte/s)": "960000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "335.0",
      "FP64 (double precision) performance (FLOP/s)": "1919000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "61420000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "122800000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "529.0",
      "Base clock (MHz)": "1860.0",
      "Boost clock (MHz)": "2499.0",
      "Memory clock (MHz)": "2500.0",
      "Memory bus (bit)": "384.0",
      "Tensor cores": "96.0",
      "Process size (nm)": "",
      "Transistors (millions)": "57700.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/radeon-rx-7900-xtx.c3941",
      "Source for the price": "https://www.pcgamer.com/amd-radeon-rx-7900-xtx-xt-price-release-date-specs/",
      "ML models": "",
      "Last modified": "2025-07-21 09:20:46+00:00",
      "Max performance": "122800000000000.0",
      "Energy efficiency": "366567164179.1045",
      "Total processing performance (bit-OP/s)": "1965440000000000.0",
      "Price-performance": "122922922922.92291",
      "ML OP/s": "122800000000000.0"
    },
    {
      "Hardware name": "NVIDIA A800 PCIe 80 GB",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2022-11-03",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "312000000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "",
      "Memory bandwidth (byte/s)": "1940000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "250.0",
      "FP64 (double precision) performance (FLOP/s)": "9746000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "19490000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "156000000000000.0",
      "FP16 (half precision) performance (FLOP/s)": "77970000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "624000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "826.0",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "1410.0",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "432.0",
      "Process size (nm)": "7.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/a800-pcie-80-gb.c3965\nhttps://www.unixcloud.ltd/product/nvidia-a800/# ",
      "Source for the price": "",
      "ML models": "Xinyu,MindLink-72B,MindLink-32B,XuanYuan-70B,Aquila2\u201170B\u2011Expr",
      "Last modified": "2026-01-06 16:01:16+00:00",
      "Max performance": "624000000000000.0",
      "Energy efficiency": "2496000000000.0",
      "Total processing performance (bit-OP/s)": "4988800000000000.0",
      "Price-performance": "",
      "ML OP/s": "624000000000000.0"
    },
    {
      "Hardware name": "Amazon Trainium1",
      "Manufacturer": "Amazon AWS",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2022-10-15",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "190000000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "",
      "Memory bandwidth (byte/s)": "820000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "47500000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "190000000000000.0",
      "FP16 (half precision) performance (FLOP/s)": "190000000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "420000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "",
      "Transistors (millions)": "",
      "Link to datasheet": "https://web.archive.org/web/20230518023215/https://awsdocs-neuron.readthedocs-hosted.com/en/v2.9.1/general/arch/neuron-hardware/trainium.html",
      "Source for the price": "",
      "ML models": "nekomata-14b,Amazon Nova Canvas,Amazon Nova Reel,Amazon Nova Micro,Amazon Nova Lite,Amazon Nova Pro",
      "Last modified": "2026-01-08 16:34:08+00:00",
      "Max performance": "420000000000000.0",
      "Energy efficiency": "",
      "Total processing performance (bit-OP/s)": "3610000000000000.0",
      "Price-performance": "",
      "ML OP/s": "420000000000000.0"
    },
    {
      "Hardware name": "GroqChip LPU v1",
      "Manufacturer": "",
      "Type": "LPU",
      "Notes": "",
      "Release date": "2022-10-15",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "230000000.0",
      "Memory bandwidth (byte/s)": "80000000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "215.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "188000000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "750000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "900.0",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "14.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://groq.com/wp-content/uploads/2024/08/GroqChip%E2%84%A2-Processor-Product-Brief-v1.7.pdf",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:21:01+00:00",
      "Max performance": "750000000000000.0",
      "Energy efficiency": "3488372093023.256",
      "Total processing performance (bit-OP/s)": "6000000000000000.0",
      "Price-performance": "",
      "ML OP/s": "750000000000000.0"
    },
    {
      "Hardware name": "NVIDIA L40",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2022-10-13",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "181000000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "48000000000.0",
      "Memory bandwidth (byte/s)": "864000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "300.0",
      "FP64 (double precision) performance (FLOP/s)": "1414000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "90520000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "90520000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "362000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "608.0",
      "Base clock (MHz)": "735.0",
      "Boost clock (MHz)": "2490.0",
      "Memory clock (MHz)": "2250.0",
      "Memory bus (bit)": "384.0",
      "Tensor cores": "568.0",
      "Process size (nm)": "",
      "Transistors (millions)": "76300.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/l40.c3959; https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/support-guide/NVIDIA-L40-Datasheet-January-2023.pdf",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:21:01+00:00",
      "Max performance": "362000000000000.0",
      "Energy efficiency": "1206666666666.6667",
      "Total processing performance (bit-OP/s)": "2896640000000000.0",
      "Price-performance": "",
      "ML OP/s": "362000000000000.0"
    },
    {
      "Hardware name": "NVIDIA H100 PCIe",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "Listed performance figures are without sparsity.",
      "Release date": "2022-09-20",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "756000000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "80000000000.0",
      "Memory bandwidth (byte/s)": "2040000000000.0",
      "Intranode bandwidth (byte/s)": "600000000000.0",
      "Internode bandwidth (bit/s)": "400000000000.0",
      "TDP (W)": "350.0",
      "FP64 (double precision) performance (FLOP/s)": "25610000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "51220000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "378000000000000.0",
      "FP16 (half precision) performance (FLOP/s)": "204900000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "814.0",
      "Base clock (MHz)": "1095.0",
      "Boost clock (MHz)": "1755.0",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "4.0",
      "Transistors (millions)": "80000.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/h100-pcie.c3899",
      "Source for the price": "",
      "ML models": "SO3LR,RNAdiffusion,RDT-1B,SmolLM-1.7B,FoxBrain,NatureLM-audio",
      "Last modified": "2025-11-13 14:52:16+00:00",
      "Max performance": "756000000000000.0",
      "Energy efficiency": "2160000000000.0",
      "Total processing performance (bit-OP/s)": "1.2096e+16",
      "Price-performance": "",
      "ML OP/s": "756000000000000.0"
    },
    {
      "Hardware name": "NVIDIA GeForce RTX 4090",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2022-09-20",
      "Release price (USD)": "1599.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "330000000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "24000000000.0",
      "Memory bandwidth (byte/s)": "1008000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "450.0",
      "FP64 (double precision) performance (FLOP/s)": "1290000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "82600000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "82600000000000.0",
      "FP16 (half precision) performance (FLOP/s)": "82600000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "660600000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "608.0",
      "Base clock (MHz)": "2235.0",
      "Boost clock (MHz)": "2520.0",
      "Memory clock (MHz)": "1313.0",
      "Memory bus (bit)": "384.0",
      "Tensor cores": "512.0",
      "Process size (nm)": "",
      "Transistors (millions)": "76300.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/geforce-rtx-4090.c3889",
      "Source for the price": "https://www.pcgamesn.com/nvidia/rtx-4090-release-date-price-spec-and-benchmarks",
      "ML models": "Diamond,JetFire (GPT2-LARGE),Rethinking Molecular Design: Integrating Latent Variable and Auto-Regressive Models for Goal Directed Generation,AtomFlow,OmniGenome,ManiGaussian,Fish-Speech 1.4",
      "Last modified": "2025-07-21 09:21:02+00:00",
      "Max performance": "660600000000000.0",
      "Energy efficiency": "1468000000000.0",
      "Total processing performance (bit-OP/s)": "5284800000000000.0",
      "Price-performance": "413133208255.1595",
      "ML OP/s": "660600000000000.0"
    },
    {
      "Hardware name": "NVIDIA H100 SXM5 80GB",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "Listed performance figures are without sparsity.",
      "Release date": "2022-09-20",
      "Release price (USD)": "33600.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "989400000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "80000000000.0",
      "Memory bandwidth (byte/s)": "3350000000000.0",
      "Intranode bandwidth (byte/s)": "900000000000.0",
      "Internode bandwidth (bit/s)": "400000000000.0",
      "TDP (W)": "700.0",
      "FP64 (double precision) performance (FLOP/s)": "33450000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "66910000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "494700000000000.0",
      "FP16 (half precision) performance (FLOP/s)": "133800000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "1979000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "814.0",
      "Base clock (MHz)": "1665.0",
      "Boost clock (MHz)": "1980.0",
      "Memory clock (MHz)": "1313.0",
      "Memory bus (bit)": "5120.0",
      "Tensor cores": "528.0",
      "Process size (nm)": "5.0",
      "Transistors (millions)": "80000.0",
      "Link to datasheet": "https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-datasheet; https://www.techpowerup.com/gpu-specs/h100-sxm5.c3900",
      "Source for the price": "No official MSRP. \n\nIndustry analyst in May 2023 reports DGX servers with 8xH100 were sold for $269k, or $33,600 per GPU. In bill of materials, 8 GPUs + baseboard are $195k, or $24.4k per GPU. It is possible that this reflects bulk/hyperscaler pricing\n\nhttps://web.archive.org/web/20230529125834/https://www.semianalysis.com/p/ai-server-cost-analysis-memory-is \n\nOne early listing had DGX at 370k EUR, or ~44.5k USD per card.\n\nhttps://web.archive.org/web/20220929115124/https://www.deltacomputer.com/nvidia-dgx-h100-640gb.html ",
      "ML models": "Inflection-1,Inflection-2,MPT-30B,StarCoder 2 7B,Llama 3-70B,DBRX,Evo,Inflection-2.5,Reka Core,Reka Flash,Stable LM 2 12B,Llama 3-8B,phi-3-mini 3.8B,Jamba,Nemotron-4 340B,Multi-Token Prediction 7B,Multi-Token Prediction 13B,Llama 3.1-405B,Mamba2-Hybrid,EXAONE 3.0,Jamba 1.5-Large,OLMoE,Falcon Mamba,Pharia-1-LLM-7B,Llama 3.2 1B,Llama 3.2 3B,Llama 3.2 11B,Llama 3.2 90B,Movie Gen Video,Movie Gen Audio,phi-3.5-mini,phi-3.5-Vision,Stable Video 4D (SV4D),Yi-Lightning,Mistral Large,NVLM-D 72B,NVLM-H 72B,Luma Dream Machine,GRITLM 8x7B,Nemotron-4 15B,Tulu 3 (T\u00fclu 3) 70B,Hermes 3 405B,Hermes 3 70B,Hermes 3 8B,DISTRO,Allegro,Zephyr 141B-A39B\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t,NVLM-X 72B,Llama 3.3 70B,Granite 3.0 8B,Granite 3.0 2B,OLMo 2 Furious 7B,OLMo 2 Furious 13B,LLaVA-CoT,Cosmos-1.0-\nDiffusion-14B Video2World,TAIWAN-LLM 13B,TAIWAN-LLM 7B,TAIDE-LX-7B,Llama-3-Taiwan-70B\n,SimPO,Yi-VL-34B,s1,NVILA 15B,NVILA 8B,Grok 3,Grok-2,Llama SEA-LION V2 8B,SEA-LION V3 Gemma2 9B,SEA-LION V3 Llama3.1 70B,EXAONE Deep 2.4B,EXAONE Deep 7.8B,EXAONE Deep 32B,Ovis-7B,Molmo 72B,Phi-4,Phi-3.5-MoE,Seaweed-7B,dnaGrinder,OpenELM-450M,OpenELM-3B,Nemotron-H 56B,MULAN,OpenPhenom-S/16,Quiet-STaR,Granite 3.1 2B,Granite 3.2 2B ,Granite 3.1 8B,Granite 3.2 8B,LongVU,ProtChatGPT,Zamba2-7B,ProteinStructureTransformer,Phi-4-Reasoning,Phi-4-Reasoning-plus,LLama 3..2 Typhoon 2 1B,LLama 3.2 Typhoon 2 3B,Typhoon 2 7B,Llama 3.1 Typhoon 2 8B,Llama 3.1 Typhoon 2 70B,Pleias-RAG-1B,Pleias-RAG-350m,Pleias 1.0 1.2B,Pleias 1.0 350m,Meissonic,GR00T N1 2B,Eagle 2,Gemma2 9B CPT Sahabat-AI,DCLM 7B,GAIA-2,rStar-Math (Qwen2.5-Math-7B base),rStar-Math (Qwen2-Math-7B base),Amazon Nova Canvas,Amazon Nova Reel,Amazon Nova Micro,Amazon Nova Lite,Amazon Nova Pro,Tulu 3 8B,Tulu 3 405B,OLMo 2 32B,Megrez-3B-Omni,Reka Edge,OpenBioLLM-Llama3-70B,OpenBioLLM-Llama3-8B,Earth-2 (cBottle-SR),Fish-Speech 1.4,Llama 4 Behemoth (preview),Mercury Coder Small,Mercury Coder Mini,AlphaGenome,gpt-oss-120b,gpt-oss-20b,Falcon3-7B,Falcon-H1,OpenThaiGPT 1.6 /  OTG-1.6 (72B),OpenThaiGPT R1 32b / OTG-R1 (32B),XY-LENTXL,MolmoAct-7B-D,Microsoft MAI-1,Stable Video 4D 2.0 (SV4D 2.0),Granite-Docling,ether0,Smaug-72B,Smaug-34B,Cosmos-Predict1-14b-Video2World,Cosmos-Predict1-7b-Video2World,Cosmos-Transfer1-7B,Trillion-7B,Cosmos-Predict2.5 2B,Cosmos-Predict2.5-14B,Tiny Recursive Model (TRM-Att),OpenDiLoCo 1.1B",
      "Last modified": "2025-11-14 18:27:23+00:00",
      "Max performance": "1979000000000000.0",
      "Energy efficiency": "2827142857142.857",
      "Total processing performance (bit-OP/s)": "1.5832e+16",
      "Price-performance": "58898809523.809525",
      "ML OP/s": "1979000000000000.0"
    },
    {
      "Hardware name": "NVIDIA GeForce RTX 4080",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2022-09-20",
      "Release price (USD)": "1199.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "16000000000.0",
      "Memory bandwidth (byte/s)": "716800000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "320.0",
      "FP64 (double precision) performance (FLOP/s)": "761500000000.0",
      "FP32 (single precision) performance (FLOP/s)": "48740000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "48740000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "390000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "379.0",
      "Base clock (MHz)": "2205.0",
      "Boost clock (MHz)": "2505.0",
      "Memory clock (MHz)": "1400.0",
      "Memory bus (bit)": "256.0",
      "Tensor cores": "304.0",
      "Process size (nm)": "",
      "Transistors (millions)": "45900.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/geforce-rtx-4080.c3888",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:21:02+00:00",
      "Max performance": "390000000000000.0",
      "Energy efficiency": "1218750000000.0",
      "Total processing performance (bit-OP/s)": "3120000000000000.0",
      "Price-performance": "325271059216.01337",
      "ML OP/s": "390000000000000.0"
    },
    {
      "Hardware name": "Biren BR100",
      "Manufacturer": "Biren",
      "Type": "GPGPU",
      "Notes": "",
      "Release date": "2022-08-22",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "1024000000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "64000000000.0",
      "Memory bandwidth (byte/s)": "1640000000000.0",
      "Intranode bandwidth (byte/s)": "412000000000.0",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "550.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "256000000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "512000000000000.0",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "2048000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "1074.0",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "7.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://web.archive.org/web/20221004073838/https://www.birentech.com/product_details/1005557637772464128.html; https://hc34.hotchips.org/assets/program/conference/day1/GPU%20HPC/HC2022.BirenTech.MikeHong.LingjieXu.v01.pdf",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-10-10 16:01:02+00:00",
      "Max performance": "2048000000000000.0",
      "Energy efficiency": "3723636363636.364",
      "Total processing performance (bit-OP/s)": "1.6384e+16",
      "Price-performance": "",
      "ML OP/s": "2048000000000000.0"
    },
    {
      "Hardware name": "NVIDIA A800 SXM",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2022-08-11",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "311800000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "",
      "Memory bandwidth (byte/s)": "2040000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "400.0",
      "FP64 (double precision) performance (FLOP/s)": "9746000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "19490000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "155900000000000.0",
      "FP16 (half precision) performance (FLOP/s)": "77970000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "826.0",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "1410.0",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "432.0",
      "Process size (nm)": "7.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/a800-sxm4-80-gb.c3966",
      "Source for the price": "",
      "ML models": "Uni-Med,Oryx 34B,Xingrui AI (\u661f\u777fAI),SkyReels-A1,SkyReels-V2,MiniCPM-4-8B",
      "Last modified": "2025-09-18 09:20:22+00:00",
      "Max performance": "311800000000000.0",
      "Energy efficiency": "779500000000.0",
      "Total processing performance (bit-OP/s)": "4988800000000000.0",
      "Price-performance": "",
      "ML OP/s": "311800000000000.0"
    },
    {
      "Hardware name": "MT-3000",
      "Manufacturer": "National University of Defense Technology",
      "Type": "Hybrid CPU",
      "Notes": "",
      "Release date": "2022-07-01",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "128000000000.0",
      "Memory bandwidth (byte/s)": "1432000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "256.0",
      "FP64 (double precision) performance (FLOP/s)": "12870000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "",
      "Transistors (millions)": "",
      "Link to datasheet": "https://link.springer.com/article/10.1007/s42514-022-00095-y",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:21:04+00:00",
      "Max performance": "12870000000000.0",
      "Energy efficiency": "50273437500.0",
      "Total processing performance (bit-OP/s)": "823680000000000.0",
      "Price-performance": "",
      "ML OP/s": "12870000000000.0"
    },
    {
      "Hardware name": "Intel Habana Gaudi2",
      "Manufacturer": "Intel",
      "Type": "",
      "Notes": "",
      "Release date": "2022-05-10",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "450000000000000.0",
      "FP8 performance (FLOP/s)": "865000000000000.0",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "96000000000.0",
      "Memory bandwidth (byte/s)": "2450000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "600.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "826.0",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "24.0",
      "Process size (nm)": "7.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://perma.cc/9SR3-HX6Q",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:21:05+00:00",
      "Max performance": "450000000000000.0",
      "Energy efficiency": "750000000000.0",
      "Total processing performance (bit-OP/s)": "7200000000000000.0",
      "Price-performance": "",
      "ML OP/s": "865000000000000.0"
    },
    {
      "Hardware name": "NVIDIA GeForce RTX 3090 Ti",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2022-03-29",
      "Release price (USD)": "1999.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "160000000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "24000000000.0",
      "Memory bandwidth (byte/s)": "1008000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "450.0",
      "FP64 (double precision) performance (FLOP/s)": "625000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "40000000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "40000000000000.0",
      "FP16 (half precision) performance (FLOP/s)": "40000000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "320000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "Samsung",
      "Die Size (mm^2)": "628.0",
      "Base clock (MHz)": "1560.0",
      "Boost clock (MHz)": "1860.0",
      "Memory clock (MHz)": "1313.0",
      "Memory bus (bit)": "384.0",
      "Tensor cores": "336.0",
      "Process size (nm)": "8.0",
      "Transistors (millions)": "28300.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/geforce-rtx-3090-ti.c3829",
      "Source for the price": "https://www.wikiwand.com/en/GeForce_30_series",
      "ML models": "YOLOv10-X,pKALM,FMMformer (2-kernel fast weight + Band20),Lightweight Fine-tuning a Pretrained Protein Language Model for Protein Secondary,ControlNet (SDv2)",
      "Last modified": "2025-09-29 14:33:13+00:00",
      "Max performance": "320000000000000.0",
      "Energy efficiency": "711111111111.1111",
      "Total processing performance (bit-OP/s)": "2560000000000000.0",
      "Price-performance": "160080040020.01",
      "ML OP/s": "320000000000000.0"
    },
    {
      "Hardware name": "Cambricon MLU370-X8",
      "Manufacturer": "Cambricon",
      "Type": "",
      "Notes": "https://www.cambricon.com/index.php?m=content&c=index&a=lists&catid=406\nhttps://archive.ph/5cIeg",
      "Release date": "2022-03-21",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "48000000000.0",
      "Memory bandwidth (byte/s)": "614400000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "250.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "24000000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "96000000000000.0",
      "INT16 performance (OP/s)": "128000000000000.0",
      "INT8 performance (OP/s)": "256000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "7.0",
      "Transistors (millions)": "",
      "Link to datasheet": "",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:21:08+00:00",
      "Max performance": "256000000000000.0",
      "Energy efficiency": "1024000000000.0",
      "Total processing performance (bit-OP/s)": "2048000000000000.0",
      "Price-performance": "",
      "ML OP/s": "256000000000000.0"
    },
    {
      "Hardware name": "Huawei Ascend 910B",
      "Manufacturer": "Huawei",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2022-03-17",
      "Release price (USD)": "16854.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "32000000000.0",
      "Memory bandwidth (byte/s)": "1228000000000.0",
      "Intranode bandwidth (byte/s)": "392000000000.0",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "400.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "94000000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "376000000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "SMIC",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "900.0",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "7.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://archive.ph/cF945\nhttps://cset.georgetown.edu/publication/pushing-the-limits-huaweis-ai-chip-tests-u-s-export-controls/#:~:text=Ascend%20910%20models%20range%20from,specially%20designed%20for%20AI%20operations\nhttps://www.trendforce.com/news/2024/06/11/news-huaweis-self-developed-ai-chip-challenges-nvidia-boasting-its-ascend-910b-to-be-equal-in-match-with-a100/\n\ndifferent sources report different performance (376 TFLOPS vs 220 TFLOPS vs 320 TFLOPS)\n\nas for release date - the earliest mentioning I found is March, 17 2022 (https://r.huaweistatic.com/s/ascendstatic/lst/files/pdf/PR425KI.pdf)\n\nenergy source: https://arthurchiao.art/blog/gpu-data-sheets/\n",
      "Source for the price": "https://www.trendforce.com/news/2024/06/11/news-huaweis-self-developed-ai-chip-challenges-nvidia-boasting-its-ascend-910b-to-be-equal-in-match-with-a100/\n120,000 yuan = 16800 USD at mid-2023 rate of 0.14 yuan per dollar.",
      "ML models": "Pangu Ultra,Pangu Ultra MoE,Spark-X1",
      "Last modified": "2025-10-10 16:01:02+00:00",
      "Max performance": "376000000000000.0",
      "Energy efficiency": "940000000000.0",
      "Total processing performance (bit-OP/s)": "6016000000000000.0",
      "Price-performance": "22309244096.35695",
      "ML OP/s": "376000000000000.0"
    },
    {
      "Hardware name": "Baidu Kunlun R100 (75W)",
      "Manufacturer": "Kunlunxin Baidu",
      "Type": "XPU-R",
      "Notes": "https://archive.ph/XFJOZ",
      "Release date": "2022-01-01",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "12000000000.0",
      "Memory bandwidth (byte/s)": "384000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "75.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "18000000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "64000000000000.0",
      "INT16 performance (OP/s)": "64000000000000.0",
      "INT8 performance (OP/s)": "128000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "",
      "Transistors (millions)": "",
      "Link to datasheet": "",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:21:11+00:00",
      "Max performance": "128000000000000.0",
      "Energy efficiency": "1706666666666.6667",
      "Total processing performance (bit-OP/s)": "1024000000000000.0",
      "Price-performance": "",
      "ML OP/s": "128000000000000.0"
    },
    {
      "Hardware name": "Baidu Kunlun R100 (100W)",
      "Manufacturer": "Kunlunxin Baidu",
      "Type": "XPU-R",
      "Notes": "https://archive.ph/XFJOZ",
      "Release date": "2022-01-01",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "12000000000.0",
      "Memory bandwidth (byte/s)": "384000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "100.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "20000000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "85000000000000.0",
      "INT16 performance (OP/s)": "85000000000000.0",
      "INT8 performance (OP/s)": "170000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "",
      "Transistors (millions)": "",
      "Link to datasheet": "",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:21:11+00:00",
      "Max performance": "170000000000000.0",
      "Energy efficiency": "1700000000000.0",
      "Total processing performance (bit-OP/s)": "1360000000000000.0",
      "Price-performance": "",
      "ML OP/s": "170000000000000.0"
    },
    {
      "Hardware name": "Baidu Kunlun R200 (150W)",
      "Manufacturer": "Kunlunxin Baidu",
      "Type": "XPU-R",
      "Notes": "https://zhuanlan.zhihu.com/p/646793342",
      "Release date": "2022-01-01",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "16000000000.0",
      "Memory bandwidth (byte/s)": "512000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "150.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "32000000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "128000000000000.0",
      "INT16 performance (OP/s)": "128000000000000.0",
      "INT8 performance (OP/s)": "256000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "7.0",
      "Transistors (millions)": "",
      "Link to datasheet": "",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:21:12+00:00",
      "Max performance": "256000000000000.0",
      "Energy efficiency": "1706666666666.6667",
      "Total processing performance (bit-OP/s)": "2048000000000000.0",
      "Price-performance": "",
      "ML OP/s": "256000000000000.0"
    },
    {
      "Hardware name": "Baidu Kunlun R200-8F (160W)",
      "Manufacturer": "Kunlunxin Baidu",
      "Type": "XPU-R",
      "Notes": "https://zhuanlan.zhihu.com/p/646793342",
      "Release date": "2022-01-01",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "32000000000.0",
      "Memory bandwidth (byte/s)": "512000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "160.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "32000000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "128000000000000.0",
      "INT16 performance (OP/s)": "128000000000000.0",
      "INT8 performance (OP/s)": "256000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "7.0",
      "Transistors (millions)": "",
      "Link to datasheet": "",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:21:12+00:00",
      "Max performance": "256000000000000.0",
      "Energy efficiency": "1600000000000.0",
      "Total processing performance (bit-OP/s)": "2048000000000000.0",
      "Price-performance": "",
      "ML OP/s": "256000000000000.0"
    },
    {
      "Hardware name": "AMD Radeon Instinct MI250X",
      "Manufacturer": "AMD",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2021-11-08",
      "Release price (USD)": "14500.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "383000000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "128000000000.0",
      "Memory bandwidth (byte/s)": "3280000000000.0",
      "Intranode bandwidth (byte/s)": "800000000000.0",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "500.0",
      "FP64 (double precision) performance (FLOP/s)": "47870000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "47870000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "95700000000000.0",
      "FP16 (half precision) performance (FLOP/s)": "383000000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "383000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "724.0",
      "Base clock (MHz)": "1000.0",
      "Boost clock (MHz)": "1700.0",
      "Memory clock (MHz)": "1600.0",
      "Memory bus (bit)": "8192.0",
      "Tensor cores": "",
      "Process size (nm)": "6.0",
      "Transistors (millions)": "58200.0",
      "Link to datasheet": "https://www.amd.com/en/products/accelerators/instinct/mi200/mi250x.html\nhttps://www.techpowerup.com/gpu-specs/radeon-instinct-mi250x.c3837",
      "Source for the price": "https://www.nextplatform.com/2021/12/06/stacking-up-amd-mi200-versus-nvidia-a100-compute-engines/",
      "ML models": "BLUUMI,Poro 34B,Viking,OLMo-7B,ColPali,OLMo-1B",
      "Last modified": "2025-10-10 16:01:02+00:00",
      "Max performance": "383000000000000.0",
      "Energy efficiency": "766000000000.0",
      "Total processing performance (bit-OP/s)": "6128000000000000.0",
      "Price-performance": "26413793103.44828",
      "ML OP/s": "383000000000000.0"
    },
    {
      "Hardware name": "Cambricon MLU370-S4/S8",
      "Manufacturer": "Cambricon",
      "Type": "",
      "Notes": "https://www.cambricon.com/index.php?m=content&c=index&a=lists&catid=365\nhttps://archive.ph/014bu",
      "Release date": "2021-11-03",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "24000000000.0",
      "Memory bandwidth (byte/s)": "307200000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "75.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "18000000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "72000000000000.0",
      "INT16 performance (OP/s)": "96000000000000.0",
      "INT8 performance (OP/s)": "192000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "7.0",
      "Transistors (millions)": "",
      "Link to datasheet": "",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:21:17+00:00",
      "Max performance": "192000000000000.0",
      "Energy efficiency": "2560000000000.0",
      "Total processing performance (bit-OP/s)": "1536000000000000.0",
      "Price-performance": "",
      "ML OP/s": "192000000000000.0"
    },
    {
      "Hardware name": "Cambricon MLU370-X4",
      "Manufacturer": "Cambricon",
      "Type": "",
      "Notes": "https://www.cambricon.com/index.php?m=content&c=index&a=lists&catid=371\nhttps://archive.ph/qFoAQ",
      "Release date": "2021-11-03",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "24000000000.0",
      "Memory bandwidth (byte/s)": "307200000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "150.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "24000000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "96000000000000.0",
      "INT16 performance (OP/s)": "128000000000000.0",
      "INT8 performance (OP/s)": "256000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "7.0",
      "Transistors (millions)": "",
      "Link to datasheet": "",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:21:17+00:00",
      "Max performance": "256000000000000.0",
      "Energy efficiency": "1706666666666.6667",
      "Total processing performance (bit-OP/s)": "2048000000000000.0",
      "Price-performance": "",
      "ML OP/s": "256000000000000.0"
    },
    {
      "Hardware name": "Cambricon MLU370-S8",
      "Manufacturer": "Cambricon",
      "Type": "",
      "Notes": "https://www.cambricon.com/index.php?m=content&c=index&a=lists&catid=365\nhttps://archive.ph/014bu",
      "Release date": "2021-11-03",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "48000000000.0",
      "Memory bandwidth (byte/s)": "307200000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "75.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "18000000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "72000000000000.0",
      "INT16 performance (OP/s)": "96000000000000.0",
      "INT8 performance (OP/s)": "192000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "7.0",
      "Transistors (millions)": "",
      "Link to datasheet": "",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:21:17+00:00",
      "Max performance": "192000000000000.0",
      "Energy efficiency": "2560000000000.0",
      "Total processing performance (bit-OP/s)": "1536000000000000.0",
      "Price-performance": "",
      "ML OP/s": "192000000000000.0"
    },
    {
      "Hardware name": "Cambricon MLU370-S4",
      "Manufacturer": "Cambricon",
      "Type": "",
      "Notes": "https://www.cambricon.com/index.php?m=content&c=index&a=lists&catid=365\nhttps://archive.ph/014bu",
      "Release date": "2021-11-03",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "24000000000.0",
      "Memory bandwidth (byte/s)": "307200000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "75.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "18000000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "72000000000000.0",
      "INT16 performance (OP/s)": "96000000000000.0",
      "INT8 performance (OP/s)": "192000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "7.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://perma.cc/AS5M-QVCE",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2026-01-10 13:53:21+00:00",
      "Max performance": "192000000000000.0",
      "Energy efficiency": "2560000000000.0",
      "Total processing performance (bit-OP/s)": "1536000000000000.0",
      "Price-performance": "",
      "ML OP/s": "192000000000000.0"
    },
    {
      "Hardware name": "Tesla D1 Dojo",
      "Manufacturer": "Tesla",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2021-08-21",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "362000000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "",
      "Memory bandwidth (byte/s)": "10000000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "400.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "22600000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "7.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://semianalysis.com/2021/08/25/the-tesla-dojo-chip-is-impressive/",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:21:19+00:00",
      "Max performance": "362000000000000.0",
      "Energy efficiency": "905000000000.0",
      "Total processing performance (bit-OP/s)": "5792000000000000.0",
      "Price-performance": "",
      "ML OP/s": "362000000000000.0"
    },
    {
      "Hardware name": "Baidu Kunlun II",
      "Manufacturer": "Kunlunxin Baidu",
      "Type": "XPU-R",
      "Notes": "https://zhuanlan.zhihu.com/p/646793342\nhttps://www.tomshardware.com/news/baidu-unveils-kunlun-ii-processor-for-ai\nhttps://www.hpcwire.com/2021/08/18/baidu-brain-7-0-ai-platform-announced-baidu-kunlun-ii-ai-chips-now-in-mass-production/",
      "Release date": "2021-08-18",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "32000000000.0",
      "Memory bandwidth (byte/s)": "512000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "150.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "128000000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "256000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "7.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://web.archive.org/web/20250505083031/https://news.qq.com/rain/a/20221129A02ZKE00",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:21:19+00:00",
      "Max performance": "256000000000000.0",
      "Energy efficiency": "1706666666666.6667",
      "Total processing performance (bit-OP/s)": "2048000000000000.0",
      "Price-performance": "",
      "ML OP/s": "256000000000000.0"
    },
    {
      "Hardware name": "Shensuan-1",
      "Manufacturer": "Hygon",
      "Type": "DCU (GPGPU)",
      "Notes": "https://pdf.dfcfw.com/pdf/H3_AP202405141633079130_1.pdf?1715720114000.pdf",
      "Release date": "2021-07-01",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "32000000000.0",
      "Memory bandwidth (byte/s)": "1024000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "350.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "24500000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "7.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://hk-official.cmbi.info/upload/b678e2ea-ba52-47b7-bc97-893d2bfdd36d.pdf",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:21:22+00:00",
      "Max performance": "24500000000000.0",
      "Energy efficiency": "70000000000.0",
      "Total processing performance (bit-OP/s)": "392000000000000.0",
      "Price-performance": "",
      "ML OP/s": "24500000000000.0"
    },
    {
      "Hardware name": "Google TPU v4",
      "Manufacturer": "Google",
      "Type": "TPU",
      "Notes": "Google does not officially state TPUv4\u2019s TDP. \n\nIn the blog post announcing TPUv7 (https://blog.google/products/google-cloud/ironwood-tpu-age-of-inference/), Google disclosed that TPUv4 is 4.9x more efficient than the TPUv2 in peak FLOP/s per TDP (measuring peak FLOP/s as either FP8 or bf16 FLOP/s when FP8 is not supported). \n\nTPUv2 does 46 TFLOP/s peak and has a TDP of 280 W.\n\nTPUv4 does 275 TFLOP/s peak. So the TPUv4\u2019s TDP is (275 TFLOP/s  / 46 TFLOP/s) * 280 W / 4.9x efficiency = ~342 W. Note that this may be a slightly approximate figure due to rounding on Google\u2019s part. For example, applying this method to TPUv3 (using Google\u2019s stated efficiency improvement in the TPUv7 blog) yields a TDP of 416 W, lower than Google\u2019s disclosed TDP of 450 W for v3. We round to 340 W.\n\nGoogle also lists \"measured max power\" of 192 W: https://docs.cloud.google.com/tpu/docs/v4\nHowever, for TPUv3 the corresponding measured max of 262 W is lower than the v3's TDP of 450 W.",
      "Release date": "2021-05-20",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "275000000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "32000000000.0",
      "Memory bandwidth (byte/s)": "1200000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "340.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "275000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "7.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://cloud.google.com/tpu/docs/system-architecture-tpu-vm",
      "Source for the price": "https://technical.city/en/video/GeForce-GTX-580",
      "ML models": "Flamingo,GLaM,CodeGen-Mono 16.1B,Ankh_large,Cohere Command,CoCa,PaLI,BASIC-L,XGen-7B,PaLM 2,Minerva (540B),UniPi,PaLM (540B),Chinchilla,AlphaCode,OpenLLaMA-13B,Gemini 1.0 Ultra,GNoME for crystal discovery,U-PaLM (540B),Ankh_base,AudioLM,Parti,Flan-PaLM 540B,Flan-T5 11B,Aya,Unified-IO (XL),Cohere Command Light,ViT-22B,Gemini 1.0 Pro,UL2,Imagen,Gemini 1.5 Flash (May 2024),Gemma 2 9B,AFM-server,GenMS,Imagen 3,Med-Gemini-2D,Med-Gemini-3D,Med-Gemini-M 1.5,SigLIP 400M,SigLiT,Gemini 1.5 Flash 8B,Gemini 1.5 Pro,Gemma 3 12B,Gemini Robotics,LLPS,Octo-Base,Octo-Small,Cambrian-1-34B,Cambrian-1-13B,Cambrian-1-8B,ProtBFN,TxGemma 2B,TxGemma 9B,TxGemma 27B,SILC-S* (86M),Gemini Robotics-ER,Marin 8B,Primer (GPT-3 XL-like 1.9B),LIMoE-H/14,AlphaEarth Foundations (AEF),Gemini Robotics-ER 1.5,Gemini Robotics 1.5 ,C2S-Scale,Gemini 1.5 Flash (Sep 2024)",
      "Last modified": "2025-12-12 22:51:16+00:00",
      "Max performance": "275000000000000.0",
      "Energy efficiency": "808823529411.7646",
      "Total processing performance (bit-OP/s)": "4400000000000000.0",
      "Price-performance": "",
      "ML OP/s": "275000000000000.0"
    },
    {
      "Hardware name": "NVIDIA RTX A4000",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2021-04-12",
      "Release price (USD)": "1000.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "16000000000.0",
      "Memory bandwidth (byte/s)": "448000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "140.0",
      "FP64 (double precision) performance (FLOP/s)": "599000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "19170000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "19170000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "Samsung",
      "Die Size (mm^2)": "392.0",
      "Base clock (MHz)": "735.0",
      "Boost clock (MHz)": "1560.0",
      "Memory clock (MHz)": "1750.0",
      "Memory bus (bit)": "256.0",
      "Tensor cores": "192.0",
      "Process size (nm)": "8.0",
      "Transistors (millions)": "17400.0",
      "Link to datasheet": "techpowerup.com/gpu-specs/rtx-a4000.c3756",
      "Source for the price": "https://develop3d.com/hardware/nvidia-rtx-a4000-and-rtx-a5000-gpus-launch/",
      "ML models": "",
      "Last modified": "2025-07-21 09:21:30+00:00",
      "Max performance": "19170000000000.0",
      "Energy efficiency": "136928571428.57144",
      "Total processing performance (bit-OP/s)": "613440000000000.0",
      "Price-performance": "19170000000.0",
      "ML OP/s": "19170000000000.0"
    },
    {
      "Hardware name": "NVIDIA A10G",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2021-04-12",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "24000000000.0",
      "Memory bandwidth (byte/s)": "600200000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "150.0",
      "FP64 (double precision) performance (FLOP/s)": "985000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "31500000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "31500000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "Samsung",
      "Die Size (mm^2)": "628.0",
      "Base clock (MHz)": "1320.0",
      "Boost clock (MHz)": "1710.0",
      "Memory clock (MHz)": "1563.0",
      "Memory bus (bit)": "384.0",
      "Tensor cores": "288.0",
      "Process size (nm)": "8.0",
      "Transistors (millions)": "28300.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/a10g.c3798",
      "Source for the price": "",
      "ML models": "TimeGPT-1",
      "Last modified": "2025-07-21 09:21:27+00:00",
      "Max performance": "31500000000000.0",
      "Energy efficiency": "210000000000.0",
      "Total processing performance (bit-OP/s)": "1008000000000000.0",
      "Price-performance": "",
      "ML OP/s": "31500000000000.0"
    },
    {
      "Hardware name": "NVIDIA RTX A5000",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2021-04-12",
      "Release price (USD)": "2250.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "24000000000.0",
      "Memory bandwidth (byte/s)": "768000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "230.0",
      "FP64 (double precision) performance (FLOP/s)": "867800000000.0",
      "FP32 (single precision) performance (FLOP/s)": "27770000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "27770000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "Samsung",
      "Die Size (mm^2)": "628.0",
      "Base clock (MHz)": "1170.0",
      "Boost clock (MHz)": "1695.0",
      "Memory clock (MHz)": "2000.0",
      "Memory bus (bit)": "384.0",
      "Tensor cores": "256.0",
      "Process size (nm)": "8.0",
      "Transistors (millions)": "28300.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/rtx-a5000.c3748",
      "Source for the price": "https://develop3d.com/hardware/nvidia-rtx-a4000-and-rtx-a5000-gpus-launch/",
      "ML models": "Refact-1.6B,BiomedGPT (182M),GITIII",
      "Last modified": "2025-07-21 09:21:31+00:00",
      "Max performance": "27770000000000.0",
      "Energy efficiency": "120739130434.7826",
      "Total processing performance (bit-OP/s)": "888640000000000.0",
      "Price-performance": "12342222222.22222",
      "ML OP/s": "27770000000000.0"
    },
    {
      "Hardware name": "NVIDIA A10 PCIe",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2021-04-12",
      "Release price (USD)": "2800.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "24000000000.0",
      "Memory bandwidth (byte/s)": "600200000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "150.0",
      "FP64 (double precision) performance (FLOP/s)": "976300000000.0",
      "FP32 (single precision) performance (FLOP/s)": "31240000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "31240000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "250000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "Samsung",
      "Die Size (mm^2)": "628.0",
      "Base clock (MHz)": "885.0",
      "Boost clock (MHz)": "1695.0",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "384.0",
      "Tensor cores": "288.0",
      "Process size (nm)": "8.0",
      "Transistors (millions)": "28300.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/a10-pcie.c3793#:~:text=The%20A10%20PCIe%20is%20a,card%20supports%20DirectX%2012%20Ultimate.",
      "Source for the price": "https://www.nextplatform.com/2021/04/15/nvidia-rounds-out-ampere-lineup-with-two-new-accelerators/",
      "ML models": "Goat-7B",
      "Last modified": "2025-07-21 09:21:27+00:00",
      "Max performance": "250000000000000.0",
      "Energy efficiency": "1666666666666.6667",
      "Total processing performance (bit-OP/s)": "2000000000000000.0",
      "Price-performance": "89285714285.71428",
      "ML OP/s": "250000000000000.0"
    },
    {
      "Hardware name": "Tianshu Zhixin Tiangai 100 (BI-V100)",
      "Manufacturer": "Iluvatar CoreX",
      "Type": "GPGPU",
      "Notes": "http://www.cloudhin.com/xk/showproduct.php?id=275\nhttps://archive.ph/TFXOA",
      "Release date": "2021-03-31",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "32000000000.0",
      "Memory bandwidth (byte/s)": "",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "250.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "32000000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "128000000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "256000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "7.0",
      "Transistors (millions)": "",
      "Link to datasheet": "",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:21:41+00:00",
      "Max performance": "256000000000000.0",
      "Energy efficiency": "1024000000000.0",
      "Total processing performance (bit-OP/s)": "2048000000000000.0",
      "Price-performance": "",
      "ML OP/s": "256000000000000.0"
    },
    {
      "Hardware name": "Cambricon MLU290-M5",
      "Manufacturer": "Cambricon",
      "Type": "",
      "Notes": "https://www.cambricon.com/index.php?m=content&c=index&a=lists&catid=340\nhttps://archive.ph/i32rD",
      "Release date": "2021-01-21",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "32000000000.0",
      "Memory bandwidth (byte/s)": "1228000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "350.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "256000000000000.0",
      "INT8 performance (OP/s)": "512000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "7.0",
      "Transistors (millions)": "",
      "Link to datasheet": "",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:22:08+00:00",
      "Max performance": "512000000000000.0",
      "Energy efficiency": "1462857142857.1428",
      "Total processing performance (bit-OP/s)": "4096000000000000.0",
      "Price-performance": "",
      "ML OP/s": "512000000000000.0"
    },
    {
      "Hardware name": "NVIDIA A100 SXM4 80 GB",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2020-11-16",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "312000000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "80000000000.0",
      "Memory bandwidth (byte/s)": "2039000000000.0",
      "Intranode bandwidth (byte/s)": "600000000000.0",
      "Internode bandwidth (bit/s)": "200000000000.0",
      "TDP (W)": "400.0",
      "FP64 (double precision) performance (FLOP/s)": "9746000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "19490000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "156000000000000.0",
      "FP16 (half precision) performance (FLOP/s)": "77970000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "624000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "826.0",
      "Base clock (MHz)": "1275.0",
      "Boost clock (MHz)": "1410.0",
      "Memory clock (MHz)": "1593.0",
      "Memory bus (bit)": "5120.0",
      "Tensor cores": "432.0",
      "Process size (nm)": "7.0",
      "Transistors (millions)": "54200.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/a100-sxm4-80-gb.c3746",
      "Source for the price": "",
      "ML models": "InternLM,Llama 2-70B,Galactica,StarCoder,Luminous-supreme,JIANG,Megatron-Turing NLG 530B,RWKV-4 14B,OPT-175B,Code Llama-34B,Llama 2-34B,CodeFuse-13B,XuanYuan 2.0,Llama 2-7B,Llama 2-13B,Fusion in Encoder,GGNN,OmegaPLM,Orca 2-13B,Code Llama-70B,MMS-1B,Llama Guard,Code Llama-7B,VideoMAE V2,Emu1 (BAAI),OpenFlamingo,StarCoder 2 3B,Volcano 13B,BLOOM-176B,NLLB,OPT-66B,A.X (Adot) 18B,Chameleon-34B,FragLlama: Next-fragment prediction for molecular design,YaART,Multi-Token Prediction 7B,Multi-Token Prediction 13B,Pharia-1-LLM-7B,LBSTER,Stable Video 3D (SV3D),Stable Video Diffusion,BELLE-LLaMA-EXT-7B,GRITLM 7B,MindEye2,MobileCLIP-B (LT),TiTok-L,Alpaca,PepPrCLIP,Amber,Code Llama-13B,StableLM - Zephyr 3B,Minitron 8B,Minitron 4B,Llama-3.1-Minitron-4B,F5-TTS, InternVL,SparseOPT-175B,Hybrid H3-2.7B,Phi-4-Multimodal,Phi-4 Mini,Transformer-XL + RMT,LLaVA-NeXT-32B-Qwen,OpenELM-270M,OpenELM-1.1B,OmniNA,Typhoon2-Vision ,HuaTuo,GPT3-2.7B (FlashAttention-2),Hyena 1.3B,Hyena-2 355M,Hyena-2 153M,HAMSTER VLM,VILA-13B,BFS-Prover,CollabLLM,Meditron-70B,Kokoro v1.0,Kokoro v0.19,Me Llama 70B,Me Llama 13B,UniDiffuser (\u591a\u6a21\u6001\u5927\u6a21\u578b),Baize-v2-13B (\u767d\u6cfd),SDXL-Lightning,ChunkLlama2-13B",
      "Last modified": "2025-11-04 01:45:50+00:00",
      "Max performance": "624000000000000.0",
      "Energy efficiency": "1560000000000.0",
      "Total processing performance (bit-OP/s)": "4992000000000000.0",
      "Price-performance": "",
      "ML OP/s": "624000000000000.0"
    },
    {
      "Hardware name": "NVIDIA RTX A6000",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2020-10-05",
      "Release price (USD)": "4649.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "48000000000.0",
      "Memory bandwidth (byte/s)": "768000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "300.0",
      "FP64 (double precision) performance (FLOP/s)": "1210000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "38700000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "38700000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "Samsung",
      "Die Size (mm^2)": "628.0",
      "Base clock (MHz)": "1410.0",
      "Boost clock (MHz)": "1800.0",
      "Memory clock (MHz)": "2000.0",
      "Memory bus (bit)": "384.0",
      "Tensor cores": "336.0",
      "Process size (nm)": "8.0",
      "Transistors (millions)": "28300.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/rtx-a6000.c3686",
      "Source for the price": "https://wccftech.com/nvidia-rtx-a6000-48-gb-workstation-graphics-card-full-ga102-gpu-4650-us/",
      "ML models": "DiffDock,QMoE: compressed SwitchTransformer,DeepUrfold,llama-3-airoboros-70b-3.3,Mdgen,Precious3GPT,P-Llama2,AbGPT,P-LLama3,ProLLaMA,DiscDiff,Improved motif-scaffolding with SE(3) flow matching,P-gemma,P-Mistral,NAEPro,RoseTTAFold All-Atom (RFAA),VASA-1,Repress",
      "Last modified": "2025-10-16 19:31:13+00:00",
      "Max performance": "38700000000000.0",
      "Energy efficiency": "129000000000.0",
      "Total processing performance (bit-OP/s)": "1238400000000000.0",
      "Price-performance": "8324370832.437083",
      "ML OP/s": "38700000000000.0"
    },
    {
      "Hardware name": "NVIDIA A40 PCIe",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2020-10-05",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "149700000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "48000000000.0",
      "Memory bandwidth (byte/s)": "695800000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "300.0",
      "FP64 (double precision) performance (FLOP/s)": "584600000000.0",
      "FP32 (single precision) performance (FLOP/s)": "37420000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "74800000000000.0",
      "FP16 (half precision) performance (FLOP/s)": "37420000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "299300000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "Samsung",
      "Die Size (mm^2)": "628.0",
      "Base clock (MHz)": "1305.0",
      "Boost clock (MHz)": "1740.0",
      "Memory clock (MHz)": "1812.0",
      "Memory bus (bit)": "384.0",
      "Tensor cores": "336.0",
      "Process size (nm)": "8.0",
      "Transistors (millions)": "28300.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/a40-pcie.c3700; https://images.nvidia.com/content/Solutions/data-center/a40/nvidia-a40-datasheet.pdf",
      "Source for the price": "",
      "ML models": "UdanDTI,RNADiffFold,DDPM,StyleTTS 2,Decaying Fast Weights Transformer (WT-103)",
      "Last modified": "2025-08-04 15:37:58+00:00",
      "Max performance": "299300000000000.0",
      "Energy efficiency": "997666666666.6666",
      "Total processing performance (bit-OP/s)": "2395200000000000.0",
      "Price-performance": "",
      "ML OP/s": "299300000000000.0"
    },
    {
      "Hardware name": "NVIDIA GeForce RTX 3080",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2020-09-01",
      "Release price (USD)": "699.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "10000000000.0",
      "Memory bandwidth (byte/s)": "760000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "320.0",
      "FP64 (double precision) performance (FLOP/s)": "465100000000.0",
      "FP32 (single precision) performance (FLOP/s)": "29770000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "29770000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "Samsung",
      "Die Size (mm^2)": "628.0",
      "Base clock (MHz)": "1440.0",
      "Boost clock (MHz)": "1710.0",
      "Memory clock (MHz)": "1188.0",
      "Memory bus (bit)": "320.0",
      "Tensor cores": "272.0",
      "Process size (nm)": "8.0",
      "Transistors (millions)": "28300.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/geforce-rtx-3080.c3621",
      "Source for the price": "https://www.tomsguide.com/news/nvidia-geforce-rtx-3080-ti",
      "ML models": "MTDP,ConoDL",
      "Last modified": "2025-07-21 09:22:30+00:00",
      "Max performance": "29770000000000.0",
      "Energy efficiency": "93031250000.0",
      "Total processing performance (bit-OP/s)": "952640000000000.0",
      "Price-performance": "42589413447.78255",
      "ML OP/s": "29770000000000.0"
    },
    {
      "Hardware name": "NVIDIA GeForce RTX 3090",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "We divide performance by 2 to convert it from performance over sparse matrices to performance over dense matrices.",
      "Release date": "2020-09-01",
      "Release price (USD)": "1499.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "24000000000.0",
      "Memory bandwidth (byte/s)": "936000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "350.0",
      "FP64 (double precision) performance (FLOP/s)": "556000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "35580000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "35580000000000.0",
      "INT16 performance (OP/s)": "143000000000000.0",
      "INT8 performance (OP/s)": "284000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "Samsung",
      "Die Size (mm^2)": "628.0",
      "Base clock (MHz)": "1395.0",
      "Boost clock (MHz)": "1695.0",
      "Memory clock (MHz)": "1219.0",
      "Memory bus (bit)": "384.0",
      "Tensor cores": "328.0",
      "Process size (nm)": "8.0",
      "Transistors (millions)": "28300.0",
      "Link to datasheet": "https://images.nvidia.com/aem-dam/en-zz/Solutions/geforce/ampere/pdf/NVIDIA-ampere-GA102-GPU-Architecture-Whitepaper-V1.pdf#page=38; https://www.techpowerup.com/gpu-specs/geforce-rtx-3090.c3622",
      "Source for the price": "https://www.tomsguide.com/news/nvidia-geforce-rtx-3090",
      "ML models": "Swift,Robot Parkour,Otter,FinGPT-13B,DeepSA,NLM,MPDF,Text2Protein,TAWFN,Transformer + GFM,Transformer-XL + PowerSGD + L-Greco,DualNetGO,scHyena,Protein-Mamba,RNA-FrameFlow,BiRNA-BERT",
      "Last modified": "2025-08-02 23:27:43+00:00",
      "Max performance": "284000000000000.0",
      "Energy efficiency": "811428571428.5714",
      "Total processing performance (bit-OP/s)": "2288000000000000.0",
      "Price-performance": "189459639759.8399",
      "ML OP/s": "284000000000000.0"
    },
    {
      "Hardware name": "NVIDIA A100 PCIe",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2020-06-22",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "312000000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "40000000000.0",
      "Memory bandwidth (byte/s)": "1940000000000.0",
      "Intranode bandwidth (byte/s)": "600000000000.0",
      "Internode bandwidth (bit/s)": "200000000000.0",
      "TDP (W)": "300.0",
      "FP64 (double precision) performance (FLOP/s)": "9746000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "19490000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "156000000000000.0",
      "FP16 (half precision) performance (FLOP/s)": "77970000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "624000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "826.0",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "5120.0",
      "Tensor cores": "432.0",
      "Process size (nm)": "7.0",
      "Transistors (millions)": "54200.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/a100-pcie-80-gb.c3821\nhttps://www.techpowerup.com/gpu-specs/a100-pcie-40-gb.c3623",
      "Source for the price": "",
      "ML models": "Discriminator Guidance,YuYan 11B,FvFold,Stable Diffusion 1.1,Stable Diffusion 1.4,Stable Diffusion 1.2,Stable Diffusion 1.5",
      "Last modified": "2025-10-29 21:57:38+00:00",
      "Max performance": "624000000000000.0",
      "Energy efficiency": "2080000000000.0",
      "Total processing performance (bit-OP/s)": "4992000000000000.0",
      "Price-performance": "",
      "ML OP/s": "624000000000000.0"
    },
    {
      "Hardware name": "NVIDIA A100 SXM4 40 GB",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2020-05-14",
      "Release price (USD)": "15000.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "312000000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "40000000000.0",
      "Memory bandwidth (byte/s)": "1560000000000.0",
      "Intranode bandwidth (byte/s)": "600000000000.0",
      "Internode bandwidth (bit/s)": "200000000000.0",
      "TDP (W)": "400.0",
      "FP64 (double precision) performance (FLOP/s)": "9746000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "19490000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "156000000000000.0",
      "FP16 (half precision) performance (FLOP/s)": "77970000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "624000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "826.0",
      "Base clock (MHz)": "1095.0",
      "Boost clock (MHz)": "1410.0",
      "Memory clock (MHz)": "1215.0",
      "Memory bus (bit)": "5120.0",
      "Tensor cores": "432.0",
      "Process size (nm)": "7.0",
      "Transistors (millions)": "54200.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/a100-sxm4-40-gb.c3506",
      "Source for the price": "https://www.nextplatform.com/2022/05/09/how-much-of-a-premium-will-nvidia-charge-for-hopper-gpus/",
      "ML models": "Florence,Luminous-supreme,Falcon-180B,GPT-3.5 (davinci-002)\n,GPT-4 (Mar 2023),StableLM-Base-Alpha-7B,Phi-1.5,WeLM,GLM-130B,BlenderBot 3,GPT-NeoX-20B,TinyLlama-1.1B (1T token checkpoint),TinyLlama-1.1B (3T token checkpoint),StableLM-2-1.6B,DINOv2,Stable Code 3B,Falcon-7B,Qarasu-14B,Flan T5-XXL + BLIP-2,BLIP-2 (Q-Former),Swin Transformer V2 (SwinV2-G),SPHINX (Llama 2 13B),EVA-01,CoRe,InstructBLIP,xTrimoPGLM -100B,MPT-7B,Pythia-12b,Pythia-2.8b,Pythia-6.9b,Pythia-160m,Pythia-1b,Pythia-1.4b,Pythia-70m,Pythia-410m,PLaMo-13B,Falcon 2 11B,Janus 1.3B,Luminous-extended,Luminous-base,TeleChat-7B,TeleChat-3B,TeleChat-12B,aiXcoder-7B Base,Janus-Pro-7B,Janus-Pro-1B,SEA-LION V1 3B,SEA-LION V1 7B,Llama-SEA-LION-v2-8B-IT,Novae,HelixProtX,Sailor-7B-Chat,SEA-LION-v1-7B-IT,SGPT BE 5.8B,ToolFormer,Stable Diffusion 2.1,AntiFormer,GPT-2 Medium (FlashAttention),Llemma 7B,Llemma 34B,Teuken 7B,Aquila2 34B,Aquila2\u201170B\u2011Expr,Deepseek OCR,GPT-4 (Jun 2023)",
      "Last modified": "2025-12-04 17:58:01+00:00",
      "Max performance": "624000000000000.0",
      "Energy efficiency": "1560000000000.0",
      "Total processing performance (bit-OP/s)": "4992000000000000.0",
      "Price-performance": "41600000000.0",
      "ML OP/s": "624000000000000.0"
    },
    {
      "Hardware name": "NVIDIA A100",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2020-03-01",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "312000000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "40000000000.0",
      "Memory bandwidth (byte/s)": "1560000000000.0",
      "Intranode bandwidth (byte/s)": "600000000000.0",
      "Internode bandwidth (bit/s)": "200000000000.0",
      "TDP (W)": "400.0",
      "FP64 (double precision) performance (FLOP/s)": "9700000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "19500000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "156000000000000.0",
      "FP16 (half precision) performance (FLOP/s)": "77970000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "624000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "826.0",
      "Base clock (MHz)": "1280.0",
      "Boost clock (MHz)": "1410.0",
      "Memory clock (MHz)": "1590.0",
      "Memory bus (bit)": "5120.0",
      "Tensor cores": "432.0",
      "Process size (nm)": "7.0",
      "Transistors (millions)": "54200.0",
      "Link to datasheet": "https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf; https://www.techpowerup.com/gpu-specs/a100-sxm4-80-gb.c3746",
      "Source for the price": "https://arstechnica.com/gadgets/2016/07/gtx-titan-x-pascal-specs-price-release-date/#:~:text=Surprise!%20%E2%80%94-,Nvidia%20unveils%20new%20GTX%20Titan%20X,teraflops%2C%2012GB%20GDDR5X%2C%20just%20%241%2C200,   https://www.extremetech.com/gaming/232924-nvidia-titan-x-offers-incredible-performance-at-a-significant-cost",
      "ML models": "DLRM-2022,Taiyi-Stable Diffusion,AR-LDM,Megatron-LM (1T),DLRM-12T,LLaMA-65B,GPT-SW3,CALM,Falcon-40B,Ferret (13B),Show-1,MoE-1.1T,StableLM-3B-4E1T,CodeT5-base,CodeT5-large,CodeT5+,ZymCTRL,BloombergGPT,LLaMA-7B,DiT-XL/2 + Discriminator Guidance,Platypus-70B,OPT-IML (175B),Fairseq-dense 13B,Polyglot-Ko-12.8B,PULI GPTrio,LLaMA-13B,Wu Dao Aquila-7B,Phi-2,Nemotron-3-8B,Japanese-LM-3.6B,EMDR,LDM-1.45B,ImageBind,Tranception,ProtGPT2,gLM,HyenaDNA,Nucleotide Transformer,Segment Anything Model,AudioGen,EnCodec,YaLM,Granite 13B,HyperCLOVA 82B,CoEdiT-xxl,XGLM-7.5B,RQ-Transformer (LSUN-cat dataset),RQ-Transformer (1.4B params ImageNet dataset),RQ-Transformer (3.8B params ImageNet dataset),LLaVA,LLaVA 1.5,LLaVA-NeXT-34B (LLaVA-1.6),CTR-BERT,DeepSeekMoE-16B,eDiff-I,Starling-LM-7B-alpha,OtterHD-8B,OneLLM,InternLM-XComposer,AlexaTM 20B,PLUG,Stable Diffusion (LDM-KL-8-G),Wu Dao Aquila-33B,MegaScale (175B),MegaScale (530B),MegaScale (Production),Evo,Reka Core,Reka Flash,OLMo-7B,MosaicML Diffusion,ruDalle: Kandinsky 3.0,ruDalle: Kandinsky 3.1,Prithvi-100M,GPT-2 (AMPS),HyperCLOVA 204B,Amazon Titan,Perfusion,BGE-M3 Embedding,Swallow,MuggleMath,MiniGPT4 (Vicuna finetune),Genie 2 (bio),OpenVLA,MGK 4 heads (medium),AlphaFold 3,FRED-T5-XL,ruGPT-3.5 13B,CHAI-1,ProteinSetTransformer,Aurora,ERNIE-ViLG 2.0,3-ensemble of Self-ensembles on CIFAR-100,ChatBit,MACE-MP-0,MetaMath 7B (LLaMa finetune),MetaMath 7B (Mistral finetune),T2V-Turbo-v2,VideoCrafter2,4D Diffusion for Dynamic Protein Structure Prediction with Reference Guided Motion Alignment,IDPFold,MolPhenix,Importance of higher-order epistasis in large protein sequence-function relationships,PlasmidGPT,Vicuna-7B v0,Vicuna-13B v0,Vicuna-13B-v1.3,Vicuna-7B-v1.3,RWKV-6 (Finch) 3B,Zephyr 7B,BaiLing TTS,BaseFold,HiFi - NN,Phi-1,HAM-TTS,HGRN 1B (WT 103),IDEFICS-9B,IDEFICS-80B,MatterSim (M3GNet - MatterSim-v1.0.0-5M),MatterSim (Grpaphomer),Apollo 7B,Apollo 3B,Apollo 1.5B,TigerBot-70B,MatterGen,xLSTM 1.4B,Jurassic-1-Jumbo,ALLaM adapted13B\n,ALLaM\u00a0adapted 70B,ALLaM 7B,ALLaM 34B,OLMo-1B,Yi-34B,RNAformer,Fugatto 1,EGRU (PTB),MLDD3UTRmRRNAS,FragLlama,Jaeger,ProtSSN,CoPRA,ProTrek,ProtT3,InstructPLM,GroundingGPT,Uni-RNA-L8,Uni-RNA-L12,ESM-GearNet,Hybrid H3-125M,Hybrid H3-355M,Hybrid H3-1.3B,GearNet,EGRU (WT2),MGK 8 heads (small),S4,Prithvi WxC,InternViT-6B,Vega v2,OmniFusion-7B (InternViT-6B-448px V1-2),EPInformer,DRGN-AI,ProCALM (Uniref9B),PocketFlow,VILA-7B,VILA1.5-13B,Oryx 7B ,Pyramid Flow,FlexSBDD,PocketGen,IgGM,RNAdiffusion,AMPLIFY,Alphaflow,ESMFlow,ProRNA3D-Single,FoldFlow2,SaProt,ESM-AA,OREAL 7B,OREAL 32B,DeepSeek-VL-7B,DeepSeek-VL-1.3B,ProstT5,RiNALMo,PTM-Mamba,Protllm,DecompDiff,Re-Dock,DiscDiff,PLLaMa,FoldFlow,PoET,Genie-SCOPe (bio),DiffSBDD (CrossDocked),RNA-FM,Animate Anyone,Animate Anyone 2,Prithvi-EO-2.0 300M,Prithvi-EO-2.0 600M,Lumina-Image-2.0,Boltz-2,GAIA-1,N\u00dcWA,Amazon Nova Canvas,Amazon Nova Reel,Amazon Nova Micro,Amazon Nova Lite,Amazon Nova Pro,ZhihaiTu AI (\u77e5\u6d77\u56fe),Reka Edge,Grounding Dino L,V-JEPA,STORM-B/8,SANA 1.5 4.8B,Diffusion Renderer,FoundationStereo,Zero-shot Monocular Scene Flow (ZeroMSF),OpenOmni,GigaChat Pro,Hymba,Wu Dao - Wen Lan,Apollo-7B,Turing ULRv5,Alibaba-NLP (mGTE),Canary 1B v2,Parakeet-tdt-0.6b-v3,SauTech,FourCastNet,Demist-2,Surya,ZhiLu-2-8B-Instruct,TerraMind,LongLoRA,Agentar-Fin-R1 8B,Agentar-Fin-R1 32B,Statement Curriculum Learning,LoongRL 7B",
      "Last modified": "2025-11-08 01:45:36+00:00",
      "Max performance": "624000000000000.0",
      "Energy efficiency": "1560000000000.0",
      "Total processing performance (bit-OP/s)": "4992000000000000.0",
      "Price-performance": "",
      "ML OP/s": "624000000000000.0"
    },
    {
      "Hardware name": "Google TPU v4i",
      "Manufacturer": "Google",
      "Type": "TPU",
      "Notes": "",
      "Release date": "2020-01-01",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "138000000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "8000000000.0",
      "Memory bandwidth (byte/s)": "614000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "175.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "400.0",
      "Base clock (MHz)": "1055.0",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "7.0",
      "Transistors (millions)": "16000.0",
      "Link to datasheet": "https://ieeexplore.ieee.org/document/9499913\nsummary here: https://parsa.epfl.ch/course-info/cs723/lectures/hw_accel.pdf ",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-12-12 23:08:31+00:00",
      "Max performance": "138000000000000.0",
      "Energy efficiency": "788571428571.4286",
      "Total processing performance (bit-OP/s)": "2208000000000000.0",
      "Price-performance": "",
      "ML OP/s": "138000000000000.0"
    },
    {
      "Hardware name": "Baidu Kunlun K200",
      "Manufacturer": "Kunlunxin Baidu",
      "Type": "XPU",
      "Notes": "https://hc32.hotchips.org/assets/program/conference/day2/HotChips2020_ML_Inference_Baidu_Kunlun_v5.pdf",
      "Release date": "2020-01-01",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "16000000000.0",
      "Memory bandwidth (byte/s)": "512000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "150.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "16000000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "64000000000000.0",
      "INT16 performance (OP/s)": "64000000000000.0",
      "INT8 performance (OP/s)": "256000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "504.0",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "14.0",
      "Transistors (millions)": "",
      "Link to datasheet": "",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:23:25+00:00",
      "Max performance": "256000000000000.0",
      "Energy efficiency": "1706666666666.6667",
      "Total processing performance (bit-OP/s)": "2048000000000000.0",
      "Price-performance": "",
      "ML OP/s": "256000000000000.0"
    },
    {
      "Hardware name": "NVIDIA Tesla V100S PCIe 32 GB",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2019-11-26",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "130000000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "32000000000.0",
      "Memory bandwidth (byte/s)": "1130000000000.0",
      "Intranode bandwidth (byte/s)": "32000000000.0",
      "Internode bandwidth (bit/s)": "100000000000.0",
      "TDP (W)": "250.0",
      "FP64 (double precision) performance (FLOP/s)": "8177000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "16350000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "32710000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "815.0",
      "Base clock (MHz)": "1245.0",
      "Boost clock (MHz)": "1597.0",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "4096.0",
      "Tensor cores": "640.0",
      "Process size (nm)": "12.0",
      "Transistors (millions)": "21100.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/tesla-v100s-pcie-32-gb.c3467",
      "Source for the price": "",
      "ML models": "ConSERT,Megatron-BERT",
      "Last modified": "2025-10-10 16:01:28+00:00",
      "Max performance": "130000000000000.0",
      "Energy efficiency": "520000000000.0",
      "Total processing performance (bit-OP/s)": "2080000000000000.0",
      "Price-performance": "",
      "ML OP/s": "130000000000000.0"
    },
    {
      "Hardware name": "T-Head Hanguang 800",
      "Manufacturer": "Alibaba",
      "Type": "NPU",
      "Notes": "https://img.102.alibaba.com/1622193035686/9898014ba4eb8adfd3f31db3b2cf26f3.pdf?spm=a2ouz.12987056.0.0.53519352dMhuEh&file=9898014ba4eb8adfd3f31db3b2cf26f3.pdf",
      "Release date": "2019-09-25",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "",
      "Memory bandwidth (byte/s)": "",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "276.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "205000000000000.0",
      "INT8 performance (OP/s)": "825000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "12.0",
      "Transistors (millions)": "",
      "Link to datasheet": "",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:23:41+00:00",
      "Max performance": "825000000000000.0",
      "Energy efficiency": "2989130434782.609",
      "Total processing performance (bit-OP/s)": "6600000000000000.0",
      "Price-performance": "",
      "ML OP/s": "825000000000000.0"
    },
    {
      "Hardware name": "Huawei Ascend 910A",
      "Manufacturer": "Huawei",
      "Type": "GPU",
      "Notes": "Source for power: https://www.huawei.com/au/news/au/2019/huawei-launches-ascend-910-the-worlds-most-powerful-ai-processor\n\"For half-precision floating point (FP16) operations, Ascend 910 delivers 256 TeraFLOPS. For integer precision calculations (INT8), it delivers 512 TeraOPS. Despite its unrivaled performance, Ascend 910's max power consumption is only 310W, much lower than its planned specs (350W).\"\n\nSource for bandwidth: https://support.huawei.com/enterprise/en/doc/EDOC1100141955/3cf58244/technical-specifications",
      "Release date": "2019-08-23",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "32000000000.0",
      "Memory bandwidth (byte/s)": "1288000000000.0",
      "Intranode bandwidth (byte/s)": "270000000000.0",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "310.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "256000000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "7.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://www.sciencedirect.com/science/article/pii/S0743731524000480",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-10-10 16:01:02+00:00",
      "Max performance": "256000000000000.0",
      "Energy efficiency": "825806451612.9032",
      "Total processing performance (bit-OP/s)": "4096000000000000.0",
      "Price-performance": "",
      "ML OP/s": "256000000000000.0"
    },
    {
      "Hardware name": "Huawei Ascend 910A Pro",
      "Manufacturer": "Huawei",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2019-08-23",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "32000000000.0",
      "Memory bandwidth (byte/s)": "1288000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "288000000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "",
      "Transistors (millions)": "",
      "Link to datasheet": "https://www.sciencedirect.com/science/article/pii/S0743731524000480",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:23:43+00:00",
      "Max performance": "288000000000000.0",
      "Energy efficiency": "",
      "Total processing performance (bit-OP/s)": "4608000000000000.0",
      "Price-performance": "",
      "ML OP/s": "288000000000000.0"
    },
    {
      "Hardware name": "Huawei Ascend 910A Premium",
      "Manufacturer": "Huawei",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2019-08-23",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "32000000000.0",
      "Memory bandwidth (byte/s)": "1288000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "320000000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "",
      "Transistors (millions)": "",
      "Link to datasheet": "",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:23:42+00:00",
      "Max performance": "320000000000000.0",
      "Energy efficiency": "",
      "Total processing performance (bit-OP/s)": "5120000000000000.0",
      "Price-performance": "",
      "ML OP/s": "320000000000000.0"
    },
    {
      "Hardware name": "Huawei Ascend 910",
      "Manufacturer": "Huawei",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2019-08-23",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "",
      "Memory bandwidth (byte/s)": "1288000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "310.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "256000000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "512000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "1228.0",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "7.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://old.hotchips.org/hc31/HC31_1.11_Huawei.Davinci.HengLiao_v4.0.pdf\nhttps://cset.georgetown.edu/publication/pushing-the-limits-huaweis-ai-chip-tests-u-s-export-controls/",
      "Source for the price": "",
      "ML models": "PanGu-\u03b1,CodeGeeX,PanGu-\u03a3,ERNIE 3.0 Titan",
      "Last modified": "2025-07-21 09:23:41+00:00",
      "Max performance": "512000000000000.0",
      "Energy efficiency": "1651612903225.8064",
      "Total processing performance (bit-OP/s)": "4096000000000000.0",
      "Price-performance": "",
      "ML OP/s": "512000000000000.0"
    },
    {
      "Hardware name": "Huawei Ascend 910 (320 TFLOPS)",
      "Manufacturer": "Huawei",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2019-08-23",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "",
      "Memory bandwidth (byte/s)": "1288000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "310.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "320000000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "640000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "1228.0",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "7.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://old.hotchips.org/hc31/HC31_1.11_Huawei.Davinci.HengLiao_v4.0.pdf\nhttps://cset.georgetown.edu/publication/pushing-the-limits-huaweis-ai-chip-tests-u-s-export-controls/",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:23:42+00:00",
      "Max performance": "640000000000000.0",
      "Energy efficiency": "2064516129032.258",
      "Total processing performance (bit-OP/s)": "5120000000000000.0",
      "Price-performance": "",
      "ML OP/s": "640000000000000.0"
    },
    {
      "Hardware name": "Cambricon MLU270-F4",
      "Manufacturer": "Cambricon",
      "Type": "ASIC",
      "Notes": "https://www.cambricon.com/index.php?m=content&c=index&a=lists&catid=37 \nhttps://archive.ph/xX1FH\n\nhttps://www.cambricon.com/index.php?m=content&c=index&a=show&catid=127&id=27\nhttps://archive.ph/zaZBg",
      "Release date": "2019-06-20",
      "Release price (USD)": "2080.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "16000000000.0",
      "Memory bandwidth (byte/s)": "102000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "150.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "64000000000000.0",
      "INT8 performance (OP/s)": "128000000000000.0",
      "INT4 performance (OP/s)": "256000000000000.0",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "",
      "Transistors (millions)": "",
      "Link to datasheet": "",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:24:01+00:00",
      "Max performance": "256000000000000.0",
      "Energy efficiency": "1706666666666.6667",
      "Total processing performance (bit-OP/s)": "1024000000000000.0",
      "Price-performance": "61538461538.46154",
      "ML OP/s": "128000000000000.0"
    },
    {
      "Hardware name": "Cambricon MLU270-S4",
      "Manufacturer": "Cambricon",
      "Type": "ASIC",
      "Notes": "https://www.cambricon.com/index.php?m=content&c=index&a=lists&catid=36\nhttps://archive.ph/Vv13F",
      "Release date": "2019-06-20",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "16000000000.0",
      "Memory bandwidth (byte/s)": "102000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "70.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "64000000000000.0",
      "INT8 performance (OP/s)": "128000000000000.0",
      "INT4 performance (OP/s)": "256000000000000.0",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "",
      "Transistors (millions)": "",
      "Link to datasheet": "",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:24:01+00:00",
      "Max performance": "256000000000000.0",
      "Energy efficiency": "3657142857142.857",
      "Total processing performance (bit-OP/s)": "1024000000000000.0",
      "Price-performance": "",
      "ML OP/s": "128000000000000.0"
    },
    {
      "Hardware name": "Intel Habana GAUDI HL-205",
      "Manufacturer": "Intel",
      "Type": "",
      "Notes": "",
      "Release date": "2019-06-18",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "",
      "Memory bandwidth (byte/s)": "",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "300.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "",
      "Transistors (millions)": "",
      "Link to datasheet": "https://www.new-techeurope.com/2019/06/18/habana-labs-announces-gaudi-ai-training-processor/",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-05-09 20:39:54+00:00",
      "Max performance": "0.0",
      "Energy efficiency": "0.0",
      "Total processing performance (bit-OP/s)": "",
      "Price-performance": "",
      "ML OP/s": "0.0"
    },
    {
      "Hardware name": "Baidu Kunlun I",
      "Manufacturer": "Kunlunxin Baidu",
      "Type": "XPU",
      "Notes": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9366056\nhttps://www.tomshardware.com/news/baidu-unveils-kunlun-ii-processor-for-ai\n",
      "Release date": "2019-01-01",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "",
      "Memory bandwidth (byte/s)": "512000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "150.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "16000000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "64000000000000.0",
      "INT16 performance (OP/s)": "64000000000000.0",
      "INT8 performance (OP/s)": "256000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "500.0",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "14.0",
      "Transistors (millions)": "",
      "Link to datasheet": "",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:26:24+00:00",
      "Max performance": "256000000000000.0",
      "Energy efficiency": "1706666666666.6667",
      "Total processing performance (bit-OP/s)": "2048000000000000.0",
      "Price-performance": "",
      "ML OP/s": "256000000000000.0"
    },
    {
      "Hardware name": "NVIDIA Quadro RTX 4000",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2018-11-13",
      "Release price (USD)": "899.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "8000000000.0",
      "Memory bandwidth (byte/s)": "416000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "160.0",
      "FP64 (double precision) performance (FLOP/s)": "222500000000.0",
      "FP32 (single precision) performance (FLOP/s)": "7119000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "14240000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "545.0",
      "Base clock (MHz)": "1005.0",
      "Boost clock (MHz)": "1545.0",
      "Memory clock (MHz)": "1625.0",
      "Memory bus (bit)": "256.0",
      "Tensor cores": "288.0",
      "Process size (nm)": "12.0",
      "Transistors (millions)": "13600.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/quadro-rtx-4000.c3336",
      "Source for the price": "https://videocardz.net/nvidia-quadro-rtx-4000",
      "ML models": "CaLM",
      "Last modified": "2025-07-21 09:53:39+00:00",
      "Max performance": "14240000000000.0",
      "Energy efficiency": "89000000000.0",
      "Total processing performance (bit-OP/s)": "227840000000000.0",
      "Price-performance": "15839822024.471636",
      "ML OP/s": "14240000000000.0"
    },
    {
      "Hardware name": "Huawei Ascend 310",
      "Manufacturer": "Huawei",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2018-10-10",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "",
      "Memory bandwidth (byte/s)": "51200000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "8.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "8000000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "16000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "105.0",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "12.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://old.hotchips.org/hc31/HC31_1.11_Huawei.Davinci.HengLiao_v4.0.pdf",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:53:47+00:00",
      "Max performance": "16000000000000.0",
      "Energy efficiency": "2000000000000.0",
      "Total processing performance (bit-OP/s)": "128000000000000.0",
      "Price-performance": "",
      "ML OP/s": "16000000000000.0"
    },
    {
      "Hardware name": "NVIDIA GeForce RTX 2080 Ti 11GB",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2018-09-20",
      "Release price (USD)": "999.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "113800000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "11000000000.0",
      "Memory bandwidth (byte/s)": "616000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "250.0",
      "FP64 (double precision) performance (FLOP/s)": "420200000000.0",
      "FP32 (single precision) performance (FLOP/s)": "13450000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "26900000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "227700000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "754.0",
      "Base clock (MHz)": "1350.0",
      "Boost clock (MHz)": "1545.0",
      "Memory clock (MHz)": "1750.0",
      "Memory bus (bit)": "352.0",
      "Tensor cores": "544.0",
      "Process size (nm)": "12.0",
      "Transistors (millions)": "18600.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/geforce-rtx-2080-ti.c3305",
      "Source for the price": "",
      "ML models": "YuYan 11B,DNABERT,EpiScan,Transformer-XL DeFINE (107M),$\\infty$-former (SM),DEQ-Transformer (Post-LN) + Jacobian Regularisation,Memformer (4 encoder + 16 decoder),LSTM-3-layer+Gadam,TaLK Convolution",
      "Last modified": "2025-07-21 09:53:50+00:00",
      "Max performance": "227700000000000.0",
      "Energy efficiency": "910800000000.0",
      "Total processing performance (bit-OP/s)": "1821600000000000.0",
      "Price-performance": "227927927927.92792",
      "ML OP/s": "227700000000000.0"
    },
    {
      "Hardware name": "NVIDIA Tesla T4",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2018-09-13",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "16000000000.0",
      "Memory bandwidth (byte/s)": "320000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "70.0",
      "FP64 (double precision) performance (FLOP/s)": "254400000000.0",
      "FP32 (single precision) performance (FLOP/s)": "8141000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "65129999999999.9",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "545.0",
      "Base clock (MHz)": "585.0",
      "Boost clock (MHz)": "1590.0",
      "Memory clock (MHz)": "1250.0",
      "Memory bus (bit)": "256.0",
      "Tensor cores": "320.0",
      "Process size (nm)": "12.0",
      "Transistors (millions)": "13600.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/tesla-t4.c3316",
      "Source for the price": "",
      "ML models": "CLR_ESP",
      "Last modified": "2025-07-21 09:53:52+00:00",
      "Max performance": "65129999999999.9",
      "Energy efficiency": "930428571428.57",
      "Total processing performance (bit-OP/s)": "1042079999999998.4",
      "Price-performance": "",
      "ML OP/s": "65129999999999.9"
    },
    {
      "Hardware name": "NVIDIA Quadro RTX 6000",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2018-08-13",
      "Release price (USD)": "6300.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "24000000000.0",
      "Memory bandwidth (byte/s)": "672000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "260.0",
      "FP64 (double precision) performance (FLOP/s)": "509800000000.0",
      "FP32 (single precision) performance (FLOP/s)": "16309999999999.9",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "32620000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "754.0",
      "Base clock (MHz)": "1440.0",
      "Boost clock (MHz)": "1770.0",
      "Memory clock (MHz)": "1750.0",
      "Memory bus (bit)": "384.0",
      "Tensor cores": "576.0",
      "Process size (nm)": "12.0",
      "Transistors (millions)": "18600.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/quadro-rtx-6000.c3307",
      "Source for the price": "https://videocardz.net/nvidia-quadro-rtx-6000",
      "ML models": "Projected GAN",
      "Last modified": "2025-07-21 09:53:58+00:00",
      "Max performance": "32620000000000.0",
      "Energy efficiency": "125461538461.53848",
      "Total processing performance (bit-OP/s)": "521920000000000.0",
      "Price-performance": "5177777777.777778",
      "ML OP/s": "32620000000000.0"
    },
    {
      "Hardware name": "NVIDIA Quadro RTX 5000",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2018-08-13",
      "Release price (USD)": "2300.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "16000000000.0",
      "Memory bandwidth (byte/s)": "448000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "230.0",
      "FP64 (double precision) performance (FLOP/s)": "348500000000.0",
      "FP32 (single precision) performance (FLOP/s)": "11150000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "22300000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "545.0",
      "Base clock (MHz)": "1620.0",
      "Boost clock (MHz)": "1815.0",
      "Memory clock (MHz)": "1750.0",
      "Memory bus (bit)": "256.0",
      "Tensor cores": "384.0",
      "Process size (nm)": "12.0",
      "Transistors (millions)": "13600.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/quadro-rtx-5000.c3308",
      "Source for the price": "https://videocardz.net/nvidia-quadro-rtx-5000",
      "ML models": "ProteinBERT",
      "Last modified": "2025-07-21 09:53:58+00:00",
      "Max performance": "22300000000000.0",
      "Energy efficiency": "96956521739.13045",
      "Total processing performance (bit-OP/s)": "356800000000000.0",
      "Price-performance": "9695652173.913044",
      "ML OP/s": "22300000000000.0"
    },
    {
      "Hardware name": "NVIDIA Quadro RTX 8000",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2018-08-13",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "48000000000.0",
      "Memory bandwidth (byte/s)": "672000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "260.0",
      "FP64 (double precision) performance (FLOP/s)": "509800000000.0",
      "FP32 (single precision) performance (FLOP/s)": "16300000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "32620000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "754.0",
      "Base clock (MHz)": "1395.0",
      "Boost clock (MHz)": "1770.0",
      "Memory clock (MHz)": "1750.0",
      "Memory bus (bit)": "384.0",
      "Tensor cores": "576.0",
      "Process size (nm)": "12.0",
      "Transistors (millions)": "18600.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/quadro-rtx-8000.c3306",
      "Source for the price": "",
      "ML models": "PolyCoder,MiniGPT4 + LRV-Instruction,Verbatim Memory Transformer (108M)",
      "Last modified": "2025-07-21 09:53:58+00:00",
      "Max performance": "32620000000000.0",
      "Energy efficiency": "125461538461.53848",
      "Total processing performance (bit-OP/s)": "521920000000000.0",
      "Price-performance": "",
      "ML OP/s": "32620000000000.0"
    },
    {
      "Hardware name": "Google TPU v3",
      "Manufacturer": "Google",
      "Type": "TPU",
      "Notes": "",
      "Release date": "2018-05-18",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "123000000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "32000000000.0",
      "Memory bandwidth (byte/s)": "900000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "450.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "16.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://cloud.google.com/tpu/docs/system-architecture-tpu-vm\nhttps://dl.acm.org/doi/pdf/10.1145/3360307 ",
      "Source for the price": "https://www.techpowerup.com/gpu-specs/geforce-gtx-1080-ti.c2877#:~:text=The%20GeForce%20GTX%201080%20Ti,on%20GeForce%20GTX%201080%20Ti.",
      "ML models": "BigGAN-deep 512x512,MnasNet-A3,MnasNet-A1 + SSDLite,T5-3B,T5-11B,SimCLR,EfficientNetV2-XL,Denoising Diffusion Probabilistic Models (LSUN Bedroom),Meena,AlphaFold 2,CoAtNet,Big Transfer (BiT-L),Conformer + Wav2vec 2.0 + Noisy Student,ContextNet + Noisy Student,ALBERT,ViT-G/14 (LiT),ViT-G/14,AlphaStar,ByT5-XXL,Gopher (280B),LaMDA,ALIGN,GShard (dense),UnifiedQA,DiT-XL/2,AlphaFold-Multimer,Chinchilla,ProGen2-xlarge,Adaptive Agent,Instruct-GPT + Mind's Eye,PaLM-SayCan,ResNet-RS,Meta Pseudo Labels,GOAT,Switch,ProtT5-XXL,LongT5,SciBERT,AlphaGeometry,GBERT-Large,German ELECTRA Large,DALL-E mega,DALL-E mini,T0-XXL,ViT-Huge/14,ALBERT-xxlarge,Gato,Noisy Student (L2),ProtBERT-BFD,FLAN 137B,ProtT5-XXL-BFD,Hawk,Griffin,Tk-Instruct,Delphi,XLNet,GPT-J-6B,PaliGemma,Tulu V2 DPO 70B,AlphaTensor,GPT-2 (1.5B),ProtBERT-UniRef,Transformer-XL (257M),RFA-GATE-Gaussian-Stateful Big,Long-range sequence Compressive Transformers,Local Transformer (WT103),Big Transfer (BiT-M),ProGen2-base,AraGPT2-Mega,AraELECTRA,AraBERT LArge v2,DCTransformer (ImageNet),SPN (CelebA HQ),SPN (ImageNet 128),AlphaGenome,Grover-Mega,GShard (600B),LIMoE-H/14,MetNet,BYOL,GQA-8-XXL",
      "Last modified": "2025-12-12 22:58:44+00:00",
      "Max performance": "123000000000000.0",
      "Energy efficiency": "273333333333.3333",
      "Total processing performance (bit-OP/s)": "1968000000000000.0",
      "Price-performance": "",
      "ML OP/s": "123000000000000.0"
    },
    {
      "Hardware name": "NVIDIA Tesla V100 DGXS 16 GB",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2018-03-27",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "125000000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "16000000000.0",
      "Memory bandwidth (byte/s)": "897000000000.0",
      "Intranode bandwidth (byte/s)": "300000000000.0",
      "Internode bandwidth (bit/s)": "100000000000.0",
      "TDP (W)": "250.0",
      "FP64 (double precision) performance (FLOP/s)": "8100000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "16200000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "32400000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "815.0",
      "Base clock (MHz)": "1354.0",
      "Boost clock (MHz)": "1582.0",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "640.0",
      "Process size (nm)": "12.0",
      "Transistors (millions)": "21100.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/tesla-v100-dgxs-16-gb.c3763",
      "Source for the price": "",
      "ML models": "KataGo,CogView,DALL-E,Big Transformer for Back-Translation,Memformer (4 encoder + 16 decoder),DistilBERT,NVAE (CIFAR 10)",
      "Last modified": "2025-10-29 21:57:21+00:00",
      "Max performance": "125000000000000.0",
      "Energy efficiency": "500000000000.0",
      "Total processing performance (bit-OP/s)": "2000000000000000.0",
      "Price-performance": "",
      "ML OP/s": "125000000000000.0"
    },
    {
      "Hardware name": "NVIDIA Tesla V100 PCIe 32 GB",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2018-03-27",
      "Release price (USD)": "11458.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "112000000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "32000000000.0",
      "Memory bandwidth (byte/s)": "897000000000.0",
      "Intranode bandwidth (byte/s)": "32000000000.0",
      "Internode bandwidth (bit/s)": "100000000000.0",
      "TDP (W)": "250.0",
      "FP64 (double precision) performance (FLOP/s)": "7066000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "14130000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "28260000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "815.0",
      "Base clock (MHz)": "1230.0",
      "Boost clock (MHz)": "1380.0",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "4096.0",
      "Tensor cores": "640.0",
      "Process size (nm)": "12.0",
      "Transistors (millions)": "21100.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/tesla-v100-pcie-32-gb.c3184",
      "Source for the price": "https://www.microway.com/hpc-tech-tips/nvidia-tesla-v100-price-analysis/",
      "ML models": "Retrieval-Augmented Generator",
      "Last modified": "2025-10-29 21:57:19+00:00",
      "Max performance": "112000000000000.0",
      "Energy efficiency": "448000000000.0",
      "Total processing performance (bit-OP/s)": "1792000000000000.0",
      "Price-performance": "9774829813.23093",
      "ML OP/s": "112000000000000.0"
    },
    {
      "Hardware name": "NVIDIA Tesla V100 DGXS 32 GB",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2018-03-27",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "125000000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "32000000000.0",
      "Memory bandwidth (byte/s)": "897000000000.0",
      "Intranode bandwidth (byte/s)": "300000000000.0",
      "Internode bandwidth (bit/s)": "100000000000.0",
      "TDP (W)": "250.0",
      "FP64 (double precision) performance (FLOP/s)": "7834000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "15670000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "31330000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "815.0",
      "Base clock (MHz)": "1297.0",
      "Boost clock (MHz)": "1530.0",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "4096.0",
      "Tensor cores": "640.0",
      "Process size (nm)": "12.0",
      "Transistors (millions)": "21100.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/tesla-v100-dgxs-32-gb.c3186",
      "Source for the price": "",
      "ML models": "Megatron-LM (8.3B),Rubik's cube ADR robot,GPT-3 175B (davinci),iGPT-XL,iGPT-L,wave2vec 2.0 LARGE,DLRM-2022,M6-T,DistilProtBert,ERNIE 3.0 Titan,Turing-NLG,MSA Transformer,XLM-RoBERTa,MedBERT,GPT-3 2.7B,GPT-3 6.7B,GPT-3 13B,GPT-3 Large,GPT-3 Medium,GPT-3 Small,GPT-3 XL,Uni-Mol Molecular Model,RoBERTa Large,ERNIE-RNA,PeTriBERT,NVAE (Celeba HQ),NVAE (FFHQ),DLWP,WDC20 / DLWP",
      "Last modified": "2025-10-29 21:57:20+00:00",
      "Max performance": "125000000000000.0",
      "Energy efficiency": "500000000000.0",
      "Total processing performance (bit-OP/s)": "2000000000000000.0",
      "Price-performance": "",
      "ML OP/s": "125000000000000.0"
    },
    {
      "Hardware name": "NVIDIA Tesla V100 SXM3 32 GB",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2018-03-27",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "125000000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "",
      "Memory bandwidth (byte/s)": "898000000000.0",
      "Intranode bandwidth (byte/s)": "300000000000.0",
      "Internode bandwidth (bit/s)": "100000000000.0",
      "TDP (W)": "300.0",
      "FP64 (double precision) performance (FLOP/s)": "7834000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "15670000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "31330000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "815.0",
      "Base clock (MHz)": "1290.0",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "12.0",
      "Transistors (millions)": "21100.0",
      "Link to datasheet": "",
      "Source for the price": "",
      "ML models": "Megatron-LM (1.2B)",
      "Last modified": "2025-10-29 21:57:06+00:00",
      "Max performance": "125000000000000.0",
      "Energy efficiency": "416666666666.6667",
      "Total processing performance (bit-OP/s)": "2000000000000000.0",
      "Price-performance": "",
      "ML OP/s": "125000000000000.0"
    },
    {
      "Hardware name": "NVIDIA Tesla V100 SXM2 32 GB",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2018-03-27",
      "Release price (USD)": "11458.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "125000000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "32000000000.0",
      "Memory bandwidth (byte/s)": "898000000000.0",
      "Intranode bandwidth (byte/s)": "300000000000.0",
      "Internode bandwidth (bit/s)": "100000000000.0",
      "TDP (W)": "300.0",
      "FP64 (double precision) performance (FLOP/s)": "7834000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "15670000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "31330000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "815.0",
      "Base clock (MHz)": "1290.0",
      "Boost clock (MHz)": "1530.0",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "4096.0",
      "Tensor cores": "640.0",
      "Process size (nm)": "12.0",
      "Transistors (millions)": "21100.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/tesla-v100-sxm2-32-gb.c3185",
      "Source for the price": "https://www.microway.com/hpc-tech-tips/nvidia-tesla-v100-price-analysis/",
      "ML models": "",
      "Last modified": "2025-10-29 21:57:06+00:00",
      "Max performance": "125000000000000.0",
      "Energy efficiency": "416666666666.6667",
      "Total processing performance (bit-OP/s)": "2000000000000000.0",
      "Price-performance": "10909408273.695234",
      "ML OP/s": "125000000000000.0"
    },
    {
      "Hardware name": "NVIDIA Titan V",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2017-12-01",
      "Release price (USD)": "2999.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "12000000000.0",
      "Memory bandwidth (byte/s)": "651000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "250.0",
      "FP64 (double precision) performance (FLOP/s)": "7450000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "14900000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "29800000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "815.0",
      "Base clock (MHz)": "1200.0",
      "Boost clock (MHz)": "1455.0",
      "Memory clock (MHz)": "848.0",
      "Memory bus (bit)": "3072.0",
      "Tensor cores": "640.0",
      "Process size (nm)": "12.0",
      "Transistors (millions)": "21100.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/titan-v.c3051",
      "Source for the price": "",
      "ML models": "SeqVec,GPT2+CoreLM+Fine-Tuning,RGC+ASQ (PTB),Specter",
      "Last modified": "2025-07-21 15:20:50+00:00",
      "Max performance": "29800000000000.0",
      "Energy efficiency": "119200000000.0",
      "Total processing performance (bit-OP/s)": "476800000000000.0",
      "Price-performance": "9936645548.516172",
      "ML OP/s": "29800000000000.0"
    },
    {
      "Hardware name": "NVIDIA Tesla V100 SXM2",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2017-07-01",
      "Release price (USD)": "11500.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "125000000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "16000000000.0",
      "Memory bandwidth (byte/s)": "897000000000.0",
      "Intranode bandwidth (byte/s)": "300000000000.0",
      "Internode bandwidth (bit/s)": "100000000000.0",
      "TDP (W)": "300.0",
      "FP64 (double precision) performance (FLOP/s)": "7800000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "15670000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "31330000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "815.0",
      "Base clock (MHz)": "1312.0",
      "Boost clock (MHz)": "1530.0",
      "Memory clock (MHz)": "876.0",
      "Memory bus (bit)": "4096.0",
      "Tensor cores": "640.0",
      "Process size (nm)": "12.0",
      "Transistors (millions)": "21100.0",
      "Link to datasheet": "https://images.nvidia.com/content/technologies/volta/pdf/437317-Volta-V100-DS-NV-US-WEB.pdf\nhttps://www.techpowerup.com/gpu-specs/tesla-v100-sxm2-16-gb.c3018\nhttps://images.nvidia.com/content/technologies/volta/pdf/volta-v100-datasheet-update-us-1165301-r5.pdf",
      "Source for the price": "",
      "ML models": "PAGnol-XL,FRED-T5-XL,ruGPT-3.5 13B",
      "Last modified": "2025-10-29 21:57:02+00:00",
      "Max performance": "125000000000000.0",
      "Energy efficiency": "416666666666.6667",
      "Total processing performance (bit-OP/s)": "2000000000000000.0",
      "Price-performance": "10869565217.391304",
      "ML OP/s": "125000000000000.0"
    },
    {
      "Hardware name": "NVIDIA Tesla V100 SXM2 16 GB",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2017-06-21",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "125000000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "",
      "Memory bandwidth (byte/s)": "897000000000.0",
      "Intranode bandwidth (byte/s)": "300000000000.0",
      "Internode bandwidth (bit/s)": "100000000000.0",
      "TDP (W)": "300.0",
      "FP64 (double precision) performance (FLOP/s)": "7834000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "15670000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "31330000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "815.0",
      "Base clock (MHz)": "1312.0",
      "Boost clock (MHz)": "1530.0",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "12.0",
      "Transistors (millions)": "21100.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/tesla-v100-sxm2-16-gb.c3018",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-10-29 21:57:01+00:00",
      "Max performance": "125000000000000.0",
      "Energy efficiency": "416666666666.6667",
      "Total processing performance (bit-OP/s)": "2000000000000000.0",
      "Price-performance": "",
      "ML OP/s": "125000000000000.0"
    },
    {
      "Hardware name": "NVIDIA Tesla V100 PCIe 16 GB",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2017-06-21",
      "Release price (USD)": "10664.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "112000000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "16000000000.0",
      "Memory bandwidth (byte/s)": "897000000000.0",
      "Intranode bandwidth (byte/s)": "32000000000.0",
      "Internode bandwidth (bit/s)": "100000000000.0",
      "TDP (W)": "250.0",
      "FP64 (double precision) performance (FLOP/s)": "7066000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "14130000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "28260000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "815.0",
      "Base clock (MHz)": "1245.0",
      "Boost clock (MHz)": "1380.0",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "4096.0",
      "Tensor cores": "640.0",
      "Process size (nm)": "12.0",
      "Transistors (millions)": "21100.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/tesla-v100-pcie-16-gb.c2957",
      "Source for the price": "https://www.microway.com/hpc-tech-tips/nvidia-tesla-v100-price-analysis/",
      "ML models": "ASE",
      "Last modified": "2025-10-29 21:57:02+00:00",
      "Max performance": "112000000000000.0",
      "Energy efficiency": "448000000000.0",
      "Total processing performance (bit-OP/s)": "1792000000000000.0",
      "Price-performance": "10502625656.414104",
      "ML OP/s": "112000000000000.0"
    },
    {
      "Hardware name": "NVIDIA V100",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2017-06-21",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "125000000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "",
      "Memory bandwidth (byte/s)": "897000000000.0",
      "Intranode bandwidth (byte/s)": "300000000000.0",
      "Internode bandwidth (bit/s)": "100000000000.0",
      "TDP (W)": "300.0",
      "FP64 (double precision) performance (FLOP/s)": "7834000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "15670000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "31330000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "815.0",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "4096.0",
      "Tensor cores": "640.0",
      "Process size (nm)": "12.0",
      "Transistors (millions)": "21100.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/tesla-v100-pcie-16-gb.c2957\nhttps://www.techpowerup.com/gpu-specs/tesla-v100-pcie-32-gb.c3184\nhttps://www.techpowerup.com/gpu-specs/tesla-v100-sxm2-16-gb.c3471\nhttps://www.techpowerup.com/gpu-specs/tesla-v100-sxm2-16-gb.c3018\nhttps://www.techpowerup.com/gpu-specs/tesla-v100-sxm2-32-gb.c3185\nhttps://www.techpowerup.com/gpu-specs/tesla-v100-sxm3-32-gb.c3472\nhttps://www.techpowerup.com/gpu-specs/tesla-v100s-pcie-32-gb.c3467",
      "Source for the price": "",
      "ML models": "DARK,ProBERTa,DD-PPO,Diffusion-GAN,Projected GAN,ADM,WizardLM-7B,DETR,DDPM-IP (CelebA),SantaCoder,Persia,Incoder-6.7B,RedPajama-INCITE-7B-Base,CTM (CIFAR-10),Transformer + Simple Recurrent Unit,ImageBind,ESM1-670M (UR50/S),ESM1-670M (UR50/D),ESM1-670M (UR100),ESM1-85M,ESM1-43M,ESM1b,ESM1v,Rita-XLarge,ProteinLM,TAPE Transformer,VALL-E,MultiBand Diffusion,VALL-E X,Transformer (Adaptive Input Embeddings) WT103,LUKE,SeamlessM4T,Pangu-Weather,Kosmos-2,mBART-50,Detic,MegaMolBART,ViT + DINO,YOLOX-X,CamemBERT,ProxylessNAS,Once for All,CLIP (ViT L/14@336px),ESM2-15B,ERNIE 3.0,CPM-Large,ESM2-650M,DeBERTa,PIXART-\u03b1,DeiT-B,Theseus 6/768,CLUE,WeNet (Penn Treebank),PermuteFormer,RFdiffusion,DiffQ Transformer (16L),CODEFUSION (Python),TrOCR,PLATO-XL,SEER,MoLFormer-XL,Soccer Robot,CodonTransformer,ProteinSGM,FastSpeech,FastSpeech 2,ResNeXt-101 32x48d,GPT-2+Active-SGD (WT2),ProtRNA,ProtT3,mini-GPT-2+Active-AdamW,Monarch-GPT-2-Small,GPT-2-Small+Pixelfly,DITTO,Monarch-GPT-2-Medium,GPT-2-Medium+Pixelfly,GPT-2 (117M, SLW 110K),T2R 75% + Pretrain (WT-103),T2R + Pretrain,SRU++ Base,SRU++ Large only 2 attention layers (k=5) (WT103),Linear Transformer (small),Subformer (83M),Subformer (96M),Megatron-LM (2.5B),GPT-2 (1.5B, Curriculum Learning 45K),Adaptive Input Transformer + RD,T2R + Random Init,GLM-10B,SRU++ Large,Subformer (122M),DeLighT,GPT3-6.7B (rerun of original),TransformerXL + spectrum control,All-attention network + adaptive span,FAIRSEQ Adaptive Inputs,OPUS-Design,FABind,PMLM-large,HJRSS,WeNet (PTB),DreamerV3,QRNN,PepINVENT,ChemBERTa,ShapeMol,LM-GVP,Profile Prediction,SSA,GemNet-OC ,Dexterous In-Hand Manipulation [control policy],StyleGAN3-T,StyleGAN3-R,StyleGAN,StyleGAN2,Hanabi 4 player,Sparse all-MLP,Brain2Qwerty,Very Deep VAEs (ImageNet-64),StyleGAN-XL,Sparse Transformer (ImageNet),Sparse Transformer (CIFAR10),Sparse Transformer (Enwik8),OC-GAN (Visual Genome),OC-GAN (COCO-Stuff),Wu Dao - Wen Yuan,Wu Dao - Wen Hui,CARP,Spec-Drafter,RoFormer,RMSNorm (Transformer-base)",
      "Last modified": "2025-11-04 18:54:26+00:00",
      "Max performance": "125000000000000.0",
      "Energy efficiency": "416666666666.6667",
      "Total processing performance (bit-OP/s)": "2000000000000000.0",
      "Price-performance": "",
      "ML OP/s": "125000000000000.0"
    },
    {
      "Hardware name": "PEZY-SC2",
      "Manufacturer": "PEZY",
      "Type": "",
      "Notes": "",
      "Release date": "2017-06-01",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "",
      "Memory bandwidth (byte/s)": "102377186750.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "180.0",
      "FP64 (double precision) performance (FLOP/s)": "4100000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "8192000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "16040000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "16.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://archive.ph/yiqZG\nhttps://en.wikichip.org/wiki/pezy/pezy-scx/pezy-sc2",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:55:44+00:00",
      "Max performance": "16040000000000.0",
      "Energy efficiency": "89111111111.11111",
      "Total processing performance (bit-OP/s)": "262400000000000.0",
      "Price-performance": "",
      "ML OP/s": "16040000000000.0"
    },
    {
      "Hardware name": "Google TPU v2",
      "Manufacturer": "Google",
      "Type": "TPU",
      "Notes": "Some sources claim 45 TFLOP/s, but Jouppi et. al., authored by Google's TPU lead, disclose 46 TFLOP/s peak",
      "Release date": "2017-05-01",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "46000000000000.0",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "16000000000.0",
      "Memory bandwidth (byte/s)": "600000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "280.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "3000000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "16.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://dl.acm.org/doi/pdf/10.1145/3360307",
      "Source for the price": "",
      "ML models": "AlphaZero,BERT-Large,Mesh-TensorFlow Transformer 4.9B (language),Mesh-TensorFlow Transformer 2.9B (translation),Soccer Robot,Transformer-XL + RelationLM,Transformer-XL + AutoDropout (PTB),AraBERT,LIMoE-H/14,Evolved Transformer,FFN SwiGLU",
      "Last modified": "2025-12-12 23:05:20+00:00",
      "Max performance": "46000000000000.0",
      "Energy efficiency": "164285714285.7143",
      "Total processing performance (bit-OP/s)": "720000000000000.0",
      "Price-performance": "",
      "ML OP/s": "46000000000000.0"
    },
    {
      "Hardware name": "NVIDIA TITAN Xp",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2017-04-06",
      "Release price (USD)": "1199.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "12000000000.0",
      "Memory bandwidth (byte/s)": "548000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "250.0",
      "FP64 (double precision) performance (FLOP/s)": "379700000000.0",
      "FP32 (single precision) performance (FLOP/s)": "12150000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "189800000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "471.0",
      "Base clock (MHz)": "1405.0",
      "Boost clock (MHz)": "1582.0",
      "Memory clock (MHz)": "1426.0",
      "Memory bus (bit)": "384.0",
      "Tensor cores": "",
      "Process size (nm)": "16.0",
      "Transistors (millions)": "11800.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/titan-xp.c2948",
      "Source for the price": "",
      "ML models": "DNA Fine-Tuned Language Model (DFLM),DensePhrases,Refined Part Pooling,GraSR",
      "Last modified": "2025-08-04 18:20:23+00:00",
      "Max performance": "12150000000000.0",
      "Energy efficiency": "48600000000.0",
      "Total processing performance (bit-OP/s)": "388800000000000.0",
      "Price-performance": "10133444537.114262",
      "ML OP/s": "12150000000000.0"
    },
    {
      "Hardware name": "NVIDIA GeForce GTX 1080 Ti",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2017-03-10",
      "Release price (USD)": "699.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "11000000000.0",
      "Memory bandwidth (byte/s)": "484000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "250.0",
      "FP64 (double precision) performance (FLOP/s)": "354400000000.0",
      "FP32 (single precision) performance (FLOP/s)": "11340000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "177200000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "471.0",
      "Base clock (MHz)": "1481.0",
      "Boost clock (MHz)": "1582.0",
      "Memory clock (MHz)": "1376.0",
      "Memory bus (bit)": "352.0",
      "Tensor cores": "",
      "Process size (nm)": "16.0",
      "Transistors (millions)": "11800.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/geforce-gtx-1080-ti.c2877",
      "Source for the price": "https://www.techpowerup.com/gpu-specs/tesla-p100-pcie-16-gb.c2888",
      "ML models": "Cross-lingual alignment,AlphaX-1,PeptideBERT,NAS+ESS (23M),Adaptive LSTM + DeFINE,Transformer-C,CT-MoS (WT2),NAS+ESS (156M),DARTS,ENAS,MEGNet (molecule model),MEGNet (crystal band gap model),MEGNet (crystal formation energy model),MEGNet (crystal elasticity model),GemNet-T (OC20)",
      "Last modified": "2025-07-21 09:55:53+00:00",
      "Max performance": "11340000000000.0",
      "Energy efficiency": "45360000000.0",
      "Total processing performance (bit-OP/s)": "362880000000000.0",
      "Price-performance": "16223175965.665236",
      "ML OP/s": "11340000000000.0"
    },
    {
      "Hardware name": "NVIDIA Quadro P600",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2017-02-07",
      "Release price (USD)": "178.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "2000000000.0",
      "Memory bandwidth (byte/s)": "64130000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "40.0",
      "FP64 (double precision) performance (FLOP/s)": "37370000000.0",
      "FP32 (single precision) performance (FLOP/s)": "1196000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "18680000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "Samsung",
      "Die Size (mm^2)": "132.0",
      "Base clock (MHz)": "1329.0",
      "Boost clock (MHz)": "1557.0",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "128.0",
      "Tensor cores": "",
      "Process size (nm)": "14.0",
      "Transistors (millions)": "3300.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/quadro-p600.c2933",
      "Source for the price": "https://technical.city/en/video/GeForce-GTX-970-vs-Quadro-P600",
      "ML models": "GPT-1",
      "Last modified": "2025-07-21 09:55:56+00:00",
      "Max performance": "1196000000000.0",
      "Energy efficiency": "29900000000.0",
      "Total processing performance (bit-OP/s)": "38272000000000.0",
      "Price-performance": "6719101123.595506",
      "ML OP/s": "1196000000000.0"
    },
    {
      "Hardware name": "NVIDIA Quadro P4000",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2017-02-06",
      "Release price (USD)": "815.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "8000000000.0",
      "Memory bandwidth (byte/s)": "243400000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "105.0",
      "FP64 (double precision) performance (FLOP/s)": "165800000000.0",
      "FP32 (single precision) performance (FLOP/s)": "5304000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "82880000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "314.0",
      "Base clock (MHz)": "1202.0",
      "Boost clock (MHz)": "1480.0",
      "Memory clock (MHz)": "1901.0",
      "Memory bus (bit)": "256.0",
      "Tensor cores": "",
      "Process size (nm)": "16.0",
      "Transistors (millions)": "7200.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/quadro-p4000.c2930",
      "Source for the price": "https://www.techpowerup.com/gpu-specs/quadro-p4000.c2930",
      "ML models": "",
      "Last modified": "2025-07-21 09:55:58+00:00",
      "Max performance": "5304000000000.0",
      "Energy efficiency": "50514285714.28571",
      "Total processing performance (bit-OP/s)": "169728000000000.0",
      "Price-performance": "6507975460.1227",
      "ML OP/s": "5304000000000.0"
    },
    {
      "Hardware name": "MT-2000",
      "Manufacturer": "National University of Defense Technology",
      "Type": "",
      "Notes": "",
      "Release date": "2017-01-01",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "",
      "Memory bandwidth (byte/s)": "153652000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "240.0",
      "FP64 (double precision) performance (FLOP/s)": "2460000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "4920000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "",
      "Transistors (millions)": "",
      "Link to datasheet": "https://archive.ph/CdMwe",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:56:11+00:00",
      "Max performance": "4920000000000.0",
      "Energy efficiency": "20500000000.0",
      "Total processing performance (bit-OP/s)": "157440000000000.0",
      "Price-performance": "",
      "ML OP/s": "4920000000000.0"
    },
    {
      "Hardware name": "NVIDIA Quadro P5000",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2016-10-01",
      "Release price (USD)": "2499.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "16000000000.0",
      "Memory bandwidth (byte/s)": "288000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "180.0",
      "FP64 (double precision) performance (FLOP/s)": "277300000000.0",
      "FP32 (single precision) performance (FLOP/s)": "8873000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "138600000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "314.0",
      "Base clock (MHz)": "1607.0",
      "Boost clock (MHz)": "1733.0",
      "Memory clock (MHz)": "1127.0",
      "Memory bus (bit)": "256.0",
      "Tensor cores": "",
      "Process size (nm)": "16.0",
      "Transistors (millions)": "7200.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/quadro-p5000.c2864",
      "Source for the price": "https://www.techpowerup.com/gpu-specs/quadro-p5000.c2864",
      "ML models": "",
      "Last modified": "2025-07-21 09:56:18+00:00",
      "Max performance": "8873000000000.0",
      "Energy efficiency": "49294444444.44444",
      "Total processing performance (bit-OP/s)": "283936000000000.0",
      "Price-performance": "3550620248.09924",
      "ML OP/s": "8873000000000.0"
    },
    {
      "Hardware name": "NVIDIA Quadro P6000",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2016-10-01",
      "Release price (USD)": "5999.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "24000000000.0",
      "Memory bandwidth (byte/s)": "433000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "250.0",
      "FP64 (double precision) performance (FLOP/s)": "394800000000.0",
      "FP32 (single precision) performance (FLOP/s)": "12630000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "197400000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "471.0",
      "Base clock (MHz)": "1506.0",
      "Boost clock (MHz)": "1645.0",
      "Memory clock (MHz)": "1127.0",
      "Memory bus (bit)": "384.0",
      "Tensor cores": "",
      "Process size (nm)": "16.0",
      "Transistors (millions)": "11800.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/quadro-p6000.c2865",
      "Source for the price": "https://www.techpowerup.com/gpu-specs/quadro-p6000.c2865",
      "ML models": "",
      "Last modified": "2025-07-21 09:56:18+00:00",
      "Max performance": "12630000000000.0",
      "Energy efficiency": "50520000000.0",
      "Total processing performance (bit-OP/s)": "404160000000000.0",
      "Price-performance": "2105350891.8153024",
      "ML OP/s": "12630000000000.0"
    },
    {
      "Hardware name": "NVIDIA P40",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2016-09-13",
      "Release price (USD)": "5699.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "24000000000.0",
      "Memory bandwidth (byte/s)": "694300000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "250.0",
      "FP64 (double precision) performance (FLOP/s)": "367400000000.0",
      "FP32 (single precision) performance (FLOP/s)": "11760000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "183700000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "471.0",
      "Base clock (MHz)": "1303.0",
      "Boost clock (MHz)": "1531.0",
      "Memory clock (MHz)": "1808.0",
      "Memory bus (bit)": "384.0",
      "Tensor cores": "",
      "Process size (nm)": "",
      "Transistors (millions)": "11800.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/tesla-p40.c2878",
      "Source for the price": "https://www.techpowerup.com/gpu-specs/tesla-p40.c2878",
      "ML models": "Tensorized Transformer (OBW),Tensorized Transformer (PTB),Tensorized Transformer (large PTB),Tensorized Transformer (W103),Tensorized Transformer (257M)",
      "Last modified": "2025-07-21 09:56:19+00:00",
      "Max performance": "11760000000000.0",
      "Energy efficiency": "47040000000.0",
      "Total processing performance (bit-OP/s)": "376320000000000.0",
      "Price-performance": "2063519915.7746973",
      "ML OP/s": "11760000000000.0"
    },
    {
      "Hardware name": "NVIDIA Tesla P4",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2016-09-13",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "8000000000.0",
      "Memory bandwidth (byte/s)": "192300000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "75.0",
      "FP64 (double precision) performance (FLOP/s)": "178200000000.0",
      "FP32 (single precision) performance (FLOP/s)": "5704000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "89120000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "22000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "314.0",
      "Base clock (MHz)": "886.0",
      "Boost clock (MHz)": "1114.0",
      "Memory clock (MHz)": "1502.0",
      "Memory bus (bit)": "256.0",
      "Tensor cores": "",
      "Process size (nm)": "16.0",
      "Transistors (millions)": "7200.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/tesla-p4.c2879",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:56:20+00:00",
      "Max performance": "22000000000000.0",
      "Energy efficiency": "293333333333.3333",
      "Total processing performance (bit-OP/s)": "182528000000000.0",
      "Price-performance": "",
      "ML OP/s": "22000000000000.0"
    },
    {
      "Hardware name": "NVIDIA Tesla P100 PCIe 16GB",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2016-06-20",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "",
      "Memory bandwidth (byte/s)": "732200000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "250.0",
      "FP64 (double precision) performance (FLOP/s)": "4763000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "9526000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "19050000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "610.0",
      "Base clock (MHz)": "1190.0",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "16.0",
      "Transistors (millions)": "15300.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/tesla-p100-pcie-16-gb.c2888",
      "Source for the price": "",
      "ML models": "RNN + char2-MS-vec,RNN + char3-MS-vec,RNN + char4-MS-vec",
      "Last modified": "2025-10-29 21:56:37+00:00",
      "Max performance": "19050000000000.0",
      "Energy efficiency": "76200000000.0",
      "Total processing performance (bit-OP/s)": "304832000000000.0",
      "Price-performance": "",
      "ML OP/s": "19050000000000.0"
    },
    {
      "Hardware name": "NVIDIA Tesla P100 PCIe 12GB",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2016-06-20",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "",
      "Memory bandwidth (byte/s)": "549100000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "250.0",
      "FP64 (double precision) performance (FLOP/s)": "4763000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "9526000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "19050000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "610.0",
      "Base clock (MHz)": "1190.0",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "16.0",
      "Transistors (millions)": "15300.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/tesla-p100-pcie-12-gb.c2915",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:56:35+00:00",
      "Max performance": "19050000000000.0",
      "Energy efficiency": "76200000000.0",
      "Total processing performance (bit-OP/s)": "304832000000000.0",
      "Price-performance": "",
      "ML OP/s": "19050000000000.0"
    },
    {
      "Hardware name": "NVIDIA P100 PCIe 16GB",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2016-06-20",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "16000000000.0",
      "Memory bandwidth (byte/s)": "732000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "250.0",
      "FP64 (double precision) performance (FLOP/s)": "4700000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "9560000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "19100000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "610.0",
      "Base clock (MHz)": "1190.0",
      "Boost clock (MHz)": "1329.0",
      "Memory clock (MHz)": "715.0",
      "Memory bus (bit)": "4096.0",
      "Tensor cores": "",
      "Process size (nm)": "16.0",
      "Transistors (millions)": "15300.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/tesla-p100-pcie-16-gb.c2888",
      "Source for the price": "",
      "ML models": "T-DMCA",
      "Last modified": "2025-11-03 21:32:22+00:00",
      "Max performance": "19100000000000.0",
      "Energy efficiency": "76400000000.0",
      "Total processing performance (bit-OP/s)": "305920000000000.0",
      "Price-performance": "",
      "ML OP/s": "19100000000000.0"
    },
    {
      "Hardware name": "NVIDIA GeForce GTX 1080",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2016-05-27",
      "Release price (USD)": "599.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "8000000000.0",
      "Memory bandwidth (byte/s)": "320300000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "180.0",
      "FP64 (double precision) performance (FLOP/s)": "277300000000.0",
      "FP32 (single precision) performance (FLOP/s)": "8873000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "138600000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "314.0",
      "Base clock (MHz)": "1607.0",
      "Boost clock (MHz)": "1733.0",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "256.0",
      "Tensor cores": "",
      "Process size (nm)": "16.0",
      "Transistors (millions)": "7200.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/geforce-gtx-1080.c2839",
      "Source for the price": "",
      "ML models": "SEST,CT-MoS (PTB),CT-MoS + DynamicEval (PTB),AWD-LSTM + DeFINE",
      "Last modified": "2025-07-21 09:56:40+00:00",
      "Max performance": "8873000000000.0",
      "Energy efficiency": "49294444444.44444",
      "Total processing performance (bit-OP/s)": "283936000000000.0",
      "Price-performance": "14813021702.838064",
      "ML OP/s": "8873000000000.0"
    },
    {
      "Hardware name": "NVIDIA Tesla P100 SXM2",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2016-04-05",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "",
      "Memory bandwidth (byte/s)": "",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "300.0",
      "FP64 (double precision) performance (FLOP/s)": "5304000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "10610000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "21220000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "610.0",
      "Base clock (MHz)": "1328.0",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "16.0",
      "Transistors (millions)": "15300.0",
      "Link to datasheet": "",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-10-29 21:56:40+00:00",
      "Max performance": "21220000000000.0",
      "Energy efficiency": "70733333333.33333",
      "Total processing performance (bit-OP/s)": "339520000000000.0",
      "Price-performance": "",
      "ML OP/s": "21220000000000.0"
    },
    {
      "Hardware name": "NVIDIA P100",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2016-04-05",
      "Release price (USD)": "5699.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "16000000000.0",
      "Memory bandwidth (byte/s)": "732000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "250.0",
      "FP64 (double precision) performance (FLOP/s)": "4700000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "9300000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "18700000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "16.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://images.nvidia.com/content/tesla/pdf/nvidia-tesla-p100-PCIe-datasheet.pdf",
      "Source for the price": "https://www.hardwarezone.com.sg/tech-news-nvidia-dgx-a100-supercomputer-super-performance-fight-covid-19#:~:text=The%20new%20DGX%20A100%20costs,has%20a%20height%20of%20444mm.",
      "ML models": "Transformer,IMPALA,ProteinGAN,RNMT+,LSTM (Hebbian, Cache, MbPA),QT-Opt,HiPPO-LegS,OpenAI Five,OpenAI Five Rerun,GPT-2+Active-SGD (WT2),mini-GPT-2+Active-AdamW,RoBERTa (PFAM),B2T connection (16L)",
      "Last modified": "2025-10-29 21:56:45+00:00",
      "Max performance": "18700000000000.0",
      "Energy efficiency": "74800000000.0",
      "Total processing performance (bit-OP/s)": "300800000000000.0",
      "Price-performance": "3281277417.090718",
      "ML OP/s": "18700000000000.0"
    },
    {
      "Hardware name": "NVIDIA Tesla P100 DGXS",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2016-04-05",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "",
      "Memory bandwidth (byte/s)": "",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "300.0",
      "FP64 (double precision) performance (FLOP/s)": "5304000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "10610000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "21220000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "610.0",
      "Base clock (MHz)": "1328.0",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "16.0",
      "Transistors (millions)": "15300.0",
      "Link to datasheet": "",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-10-29 21:56:39+00:00",
      "Max performance": "21220000000000.0",
      "Energy efficiency": "70733333333.33333",
      "Total processing performance (bit-OP/s)": "339520000000000.0",
      "Price-performance": "",
      "ML OP/s": "21220000000000.0"
    },
    {
      "Hardware name": "NVIDIA M40",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2015-11-10",
      "Release price (USD)": "10799.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "12000000000.0",
      "Memory bandwidth (byte/s)": "288000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "250.0",
      "FP64 (double precision) performance (FLOP/s)": "213500000000.0",
      "FP32 (single precision) performance (FLOP/s)": "6832000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "601.0",
      "Base clock (MHz)": "948.0",
      "Boost clock (MHz)": "1112.0",
      "Memory clock (MHz)": "1502.0",
      "Memory bus (bit)": "384.0",
      "Tensor cores": "",
      "Process size (nm)": "",
      "Transistors (millions)": "8000.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/tesla-m40.c2771",
      "Source for the price": "https://texas.gs.shi.com/product/32445647/NVIDIA-Tesla-M40-GPU-computing-processor",
      "ML models": "YOLOv3,RetinaNet-R101,MemoReader,ConvS2S (ensemble of 8 models)",
      "Last modified": "2025-07-21 09:57:17+00:00",
      "Max performance": "6832000000000.0",
      "Energy efficiency": "27328000000.0",
      "Total processing performance (bit-OP/s)": "218624000000000.0",
      "Price-performance": "632651171.4047596",
      "ML OP/s": "6832000000000.0"
    },
    {
      "Hardware name": "NVIDIA TESLA M60",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2015-08-30",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "16000000000.0",
      "Memory bandwidth (byte/s)": "320800000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "300.0",
      "FP64 (double precision) performance (FLOP/s)": "301600000000.0",
      "FP32 (single precision) performance (FLOP/s)": "9650000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "398.0",
      "Base clock (MHz)": "557.0",
      "Boost clock (MHz)": "1178.0",
      "Memory clock (MHz)": "1253.0",
      "Memory bus (bit)": "512.0",
      "Tensor cores": "",
      "Process size (nm)": "28.0",
      "Transistors (millions)": "5200.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/tesla-m60.c2760",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:57:39+00:00",
      "Max performance": "9650000000000.0",
      "Energy efficiency": "32166666666.666668",
      "Total processing performance (bit-OP/s)": "308800000000000.0",
      "Price-performance": "",
      "ML OP/s": "9650000000000.0"
    },
    {
      "Hardware name": "NVIDIA Quadro M4000",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2015-06-29",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "8000000000.0",
      "Memory bandwidth (byte/s)": "192000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "120.0",
      "FP64 (double precision) performance (FLOP/s)": "80390000000.0",
      "FP32 (single precision) performance (FLOP/s)": "2573000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "398.0",
      "Base clock (MHz)": "773.0",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "1502.0",
      "Memory bus (bit)": "256.0",
      "Tensor cores": "",
      "Process size (nm)": "28.0",
      "Transistors (millions)": "5200.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/quadro-m4000.c2757",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:57:45+00:00",
      "Max performance": "2573000000000.0",
      "Energy efficiency": "21441666666.666668",
      "Total processing performance (bit-OP/s)": "82336000000000.0",
      "Price-performance": "",
      "ML OP/s": "2573000000000.0"
    },
    {
      "Hardware name": "Google TPU v1",
      "Manufacturer": "Google",
      "Type": "TPU",
      "Notes": "",
      "Release date": "2015-05-15",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "8589934592.0",
      "Memory bandwidth (byte/s)": "34000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "75.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "92000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "331.0",
      "Base clock (MHz)": "700.0",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "28.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9499913",
      "Source for the price": "",
      "ML models": "AlphaGo Master,AlphaGo Zero,AlphaZero",
      "Last modified": "2025-10-29 21:38:06+00:00",
      "Max performance": "92000000000000.0",
      "Energy efficiency": "1226666666666.6667",
      "Total processing performance (bit-OP/s)": "736000000000000.0",
      "Price-performance": "",
      "ML OP/s": "92000000000000.0"
    },
    {
      "Hardware name": "NVIDIA GeForce GTX TITAN X",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2015-03-17",
      "Release price (USD)": "999.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "12000000000.0",
      "Memory bandwidth (byte/s)": "336600000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "250.0",
      "FP64 (double precision) performance (FLOP/s)": "209100000000.0",
      "FP32 (single precision) performance (FLOP/s)": "6691000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "601.0",
      "Base clock (MHz)": "1000.0",
      "Boost clock (MHz)": "1089.0",
      "Memory clock (MHz)": "1753.0",
      "Memory bus (bit)": "384.0",
      "Tensor cores": "",
      "Process size (nm)": "28.0",
      "Transistors (millions)": "8000.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/geforce-gtx-titan-x.c2632",
      "Source for the price": "",
      "ML models": "DeepSpeech2 (English),YOLOv3,Mono3D++,BIDAF,PolyNet,GELU for CIFAR-10,SSD,Part-of-sentence tagging model,Named Entity Recognition model,Graph-based Semi-Supervised Learning (GSSL) Model on MNIST,SAF R-CNN,DeepLoc",
      "Last modified": "2025-09-05 20:46:37+00:00",
      "Max performance": "6691000000000.0",
      "Energy efficiency": "26764000000.0",
      "Total processing performance (bit-OP/s)": "214112000000000.0",
      "Price-performance": "6697697697.697698",
      "ML OP/s": "6691000000000.0"
    },
    {
      "Hardware name": "NVIDIA Quadro K1200",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2015-01-28",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "4000000000.0",
      "Memory bandwidth (byte/s)": "80000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "45.0",
      "FP64 (double precision) performance (FLOP/s)": "35970000000.0",
      "FP32 (single precision) performance (FLOP/s)": "1151000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "148.0",
      "Base clock (MHz)": "1058.0",
      "Boost clock (MHz)": "1124.0",
      "Memory clock (MHz)": "1250.0",
      "Memory bus (bit)": "128.0",
      "Tensor cores": "",
      "Process size (nm)": "28.0",
      "Transistors (millions)": "1870.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/quadro-k1200.c2641",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 09:58:24+00:00",
      "Max performance": "1151000000000.0",
      "Energy efficiency": "25577777777.77778",
      "Total processing performance (bit-OP/s)": "36832000000000.0",
      "Price-performance": "",
      "ML OP/s": "1151000000000.0"
    },
    {
      "Hardware name": "NVIDIA Tesla K80",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2014-11-17",
      "Release price (USD)": "5000.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "24000000000.0",
      "Memory bandwidth (byte/s)": "241000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "300.0",
      "FP64 (double precision) performance (FLOP/s)": "2742000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "8126000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "561.0",
      "Base clock (MHz)": "562.0",
      "Boost clock (MHz)": "824.0",
      "Memory clock (MHz)": "1253.0",
      "Memory bus (bit)": "768.0",
      "Tensor cores": "",
      "Process size (nm)": "28.0",
      "Transistors (millions)": "7100.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/tesla-k80.c2616; https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/tesla-product-literature/TeslaK80-datasheet.pdf",
      "Source for the price": "https://www.anandtech.com/show/8729/nvidia-launches-tesla-k80-gk210-gpu",
      "ML models": "GNMT,Xception,JFT,Big-Little Net,OR-WideResNet,UniRep,Fold2Seq,Cube-Space AutoEncoder,code2vec,MoLeR,Zoneout + Variational LSTM (WT2)",
      "Last modified": "2025-10-29 21:56:18+00:00",
      "Max performance": "8126000000000.0",
      "Energy efficiency": "27086666666.666668",
      "Total processing performance (bit-OP/s)": "260032000000000.0",
      "Price-performance": "1625200000.0",
      "ML OP/s": "8126000000000.0"
    },
    {
      "Hardware name": "NVIDIA GeForce GTX 980",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2014-09-19",
      "Release price (USD)": "549.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "4000000000.0",
      "Memory bandwidth (byte/s)": "224400000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "165.0",
      "FP64 (double precision) performance (FLOP/s)": "155600000000.0",
      "FP32 (single precision) performance (FLOP/s)": "4981000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "398.0",
      "Base clock (MHz)": "1127.0",
      "Boost clock (MHz)": "1216.0",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "256.0",
      "Tensor cores": "",
      "Process size (nm)": "28.0",
      "Transistors (millions)": "5200.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/geforce-gtx-980.c2621",
      "Source for the price": "",
      "ML models": "SimpleNet",
      "Last modified": "2025-07-21 09:58:32+00:00",
      "Max performance": "4981000000000.0",
      "Energy efficiency": "30187878787.878788",
      "Total processing performance (bit-OP/s)": "159392000000000.0",
      "Price-performance": "9072859744.990892",
      "ML OP/s": "4981000000000.0"
    },
    {
      "Hardware name": "NVIDIA GeForce GTX Titan Black",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2014-02-18",
      "Release price (USD)": "999.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "6000000000.0",
      "Memory bandwidth (byte/s)": "336000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "250.0",
      "FP64 (double precision) performance (FLOP/s)": "1882000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "5645000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "561.0",
      "Base clock (MHz)": "889.0",
      "Boost clock (MHz)": "980.0",
      "Memory clock (MHz)": "1750.0",
      "Memory bus (bit)": "384.0",
      "Tensor cores": "",
      "Process size (nm)": "28.0",
      "Transistors (millions)": "7080.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/geforce-gtx-titan-black.c2549",
      "Source for the price": "https://www.techpowerup.com/gpu-specs/geforce-gtx-titan-black.c2549",
      "ML models": "VGG16,VGG-Face",
      "Last modified": "2025-10-16 15:09:25+00:00",
      "Max performance": "5645000000000.0",
      "Energy efficiency": "22580000000.0",
      "Total processing performance (bit-OP/s)": "180640000000000.0",
      "Price-performance": "5650650650.650651",
      "ML OP/s": "5645000000000.0"
    },
    {
      "Hardware name": "NVIDIA Tesla K40s",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2013-11-22",
      "Release price (USD)": "7699.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "12000000000.0",
      "Memory bandwidth (byte/s)": "288000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "245.0",
      "FP64 (double precision) performance (FLOP/s)": "1682000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "5046000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "561.0",
      "Base clock (MHz)": "745.0",
      "Boost clock (MHz)": "876.0",
      "Memory clock (MHz)": "1502.0",
      "Memory bus (bit)": "384.0",
      "Tensor cores": "",
      "Process size (nm)": "28.0",
      "Transistors (millions)": "7080.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/tesla-k40s.c2528",
      "Source for the price": "https://www.techpowerup.com/gpu-specs/tesla-k40s.c2528",
      "ML models": "AmoebaNet-A (F=448),VDCNN (on Amazon Review Full dataset),BIG LSTM+CNN INPUTS ,10 LSTMS + KN-5 (OPTIMAL WEIGHTS),genCNN + dyn eval",
      "Last modified": "2025-07-21 09:59:39+00:00",
      "Max performance": "5046000000000.0",
      "Energy efficiency": "20595918367.34694",
      "Total processing performance (bit-OP/s)": "161472000000000.0",
      "Price-performance": "655409793.4796727",
      "ML OP/s": "5046000000000.0"
    },
    {
      "Hardware name": "NVIDIA Tesla K40t",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2013-11-22",
      "Release price (USD)": "7699.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "12000000000.0",
      "Memory bandwidth (byte/s)": "288400000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "245.0",
      "FP64 (double precision) performance (FLOP/s)": "1682000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "5046000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "561.0",
      "Base clock (MHz)": "745.0",
      "Boost clock (MHz)": "876.0",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "384.0",
      "Tensor cores": "",
      "Process size (nm)": "28.0",
      "Transistors (millions)": "7080.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/tesla-k40t.c3403",
      "Source for the price": "https://www.techpowerup.com/gpu-specs/tesla-k40t.c3403#:~:text=Tesla%20K40t%20is%20connected%20to,launch%20was%207699%20US%20Dollars",
      "ML models": "MoE-Multi,MSRA (C, PReLU)",
      "Last modified": "2025-07-21 09:59:39+00:00",
      "Max performance": "5046000000000.0",
      "Energy efficiency": "20595918367.34694",
      "Total processing performance (bit-OP/s)": "161472000000000.0",
      "Price-performance": "655409793.4796727",
      "ML OP/s": "5046000000000.0"
    },
    {
      "Hardware name": "NVIDIA Quadro K6000",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2013-07-23",
      "Release price (USD)": "5265.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "12000000000.0",
      "Memory bandwidth (byte/s)": "288000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "225.0",
      "FP64 (double precision) performance (FLOP/s)": "1732000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "5196000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "561.0",
      "Base clock (MHz)": "797.0",
      "Boost clock (MHz)": "902.0",
      "Memory clock (MHz)": "1502.0",
      "Memory bus (bit)": "384.0",
      "Tensor cores": "",
      "Process size (nm)": "28.0",
      "Transistors (millions)": "7080.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/quadro-k6000.c2426",
      "Source for the price": "https://www.techpowerup.com/gpu-specs/quadro-k6000.c2426",
      "ML models": "RNNsearch-50*",
      "Last modified": "2025-07-21 10:00:26+00:00",
      "Max performance": "5196000000000.0",
      "Energy efficiency": "23093333333.33333",
      "Total processing performance (bit-OP/s)": "166272000000000.0",
      "Price-performance": "986894586.8945868",
      "ML OP/s": "5196000000000.0"
    },
    {
      "Hardware name": "NVIDIA GeForce GTX 780",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2013-05-23",
      "Release price (USD)": "649.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "3000000000.0",
      "Memory bandwidth (byte/s)": "288400000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "250.0",
      "FP64 (double precision) performance (FLOP/s)": "173200000000.0",
      "FP32 (single precision) performance (FLOP/s)": "4156000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "561.0",
      "Base clock (MHz)": "863.0",
      "Boost clock (MHz)": "902.0",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "384.0",
      "Tensor cores": "",
      "Process size (nm)": "28.0",
      "Transistors (millions)": "7080.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/geforce-gtx-780.c1701",
      "Source for the price": "",
      "ML models": "Fractional Max-Pooling",
      "Last modified": "2025-07-21 10:00:52+00:00",
      "Max performance": "4156000000000.0",
      "Energy efficiency": "16624000000.0",
      "Total processing performance (bit-OP/s)": "132992000000000.0",
      "Price-performance": "6403697996.918336",
      "ML OP/s": "4156000000000.0"
    },
    {
      "Hardware name": "NVIDIA Quadro K4000",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2013-03-01",
      "Release price (USD)": "1269.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "3000000000.0",
      "Memory bandwidth (byte/s)": "134800000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "80.0",
      "FP64 (double precision) performance (FLOP/s)": "51840000000.0",
      "FP32 (single precision) performance (FLOP/s)": "1244000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "221.0",
      "Base clock (MHz)": "810.0",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "192.0",
      "Tensor cores": "",
      "Process size (nm)": "28.0",
      "Transistors (millions)": "2540.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/quadro-k4000.c1841",
      "Source for the price": "https://videocardz.net/nvidia-quadro-k4000",
      "ML models": "Attend-Infer-Repeat",
      "Last modified": "2025-07-21 10:01:14+00:00",
      "Max performance": "1244000000000.0",
      "Energy efficiency": "15550000000.0",
      "Total processing performance (bit-OP/s)": "39808000000000.0",
      "Price-performance": "980299448.3845547",
      "ML OP/s": "1244000000000.0"
    },
    {
      "Hardware name": "NVIDIA GeForce GTX TITAN",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2013-02-19",
      "Release price (USD)": "999.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "6000000000.0",
      "Memory bandwidth (byte/s)": "288400000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "250.0",
      "FP64 (double precision) performance (FLOP/s)": "1570000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "4709000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "561.0",
      "Base clock (MHz)": "836.0",
      "Boost clock (MHz)": "876.0",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "384.0",
      "Tensor cores": "",
      "Process size (nm)": "28.0",
      "Transistors (millions)": "7080.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/geforce-gtx-titan.c1996",
      "Source for the price": "",
      "ML models": "SPPNet",
      "Last modified": "2025-07-21 10:01:15+00:00",
      "Max performance": "4709000000000.0",
      "Energy efficiency": "18836000000.0",
      "Total processing performance (bit-OP/s)": "150688000000000.0",
      "Price-performance": "4713713713.713714",
      "ML OP/s": "4709000000000.0"
    },
    {
      "Hardware name": "NVIDIA Tesla K20c",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2012-11-12",
      "Release price (USD)": "3199.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "5000000000.0",
      "Memory bandwidth (byte/s)": "208000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "225.0",
      "FP64 (double precision) performance (FLOP/s)": "1170000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "3520000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "561.0",
      "Base clock (MHz)": "706.0",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "1300.0",
      "Memory bus (bit)": "320.0",
      "Tensor cores": "",
      "Process size (nm)": "28.0",
      "Transistors (millions)": "7080.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/tesla-k20c.c564",
      "Source for the price": "",
      "ML models": "Large regularized LSTM",
      "Last modified": "2025-07-21 10:01:33+00:00",
      "Max performance": "3520000000000.0",
      "Energy efficiency": "15644444444.444445",
      "Total processing performance (bit-OP/s)": "112640000000000.0",
      "Price-performance": "1100343857.4554548",
      "ML OP/s": "3520000000000.0"
    },
    {
      "Hardware name": "NVIDIA Tesla K20X",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2012-11-12",
      "Release price (USD)": "7699.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "",
      "Memory bandwidth (byte/s)": "",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "235.0",
      "FP64 (double precision) performance (FLOP/s)": "1312000000000.0",
      "FP32 (single precision) performance (FLOP/s)": "3935000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "561.0",
      "Base clock (MHz)": "732.0",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "28.0",
      "Transistors (millions)": "7080.0",
      "Link to datasheet": "",
      "Source for the price": "https://archive.ph/Jxl43",
      "ML models": "OverFeat",
      "Last modified": "2025-07-21 10:01:33+00:00",
      "Max performance": "3935000000000.0",
      "Energy efficiency": "16744680851.06383",
      "Total processing performance (bit-OP/s)": "125920000000000.0",
      "Price-performance": "511105338.3556306",
      "ML OP/s": "3935000000000.0"
    },
    {
      "Hardware name": "NVIDIA Tesla C2050",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2011-07-25",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "",
      "Memory bandwidth (byte/s)": "",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "238.0",
      "FP64 (double precision) performance (FLOP/s)": "513900000000.0",
      "FP32 (single precision) performance (FLOP/s)": "1028000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "529.0",
      "Base clock (MHz)": "574.0",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "40.0",
      "Transistors (millions)": "3100.0",
      "Link to datasheet": "",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 10:03:40+00:00",
      "Max performance": "1028000000000.0",
      "Energy efficiency": "4319327731.092437",
      "Total processing performance (bit-OP/s)": "32896000000000.0",
      "Price-performance": "",
      "ML OP/s": "1028000000000.0"
    },
    {
      "Hardware name": "NVIDIA GeForce GTX 580",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2010-11-09",
      "Release price (USD)": "499.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "1536000000.0",
      "Memory bandwidth (byte/s)": "192400000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "244.0",
      "FP64 (double precision) performance (FLOP/s)": "197600000000.0",
      "FP32 (single precision) performance (FLOP/s)": "1581000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "520.0",
      "Base clock (MHz)": "772.0",
      "Boost clock (MHz)": "1544.0",
      "Memory clock (MHz)": "1002.0",
      "Memory bus (bit)": "384.0",
      "Tensor cores": "",
      "Process size (nm)": "40.0",
      "Transistors (millions)": "3000.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/geforce-gtx-580.c270",
      "Source for the price": "https://www.techpowerup.com/gpu-specs/geforce-gtx-580.c270",
      "ML models": "Dropout (CIFAR),Dropout (ImageNet),Dropout (MNIST),Dropout (TIMIT),AlexNet,Visualizing CNNs,DNN EM segmentation,CNN Committee (NIST),CNN Committee (MNIST),High Performance CNN (NORB),CNN committee (traffic sign)",
      "Last modified": "2025-07-21 10:05:03+00:00",
      "Max performance": "1581000000000.0",
      "Energy efficiency": "6479508196.721312",
      "Total processing performance (bit-OP/s)": "50592000000000.0",
      "Price-performance": "3168336673.346693",
      "ML OP/s": "1581000000000.0"
    },
    {
      "Hardware name": "NVIDIA GeForce GTX 280",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "2008-06-16",
      "Release price (USD)": "649.0",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "1024000000.0",
      "Memory bandwidth (byte/s)": "141100000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "236.0",
      "FP64 (double precision) performance (FLOP/s)": "77760000000.0",
      "FP32 (single precision) performance (FLOP/s)": "622100000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "TSMC",
      "Die Size (mm^2)": "576.0",
      "Base clock (MHz)": "602.0",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "1107.0",
      "Memory bus (bit)": "512.0",
      "Tensor cores": "",
      "Process size (nm)": "65.0",
      "Transistors (millions)": "1400.0",
      "Link to datasheet": "https://www.techpowerup.com/gpu-specs/geforce-gtx-280.c216",
      "Source for the price": "",
      "ML models": "6-layer MLP (MNIST)",
      "Last modified": "2025-07-21 10:08:23+00:00",
      "Max performance": "622100000000.0",
      "Energy efficiency": "2636016949.1525426",
      "Total processing performance (bit-OP/s)": "19907200000000.0",
      "Price-performance": "958551617.8736516",
      "ML OP/s": "622100000000.0"
    },
    {
      "Hardware name": "Huawei Ascend 910B Pro",
      "Manufacturer": "Huawei",
      "Type": "GPU",
      "Notes": "",
      "Release date": "",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "32000000000.0",
      "Memory bandwidth (byte/s)": "1228000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "280000000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "SMIC",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "1150.0",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "7.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://cset.georgetown.edu/publication/pushing-the-limits-huaweis-ai-chip-tests-u-s-export-controls/#:~:text=Ascend%20910%20models%20range%20from,specially%20designed%20for%20AI%20operations",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 10:16:59+00:00",
      "Max performance": "280000000000000.0",
      "Energy efficiency": "",
      "Total processing performance (bit-OP/s)": "4480000000000000.0",
      "Price-performance": "",
      "ML OP/s": "280000000000000.0"
    },
    {
      "Hardware name": "Huawei Ascend 910B1 (v1)",
      "Manufacturer": "Huawei",
      "Type": "GPU",
      "Notes": "",
      "Release date": "",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "64000000000.0",
      "Memory bandwidth (byte/s)": "1600000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "400000000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "SMIC",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "1850.0",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "7.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://cset.georgetown.edu/publication/pushing-the-limits-huaweis-ai-chip-tests-u-s-export-controls/#:~:text=Ascend%20910%20models%20range%20from,specially%20designed%20for%20AI%20operations",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 10:16:59+00:00",
      "Max performance": "400000000000000.0",
      "Energy efficiency": "",
      "Total processing performance (bit-OP/s)": "6400000000000000.0",
      "Price-performance": "",
      "ML OP/s": "400000000000000.0"
    },
    {
      "Hardware name": "Huawei Ascend 910B1 (v2)",
      "Manufacturer": "Huawei",
      "Type": "GPU",
      "Notes": "",
      "Release date": "",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "64000000000.0",
      "Memory bandwidth (byte/s)": "1600000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "SMIC",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "1850.0",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "7.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://cset.georgetown.edu/publication/pushing-the-limits-huaweis-ai-chip-tests-u-s-export-controls/#:~:text=Ascend%20910%20models%20range%20from,specially%20designed%20for%20AI%20operations",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-05-07 14:20:18+00:00",
      "Max performance": "0.0",
      "Energy efficiency": "",
      "Total processing performance (bit-OP/s)": "",
      "Price-performance": "",
      "ML OP/s": "0.0"
    },
    {
      "Hardware name": "Huawei Ascend 910B2",
      "Manufacturer": "Huawei",
      "Type": "GPU",
      "Notes": "",
      "Release date": "",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "64000000000.0",
      "Memory bandwidth (byte/s)": "1600000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "376000000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "SMIC",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "1800.0",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "7.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://cset.georgetown.edu/publication/pushing-the-limits-huaweis-ai-chip-tests-u-s-export-controls/#:~:text=Ascend%20910%20models%20range%20from,specially%20designed%20for%20AI%20operations",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 10:17:00+00:00",
      "Max performance": "376000000000000.0",
      "Energy efficiency": "",
      "Total processing performance (bit-OP/s)": "6016000000000000.0",
      "Price-performance": "",
      "ML OP/s": "376000000000000.0"
    },
    {
      "Hardware name": "Huawei Ascend 910B3",
      "Manufacturer": "Huawei",
      "Type": "GPU",
      "Notes": "",
      "Release date": "",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "64000000000.0",
      "Memory bandwidth (byte/s)": "1600000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "313000000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "SMIC",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "1800.0",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "7.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://cset.georgetown.edu/publication/pushing-the-limits-huaweis-ai-chip-tests-u-s-export-controls/#:~:text=Ascend%20910%20models%20range%20from,specially%20designed%20for%20AI%20operations",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 10:17:00+00:00",
      "Max performance": "313000000000000.0",
      "Energy efficiency": "",
      "Total processing performance (bit-OP/s)": "5008000000000000.0",
      "Price-performance": "",
      "ML OP/s": "313000000000000.0"
    },
    {
      "Hardware name": "Huawei Ascend 910B4",
      "Manufacturer": "Huawei",
      "Type": "GPU",
      "Notes": "",
      "Release date": "",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "32000000000.0",
      "Memory bandwidth (byte/s)": "800000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "72000000000000.0",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "280000000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "SMIC",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "1650.0",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "7.0",
      "Transistors (millions)": "",
      "Link to datasheet": "https://cset.georgetown.edu/publication/pushing-the-limits-huaweis-ai-chip-tests-u-s-export-controls/#:~:text=Ascend%20910%20models%20range%20from,specially%20designed%20for%20AI%20operations\nhttps://perma.cc/9B7N-X5XT",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-07-21 10:17:00+00:00",
      "Max performance": "280000000000000.0",
      "Energy efficiency": "",
      "Total processing performance (bit-OP/s)": "4480000000000000.0",
      "Price-performance": "",
      "ML OP/s": "280000000000000.0"
    },
    {
      "Hardware name": "Huawei Ascend 800T A2",
      "Manufacturer": "Huawei",
      "Type": "GPU",
      "Notes": "",
      "Release date": "",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "",
      "Memory (bytes)": "",
      "Memory bandwidth (byte/s)": "",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "310.0",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "256000000000000.0",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "512000000000000.0",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "",
      "Transistors (millions)": "",
      "Link to datasheet": "https://arxiv.org/pdf/2505.21411",
      "Source for the price": "",
      "ML models": "Pangu Pro MoE",
      "Last modified": "2025-07-23 16:41:34+00:00",
      "Max performance": "512000000000000.0",
      "Energy efficiency": "1651612903225.8064",
      "Total processing performance (bit-OP/s)": "",
      "Price-performance": "",
      "ML OP/s": "512000000000000.0"
    },
    {
      "Hardware name": "NVIDIA Rubin CPX",
      "Manufacturer": "NVIDIA",
      "Type": "GPU",
      "Notes": "",
      "Release date": "",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "",
      "FP4 performance (FLOP/s)": "3e+16",
      "Memory (bytes)": "128000000000.0",
      "Memory bandwidth (byte/s)": "",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "",
      "TDP (W)": "",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "",
      "Transistors (millions)": "",
      "Link to datasheet": "https://nvidianews.nvidia.com/news/nvidia-unveils-rubin-cpx-a-new-class-of-gpu-designed-for-massive-context-inference",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-09-11 17:27:44+00:00",
      "Max performance": "0.0",
      "Energy efficiency": "",
      "Total processing performance (bit-OP/s)": "",
      "Price-performance": "",
      "ML OP/s": "0.0"
    },
    {
      "Hardware name": "Huawei Ascend 950DT",
      "Manufacturer": "Huawei",
      "Type": "NPU",
      "Notes": "",
      "Release date": "",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "1000000000000000.0",
      "FP4 performance (FLOP/s)": "2000000000000000.0",
      "Memory (bytes)": "",
      "Memory bandwidth (byte/s)": "4000000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "2000000000000.0",
      "TDP (W)": "",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "",
      "Transistors (millions)": "",
      "Link to datasheet": "https://www.techradar.com/pro/huawei-ascend-950-vs-nvidia-h200-vs-amd-mi300-instinct-how-do-they-compare\nhttps://www.tomshardware.com/tech-industry/semiconductors/huawei-unveils-ascend-roadmap-backed-by-in-house-hbm",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-10-14 14:13:33+00:00",
      "Max performance": "0.0",
      "Energy efficiency": "",
      "Total processing performance (bit-OP/s)": "",
      "Price-performance": "",
      "ML OP/s": "1000000000000000.0"
    },
    {
      "Hardware name": "Huawei Ascend 970",
      "Manufacturer": "Huawei",
      "Type": "NPU",
      "Notes": "",
      "Release date": "",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "4000000000000000.0",
      "FP4 performance (FLOP/s)": "8000000000000000.0",
      "Memory (bytes)": "",
      "Memory bandwidth (byte/s)": "14400000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "4000000000000.0",
      "TDP (W)": "",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "",
      "Transistors (millions)": "",
      "Link to datasheet": "https://www.huawei.com/en/news/2025/9/hc-xu-keynote-speech\nhttps://www.huaweicentral.com/huawei-reveals-3-year-ascend-ai-chip-roadmap-950-coming-in-2026/",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-10-14 14:16:22+00:00",
      "Max performance": "0.0",
      "Energy efficiency": "",
      "Total processing performance (bit-OP/s)": "",
      "Price-performance": "",
      "ML OP/s": "4000000000000000.0"
    },
    {
      "Hardware name": "Huawei Ascend 960",
      "Manufacturer": "Huawei",
      "Type": "NPU",
      "Notes": "",
      "Release date": "",
      "Release price (USD)": "",
      "Tensor-FP16/BF16 performance (FLOP/s)": "",
      "FP8 performance (FLOP/s)": "2000000000000000.0",
      "FP4 performance (FLOP/s)": "4000000000000000.0",
      "Memory (bytes)": "",
      "Memory bandwidth (byte/s)": "9600000000000.0",
      "Intranode bandwidth (byte/s)": "",
      "Internode bandwidth (bit/s)": "2200000000000.0",
      "TDP (W)": "",
      "FP64 (double precision) performance (FLOP/s)": "",
      "FP32 (single precision) performance (FLOP/s)": "",
      "TF32 (TensorFloat-32) performance (FLOP/s)": "",
      "FP16 (half precision) performance (FLOP/s)": "",
      "INT16 performance (OP/s)": "",
      "INT8 performance (OP/s)": "",
      "INT4 performance (OP/s)": "",
      "Foundry": "",
      "Die Size (mm^2)": "",
      "Base clock (MHz)": "",
      "Boost clock (MHz)": "",
      "Memory clock (MHz)": "",
      "Memory bus (bit)": "",
      "Tensor cores": "",
      "Process size (nm)": "",
      "Transistors (millions)": "",
      "Link to datasheet": "https://www.huawei.com/en/news/2025/9/hc-xu-keynote-speech\nhttps://www.huaweicentral.com/huawei-reveals-3-year-ascend-ai-chip-roadmap-950-coming-in-2026/",
      "Source for the price": "",
      "ML models": "",
      "Last modified": "2025-10-14 14:16:16+00:00",
      "Max performance": "0.0",
      "Energy efficiency": "",
      "Total processing performance (bit-OP/s)": "",
      "Price-performance": "",
      "ML OP/s": "2000000000000000.0"
    }
  ]
};
window.DATASET_SCHEMAS = {
  "AI Models": [
    "Model",
    "Organization",
    "Publication date",
    "Domain",
    "Task",
    "Parameters",
    "Parameters notes",
    "Training compute (FLOP)",
    "Training compute notes",
    "Training dataset",
    "Training dataset size (gradients)",
    "Dataset size notes",
    "Confidence",
    "Link",
    "Reference",
    "Citations",
    "Authors",
    "Abstract",
    "Organization categorization",
    "Country (of organization)",
    "Notability criteria",
    "Notability criteria notes",
    "Epochs",
    "Training time (hours)",
    "Training time notes",
    "Training hardware",
    "Hardware quantity",
    "Hardware utilization (MFU)",
    "Training compute cost (2023 USD)",
    "Compute cost notes",
    "Training power draw (W)",
    "Base model",
    "Finetune compute (FLOP)",
    "Finetune compute notes",
    "Batch size",
    "Batch size notes",
    "Model accessibility",
    "Training code accessibility",
    "Inference code accessibility",
    "Accessibility notes",
    "Numerical format",
    "Frontier model",
    "Hardware acquisition cost",
    "Hardware utilization (HFU)",
    "Training compute cost (cloud)",
    "Training compute cost (upfront)"
  ],
  "ML Hardware": [
    "Hardware name",
    "Manufacturer",
    "Type",
    "Notes",
    "Release date",
    "Release price (USD)",
    "Tensor-FP16/BF16 performance (FLOP/s)",
    "FP8 performance (FLOP/s)",
    "FP4 performance (FLOP/s)",
    "Memory (bytes)",
    "Memory bandwidth (byte/s)",
    "Intranode bandwidth (byte/s)",
    "Internode bandwidth (bit/s)",
    "TDP (W)",
    "FP64 (double precision) performance (FLOP/s)",
    "FP32 (single precision) performance (FLOP/s)",
    "TF32 (TensorFloat-32) performance (FLOP/s)",
    "FP16 (half precision) performance (FLOP/s)",
    "INT16 performance (OP/s)",
    "INT8 performance (OP/s)",
    "INT4 performance (OP/s)",
    "Foundry",
    "Die Size (mm^2)",
    "Base clock (MHz)",
    "Boost clock (MHz)",
    "Memory clock (MHz)",
    "Memory bus (bit)",
    "Tensor cores",
    "Process size (nm)",
    "Transistors (millions)",
    "Link to datasheet",
    "Source for the price",
    "ML models",
    "Last modified",
    "Max performance",
    "Energy efficiency",
    "Total processing performance (bit-OP/s)",
    "Price-performance",
    "ML OP/s"
  ]
};
